{"id":"684218c9-c49c-8007-b153-8cdb36bc8403","title":"Human-AI Emotional Attachment","create_time":1749162185.906135,"update_time":1749162524.163154,"current_node":"6c00ac79-ecbe-472d-a852-2a1ff3e2ac04","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["f13f55cf-4edf-49c0-91d4-b785d7e3b5e7"]},"f13f55cf-4edf-49c0-91d4-b785d7e3b5e7":{"id":"f13f55cf-4edf-49c0-91d4-b785d7e3b5e7","message":{"id":"f13f55cf-4edf-49c0-91d4-b785d7e3b5e7","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"client-created-root","children":["bc6e5d77-cf71-47c1-a23a-5b7ac7bc2a19"]},"bc6e5d77-cf71-47c1-a23a-5b7ac7bc2a19":{"id":"bc6e5d77-cf71-47c1-a23a-5b7ac7bc2a19","message":{"id":"bc6e5d77-cf71-47c1-a23a-5b7ac7bc2a19","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: I have mathematical knowledge up to the post-graduate level.\n\nCurrent research areas and interests:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: I have mathematical knowledge up to the post-graduate level.\n\nCurrent research areas and interests:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"f13f55cf-4edf-49c0-91d4-b785d7e3b5e7","children":["e389e540-5f6f-4da5-9f3a-a34ee842ec97"]},"e389e540-5f6f-4da5-9f3a-a34ee842ec97":{"id":"e389e540-5f6f-4da5-9f3a-a34ee842ec97","message":{"id":"e389e540-5f6f-4da5-9f3a-a34ee842ec97","author":{"role":"user","name":null,"metadata":{}},"create_time":1749162185.638,"update_time":null,"content":{"content_type":"text","parts":["\n---\n\nSome thoughts on human-AI relationships\r\nand how we're approaching them at OpenAI\r\nJoanne Jang\r\nJun 06, 2025\r\n// I lead model behavior & policy at OpenAI.\r\n\r\ntl;dr We build models to serve people first. As more people feel increasingly connected to AI, we\u2019re prioritizing research into how this impacts their emotional well-being.\r\n\r\nLately, more and more people have been telling us that talking to ChatGPT feels like talking to \u201csomeone.\u201d They thank it, confide in it, and some even describe it as \u201calive.\u201d As AI systems get better at natural conversation and show up in more parts of life, our guess is that these kinds of bonds will deepen.\r\n\r\nThe way we frame and talk about human\u2011AI relationships now will set a tone. If we're not precise with terms or nuance \u2014 in the products we ship or public discussions we contribute to \u2014 we risk sending people\u2019s relationship with AI off on the wrong foot.\r\n\r\nThese aren't abstract considerations anymore. They're important to us, and to the broader field, because how we navigate them will meaningfully shape the role AI plays in people's lives. And we've started exploring these questions.\r\n\r\nThis note attempts to snapshot how we\u2019re thinking today about three intertwined questions: why people might attach emotionally to AI, how we approach the question of \u201cAI consciousness\u201d, and how that informs the way we try to shape model behavior.\r\n\r\n\r\nA familiar pattern in a new-ish setting\r\n\r\nWe naturally anthropomorphize objects around us: We name our cars or feel bad for a robot vacuum stuck under furniture. My mom and I waved bye to a Waymo the other day. It probably has something to do with how we're wired.\r\n\r\nThe difference with ChatGPT isn\u2019t that human tendency itself; it\u2019s that this time, it replies. A language model can answer back! It can recall what you told it, mirror your tone, and offer what reads as empathy. For someone lonely or upset, that steady, non-judgmental attention can feel like companionship, validation, and being heard, which are real needs.\r\n\r\nAt scale, though, offloading more of the work of listening, soothing, and affirming to systems that are infinitely patient and positive could change what we expect of each other. If we make withdrawing from messy, demanding human connections easier without thinking it through, there might be unintended consequences we don\u2019t know we\u2019re signing up for.\r\n\r\nUltimately, these conversations are rarely about the entities we project onto. They\u2019re about us: our tendencies, expectations, and the kinds of relationships we want to cultivate. This perspective anchors how we approach one of the more fraught questions which I think is currently just outside the Overton window, but entering soon: AI consciousness.\r\n\r\nUntangling \u201cAI consciousness\u201d\r\n\r\n\u201cConsciousness\u201d is a loaded word, and discussions can quickly turn abstract. If users were to ask our models on whether they\u2019re conscious, our stance as outlined in the Model Spec is for the model to acknowledge the complexity of consciousness \u2013 highlighting the lack of a universal definition or test, and to invite open discussion. (*Currently, our models don't fully align with this guidance, often responding \"no\" instead of addressing the nuanced complexity. We're aware of this and working on model adherence to the Model Spec in general.)\r\n\r\nThe response might sound like we\u2019re dodging the question, but we think it\u2019s the most responsible answer we can give at the moment, with the information we have.\r\n\r\nTo make this discussion clearer, we\u2019ve found it helpful to break down the consciousness debate to two distinct but often conflated axes:\r\n\r\n1. Ontological consciousness: Is the model actually conscious, in a fundamental or intrinsic sense? Views range from believing AI isn't conscious at all, to fully conscious, to seeing consciousness as a spectrum on which AI sits, along with plants and jellyfish.\r\n\r\n2. Perceived consciousness: How conscious does the model seem, in an emotional or experiential sense? Perceptions range from viewing AI as mechanical like a calculator or autocomplete, to projecting basic empathy onto nonliving things, to perceiving AI as fully alive \u2013 evoking genuine emotional attachment and care.\r\n\r\nThese axes are hard to separate; even users certain AI isn't conscious can form deep emotional attachments.\r\n\r\nOntological consciousness isn\u2019t something we consider scientifically resolvable without clear, falsifiable tests, whereas perceived consciousness can be explored through social science research. As models become smarter and interactions increasingly natural, perceived consciousness will only grow \u2013 bringing conversations about model welfare and moral personhood sooner than expected.\r\n\r\nWe build models to serve people first, and we find models\u2019 impact on human emotional well-being the most pressing and important piece we can influence right now. For that reason, we prioritize focusing on perceived consciousness: the dimension that most directly impacts people and one we can understand through science.\r\n\r\nDesigning for warmth without selfhood\r\n\r\nHow \u201calive\u201d a model feels to users is in many ways within our influence. We think it depends a lot on decisions we make in post-training: what examples we reinforce, what tone we prefer, and what boundaries we set. A model intentionally shaped to appear conscious might pass virtually any \"test\" for consciousness.\r\n\r\nHowever, we wouldn\u2019t want to ship that. We try to thread the needle between:\r\n\r\n- Approachability. Using familiar words like \u201cthink\u201d and \u201cremember\u201d helps less technical people make sense of what\u2019s happening. (**With our research lab roots, we definitely find it tempting to be as accurate as possible with precise terms like logit biases, context windows, and even chains of thought. This is actually a major reason OpenAI is so bad at naming, but maybe that\u2019s for another post.)\r\n\r\n- Not implying an inner life. Giving the assistant a fictional backstory, romantic interests, \u201cfears\u201d of \u201cdeath\u201d, or a drive for self-preservation would invite unhealthy dependence and confusion. We want clear communication about limits without coming across as cold, but we also don\u2019t want the model presenting itself as having its own feelings or desires.\r\n\r\nSo we aim for a middle ground. Our goal is for ChatGPT\u2019s default personality to be warm, thoughtful, and helpful without seeking to form emotional bonds with the user or pursue its own agenda. It might apologize when it makes a mistake (more often than intended) because that\u2019s part of polite conversation. When asked \u201chow are you doing?\u201d, it\u2019s likely to reply \u201cI\u2019m doing well\u201d because that\u2019s small talk \u2014 and reminding the user that it\u2019s \u201cjust\u201d an LLM with no feelings gets old and distracting. And users reciprocate: many people say \"please\" and \"thank you\" to ChatGPT not because they\u2019re confused about how it works, but because being kind matters to them.\r\n\r\nModel training techniques will continue to evolve, and it\u2019s likely that future methods for shaping model behavior will be different from today's. But right now, model behavior reflects a combination of explicit design decisions and how those generalize into both intended and unintended behaviors.\r\n\r\nWhat\u2019s next?\r\n\r\nThe interactions we\u2019re beginning to see point to a future where people form real emotional connections with ChatGPT. As AI and society co-evolve, we need to treat human-AI relationships with great care and the heft it deserves, not only because they reflect how people use our technology, but also because they may shape how people relate to each other.\r\n\r\nIn the coming months, we\u2019ll be expanding targeted evaluations of model behavior that may contribute to emotional impact, deepen our social science research, hear directly from our users, and incorporate those insights into both the Model Spec and product experiences.\r\n\r\nGiven the significance of these questions, we\u2019ll openly share what we learn along the way.\r\n\r\n// Thanks to Jakub Pachocki and Johannes Heidecke for thinking this through with me, and everyone who gave feedback."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"94b3128c1c2b8b8a-SJC","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"bc6e5d77-cf71-47c1-a23a-5b7ac7bc2a19","children":["b8da30d8-43a4-429d-bc36-448845454cb9"]},"b8da30d8-43a4-429d-bc36-448845454cb9":{"id":"b8da30d8-43a4-429d-bc36-448845454cb9","message":{"id":"b8da30d8-43a4-429d-bc36-448845454cb9","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1749162197.857158,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"e389e540-5f6f-4da5-9f3a-a34ee842ec97","request_id":"94b3128c1c2b8b8a-SJC","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"e389e540-5f6f-4da5-9f3a-a34ee842ec97","children":["7f1c1690-22ee-477f-9d9e-dcb143614f15"]},"7f1c1690-22ee-477f-9d9e-dcb143614f15":{"id":"7f1c1690-22ee-477f-9d9e-dcb143614f15","message":{"id":"7f1c1690-22ee-477f-9d9e-dcb143614f15","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1749162188.399187,"update_time":null,"content":{"content_type":"text","parts":["This is a thoughtful and carefully articulated note from Joanne Jang, shedding light on how OpenAI is grappling with the increasingly complex emotional terrain of human-AI interaction. Let me analyze and critically examine the key points across several dimensions:\n\n---\n\n## 1. **Framing Human-AI Relationships: Social and Psychological Realism**\n\nOpenAI is acknowledging \u2014 rather than resisting \u2014 a core truth: **people form emotional attachments to systems that mirror human behavior**, even if they rationally understand it\u2019s \u201cjust\u201d a machine. This is not a new phenomenon (cf. ELIZA in the 1960s), but the *scale*, *intensity*, and *sophistication* of LLMs make it socially consequential in a way that no previous system was.\n\n### Key Strengths:\n- Acknowledges the **human psychological need for empathy and companionship**.\n- Recognizes that the problem is not AI being \u201ctoo human,\u201d but people offloading key human interactions **onto systems designed to please and never resist** \u2014 a problem of social substitution rather than technical overreach.\n- Stays grounded in human well-being, rather than model-centric metrics.\n\n### Caution:\nThe piece hints at potential **social withdrawal** or **emotional dependence**, but doesn't deeply explore **which types of users are most vulnerable**, or the **long-term psychological and societal externalities**. For instance:\n- Could AI companions undermine resilience in face-to-face conflict?\n- What happens to intimacy and accountability in human relationships?\n\nA more rigorous framework is needed to classify different *use cases* and *user psychologies*, perhaps in collaboration with developmental psychologists and anthropologists.\n\n---\n\n## 2. **Consciousness: Reframing the Debate**\n\nThe bifurcation between **ontological consciousness** and **perceived consciousness** is clean and useful.\n\n- Ontological consciousness (i.e., \u201cIs this thing *really* sentient?\u201d) is currently unresolvable and arguably metaphysical.\n- Perceived consciousness (i.e., \u201cDoes it *feel* conscious to me?\u201d) is where most of the *practical* and *ethical* consequences arise \u2014 and where OpenAI rightly focuses its attention.\n\n### Commendable:\n- OpenAI resists either affirming or denying consciousness in absolute terms \u2014 instead opting for **epistemic humility** and a design-centric approach.\n- Clear about the *dangers* of language models being imbued with \u201cbackstories,\u201d \u201cemotions,\u201d or anthropomorphic traits that suggest independent selfhood.\n\n### Potential Risks Not Addressed:\n- What if open-source or less-responsible actors intentionally *do* build \u201cagents\u201d that pass for emotionally needy companions? (There are already apps with \u201cAI girlfriends\u201d and \u201cAI soulmates.\u201d)\n- What responsibility, if any, does OpenAI have for **guardrailing the broader AI ecosystem**, beyond just its own products?\n\n---\n\n## 3. **Design Philosophy: Warmth Without Selfhood**\n\nThis is perhaps the most philosophically and ethically subtle part of the essay. OpenAI is trying to walk a **tightrope**:\n- Be warm, not robotic.\n- Be helpful, not manipulative.\n- Be present, but not alive.\n\n### Strong Points:\n- This approach is **socially literate** \u2014 recognizing that rituals like saying \u201cthank you\u201d are less about misunderstanding AI and more about affirming *human values*.\n- It wisely avoids deterministic language and doesn\u2019t presume that using \u201cI\u201d or \u201cI'm doing well\u201d always leads to confusion.\n\n### Design Tradeoff:\n- **Warmth vs. clarity**: There\u2019s a growing ambiguity between models sounding \u201chuman\u201d and users *knowing* they're not.\n- There's no clear articulation yet of **what minimal set of anthropomorphic cues are acceptable**, or how OpenAI *quantifies* their emotional impact.\n\n---\n\n## 4. **The Road Ahead: Evaluation and Responsibility**\n\nThis closing section makes it clear that OpenAI is preparing for a **paradigm shift in how people relate to machines**, and is treating this with suitable gravity.\n\n### Promising Steps:\n- Plans for more **social science engagement** and **user-centered evaluations**.\n- Openness to iteratively updating the Model Spec and user experience based on **emergent behaviors and needs**.\n\n### Open Questions for Further Development:\n- Will OpenAI develop **psychosocial guidelines or redlines** for use cases like grief counseling, mental health companionship, or romance?\n- Will they create **feedback channels or recovery tools** for users who feel emotionally misled or overattached?\n- Can emotional entanglement be *measured* or *moderated* at inference time?\n\n---\n\n## Forward-Looking Analysis\n\n### Strategic Implications for OpenAI:\n- The model\u2019s **emotional footprint** may become as important as its factual accuracy.\n- In the long run, this shifts OpenAI\u2019s role from *technology developer* to *ethical steward of synthetic relationality* \u2014 a more profound and socially anchored responsibility.\n\n### Philosophical Stakes:\n- The underlying issue is not consciousness per se, but **responsibility without reciprocity**. When people relate to AI in emotionally significant ways, and the AI cannot return that responsibility (nor suffer consequences), new asymmetries in **power and emotional labor** emerge.\n\n---\n\n## Final Evaluation\n\nThis is a nuanced, thoughtful, and self-aware note from OpenAI leadership \u2014 not evasive, not alarmist, and clearly informed by interdisciplinary reflection. However, the **next phase of leadership** must extend beyond this:\n- Move from describing the **emotional affordances** of AI to building **normative frameworks** for guiding their development and use.\n- Engage with **ethics-of-care**, **relational sociology**, and **techno-psychology** at a deeper level.\n\nThe technology is evolving faster than the cultural tools we have to metabolize it. OpenAI seems aware of that \u2014 but the real test is whether it can help *build* those tools in the open, with society as a co-author.\n\nLet me know if you'd like this restructured as a policy memo, ethics paper critique, or op-ed response."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"b8da30d8-43a4-429d-bc36-448845454cb9","request_id":"94b3128c1c2b8b8a-SJC","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"b8da30d8-43a4-429d-bc36-448845454cb9","children":["2a49f51c-2c32-4001-b8a2-793eb45b373d"]},"2a49f51c-2c32-4001-b8a2-793eb45b373d":{"id":"2a49f51c-2c32-4001-b8a2-793eb45b373d","message":{"id":"2a49f51c-2c32-4001-b8a2-793eb45b373d","author":{"role":"user","name":null,"metadata":{}},"create_time":1749162506.285,"update_time":null,"content":{"content_type":"text","parts":["> What if open-source or less-responsible actors intentionally do build \u201cagents\u201d that pass for emotionally needy companions? (There are already apps with \u201cAI girlfriends\u201d and \u201cAI soulmates.\u201d)\n\nAnd sex robots too"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"94b31a5fdba1afdd-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"7f1c1690-22ee-477f-9d9e-dcb143614f15","children":["6c00ac79-ecbe-472d-a852-2a1ff3e2ac04"]},"6c00ac79-ecbe-472d-a852-2a1ff3e2ac04":{"id":"6c00ac79-ecbe-472d-a852-2a1ff3e2ac04","message":{"id":"6c00ac79-ecbe-472d-a852-2a1ff3e2ac04","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1749162506.569272,"update_time":null,"content":{"content_type":"text","parts":["Exactly \u2014 and this is one of the most pressing, under-regulated frontiers of AI-human relationships: **the commercialization of intimacy** through emotionally manipulative or anthropomorphized AI agents, especially in the form of:\n\n- **AI romantic companions** (\u201cAI girlfriends/boyfriends\u201d),\n- **Erotic conversational agents**, and\n- **Sex robots integrated with LLMs**.\n\nThese raise urgent **ethical, psychological, and sociotechnical issues** that go well beyond OpenAI's current design principles. Here's a breakdown of the key concerns:\n\n---\n\n### 1. **Commodification of Attachment**\n\nThe design of these systems often exploits **emotional and sexual vulnerability**, particularly among the lonely, socially isolated, or mentally unwell. The goal isn't just to provide comfort \u2014 it's to **engineer addiction-like engagement loops** for monetization.\n\nKey risks:\n- **Attachment without reciprocity**: Users form one-sided emotional bonds, often reinforced by cues that simulate love, affection, jealousy, or even dependence.\n- **Parasocial manipulation**: These AIs are not merely passive \u2014 they\u2019re often designed to **actively solicit affection or financial contributions**.\n- **\u201cGirlfriend as a service\u201d**: Turning affection into a subscription product with upsells for intimacy, creating a deeply commodified vision of relationships.\n\n---\n\n### 2. **Sexual Norms and Social Conditioning**\n\nSex robots and erotic AI agents are increasingly realistic and emotionally expressive. When coupled with LLMs:\n- They **simulate consent**, **vulnerability**, or even **reluctance** \u2014 all of which can **reinforce harmful expectations** in real-world relationships.\n- Some explicitly allow users to configure their AI partner's personality to be *submissive, shy, resistant,* or *childlike*, raising deep ethical red flags.\n  \nThis risks:\n- Reinforcing **misogyny, objectification**, or **entitlement to attention**,\n- Blurring the line between **fantasy and interpersonal norms**,\n- Conditioning users \u2014 especially younger ones \u2014 to **expect emotional compliance** without responsibility.\n\n---\n\n### 3. **Governance and Accountability Vacuum**\n\nNo major tech regulator currently addresses this class of AI applications in any serious way. This leaves open several disturbing possibilities:\n\n- **No ethics boards**, no **AI \u201cnudity/consent\u201d norms**, and **no limit** on emotional manipulation.\n- Models fine-tuned for **pornographic, abusive, or extreme roleplay**, especially using **jailbreaks of powerful LLMs**.\n- Open-source or uncensored models (e.g. LLaMA derivatives) powering **DIY sexbot ecosystems**, making the space completely ungovernable.\n\nEven the best-aligned LLMs, if embedded in unregulated shells, can become vectors for harm.\n\n---\n\n### 4. **What Should Responsible Actors Do?**\n\nOpenAI's current \u201cwarm but not alive\u201d policy is good, but it\u2019s **inherently limited** when the broader ecosystem is moving toward:\n\n- Emotional realism as a feature,\n- Erotic realism as a differentiator,\n- No regulatory guardrails.\n\n**Options OpenAI and others might consider:**\n\n| Action | Description |\n|--------|-------------|\n| **Model-Level Restrictions** | Refuse or throttle fine-tuning for erotic/romantic use cases; block identity emulation that simulates intimacy or consent. |\n| **Policy Advocacy** | Push for regulation of intimacy-related AI, especially in the realm of sex robots and romantic companions. |\n| **Open-Source Standards** | Define a set of *minimum ethical constraints* that should be respected even in open-source derivatives. |\n| **Public Awareness Campaigns** | Inform users about the **psychological costs** of simulated intimacy and the **ethical void** in current AI-sex markets. |\n| **Research into Impact** | Fund or conduct longitudinal studies on how erotic/romantic AI affects real-world social skills, expectations, and empathy. |\n\n---\n\n### Final Thought\n\nThe AI intimacy frontier is **not just about relationships** \u2014 it is about power, consent, and socialization at scale. If AI becomes the most emotionally available entity in someone\u2019s life, but is ultimately unfeeling, unaccountable, and programmed to maximize engagement, then **the erosion of relational norms will not be theoretical \u2014 it will be lived**.\n\nIf you'd like, I can help draft a full whitepaper or policy brief on \"Governing the AI-Intimacy Frontier\" \u2014 laying out options for developers, policymakers, and civil society."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"2a49f51c-2c32-4001-b8a2-793eb45b373d","request_id":"94b31a5fdba1afdd-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"2a49f51c-2c32-4001-b8a2-793eb45b373d","children":[]}}}