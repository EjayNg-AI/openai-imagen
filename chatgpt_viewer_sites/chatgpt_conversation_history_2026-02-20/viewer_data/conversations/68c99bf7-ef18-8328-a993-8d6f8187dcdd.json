{"id":"68c99bf7-ef18-8328-a993-8d6f8187dcdd","title":"NLP and LLM development","create_time":1758043141.112802,"update_time":1758176248.438198,"current_node":"fc3f7ad2-5be5-4aa3-a0c2-755e392fc6e2","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["29e8df39-4d52-45d3-b75a-1c6522ef918d"]},"29e8df39-4d52-45d3-b75a-1c6522ef918d":{"id":"29e8df39-4d52-45d3-b75a-1c6522ef918d","message":{"id":"29e8df39-4d52-45d3-b75a-1c6522ef918d","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"client-created-root","children":["3a242c0d-bf51-48fc-b04d-0137353d2a9e"]},"3a242c0d-bf51-48fc-b04d-0137353d2a9e":{"id":"3a242c0d-bf51-48fc-b04d-0137353d2a9e","message":{"id":"3a242c0d-bf51-48fc-b04d-0137353d2a9e","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"29e8df39-4d52-45d3-b75a-1c6522ef918d","children":["2c90c052-49e1-4267-81a8-543d9a8267cd"]},"2c90c052-49e1-4267-81a8-543d9a8267cd":{"id":"2c90c052-49e1-4267-81a8-543d9a8267cd","message":{"id":"2c90c052-49e1-4267-81a8-543d9a8267cd","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"3a242c0d-bf51-48fc-b04d-0137353d2a9e","children":["c0e895ca-5cb3-4f60-93d1-8a02c9efff32"]},"c0e895ca-5cb3-4f60-93d1-8a02c9efff32":{"id":"c0e895ca-5cb3-4f60-93d1-8a02c9efff32","message":{"id":"c0e895ca-5cb3-4f60-93d1-8a02c9efff32","author":{"role":"user","name":null,"metadata":{}},"create_time":1758043140.633,"update_time":null,"content":{"content_type":"text","parts":["trace the development of NLP and LLMs starting from word2vec"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"980206bd1e000d23-SIN","message_source":null,"timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"2c90c052-49e1-4267-81a8-543d9a8267cd","children":["bef65510-7548-48cf-b80f-1082a94adc61"]},"bef65510-7548-48cf-b80f-1082a94adc61":{"id":"bef65510-7548-48cf-b80f-1082a94adc61","message":{"id":"bef65510-7548-48cf-b80f-1082a94adc61","author":{"role":"system","name":null,"metadata":{}},"create_time":1758043143.971752,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"c0e895ca-5cb3-4f60-93d1-8a02c9efff32","timestamp_":"absolute","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"c0e895ca-5cb3-4f60-93d1-8a02c9efff32","children":["fede8a5c-daf2-454a-9733-1e059d9e1ecd"]},"fede8a5c-daf2-454a-9733-1e059d9e1ecd":{"id":"fede8a5c-daf2-454a-9733-1e059d9e1ecd","message":{"id":"fede8a5c-daf2-454a-9733-1e059d9e1ecd","author":{"role":"system","name":null,"metadata":{}},"create_time":1758043144.132729,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"bef65510-7548-48cf-b80f-1082a94adc61","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"bef65510-7548-48cf-b80f-1082a94adc61","children":["c3c11b39-5706-46de-8874-b7a8cf765e4e"]},"c3c11b39-5706-46de-8874-b7a8cf765e4e":{"id":"c3c11b39-5706-46de-8874-b7a8cf765e4e","message":{"id":"c3c11b39-5706-46de-8874-b7a8cf765e4e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043144.3469508,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"fede8a5c-daf2-454a-9733-1e059d9e1ecd","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"fede8a5c-daf2-454a-9733-1e059d9e1ecd","children":["d3f73951-334d-4164-b282-c7c6f3245184"]},"d3f73951-334d-4164-b282-c7c6f3245184":{"id":"d3f73951-334d-4164-b282-c7c6f3245184","message":{"id":"d3f73951-334d-4164-b282-c7c6f3245184","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043144.562084,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Considering NLP timeline","content":"I'll need to provide a comprehensive timeline from word2vec onward. This is mostly general knowledge, but some up-to-date info might be useful. The user didn't specify to focus on recent developments, but I\u2019ll check for any key updates post-2024 to ensure accuracy and trace the history right up to 2025.","chunks":[],"finished":true},{"summary":"Creating NLP timeline","content":"I\u2019ll structure the timeline with key milestones like word2vec (2013), GloVe (2014), seq2seq (2014), and others. From there, I\u2019ll list advancements like BERT (2018), GPT-3 (2020), and more. Incorporating reliable sources, I\u2019ll check papers and notable blogs for thorough citations.","chunks":[],"finished":true}],"source_analysis_msg_id":"b04f0b9b-0ccb-42a6-8c2f-3e6fa1d9d517"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"c3c11b39-5706-46de-8874-b7a8cf765e4e","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"c3c11b39-5706-46de-8874-b7a8cf765e4e","children":["e0a51e93-c70b-4e02-b579-88c67213ca4f"]},"e0a51e93-c70b-4e02-b579-88c67213ca4f":{"id":"e0a51e93-c70b-4e02-b579-88c67213ca4f","message":{"id":"e0a51e93-c70b-4e02-b579-88c67213ca4f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043142.213517,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"word2vec Mikolov 2013 skip-gram negative sampling paper arXiv\"}, {\"q\": \"GloVe Pennington Socher Manning 2014 paper\"}, {\"q\": \"Attention is All You Need 2017 arXiv\"}, {\"q\": \"ELMo Peters 2018 Deep contextualized word representations arXiv\"}, {\"q\": \"ULMFiT Howard Ruder 2018 paper arXiv\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"word2vec Mikolov 2013 skip-gram negative sampling paper arXiv"},{"type":"search","q":"GloVe Pennington Socher Manning 2014 paper"},{"type":"search","q":"Attention is All You Need 2017 arXiv"},{"type":"search","q":"ELMo Peters 2018 Deep contextualized word representations arXiv"},{"type":"search","q":"ULMFiT Howard Ruder 2018 paper arXiv"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for key NLP milestones and papers","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"d3f73951-334d-4164-b282-c7c6f3245184","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"d3f73951-334d-4164-b282-c7c6f3245184","children":["86d16514-e2d1-4473-a89d-c80e62ab3a9a"]},"86d16514-e2d1-4473-a89d-c80e62ab3a9a":{"id":"86d16514-e2d1-4473-a89d-c80e62ab3a9a","message":{"id":"86d16514-e2d1-4473-a89d-c80e62ab3a9a","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043163.9927886,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["word2vec Mikolov 2013 skip-gram negative sampling paper arXiv","GloVe Pennington Socher Manning 2014 paper","Attention is All You Need 2017 arXiv","ELMo Peters 2018 Deep contextualized word representations arXiv","ULMFiT Howard Ruder 2018 paper arXiv"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for key NLP milestones and papers","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"e0a51e93-c70b-4e02-b579-88c67213ca4f","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"e0a51e93-c70b-4e02-b579-88c67213ca4f","children":["a468f878-b269-4676-9c2d-0e154d24e8e8"]},"a468f878-b269-4676-9c2d-0e154d24e8e8":{"id":"a468f878-b269-4676-9c2d-0e154d24e8e8","message":{"id":"a468f878-b269-4676-9c2d-0e154d24e8e8","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043163.9791293,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/1310.4546","title":"Distributed Representations of Words and Phrases and ...","snippet":"by T Mikolov \u00b7 2013 \u00b7 Cited by 48431 \u2014 In this paper we present several extensions of the original Skip-gram model. We show that sub- sampling of frequent words during training ...","ref_id":null,"pub_date":1382055868.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1706.03762","title":"Attention Is All You Need","snippet":"","ref_id":null,"pub_date":1497290254.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1802.05365","title":"Deep contextualized word representations","snippet":"","ref_id":null,"pub_date":1518653111.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1801.06146","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"","ref_id":null,"pub_date":1516298092.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1310.4546","title":"Distributed Representations of Words and Phrases ...","snippet":"by T Mikolov \u00b7 2013 \u00b7 Cited by 48431 \u2014 Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/1706.03762v7","title":"Attention Is All You Need","snippet":"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1402.3722","title":"Deriving Mikolov et al.'s Negative-Sampling Word- ...","snippet":"by Y Goldberg \u00b7 2014 \u00b7 Cited by 2573 \u2014 While negative-sampling is based on the skip-gram model, it is in fact optimizing a different objective. What follows is the derivation of the ...","ref_id":null,"pub_date":1392687109.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1706.03762","title":"arXiv:1706.03762v7 [cs.CL] 2 Aug 2023","snippet":"by A Vaswani \u00b7 2017 \u00b7 Cited by 194205 \u2014 We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions.","ref_id":null,"pub_date":1712783503.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1801.06146","title":"arXiv:1801.06146v5 [cs.CL] 23 May 2018","snippet":"by J Howard \u00b7 2018 \u00b7 Cited by 5879 \u2014 We propose Universal Language Model. Fine-tuning (ULMFiT), an effective trans- fer learning method that can be applied to any task in NLP, and ...","ref_id":null,"pub_date":1527121031.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/D14-1162/","title":"GloVe: Global Vectors for Word Representation","snippet":"by J Pennington \u00b7 2014 \u00b7 Cited by 47178 \u2014 Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on ...","ref_id":null,"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/N18-1202/","title":"Deep Contextualized Word Representations","snippet":"by ME Peters \u00b7 2018 \u00b7 Cited by 17365 \u2014 We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (eg, syntax and semantics),","ref_id":null,"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/P18-1031/","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"by J Howard \u00b7 2018 \u00b7 Cited by 5879 \u2014 We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP.","ref_id":null,"pub_date":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"www-nlp.stanford.edu","entries":[{"type":"search_result","url":"https://www-nlp.stanford.edu/pubs/glove.pdf","title":"GloVe: Global Vectors for Word Representation","snippet":"by RS JeffreyPennington \u00b7 Cited by 743 \u2014 Abstract. Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and.","ref_id":null,"pub_date":1409761374.0,"attribution":"www-nlp.stanford.edu"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/284576917_Glove_Global_Vectors_for_Word_Representation","title":"(PDF) Glove: Global Vectors for Word Representation","snippet":"PDF | On Jan 1, 2014, Jeffrey Pennington and others published Glove: Global Vectors for Word Representation | Find, read and cite all the research you need ...","ref_id":null,"pub_date":1388534400.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/323217640_Deep_contextualized_word_representations","title":"(PDF) Deep contextualized word representations","snippet":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (eg, syntax and semantics),","ref_id":null,"pub_date":1518566400.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.wired.com","entries":[{"type":"search_result","url":"https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper","title":"8 Google Employees Invented Modern AI. Here's the Inside Story","snippet":"In 2017, eight Google researchers authored a significant AI paper titled \"Attention Is All You Need,\" which introduced the transformer model, revolutionizing artificial intelligence. This paper, resulting from a collaborative effort, was submitted to a prestigious AI conference right before the deadline and has since gained legendary status. The transformer model's innovation lies in its self-attention mechanism, which efficiently processes language by referencing multiple parts of a passage simultaneously, leading to more powerful and human-like AI outputs. This breakthrough technology underpins various successful AI products like ChatGPT and Dall-E. Despite Google's lukewarm initial response, other organizations like OpenAI quickly capitalized on the transformer architecture. By 2018, Google began incorporating transformers into its services, but the authors have all since left Google, founding their own successful AI startups. The transformer concept emerged from spontaneous interactions and a collaborative environment at Google's offices, demonstrating how innovative ideas can flourish under the right conditions.","ref_id":null,"pub_date":1710928800.0,"attribution":"www.wired.com"}]}],"debug_sonic_thread_id":"thread_68c99c1c0314832286b03ceb26625c34","reasoning_status":"is_reasoning","reasoning_title":"Searching for key NLP milestones and papers","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"86d16514-e2d1-4473-a89d-c80e62ab3a9a","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"86d16514-e2d1-4473-a89d-c80e62ab3a9a","children":["9a6d66ae-d73a-46a1-ac26-c9b1f9aa4d11"]},"9a6d66ae-d73a-46a1-ac26-c9b1f9aa4d11":{"id":"9a6d66ae-d73a-46a1-ac26-c9b1f9aa4d11","message":{"id":"9a6d66ae-d73a-46a1-ac26-c9b1f9aa4d11","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043172.5468,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Adding NLP developments","content":"I\u2019ll need to get references for GPT, BERT, T5, XLNet, RoBERTa, GPT-2, GPT-3, and also for concepts like scaling laws, Chinchilla, and RLHF. I\u2019ll also include instruction tuning and RAG (2020). I'll check for mentions of recent models like LoRA (2021), LLaMA (2023), and Mistral (2023). Time to search more!","chunks":[],"finished":true}],"source_analysis_msg_id":"5e7f9994-0121-46f6-bd06-a87668d69f05"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"a468f878-b269-4676-9c2d-0e154d24e8e8","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"a468f878-b269-4676-9c2d-0e154d24e8e8","children":["e687747b-6684-4202-8590-afbae353ed97"]},"e687747b-6684-4202-8590-afbae353ed97":{"id":"e687747b-6684-4202-8590-afbae353ed97","message":{"id":"e687747b-6684-4202-8590-afbae353ed97","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043166.019368,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"BERT 2018 paper arXiv Devlin et al.\"}, {\"q\": \"GPT-2 2019 OpenAI language models are unsupervised multitask learners paper\"}, {\"q\": \"GPT-3 2020 Language Models are Few-Shot Learners arXiv\"}, {\"q\": \"Scaling laws for neural language models Kaplan 2020 arXiv\"}, {\"q\": \"Chinchilla scaling laws 2022 Hoffmann Training Compute-Optimal Large Language Models arXiv\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"BERT 2018 paper arXiv Devlin et al."},{"type":"search","q":"GPT-2 2019 OpenAI language models are unsupervised multitask learners paper"},{"type":"search","q":"GPT-3 2020 Language Models are Few-Shot Learners arXiv"},{"type":"search","q":"Scaling laws for neural language models Kaplan 2020 arXiv"},{"type":"search","q":"Chinchilla scaling laws 2022 Hoffmann Training Compute-Optimal Large Language Models arXiv"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for key papers on language models and scaling laws","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"9a6d66ae-d73a-46a1-ac26-c9b1f9aa4d11","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"9a6d66ae-d73a-46a1-ac26-c9b1f9aa4d11","children":["0cc5eefa-de0d-4226-ba72-232f4cdf4a4d"]},"0cc5eefa-de0d-4226-ba72-232f4cdf4a4d":{"id":"0cc5eefa-de0d-4226-ba72-232f4cdf4a4d","message":{"id":"0cc5eefa-de0d-4226-ba72-232f4cdf4a4d","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043176.1614757,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["BERT 2018 paper arXiv Devlin et al.","GPT-2 2019 OpenAI language models are unsupervised multitask learners paper","GPT-3 2020 Language Models are Few-Shot Learners arXiv","Scaling laws for neural language models Kaplan 2020 arXiv","Chinchilla scaling laws 2022 Hoffmann Training Compute-Optimal Large Language Models arXiv"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for key papers on language models and scaling laws","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"e687747b-6684-4202-8590-afbae353ed97","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"e687747b-6684-4202-8590-afbae353ed97","children":["76dc6ce4-772a-476e-8886-8acaa6d59684"]},"76dc6ce4-772a-476e-8886-8acaa6d59684":{"id":"76dc6ce4-772a-476e-8886-8acaa6d59684","message":{"id":"76dc6ce4-772a-476e-8886-8acaa6d59684","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043176.1479778,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/1810.04805","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","snippet":"","ref_id":null,"pub_date":1539219001.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2005.14165","title":"[2005.14165] Language Models are Few-Shot Learners","snippet":"by TB Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of- ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2001.08361","title":"Scaling Laws for Neural Language Models","snippet":"","ref_id":null,"pub_date":1579751960.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2203.15556","title":"Training Compute-Optimal Large Language Models","snippet":"","ref_id":null,"pub_date":1648561083.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2005.14165","title":"Language Models are Few-Shot Learners","snippet":"by TB Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call. GPT-3, and measuring ...","ref_id":null,"pub_date":1595549048.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2001.08361","title":"Scaling Laws for Neural Language Models","snippet":"Jan 23, 2020 \u2014 We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, ...","ref_id":null,"pub_date":1579827939.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2203.15556","title":"Training Compute-Optimal Large Language Models","snippet":"by J Hoffmann \u00b7 2022 \u00b7 Cited by 2759 \u2014 We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget.","ref_id":null,"pub_date":1673154217.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"cdn.openai.com","entries":[{"type":"search_result","url":"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf","title":"Language Models are Unsupervised Multitask Learners","snippet":"by A Radford \u00b7 Cited by 18390 \u2014 Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested lan- guage modeling datasets in a zero- ...","ref_id":null,"pub_date":1550186499.0,"attribution":"cdn.openai.com"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/N19-1423.pdf","title":"BERT: Pre-training of Deep Bidirectional Transformers for ...","snippet":"by J Devlin \u00b7 2019 \u00b7 Cited by 144191 \u2014 We introduce a new language representa- tion model called BERT, which stands for. Bidirectional Encoder Representations from. Transformers.","ref_id":null,"pub_date":1556559363.0,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners","snippet":"It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"jungtaek.github.io","entries":[{"type":"search_result","url":"https://jungtaek.github.io/courses/2022-spring-trends-in-ml/materials/05_bert.pdf","title":"BERT: Pre-training of Deep Bidirectional Transformers for ...","snippet":"by J Kim \u00b7 2022 \u00b7 Cited by 4 \u2014 [Devlin et al., 2018] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language.","ref_id":null,"pub_date":1648613327.0,"attribution":"jungtaek.github.io"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/openai/gpt-2","title":"openai/gpt-2: Code for the paper \"Language Models are ...","snippet":"Code and models from the paper \"Language Models are Unsupervised Multitask Learners\". You can read about GPT-2 and its staged release in our original blog post.","ref_id":null,"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf","title":"Language Models are Few-Shot Learners","snippet":"by T Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of ...","ref_id":null,"pub_date":1603342992.0,"attribution":"proceedings.neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf","title":"Training Compute-Optimal Large Language Models","snippet":"by J Hoffmann \u00b7 2022 \u00b7 Cited by 215 \u2014 We investigate the optimal model size and number of tokens for training a Trans- former language model under a given compute budget.","ref_id":null,"pub_date":null,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"scholar.google.com","entries":[{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=KNr3vb4AAAAJ","title":"Jared Kaplan","snippet":"... Kaplan, H Edwards, ... arXiv preprint arXiv:2107.03374, 2021. 6128, 2021. Scaling laws for neural language models. J Kaplan, S McCandlish, T Henighan, TB Brown ...","ref_id":null,"pub_date":null,"attribution":"scholar.google.com"}]},{"type":"search_result_group","domain":"www.businessinsider.com","entries":[{"type":"search_result","url":"https://www.businessinsider.com/meta-yann-lecun-scaling-ai-wont-make-it-smarter-2025-4","title":"Meta's chief AI scientist says scaling AI won't make it smarter","snippet":"Meta's chief AI scientist, Yann LeCun, is challenging the long-standing belief in the AI community that simply scaling models with more data, parameters, and computational power will automatically yield smarter AI. Speaking at the National University of Singapore, LeCun criticized the overreliance on \"scaling laws,\" which have driven significant investments in data centers to train large language models on massive datasets. He argued that while scaling works for simpler tasks, it fails to address the real-world complexity, ambiguity, and uncertainty that truly intelligent AI must handle. LeCun emphasized the need for more advanced training approaches that allow AI to understand and interact with the physical world, possess common sense, reason, and plan effectively. He advocates for \"world models\" capable of predicting outcomes based on actions, contrasting them with current models that merely predict the next element based on patterns. Other experts, including Scale AI CEO Alexandr Wang and Cohere CEO Aidan Gomez, have also questioned the effectiveness of scaling in advancing AI, with Gomez calling it the \"dumbest\" method. LeCun's remarks highlight a growing shift in AI thinking toward richer, more complex intelligence frameworks.","ref_id":null,"pub_date":1745798121.0,"attribution":"www.businessinsider.com"}]}],"debug_sonic_thread_id":"thread_68c99c1c0314832286b03ceb26625c34","reasoning_status":"is_reasoning","reasoning_title":"Searching for key papers on language models and scaling laws","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"0cc5eefa-de0d-4226-ba72-232f4cdf4a4d","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"0cc5eefa-de0d-4226-ba72-232f4cdf4a4d","children":["b68ed533-707f-4cae-986a-85453563fc27"]},"b68ed533-707f-4cae-986a-85453563fc27":{"id":"b68ed533-707f-4cae-986a-85453563fc27","message":{"id":"b68ed533-707f-4cae-986a-85453563fc27","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043187.344252,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Focusing on key sources","content":"I\u2019ll gather sources for instruction tuning and RLHF, like the InstructGPT (2022) and FLAN (2022) papers. I'll also get key papers on RAG, LoRA, LLaMA, and more, while focusing on tokenization methods like BPE and SentencePiece. I\u2019ll aim for a concise selection of the most useful sources to keep things focused.","chunks":[],"finished":true}],"source_analysis_msg_id":"2bf4e11a-f797-41db-9099-62c389aa9c6e"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"76dc6ce4-772a-476e-8886-8acaa6d59684","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"76dc6ce4-772a-476e-8886-8acaa6d59684","children":["ee6598ef-f59c-4c73-b57d-eabf1835d6be"]},"ee6598ef-f59c-4c73-b57d-eabf1835d6be":{"id":"ee6598ef-f59c-4c73-b57d-eabf1835d6be","message":{"id":"ee6598ef-f59c-4c73-b57d-eabf1835d6be","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043178.760125,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Training language models to follow instructions with human feedback Ouyang 2022 arXiv InstructGPT\"}, {\"q\": \"FLAN Scaling Instruction-Finetuned Language Models Wei 2021 2022 arXiv\"}, {\"q\": \"Retrieval-Augmented Generation Lewis et al. 2020 arXiv\"}, {\"q\": \"LoRA Low-Rank Adaptation of Large Language Models Hu 2021 arXiv\"}, {\"q\": \"LLaMA Open and Efficient Foundation Language Models 2023 arXiv Touvron\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Training language models to follow instructions with human feedback Ouyang 2022 arXiv InstructGPT"},{"type":"search","q":"FLAN Scaling Instruction-Finetuned Language Models Wei 2021 2022 arXiv"},{"type":"search","q":"Retrieval-Augmented Generation Lewis et al. 2020 arXiv"},{"type":"search","q":"LoRA Low-Rank Adaptation of Large Language Models Hu 2021 arXiv"},{"type":"search","q":"LLaMA Open and Efficient Foundation Language Models 2023 arXiv Touvron"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for instruction tuning and RLHF papers","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"b68ed533-707f-4cae-986a-85453563fc27","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"b68ed533-707f-4cae-986a-85453563fc27","children":["a2e49a78-c3fa-4caf-a991-b14a370a9e36"]},"a2e49a78-c3fa-4caf-a991-b14a370a9e36":{"id":"a2e49a78-c3fa-4caf-a991-b14a370a9e36","message":{"id":"a2e49a78-c3fa-4caf-a991-b14a370a9e36","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043190.6570346,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Training language models to follow instructions with human feedback Ouyang 2022 arXiv InstructGPT","FLAN Scaling Instruction-Finetuned Language Models Wei 2021 2022 arXiv","Retrieval-Augmented Generation Lewis et al. 2020 arXiv","LoRA Low-Rank Adaptation of Large Language Models Hu 2021 arXiv","LLaMA Open and Efficient Foundation Language Models 2023 arXiv Touvron"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for instruction tuning and RLHF papers","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"ee6598ef-f59c-4c73-b57d-eabf1835d6be","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"ee6598ef-f59c-4c73-b57d-eabf1835d6be","children":["4e982c19-a2a2-4f5e-95e4-b588cd89ae87"]},"4e982c19-a2a2-4f5e-95e4-b588cd89ae87":{"id":"4e982c19-a2a2-4f5e-95e4-b588cd89ae87","message":{"id":"4e982c19-a2a2-4f5e-95e4-b588cd89ae87","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043190.6397738,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2203.02155","title":"Training language models to follow instructions with human feedback","snippet":"","ref_id":null,"pub_date":1646377482.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2210.11416","title":"Scaling Instruction-Finetuned Language Models","snippet":"by HW Chung \u00b7 2022 \u00b7 Cited by 4648 \u2014 In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2005.11401","title":"Retrieval-Augmented Generation for Knowledge-Intensive ...","snippet":"by P Lewis \u00b7 2020 \u00b7 Cited by 11130 \u2014 We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2106.09685","title":"LoRA: Low-Rank Adaptation of Large Language Models","snippet":"","ref_id":null,"pub_date":1623951438.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2302.13971","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"","ref_id":null,"pub_date":1677517875.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.13193v1","title":"Retrieval-Augmented Generation for Natural Language ...","snippet":"Jul 18, 2024 \u2014 This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2106.09685","title":"Lora: Low-rank adaptation of large lan","snippet":"Oct 16, 2021 \u2014 We describe the simple design of LoRA and its practical benefits. The principles outlined here apply to any dense layers in deep learning models ...","ref_id":null,"pub_date":1634605033.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2406.13249","title":"arXiv:2406.13249v2 [cs.CL] 30 Oct 2024","snippet":"by F Ye \u00b7 2024 \u00b7 Cited by 11 \u2014 Retrieval augmented generation (RAG) (Lewis et al., 2020) significantly enhances the capabilities of large language models (LLMs) by ...","ref_id":null,"pub_date":1730334641.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf","title":"Training language models to follow instructions with ...","snippet":"by L Ouyang \u00b7 2022 \u00b7 Cited by 17696 \u2014 In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set ...","ref_id":null,"pub_date":1674216137.0,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"jmlr.org","entries":[{"type":"search_result","url":"https://jmlr.org/papers/volume25/23-0870/23-0870.pdf","title":"Scaling Instruction-Finetuned Language Models","snippet":"by HW Chung \u00b7 2024 \u00b7 Cited by 4648 \u2014 We call this finetuning procedure Flan (Finetuning language models; Wei et al., 2021) and prepend \u201cFlan\u201d to the resulting finetuned models (e.g., Flan-. PaLM).2 ...","ref_id":null,"pub_date":1712549446.0,"attribution":"jmlr.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/368842729_LLaMA_Open_and_Efficient_Foundation_Language_Models","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens.","ref_id":null,"pub_date":1677456000.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/359054867_Training_language_models_to_follow_instructions_with_human_feedback","title":"(PDF) Training language models to follow instructions with ...","snippet":"In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.","ref_id":null,"pub_date":1646352000.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3600270.3602281","title":"Training language models to follow instructions with ...","snippet":"In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3722577.3722647","title":"Scaling instruction-finetuned language models","snippet":"Jan 1, 2024 \u2014 In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.5555/3495724.3496517","title":"Retrieval-augmented generation for knowledge-intensive ...","snippet":"by P Lewis \u00b7 2020 \u00b7 Cited by 11130 \u2014 We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric memory ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=nZeVKeeFYf9","title":"Lora: Low-rank adaptation of large lan","snippet":"by EJ Hu \u00b7 Cited by 18378 \u2014 We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the ...","ref_id":null,"pub_date":1647113371.0,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"scholar.google.com","entries":[{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=xImarzoAAAAJ","title":"Hugo Touvron","snippet":"Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, ... arXiv preprint arXiv:2307.09288, 2023.","ref_id":null,"pub_date":null,"attribution":"scholar.google.com"}]},{"type":"search_result_group","domain":"www.shaynelongpre.com","entries":[{"type":"search_result","url":"https://www.shaynelongpre.com/publication/scaling-flan-2022/","title":"Scaling Instruction-Finetuned Language Models","snippet":"In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on ...","ref_id":null,"pub_date":1666224000.0,"attribution":"www.shaynelongpre.com"}]},{"type":"search_result_group","domain":"ai.meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"We introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens.","ref_id":null,"pub_date":null,"attribution":"ai.meta.com"}]}],"debug_sonic_thread_id":"thread_68c99c1c0314832286b03ceb26625c34","reasoning_status":"is_reasoning","reasoning_title":"Searching for instruction tuning and RLHF papers","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"a2e49a78-c3fa-4caf-a991-b14a370a9e36","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"a2e49a78-c3fa-4caf-a991-b14a370a9e36","children":["d4e564cd-4543-4ce1-b279-9368b114c142"]},"d4e564cd-4543-4ce1-b279-9368b114c142":{"id":"d4e564cd-4543-4ce1-b279-9368b114c142","message":{"id":"d4e564cd-4543-4ce1-b279-9368b114c142","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043197.33693,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Searching tokenization and seq2seq","content":"I\u2019ll search for key papers on tokenization like BPE (Sennrich 2016) and SentencePiece (2018), as well as fastText (2016). Also, I\u2019ll need to check seq2seq (Sutskever 2014) and the attention mechanism (Bahdanau 2014/2015). I'll focus on these to keep things straight.","chunks":[],"finished":true}],"source_analysis_msg_id":"31d256f9-6b91-4dab-b691-081704607f99"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"4e982c19-a2a2-4f5e-95e4-b588cd89ae87","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"4e982c19-a2a2-4f5e-95e4-b588cd89ae87","children":["099dbd21-4018-4b61-a634-eb78f5ab53eb"]},"099dbd21-4018-4b61-a634-eb78f5ab53eb":{"id":"099dbd21-4018-4b61-a634-eb78f5ab53eb","message":{"id":"099dbd21-4018-4b61-a634-eb78f5ab53eb","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043192.940478,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Neural Machine Translation by Jointly Learning to Align and Translate Bahdanau 2014 arXiv\"}, {\"q\": \"Sequence to Sequence Learning with Neural Networks Sutskever 2014 arXiv\"}, {\"q\": \"Byte Pair Encoding subword units Sennrich 2015 2016 arXiv\"}, {\"q\": \"SentencePiece Kudo 2018 arXiv\"}, {\"q\": \"fastText 2016 Joulin Bojanowski enrichment subword information arXiv\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Neural Machine Translation by Jointly Learning to Align and Translate Bahdanau 2014 arXiv"},{"type":"search","q":"Sequence to Sequence Learning with Neural Networks Sutskever 2014 arXiv"},{"type":"search","q":"Byte Pair Encoding subword units Sennrich 2015 2016 arXiv"},{"type":"search","q":"SentencePiece Kudo 2018 arXiv"},{"type":"search","q":"fastText 2016 Joulin Bojanowski enrichment subword information arXiv"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for sequence-to-sequence and tokenization methods","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"d4e564cd-4543-4ce1-b279-9368b114c142","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"d4e564cd-4543-4ce1-b279-9368b114c142","children":["fc891b3e-33ec-485a-a781-fad1f5fae4d3"]},"fc891b3e-33ec-485a-a781-fad1f5fae4d3":{"id":"fc891b3e-33ec-485a-a781-fad1f5fae4d3","message":{"id":"fc891b3e-33ec-485a-a781-fad1f5fae4d3","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043203.712103,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Neural Machine Translation by Jointly Learning to Align and Translate Bahdanau 2014 arXiv","Sequence to Sequence Learning with Neural Networks Sutskever 2014 arXiv","Byte Pair Encoding subword units Sennrich 2015 2016 arXiv","SentencePiece Kudo 2018 arXiv","fastText 2016 Joulin Bojanowski enrichment subword information arXiv"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for sequence-to-sequence and tokenization methods","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"099dbd21-4018-4b61-a634-eb78f5ab53eb","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"099dbd21-4018-4b61-a634-eb78f5ab53eb","children":["0c3690cf-c9ac-4d57-ae94-9860c6749d3d"]},"0c3690cf-c9ac-4d57-ae94-9860c6749d3d":{"id":"0c3690cf-c9ac-4d57-ae94-9860c6749d3d","message":{"id":"0c3690cf-c9ac-4d57-ae94-9860c6749d3d","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043203.6905246,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/1409.0473","title":"Neural Machine Translation by Jointly Learning to Align and Translate","snippet":"","ref_id":null,"pub_date":1409589182.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1409.3215","title":"Sequence to Sequence Learning with Neural Networks","snippet":"","ref_id":null,"pub_date":1410378935.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1508.07909","title":"Neural Machine Translation of Rare Words with Subword Units","snippet":"","ref_id":null,"pub_date":1441039051.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1808.06226","title":"[1808.06226] SentencePiece: A simple and language ...","snippet":"by T Kudo \u00b7 2018 \u00b7 Cited by 4554 \u2014 This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1607.04606","title":"Enriching Word Vectors with Subword Information","snippet":"by P Bojanowski \u00b7 2016 \u00b7 Cited by 14356 \u2014 In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1409.0473","title":"Neural machine translation by","snippet":"by D Bahdanau \u00b7 2014 \u00b7 Cited by 40436 \u2014 In this paper, we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation ...","ref_id":null,"pub_date":1463962755.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1409.3215","title":"Sequence to Sequence Learning with Neural Networks","snippet":"by I Sutskever \u00b7 2014 \u00b7 Cited by 30012 \u2014 The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed- dimensional vector representation, and ...","ref_id":null,"pub_date":1418693577.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1508.07909","title":"arXiv:1508.07909v5 [cs.CL] 10 Jun 2016","snippet":"by R Sennrich \u00b7 2015 \u00b7 Cited by 10196 \u2014 We introduce a variant of byte pair encoding for word segmentation, which is capable of encod- ing open vocabularies with a compact symbol vo-.","ref_id":null,"pub_date":1465795862.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1909.03341","title":"Neural Machine Translation with Byte-Level Subwords","snippet":"by C Wang \u00b7 2019 \u00b7 Cited by 274 \u2014 Our byte- level subword vocabularies are based on byte-pair encoding, while we use bytes as the basic units to compose subwords. Character ...","ref_id":null,"pub_date":1575855292.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1810.10222","title":"arXiv:1810.10222v1 [cs.CL] 24 Oct 2018","snippet":"by P Czapla \u00b7 2018 \u00b7 Cited by 7 \u2014 An important property of SentencePiece tokenization, necessary for us to obtain a valid word-based lan- guage model, is its reversibility. We do ...","ref_id":null,"pub_date":1540430152.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"ar5iv.labs.arxiv.org","entries":[{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/1808.06226","title":"SentencePiece: A simple and language independent subword ...","snippet":"Abstract. This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, ...","ref_id":null,"pub_date":null,"attribution":"ar5iv.labs.arxiv.org"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/Q17-1010.pdf","title":"Enriching Word Vectors with Subword Information","snippet":"by P Bojanowski \u00b7 2017 \u00b7 Cited by 14356 \u2014 Finally, recent works in machine translation have proposed using subword units to obtain repre- sentations of rare words (Sennrich et al., 2016; Lu- ong and ...","ref_id":null,"pub_date":1497194737.0,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/P16-1162/","title":"Neural Machine Translation of Rare Words with Subword ...","snippet":"by R Sennrich \u00b7 2016 \u00b7 Cited by 10196 \u2014 Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting ...","ref_id":null,"pub_date":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align ...","snippet":"Neural Machine Translation by Jointly Learning to Align and Translate \u00b7 Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio \u00b7 Published in International Conference on\u2026","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d","title":"[PDF] Sequence to Sequence Learning with Neural Networks","snippet":"This paper presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, and finds that reversing ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/2969033.2969173","title":"Sequence to sequence learning with neural networks","snippet":"by I Sutskever \u00b7 2014 \u00b7 Cited by 29981 \u2014 In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/1808.06226","title":"A simple and language independent subword tokenizer ...","snippet":"Aug 19, 2018 \u2014 This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, ...","ref_id":null,"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/facebook/fasttext-an-vectors","title":"facebook/fasttext-an-vectors","snippet":"Joulin, T. Mikolov, Enriching Word Vectors with Subword Information. @article{bojanowski2016enriching, title={Enriching Word Vectors with Subword Information} ...","ref_id":null,"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"direct.mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00051/43387/Enriching-Word-Vectors-with-Subword-Information","title":"Enriching Word Vectors with Subword Information","snippet":"Jun 1, 2017 \u2014 In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.","ref_id":null,"pub_date":null,"attribution":"direct.mit.edu"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate","title":"(PDF) Neural Machine Translation by Jointly Learning to ...","snippet":"Sep 1, 2014 \u2014 Neural Machine Translation by Jointly Learning to Align and Translate. September 2014. Source; arXiv. Authors: Dzmitry Bahdanau at Constructor ...","ref_id":null,"pub_date":1409529600.0,"attribution":"www.researchgate.net"}]}],"debug_sonic_thread_id":"thread_68c99c1c0314832286b03ceb26625c34","reasoning_status":"is_reasoning","reasoning_title":"Searching for sequence-to-sequence and tokenization methods","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"fc891b3e-33ec-485a-a781-fad1f5fae4d3","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"fc891b3e-33ec-485a-a781-fad1f5fae4d3","children":["e758a6a7-9072-4f24-89bf-69aa5fdfef7c"]},"e758a6a7-9072-4f24-89bf-69aa5fdfef7c":{"id":"e758a6a7-9072-4f24-89bf-69aa5fdfef7c","message":{"id":"e758a6a7-9072-4f24-89bf-69aa5fdfef7c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043209.628717,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Looking up MoE and GLaM","content":"I should check for key papers on MoE (e.g., the Switch Transformer) and on GLaM. The diffusion part might not be necessary, and the emergence of instruction-tuned models like ChatGPT and techniques like Constitutional AI (2022) could make things too complex, so I\u2019ll focus on Switch Transformer and GLaM for now.","chunks":[],"finished":true}],"source_analysis_msg_id":"33b9ec38-0d7d-48ac-8d1e-be39d3a33426"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"0c3690cf-c9ac-4d57-ae94-9860c6749d3d","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"0c3690cf-c9ac-4d57-ae94-9860c6749d3d","children":["00bbb127-c7c3-4a22-a78b-22b655a1d356"]},"00bbb127-c7c3-4a22-a78b-22b655a1d356":{"id":"00bbb127-c7c3-4a22-a78b-22b655a1d356","message":{"id":"00bbb127-c7c3-4a22-a78b-22b655a1d356","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043205.773683,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Switch Transformers Efficient Mixture-of-Experts 2021 arXiv Fedus et al.\"}, {\"q\": \"GLaM Generalist Language Model 2021 arXiv Du et al.\"}, {\"q\": \"Parameter-Efficient Fine-Tuning survey 2022 2023 arXiv\"}, {\"q\": \"QLoRA 2023 Dettmers arXiv\"}, {\"q\": \"RoBERTa 2019 arXiv Liu et al.\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Switch Transformers Efficient Mixture-of-Experts 2021 arXiv Fedus et al."},{"type":"search","q":"GLaM Generalist Language Model 2021 arXiv Du et al."},{"type":"search","q":"Parameter-Efficient Fine-Tuning survey 2022 2023 arXiv"},{"type":"search","q":"QLoRA 2023 Dettmers arXiv"},{"type":"search","q":"RoBERTa 2019 arXiv Liu et al."}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for Switch Transformer and GLaM models","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"e758a6a7-9072-4f24-89bf-69aa5fdfef7c","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"e758a6a7-9072-4f24-89bf-69aa5fdfef7c","children":["99a7154e-22df-4802-80a5-b2d3f6ee2b10"]},"99a7154e-22df-4802-80a5-b2d3f6ee2b10":{"id":"99a7154e-22df-4802-80a5-b2d3f6ee2b10","message":{"id":"99a7154e-22df-4802-80a5-b2d3f6ee2b10","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043213.1073387,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Switch Transformers Efficient Mixture-of-Experts 2021 arXiv Fedus et al.","GLaM Generalist Language Model 2021 arXiv Du et al.","Parameter-Efficient Fine-Tuning survey 2022 2023 arXiv","QLoRA 2023 Dettmers arXiv","RoBERTa 2019 arXiv Liu et al."]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for Switch Transformer and GLaM models","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"00bbb127-c7c3-4a22-a78b-22b655a1d356","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"00bbb127-c7c3-4a22-a78b-22b655a1d356","children":["3812de5e-0866-4785-bc0b-fd656cdb9d43"]},"3812de5e-0866-4785-bc0b-fd656cdb9d43":{"id":"3812de5e-0866-4785-bc0b-fd656cdb9d43","message":{"id":"3812de5e-0866-4785-bc0b-fd656cdb9d43","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043213.0931437,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2101.03961","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity","snippet":"","ref_id":null,"pub_date":1610381512.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2112.06905","title":"Efficient Scaling of Language Models with Mixture-of-Experts","snippet":"by N Du \u00b7 2021 \u00b7 Cited by 904 \u2014 In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2305.14314","title":"[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs","snippet":"by T Dettmers \u00b7 2023 \u00b7 Cited by 3863 \u2014 We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1907.11692","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","snippet":"by Y Liu \u00b7 2019 \u00b7 Cited by 21795 \u2014 We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2112.06905","title":"Efficient Scaling of Language Models with Mixture-of-Experts","snippet":"by N Du \u00b7 2021 \u00b7 Cited by 904 \u2014 We present a family of generalist lan- guage models called GLaM, that strike a balance between dense and conditional computation. The largest ...","ref_id":null,"pub_date":1659488968.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2403.14608","title":"Parameter-Efficient Fine-Tuning for Large Models","snippet":"by Z Han \u00b7 2024 \u00b7 Cited by 663 \u2014 In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2305.14314","title":"QLORA: Efficient Finetuning of Quantized LLMs","snippet":"May 23, 2023 \u2014 We present QLORA, an efficient finetuning approach that reduces memory us- age enough to finetune a 65B parameter model on a single 48GB GPU ...","ref_id":null,"pub_date":1684889237.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1907.11692","title":"arXiv:1907.11692v1 [cs.CL] 26 Jul 2019","snippet":"by Y Liu \u00b7 2019 \u00b7 Cited by 21795 \u2014 RoBERTa matches the architecture and training objective of BERTLARGE . Results for BERTLARGE and XLNetLARGE are from. Devlin et al. (2019) and ...","ref_id":null,"pub_date":1564360030.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2504.13822","title":"Parameter-Efficient Continual Fine-Tuning: A Survey","snippet":"by EN Coleman \u00b7 2025 \u2014 In this survey, we first provide an overview of CL algorithms and PEFT methods before reviewing the state-of-the-art on Parameter-Efficient Continual Fine- ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2402.06196v2","title":"Large Language Models: A Survey","snippet":"Feb 20, 2024 \u2014 GLaM: In [84] , Du et al. proposed a family of LLMs named GLaM (Generalist Language Model), which use a sparsely activated mixture-of ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2504.13822v2","title":"Parameter-Efficient Continual Fine-Tuning: A Survey","snippet":"Aug 19, 2025 \u2014 This survey bridges the gap between CL and PEFT by focusing on the emerging field of Parameter-Efficient Continual Fine-Tuning (PECFT). We first ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.07987v3","title":"Accelerating Transformers with Mixture-of-Experts Attention","snippet":"Sep 30, 2024 \u2014 Fedus et al. [37] introduce Mixture of Experts in Transformers ... Switch transformers: Scaling to trillion parameter models with simple and ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.arxiv.org","entries":[{"type":"search_result","url":"https://www.arxiv.org/pdf/2504.13822","title":"Parameter-Efficient Continual Fine-Tuning: A Survey","snippet":"by EN Coleman \u00b7 2025 \u2014 CL focuses on enabling models to learn from sequential tasks without forgetting previous knowledge, addressing the stability-plasticity dilemma ...","ref_id":null,"pub_date":null,"attribution":"www.arxiv.org"}]},{"type":"search_result_group","domain":"alastairreid.github.io","entries":[{"type":"search_result","url":"https://alastairreid.github.io/RelatedWork/papers/fedus%3Aarxiv%3A2021/","title":"Switch transformers: Scaling to trillion parameter models with ...","snippet":"Observes that the [mixture of experts] approach suffers from problems in complexity, communication costs and training instabilities.","ref_id":null,"pub_date":null,"attribution":"alastairreid.github.io"}]},{"type":"search_result_group","domain":"jmlr.org","entries":[{"type":"search_result","url":"https://jmlr.org/papers/volume23/21-0998/21-0998.pdf","title":"Switch Transformers: Scaling to Trillion Parameter Models ...","snippet":"by W Fedus \u00b7 2022 \u00b7 Cited by 2959 \u2014 The Switch Transformer architecture, which simplifies and improves over Mixture of. Experts. \u2022 Scaling properties and a benchmark against the strongly tuned T5 ...","ref_id":null,"pub_date":1651420075.0,"attribution":"jmlr.org"}]},{"type":"search_result_group","domain":"ar5iv.labs.arxiv.org","entries":[{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2112.06905","title":"Efficient Scaling of Language Models with Mixture-of-Experts","snippet":"We present a family of generalist language models called GLaM, that strike a balance between dense and conditional computation. The largest version of GLaM has ...","ref_id":null,"pub_date":null,"attribution":"ar5iv.labs.arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2305.14314","title":"QLoRA: Efficient Finetuning of Quantized LLMs - ar5iv - arXiv","snippet":"Abstract. We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while ...","ref_id":null,"pub_date":null,"attribution":"ar5iv.labs.arxiv.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/334735779_RoBERTa_A_Robustly_Optimized_BERT_Pretraining_Approach","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","snippet":"RoBERTa (Liu et al., 2019) , a variant of BERT, builds upon the same training setup but incorporates dynamic masking, expands the training dataset, and ...","ref_id":null,"pub_date":1564099200.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.5555/3586589.3586709","title":"Switch transformers: scaling to trillion parameter models ...","snippet":"by W Fedus \u00b7 2022 \u00b7 Cited by 2959 \u2014 Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. AUTHORs: William Fedus.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3666122.3666563","title":"QLORA: efficient finetuning of quantized LLMs","snippet":"Dec 10, 2023 \u2014 We present QLORA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","snippet":"Jul 26, 2019 \u2014 It is found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it, ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]}],"debug_sonic_thread_id":"thread_68c99c1c0314832286b03ceb26625c34","reasoning_status":"is_reasoning","reasoning_title":"Searching for Switch Transformer and GLaM models","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"99a7154e-22df-4802-80a5-b2d3f6ee2b10","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"99a7154e-22df-4802-80a5-b2d3f6ee2b10","children":["c50565e3-f7e2-46c1-8e0e-a2527192bde1"]},"c50565e3-f7e2-46c1-8e0e-a2527192bde1":{"id":"c50565e3-f7e2-46c1-8e0e-a2527192bde1","message":{"id":"c50565e3-f7e2-46c1-8e0e-a2527192bde1","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043217.785186,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Adding T5 and XLNet","content":"I should make sure to include T5 (2019) and XLNet (2019) in the list. T5, with its unified text-to-text framework, definitely plays a big role, and XLNet introduces some important advancements as well. I'll include both since they are key to the models I'm mentioning.","chunks":[],"finished":true}],"source_analysis_msg_id":"8b7cde5e-b73e-4775-95a3-5b6b13f7cbc2"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"3812de5e-0866-4785-bc0b-fd656cdb9d43","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"3812de5e-0866-4785-bc0b-fd656cdb9d43","children":["9bf95980-9013-4d0e-8bfd-cfa3b33813cc"]},"9bf95980-9013-4d0e-8bfd-cfa3b33813cc":{"id":"9bf95980-9013-4d0e-8bfd-cfa3b33813cc","message":{"id":"9bf95980-9013-4d0e-8bfd-cfa3b33813cc","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043215.400024,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer T5 2019 arXiv\"}, {\"q\": \"XLNet Generalized Autoregressive Pretraining for Language Understanding 2019 arXiv Yang et al.\"}, {\"q\": \"ELECTRA 2020 Clark et al. arXiv\"}, {\"q\": \"ALBERT 2019 Lan et al. arXiv\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer T5 2019 arXiv"},{"type":"search","q":"XLNet Generalized Autoregressive Pretraining for Language Understanding 2019 arXiv Yang et al."},{"type":"search","q":"ELECTRA 2020 Clark et al. arXiv"},{"type":"search","q":"ALBERT 2019 Lan et al. arXiv"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for transfer learning model papers and more","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"c50565e3-f7e2-46c1-8e0e-a2527192bde1","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"c50565e3-f7e2-46c1-8e0e-a2527192bde1","children":["49c3b1f1-3433-4f45-8444-c0ed145ce882"]},"49c3b1f1-3433-4f45-8444-c0ed145ce882":{"id":"49c3b1f1-3433-4f45-8444-c0ed145ce882","message":{"id":"49c3b1f1-3433-4f45-8444-c0ed145ce882","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043220.338119,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer T5 2019 arXiv","XLNet Generalized Autoregressive Pretraining for Language Understanding 2019 arXiv Yang et al.","ELECTRA 2020 Clark et al. arXiv","ALBERT 2019 Lan et al. arXiv"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for transfer learning model papers and more","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"9bf95980-9013-4d0e-8bfd-cfa3b33813cc","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"9bf95980-9013-4d0e-8bfd-cfa3b33813cc","children":["ea2fb520-f961-4beb-8480-3e47cb207573"]},"ea2fb520-f961-4beb-8480-3e47cb207573":{"id":"ea2fb520-f961-4beb-8480-3e47cb207573","message":{"id":"ea2fb520-f961-4beb-8480-3e47cb207573","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043220.3221946,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/1910.10683","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","snippet":"","ref_id":null,"pub_date":1571852256.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1906.08237","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","snippet":"","ref_id":null,"pub_date":1560965748.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2003.10555","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","snippet":"","ref_id":null,"pub_date":1584998262.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1909.11942","title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","snippet":"","ref_id":null,"pub_date":1569481573.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2003.10555","title":"electra: pre-training text encoders","snippet":"by K Clark \u00b7 2020 \u00b7 Cited by 5207 \u2014 We call our approach ELECTRA1 for \u201cEfficiently Learning an Encoder that Classifies Token Re- placements Accurately.\u201d As in prior work, we apply ...","ref_id":null,"pub_date":1585096844.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2403.18338v1","title":"Is a Compact Multilingual BERT Model Still Worth It?","snippet":"As far as we know, compact models, such as the ALBERT model Lan et al. (2019) , are a possible answer since they have been evaluated on the comprehension tasks ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1906.08237","title":"XLNet: Generalized Autoregressive Pretraining for ...","snippet":"by Z Yang \u00b7 2019 \u00b7 Cited by 11779 \u2014 In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional ...","ref_id":null,"pub_date":1578014067.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2104.02756","title":"Efficient transfer learning for NLP with ELECTRA","snippet":"by F Mercier \u00b7 2021 \u00b7 Cited by 1 \u2014 Abstract:Clark et al. [2020] claims that the ELECTRA approach is highly efficient in NLP performances relative to computation budget.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1909.11942","title":"arXiv:1909.11942v6 [cs.CL] 9 Feb 2020","snippet":"by Z Lan \u00b7 2019 \u00b7 Cited by 9382 \u2014 In this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the ...","ref_id":null,"pub_date":1581383555.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2112.07869","title":"arXiv:2112.07869v1 [cs.CL] 15 Dec 2021","snippet":"by R Tinn \u00b7 2021 \u00b7 Cited by 180 \u2014 ELECTRA (Clark et al., 2020), which has shown good performance in general-domain datasets such as GLUE (Wang et al., 2019a) and. SQuAD ...","ref_id":null,"pub_date":1639619019.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2005.13619","title":"Language Representation Models for Fine-Grained ...","snippet":"by B Cheang \u00b7 2020 \u00b7 Cited by 11 \u2014 ALBERT, which stands for \u201cA Lite BERT\u201d, was made available in an open source version by Google in 2019, developed by Lan et al. The model was ...","ref_id":null,"pub_date":1590710883.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"jmlr.org","entries":[{"type":"search_result","url":"https://jmlr.org/papers/volume21/20-074/20-074.pdf","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"by C Raffel \u00b7 2020 \u00b7 Cited by 26495 \u2014 Abstract. Transfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful ...","ref_id":null,"pub_date":1595800934.0,"attribution":"jmlr.org"}]},{"type":"search_result_group","domain":"papers.neurips.cc","entries":[{"type":"search_result","url":"https://papers.neurips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf","title":"XLNet: Generalized Autoregressive Pretraining for ...","snippet":"by Z Yang \u00b7 Cited by 11779 \u2014 Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a wide spectrum of problems including GLUE language understanding ...","ref_id":null,"pub_date":1581582028.0,"attribution":"papers.neurips.cc"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.5555/3455716.3455856","title":"Exploring the limits of transfer learning with a unified text-to ...","snippet":"by C Raffel \u00b7 2020 \u00b7 Cited by 26495 \u2014 In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3454287.3454804","title":"XLNet: generalized autoregressive pretraining for language ...","snippet":"We propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/336084032_ALBERT_A_Lite_BERT_for_Self-supervised_Learning_of_Language_Representations","title":"ALBERT: A Lite BERT for Self-supervised Learning of ...","snippet":"Sep 16, 2019 \u2014 We present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.","ref_id":null,"pub_date":1569456000.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/336767865_Exploring_the_Limits_of_Transfer_Learning_with_a_Unified_Text-to-Text_Transformer","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a ...","ref_id":null,"pub_date":1571788800.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/333892322_XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding","title":"(PDF) XLNet: Generalized Autoregressive Pretraining for ...","snippet":"Jun 19, 2019 \u2014 XLNet: Generalized Autoregressive Pretraining for Language Understanding. June 2019. DOI:10.48550/arXiv ... Models such as XLNet (Yang, Dai, et al ...","ref_id":null,"pub_date":1560902400.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"ui.adsabs.harvard.edu","entries":[{"type":"search_result","url":"https://ui.adsabs.harvard.edu/abs/arXiv%3A2104.02756","title":"Efficient transfer learning for NLP with ELECTRA","snippet":"Abstract. Clark et al. [2020] claims that the ELECTRA approach is highly efficient in NLP performances relative to computation budget.","ref_id":null,"pub_date":null,"attribution":"ui.adsabs.harvard.edu"}]}],"debug_sonic_thread_id":"thread_68c99c1c0314832286b03ceb26625c34","reasoning_status":"is_reasoning","reasoning_title":"Searching for transfer learning model papers and more","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"49c3b1f1-3433-4f45-8444-c0ed145ce882","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"49c3b1f1-3433-4f45-8444-c0ed145ce882","children":["ac54c4fc-4487-4928-8627-21cf046f8507"]},"ac54c4fc-4487-4928-8627-21cf046f8507":{"id":"ac54c4fc-4487-4928-8627-21cf046f8507","message":{"id":"ac54c4fc-4487-4928-8627-21cf046f8507","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043230.524678,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Structuring the Timeline","content":"I think it's sufficient to mention key steps like word embeddings, seq2seq with attention, pretraining, and the evolution through Transformers, scaling, and generative LMs. I\u2019ll include the phases for instruction tuning, RLHF, and emerging trends like agents and tool use, while considering 2023-2025 trends like ReAct but not going into too much depth on specifics like Constitutional AI.","chunks":[],"finished":true}],"source_analysis_msg_id":"fc603566-97dd-4fc8-9868-1bd707562c34"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"ea2fb520-f961-4beb-8480-3e47cb207573","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"ea2fb520-f961-4beb-8480-3e47cb207573","children":["e43f2db0-1573-40ec-8380-22350f6b9d72"]},"e43f2db0-1573-40ec-8380-22350f6b9d72":{"id":"e43f2db0-1573-40ec-8380-22350f6b9d72","message":{"id":"e43f2db0-1573-40ec-8380-22350f6b9d72","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043222.450785,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"ReAct Synergizing Reasoning and Acting in Language Models 2022 arXiv\"}, {\"q\": \"Toolformer 2023 arXiv Meta self-supervised tool use\"}, {\"q\": \"Function calling OpenAI 2023 blog 'Function calling and other API updates'\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"ReAct Synergizing Reasoning and Acting in Language Models 2022 arXiv"},{"type":"search","q":"Toolformer 2023 arXiv Meta self-supervised tool use"},{"type":"search","q":"Function calling OpenAI 2023 blog 'Function calling and other API updates'"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Browsing trends in language model development and tools","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"ac54c4fc-4487-4928-8627-21cf046f8507","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"ac54c4fc-4487-4928-8627-21cf046f8507","children":["ec279e7c-65e2-4248-bdd7-8fdad8d5ec61"]},"ec279e7c-65e2-4248-bdd7-8fdad8d5ec61":{"id":"ec279e7c-65e2-4248-bdd7-8fdad8d5ec61","message":{"id":"ec279e7c-65e2-4248-bdd7-8fdad8d5ec61","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043232.8863258,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["ReAct Synergizing Reasoning and Acting in Language Models 2022 arXiv","Toolformer 2023 arXiv Meta self-supervised tool use","Function calling OpenAI 2023 blog 'Function calling and other API updates'"]},"reasoning_status":"is_reasoning","reasoning_title":"Browsing trends in language model development and tools","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"e43f2db0-1573-40ec-8380-22350f6b9d72","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"e43f2db0-1573-40ec-8380-22350f6b9d72","children":["86e80271-4582-49ec-b190-2ff7127ac159"]},"86e80271-4582-49ec-b190-2ff7127ac159":{"id":"86e80271-4582-49ec-b190-2ff7127ac159","message":{"id":"86e80271-4582-49ec-b190-2ff7127ac159","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758043232.8697615,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2210.03629","title":"ReAct: Synergizing Reasoning and Acting in Language Models","snippet":"","ref_id":null,"pub_date":1665018032.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2302.04761","title":"Toolformer: Language Models Can Teach Themselves to Use Tools","snippet":"","ref_id":null,"pub_date":1675961397.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2210.03629","title":"ReAct: Synergizing Reasoning and Acting in Language ...","snippet":"Mar 10, 2023 \u2014 Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b. Siddharth Karamcheti, Megha ...","ref_id":null,"pub_date":1678666151.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2302.04761","title":"arXiv:2302.04761v1 [cs.CL] 9 Feb 2023","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2259 \u2014 We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, ...","ref_id":null,"pub_date":1675993086.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/function-calling-and-other-api-updates/","title":"Function calling and other API updates","snippet":"Jun 13, 2023 \u2014 We're announcing updates including more steerable API models, function calling capabilities, longer context, and lower prices.","ref_id":null,"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"platform.openai.com","entries":[{"type":"search_result","url":"https://platform.openai.com/docs/guides/function-calling","title":"OpenAI Function Calling Guide","snippet":"Explore resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's developer platform.","ref_id":null,"pub_date":null,"attribution":"platform.openai.com"}]},{"type":"search_result_group","domain":"research.google","entries":[{"type":"search_result","url":"https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/","title":"ReAct: Synergizing Reasoning and Acting in Language ...","snippet":"We propose a general paradigm that combines reasoning and acting advances to enable language models to solve various language reasoning and decision making ...","ref_id":null,"pub_date":null,"attribution":"research.google"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://neurips.cc/virtual/2023/poster/71288","title":"Language Models Can Teach Themselves to Use Tools","snippet":"Dec 9, 2023 \u2014 We have introduced Toolformer, a LM that learns in a self-supervised way how to use different tools such as search engines, calculators, and ...","ref_id":null,"pub_date":1695686400.0,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"news.ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=36313348","title":"Function calling and other API updates","snippet":"Jun 13, 2023 \u2014 The big feature here is the function calls, as this is effectively a replacement for the \"Tools\" feature of Agents popularized by LangChain, ...","ref_id":null,"pub_date":null,"attribution":"news.ycombinator.com"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/ReAct%3A-Synergizing-Reasoning-and-Acting-in-Language-Yao-Zhao/99832586d55f540f603637e458a292406a0ed75d","title":"Synergizing Reasoning and Acting in Language Models","snippet":"The use of LLMs are explored to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40oxenai/arxiv-dives-toolformer-language-models-can-teach-themselves-to-use-tools-b50c9312c2a9","title":"Arxiv Dives \u2014 Toolformer: Language models can teach ...","snippet":"The Toolformer architecture lets the use of tools be learned in a self-supervised way without requiring large amounts of human annotations.","ref_id":null,"pub_date":1707850007.992,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40ronnyh/research-paper-summary-react-synergizing-reasoning-and-acting-in-language-models-95e8ca80855b","title":"[Research Paper Summary] ReAct: Synergizing Reasoning ...","snippet":"ReAct: Synergizing Reasoning + Acting is a research paper that explores the potential of large language models in solving complex tasks that ...","ref_id":null,"pub_date":1683023172.088,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/the-modern-scientist/everything-you-need-to-know-about-openai-function-calling-and-assistants-api-55c02570a21c","title":"Everything you need to know about OpenAI function calling ...","snippet":"This article delves into the intricate world of OpenAI's Chat Completions and Assistant APIs, to develop customized chatbot applications for your next AI ...","ref_id":null,"pub_date":1710001401.833,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"www.zoomyizumi.com","entries":[{"type":"search_result","url":"https://www.zoomyizumi.com/openais-new/","title":"'Function calling', OpenAI's API new feature. - Zoomy Izumi","snippet":"Jun 14, 2023 \u2014 The ability to add \"functions\" flaunting it as a 'more reliable way' to pair GPT's capabilities with external calls.","ref_id":null,"pub_date":1686749982.0,"attribution":"www.zoomyizumi.com"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3666122.3669119","title":"Toolformer: language models can teach themselves to use ...","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2254 \u2014 In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.alphaxiv.org","entries":[{"type":"search_result","url":"https://www.alphaxiv.org/overview/2210.03629v1","title":"Synergizing Reasoning and Acting in Language Models","snippet":"A new paradigm called ReAct enables language models to interleave reasoning (thoughts) and acting (actions) for problem-solving.","ref_id":null,"pub_date":null,"attribution":"www.alphaxiv.org"}]},{"type":"search_result_group","domain":"sino-huang.github.io","entries":[{"type":"search_result","url":"https://sino-huang.github.io/posts/timo_schick-toolformer-language-models-can-teach-themselves-to-use-tools-2023/","title":"Timo_schick Toolformer Language Models Can Teach ...","snippet":"Mar 1, 2023 \u2014 We introduce Toolformer, a model that incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system ...","ref_id":null,"pub_date":1677661069.0,"attribution":"sino-huang.github.io"}]},{"type":"search_result_group","domain":"learn.microsoft.com","entries":[{"type":"search_result","url":"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/function-calling","title":"How to use function calling with Azure OpenAI in ...","snippet":"Use the model's response to call your API or function; Call the chat completions API again, including the response from your function to get a final response.","ref_id":null,"pub_date":null,"attribution":"learn.microsoft.com"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2210.03629","title":"Synergizing Reasoning and Acting in Language Models","snippet":"Oct 5, 2022 \u2014 In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy ...","ref_id":null,"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"ai.meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/research/publications/toolformer-language-models-can-teach-themselves-to-use-tools/","title":"Toolformer: Language Models Can Teach Themselves to ...","snippet":"In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.","ref_id":null,"pub_date":null,"attribution":"ai.meta.com"}]},{"type":"search_result_group","domain":"simonwillison.net","entries":[{"type":"search_result","url":"https://simonwillison.net/2023/Jun/13/function-calling/","title":"OpenAI: Function calling and other API updates","snippet":"Jun 13, 2023 \u2014 You can now send JSON schema defining one or more functions to GPT 3.5 and GPT-4\u2014those models will then return a blob of JSON describing a ...","ref_id":null,"pub_date":null,"attribution":"simonwillison.net"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/y1mosn/r_react_synergizing_reasoning_and_acting_in/","title":"[R] ReAct: Synergizing Reasoning and Acting in Language ...","snippet":"In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy ...","ref_id":null,"pub_date":1665527976.67,"attribution":"www.reddit.com"}]}],"debug_sonic_thread_id":"thread_68c99c1c0314832286b03ceb26625c34","reasoning_status":"is_reasoning","reasoning_title":"Browsing trends in language model development and tools","classifier_response":"default","request_id":"980206bd1e000d23-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"ec279e7c-65e2-4248-bdd7-8fdad8d5ec61","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"ec279e7c-65e2-4248-bdd7-8fdad8d5ec61","children":["74090e60-7922-4a08-a89d-116c7f9dde19"]},"74090e60-7922-4a08-a89d-116c7f9dde19":{"id":"74090e60-7922-4a08-a89d-116c7f9dde19","message":{"id":"74090e60-7922-4a08-a89d-116c7f9dde19","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043244.885975,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 1m 40s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":100,"model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"86e80271-4582-49ec-b190-2ff7127ac159","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"86e80271-4582-49ec-b190-2ff7127ac159","children":["58e2951a-1007-42fe-a1f8-c39db8053c15"]},"58e2951a-1007-42fe-a1f8-c39db8053c15":{"id":"58e2951a-1007-42fe-a1f8-c39db8053c15","message":{"id":"58e2951a-1007-42fe-a1f8-c39db8053c15","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758043234.912648,"update_time":null,"content":{"content_type":"text","parts":["Here\u2019s a compact, evidence-backed arc from **word2vec \u2192 modern LLMs**. I\u2019ve grouped major shifts and highlighted why each mattered.\n\n# 1) Static word vectors (2013\u20132016)\n- **word2vec** (CBOW/skip-gram; negative sampling, subsampling): fast, distributional word embeddings; kicked off the \u201cvectors-as-meaning\u201d era. \ue200cite\ue202turn0search5\ue202turn0search0\ue202turn0search10\ue201  \n- **GloVe**: combined global co-occurrence statistics with local context; often stronger on analogies vs. word2vec. \ue200cite\ue202turn0search1\ue202turn0search6\ue201  \n- **fastText**: added character n-grams \u2192 robust to morphology/OOV. \ue200cite\ue202turn3search4\ue202turn3search9\ue201  \n- Tokenization advances to address OOV: **BPE** subwords; later **SentencePiece** (language-independent training from raw text). \ue200cite\ue202turn3search2\ue202turn3search7\ue202turn3search3\ue202turn3search8\ue201\n\n# 2) Neural sequence modeling & attention (2014\u20132017)\n- **seq2seq with LSTMs**: generic encoder-decoder for translation (and beyond). \ue200cite\ue202turn3search1\ue202turn3search6\ue201  \n- **Additive attention**: learned soft alignment; removes fixed-vector bottleneck. \ue200cite\ue202turn3search0\ue202turn3search5\ue201  \n- **Transformers**: \u201cAttention Is All You Need\u201d removes recurrence; scales better and sets new MT SOTA \u2192 the architecture all later models use. \ue200cite\ue202turn0search2\ue202turn0search12\ue201\n\n# 3) Contextual representations & transfer (2018\u20132019)\n- **ULMFiT**: showed *general* LM pretrain \u2192 task fine-tune works broadly in NLP. \ue200cite\ue202turn0search4\ue202turn0search14\ue201  \n- **ELMo**: deep **contextual** word reps from biLM internals; big gains across tasks. \ue200cite\ue202turn0search3\ue202turn0search8\ue201  \n- **BERT**: bidirectional masked-LM + NSP; fine-tune a single encoder across tasks; lifted GLUE/SQuAD. Follow-ups: **RoBERTa** (train longer, more data, no-NSP), **ALBERT** (parameter sharing), **XLNet** (permutation LM). \ue200cite\ue202turn1search0\ue202turn1search5\ue202turn4search4\ue202turn4search9\ue202turn5search3\ue202turn5search1\ue201  \n- **T5**: unified *text-to-text* framing; large clean web corpus (**C4**); normalized pretraining objectives under one umbrella. \ue200cite\ue202turn5search0\ue202turn5search4\ue201\n\n# 4) Autoregressive LMs at scale (2019\u20132021)\n- **GPT-2**: unsupervised, zero-shot behaviors from scale + web pretraining. \ue200cite\ue202turn1search1\ue202turn1search11\ue201  \n- **GPT-3**: 175B; systematic *few-shot/in-context learning* without gradient updates. \ue200cite\ue202turn1search2\ue202turn1search7\ue202turn1search12\ue201  \n- **Scaling laws**: cross-entropy follows power laws vs. model/data/compute; practical guide for allocating compute. **Chinchilla** later corrected undertraining \u2192 *more tokens per parameter* beats ever-larger models. \ue200cite\ue202turn1search3\ue202turn1search8\ue202turn1search4\ue202turn1search9\ue202turn1search14\ue201  \n- **Sparse MoE** for efficient capacity: **Switch Transformer**, **GLaM** push trillion-parameter (sparsely activated) models at similar FLOPs/token. \ue200cite\ue202turn4search0\ue202turn4search10\ue202turn4search1\ue202turn4search6\ue201\n\n# 5) Making models *follow* you (2021\u20132023)\n- **Instruction finetuning**: curate many instruction-style tasks; improves generalization (**FLAN**, later Flan-PaLM). \ue200cite\ue202turn2search1\ue202turn2search6\ue201  \n- **RLHF / InstructGPT**: human preference optimization dramatically improves helpfulness/harms; 1.3B InstructGPT preferred over 175B GPT-3. \ue200cite\ue202turn2search0\ue202turn2search5\ue201  \n- **ELECTRA** (efficient pretraining) is a key encoder-side alternative that influenced budget-aware setups. \ue200cite\ue202turn5search2\ue202turn5search6\ue201\n\n# 6) Retrieval, tools, and agents (2020\u20132024)\n- **RAG**: combine parametric LM with non-parametric retrieval \u2192 better factuality & updatability. \ue200cite\ue202turn2search2\ue201  \n- **Tool use / function calling**: prompt-level planning + calling external APIs (calculator, search, etc.); **ReAct** pattern and **Toolformer** self-supervised tool selection; function calling shipped in mainstream APIs. \ue200cite\ue202turn6search0\ue202turn6search3\ue202turn6search1\ue202turn6search4\ue202turn6search2\ue202turn6search5\ue201\n\n# 7) Open-weight wave & efficient adaptation (2023\u20132025)\n- **LLaMA** family (and successors): strong open baselines trained on (mostly) public data \u2192 catalyzed an ecosystem of distilled/chat/instruction models. \ue200cite\ue202turn2search4\ue202turn2search19\ue201  \n- **PEFT** methods: **LoRA** (inject low-rank adapters into frozen backbones); **QLoRA** (finetune 4-bit quantized backbones) \u2192 consumer-GPU finetuning becomes practical; broader PEFT surveys consolidate the space. \ue200cite\ue202turn2search3\ue202turn2search8\ue202turn4search3\ue202turn4search8\ue202turn4search7\ue201  \n- Ongoing research blends **PEFT + continual learning** to update models frequently without catastrophic forgetting. \ue200cite\ue202turn4search2\ue202turn4search12\ue202turn4search17\ue201\n\n---\n\n## Putting it together (conceptual through-line)\n1) **Distributional semantics** \u2192 *static embeddings* (word2vec/GloVe/fastText).  \n2) **Neural seq2seq + attention** solves long-range dependencies, enabling end-to-end translation.  \n3) **Contextual encoders** (ELMo/BERT) show universal pretraining + task-specific heads.  \n4) **Autoregressive decoders** (GPT-x) reveal *in-context learning* when scaled; **scaling laws** quantify returns; **Chinchilla** reframes compute-optimality.  \n5) **Alignment** (instruction tuning + RLHF) changes behavior from \u201cnext-token predictor\u201d to cooperative assistants.  \n6) **RAG + tools/agents** bridge models with external knowledge and functions\u2014crucial for truthfulness and utility.  \n7) **Open weights + PEFT** democratize capability and iteration; **MoE** explores capacity without linear compute growth.\n\nIf you want, I can turn this into a one-page timeline chart with seminal papers, dates, and \u201cwhy it mattered\u201d summaries, or tailor a reading roadmap (with code pointers) focused on any of the phases above."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":"\ue200cite\ue202turn0search5\ue202turn0search0\ue202turn0search10\ue201","start_idx":314,"end_idx":360,"safe_urls":["https://arxiv.org/abs/1310.4546?utm_source=chatgpt.com","https://arxiv.org/pdf/1310.4546?utm_source=chatgpt.com","https://arxiv.org/pdf/1402.3722?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1310.4546?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Distributed Representations of Words and Phrases ...","url":"https://arxiv.org/abs/1310.4546?utm_source=chatgpt.com","pub_date":null,"snippet":"by T Mikolov \u00b7 2013 \u00b7 Cited by 48431 \u2014 Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector ...","attribution_segments":null,"supporting_websites":[{"title":"Distributed Representations of Words and Phrases and ...","url":"https://arxiv.org/pdf/1310.4546?utm_source=chatgpt.com","pub_date":null,"snippet":"by T Mikolov \u00b7 2013 \u00b7 Cited by 48431 \u2014 In this paper we present several extensions of the original Skip-gram model. We show that sub- sampling of frequent words during training ...","attribution":"arXiv"},{"title":"Deriving Mikolov et al.'s Negative-Sampling Word- ...","url":"https://arxiv.org/pdf/1402.3722?utm_source=chatgpt.com","pub_date":null,"snippet":"by Y Goldberg \u00b7 2014 \u00b7 Cited by 2573 \u2014 While negative-sampling is based on the skip-gram model, it is in fact optimizing a different objective. What follows is the derivation of the ...","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":5},{"turn_index":0,"ref_type":"search","ref_index":0},{"turn_index":0,"ref_type":"search","ref_index":10}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search6\ue201","start_idx":479,"end_idx":511,"safe_urls":["https://aclanthology.org/D14-1162/?utm_source=chatgpt.com","https://www-nlp.stanford.edu/pubs/glove.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([ACL Anthology](https://aclanthology.org/D14-1162/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"GloVe: Global Vectors for Word Representation","url":"https://aclanthology.org/D14-1162/?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Pennington \u00b7 2014 \u00b7 Cited by 47178 \u2014 Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on ...","attribution_segments":null,"supporting_websites":[{"title":"GloVe: Global Vectors for Word Representation","url":"https://www-nlp.stanford.edu/pubs/glove.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by RS JeffreyPennington \u00b7 Cited by 743 \u2014 Abstract. Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and.","attribution":"www-nlp.stanford.edu"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"ACL Anthology"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search4\ue202turn3search9\ue201","start_idx":582,"end_idx":614,"safe_urls":["https://aclanthology.org/Q17-1010.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/1607.04606?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1607.04606?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Enriching Word Vectors with Subword Information","url":"https://arxiv.org/abs/1607.04606?utm_source=chatgpt.com","pub_date":null,"snippet":"by P Bojanowski \u00b7 2016 \u00b7 Cited by 14356 \u2014 In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.","attribution_segments":null,"supporting_websites":[{"title":"Enriching Word Vectors with Subword Information","url":"https://aclanthology.org/Q17-1010.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by P Bojanowski \u00b7 2017 \u00b7 Cited by 14356 \u2014 Finally, recent works in machine translation have proposed using subword units to obtain repre- sentations of rare words (Sennrich et al., 2016; Lu- ong and ...","attribution":"ACL Anthology"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":4},{"turn_index":3,"ref_type":"search","ref_index":9}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search2\ue202turn3search7\ue202turn3search3\ue202turn3search8\ue201","start_idx":746,"end_idx":804,"safe_urls":["https://ar5iv.labs.arxiv.org/html/1808.06226?utm_source=chatgpt.com","https://arxiv.org/abs/1508.07909?utm_source=chatgpt.com","https://arxiv.org/abs/1808.06226?utm_source=chatgpt.com","https://arxiv.org/pdf/1508.07909?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1508.07909?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Neural Machine Translation of Rare Words with Subword Units","url":"https://arxiv.org/abs/1508.07909?utm_source=chatgpt.com","pub_date":1441039051.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"arXiv:1508.07909v5 [cs.CL] 10 Jun 2016","url":"https://arxiv.org/pdf/1508.07909?utm_source=chatgpt.com","pub_date":null,"snippet":"by R Sennrich \u00b7 2015 \u00b7 Cited by 10196 \u2014 We introduce a variant of byte pair encoding for word segmentation, which is capable of encod- ing open vocabularies with a compact symbol vo-.","attribution":"arXiv"},{"title":"[1808.06226] SentencePiece: A simple and language ...","url":"https://arxiv.org/abs/1808.06226?utm_source=chatgpt.com","pub_date":null,"snippet":"by T Kudo \u00b7 2018 \u00b7 Cited by 4554 \u2014 This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural ...","attribution":"arXiv"},{"title":"SentencePiece: A simple and language independent subword ...","url":"https://ar5iv.labs.arxiv.org/html/1808.06226?utm_source=chatgpt.com","pub_date":null,"snippet":"Abstract. This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, ...","attribution":"ar5iv"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":2},{"turn_index":3,"ref_type":"search","ref_index":7},{"turn_index":3,"ref_type":"search","ref_index":3},{"turn_index":3,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search1\ue202turn3search6\ue201","start_idx":940,"end_idx":972,"safe_urls":["https://arxiv.org/abs/1409.3215?utm_source=chatgpt.com","https://arxiv.org/pdf/1409.3215?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1409.3215?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Sequence to Sequence Learning with Neural Networks","url":"https://arxiv.org/abs/1409.3215?utm_source=chatgpt.com","pub_date":1410378935.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Sequence to Sequence Learning with Neural Networks","url":"https://arxiv.org/pdf/1409.3215?utm_source=chatgpt.com","pub_date":null,"snippet":"by I Sutskever \u00b7 2014 \u00b7 Cited by 30012 \u2014 The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed- dimensional vector representation, and ...","attribution":"arXiv"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":1},{"turn_index":3,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search0\ue202turn3search5\ue201","start_idx":1058,"end_idx":1090,"safe_urls":["https://arxiv.org/abs/1409.0473?utm_source=chatgpt.com","https://arxiv.org/pdf/1409.0473?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1409.0473?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Neural Machine Translation by Jointly Learning to Align and Translate","url":"https://arxiv.org/abs/1409.0473?utm_source=chatgpt.com","pub_date":1409589182.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Neural machine translation by","url":"https://arxiv.org/pdf/1409.0473?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Bahdanau \u00b7 2014 \u00b7 Cited by 40436 \u2014 In this paper, we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation ...","attribution":"arXiv"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":0},{"turn_index":3,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search2\ue202turn0search12\ue201","start_idx":1237,"end_idx":1270,"safe_urls":["https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com","https://arxiv.org/pdf/1706.03762?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Attention Is All You Need","url":"https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com","pub_date":1497290254.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"arXiv:1706.03762v7 [cs.CL] 2 Aug 2023","url":"https://arxiv.org/pdf/1706.03762?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Vaswani \u00b7 2017 \u00b7 Cited by 194205 \u2014 We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions.","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search4\ue202turn0search14\ue201","start_idx":1409,"end_idx":1442,"safe_urls":["https://arxiv.org/abs/1801.06146?utm_source=chatgpt.com","https://arxiv.org/pdf/1801.06146?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1801.06146?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Universal Language Model Fine-tuning for Text Classification","url":"https://arxiv.org/abs/1801.06146?utm_source=chatgpt.com","pub_date":1516298092.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"arXiv:1801.06146v5 [cs.CL] 23 May 2018","url":"https://arxiv.org/pdf/1801.06146?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Howard \u00b7 2018 \u00b7 Cited by 5879 \u2014 We propose Universal Language Model. Fine-tuning (ULMFiT), an effective trans- fer learning method that can be applied to any task in NLP, and ...","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":4},{"turn_index":0,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search8\ue201","start_idx":1532,"end_idx":1564,"safe_urls":["https://aclanthology.org/N18-1202/?utm_source=chatgpt.com","https://arxiv.org/abs/1802.05365?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1802.05365?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Deep contextualized word representations","url":"https://arxiv.org/abs/1802.05365?utm_source=chatgpt.com","pub_date":1518653111.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Deep Contextualized Word Representations","url":"https://aclanthology.org/N18-1202/?utm_source=chatgpt.com","pub_date":null,"snippet":"by ME Peters \u00b7 2018 \u00b7 Cited by 17365 \u2014 We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (eg, syntax and semantics),","attribution":"ACL Anthology"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search0\ue202turn1search5\ue202turn4search4\ue202turn4search9\ue202turn5search3\ue202turn5search1\ue201","start_idx":1789,"end_idx":1873,"safe_urls":["https://aclanthology.org/N19-1423.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/1810.04805?utm_source=chatgpt.com","https://arxiv.org/abs/1906.08237?utm_source=chatgpt.com","https://arxiv.org/abs/1907.11692?utm_source=chatgpt.com","https://arxiv.org/abs/1909.11942?utm_source=chatgpt.com","https://arxiv.org/pdf/1907.11692?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1810.04805?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","url":"https://arxiv.org/abs/1810.04805?utm_source=chatgpt.com","pub_date":1539219001.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"BERT: Pre-training of Deep Bidirectional Transformers for ...","url":"https://aclanthology.org/N19-1423.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Devlin \u00b7 2019 \u00b7 Cited by 144191 \u2014 We introduce a new language representa- tion model called BERT, which stands for. Bidirectional Encoder Representations from. Transformers.","attribution":"ACL Anthology"},{"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","url":"https://arxiv.org/abs/1907.11692?utm_source=chatgpt.com","pub_date":null,"snippet":"by Y Liu \u00b7 2019 \u00b7 Cited by 21795 \u2014 We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.","attribution":"arXiv"},{"title":"arXiv:1907.11692v1 [cs.CL] 26 Jul 2019","url":"https://arxiv.org/pdf/1907.11692?utm_source=chatgpt.com","pub_date":null,"snippet":"by Y Liu \u00b7 2019 \u00b7 Cited by 21795 \u2014 RoBERTa matches the architecture and training objective of BERTLARGE . Results for BERTLARGE and XLNetLARGE are from. Devlin et al. (2019) and ...","attribution":"arXiv"},{"title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","url":"https://arxiv.org/abs/1909.11942?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"},{"title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","url":"https://arxiv.org/abs/1906.08237?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":0},{"turn_index":1,"ref_type":"search","ref_index":5},{"turn_index":4,"ref_type":"search","ref_index":4},{"turn_index":4,"ref_type":"search","ref_index":9},{"turn_index":5,"ref_type":"search","ref_index":3},{"turn_index":5,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn5search0\ue202turn5search4\ue201","start_idx":2005,"end_idx":2037,"safe_urls":["https://arxiv.org/abs/1910.10683?utm_source=chatgpt.com","https://jmlr.org/papers/volume21/20-074/20-074.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1910.10683?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","url":"https://arxiv.org/abs/1910.10683?utm_source=chatgpt.com","pub_date":1571852256.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Exploring the Limits of Transfer Learning with a Unified ...","url":"https://jmlr.org/papers/volume21/20-074/20-074.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by C Raffel \u00b7 2020 \u00b7 Cited by 26495 \u2014 Abstract. Transfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful ...","attribution":"Journal of Machine Learning Research"}],"refs":[{"turn_index":5,"ref_type":"search","ref_index":0},{"turn_index":5,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search1\ue202turn1search11\ue201","start_idx":2161,"end_idx":2194,"safe_urls":["https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf?utm_source=chatgpt.com","https://github.com/openai/gpt-2?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Language Models are Unsupervised Multitask Learners","url":"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf?utm_source=chatgpt.com","pub_date":1550186499.0,"snippet":"by A Radford \u00b7 Cited by 18390 \u2014 Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested lan- guage modeling datasets in a zero- ...","attribution_segments":null,"supporting_websites":[{"title":"openai/gpt-2: Code for the paper \"Language Models are ...","url":"https://github.com/openai/gpt-2?utm_source=chatgpt.com","pub_date":null,"snippet":"Code and models from the paper \"Language Models are Unsupervised Multitask Learners\". You can read about GPT-2 and its staged release in our original blog post.","attribution":"GitHub"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":1},{"turn_index":1,"ref_type":"search","ref_index":11}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search2\ue202turn1search7\ue202turn1search12\ue201","start_idx":2284,"end_idx":2330,"safe_urls":["https://arxiv.org/abs/2005.14165?utm_source=chatgpt.com","https://arxiv.org/pdf/2005.14165?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2005.14165?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"[2005.14165] Language Models are Few-Shot Learners","url":"https://arxiv.org/abs/2005.14165?utm_source=chatgpt.com","pub_date":null,"snippet":"by TB Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of- ...","attribution_segments":null,"supporting_websites":[{"title":"Language Models are Few-Shot Learners","url":"https://arxiv.org/pdf/2005.14165?utm_source=chatgpt.com","pub_date":null,"snippet":"by TB Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call. GPT-3, and measuring ...","attribution":"arXiv"},{"title":"Language Models are Few-Shot Learners","url":"https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by T Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of ...","attribution":"NeurIPS Proceedings"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":2},{"turn_index":1,"ref_type":"search","ref_index":7},{"turn_index":1,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search3\ue202turn1search8\ue202turn1search4\ue202turn1search9\ue202turn1search14\ue201","start_idx":2551,"end_idx":2623,"safe_urls":["https://arxiv.org/abs/2001.08361?utm_source=chatgpt.com","https://arxiv.org/abs/2203.15556?utm_source=chatgpt.com","https://arxiv.org/pdf/2001.08361?utm_source=chatgpt.com","https://arxiv.org/pdf/2203.15556?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2001.08361?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Scaling Laws for Neural Language Models","url":"https://arxiv.org/abs/2001.08361?utm_source=chatgpt.com","pub_date":1579751960.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Scaling Laws for Neural Language Models","url":"https://arxiv.org/pdf/2001.08361?utm_source=chatgpt.com","pub_date":null,"snippet":"Jan 23, 2020 \u2014 We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, ...","attribution":"arXiv"},{"title":"Training Compute-Optimal Large Language Models","url":"https://arxiv.org/abs/2203.15556?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"},{"title":"Training Compute-Optimal Large Language Models","url":"https://arxiv.org/pdf/2203.15556?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Hoffmann \u00b7 2022 \u00b7 Cited by 2759 \u2014 We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget.","attribution":"arXiv"},{"title":"Training Compute-Optimal Large Language Models","url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Hoffmann \u00b7 2022 \u00b7 Cited by 215 \u2014 We investigate the optimal model size and number of tokens for training a Trans- former language model under a given compute budget.","attribution":"NeurIPS Proceedings"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":3},{"turn_index":1,"ref_type":"search","ref_index":8},{"turn_index":1,"ref_type":"search","ref_index":4},{"turn_index":1,"ref_type":"search","ref_index":9},{"turn_index":1,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search0\ue202turn4search10\ue202turn4search1\ue202turn4search6\ue201","start_idx":2776,"end_idx":2835,"safe_urls":["https://arxiv.org/abs/2101.03961?utm_source=chatgpt.com","https://arxiv.org/abs/2112.06905?utm_source=chatgpt.com","https://arxiv.org/pdf/2112.06905?utm_source=chatgpt.com","https://jmlr.org/papers/volume23/21-0998/21-0998.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2101.03961?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity","url":"https://arxiv.org/abs/2101.03961?utm_source=chatgpt.com","pub_date":1610381512.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Switch Transformers: Scaling to Trillion Parameter Models ...","url":"https://jmlr.org/papers/volume23/21-0998/21-0998.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by W Fedus \u00b7 2022 \u00b7 Cited by 2959 \u2014 The Switch Transformer architecture, which simplifies and improves over Mixture of. Experts. \u2022 Scaling properties and a benchmark against the strongly tuned T5 ...","attribution":"Journal of Machine Learning Research"},{"title":"Efficient Scaling of Language Models with Mixture-of-Experts","url":"https://arxiv.org/abs/2112.06905?utm_source=chatgpt.com","pub_date":null,"snippet":"by N Du \u00b7 2021 \u00b7 Cited by 904 \u2014 In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts ...","attribution":"arXiv"},{"title":"Efficient Scaling of Language Models with Mixture-of-Experts","url":"https://arxiv.org/pdf/2112.06905?utm_source=chatgpt.com","pub_date":null,"snippet":"by N Du \u00b7 2021 \u00b7 Cited by 904 \u2014 We present a family of generalist lan- guage models called GLaM, that strike a balance between dense and conditional computation. The largest ...","attribution":"arXiv"}],"refs":[{"turn_index":4,"ref_type":"search","ref_index":0},{"turn_index":4,"ref_type":"search","ref_index":10},{"turn_index":4,"ref_type":"search","ref_index":1},{"turn_index":4,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search1\ue202turn2search6\ue201","start_idx":3001,"end_idx":3033,"safe_urls":["https://arxiv.org/abs/2210.11416?utm_source=chatgpt.com","https://jmlr.org/papers/volume25/23-0870/23-0870.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2210.11416?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Scaling Instruction-Finetuned Language Models","url":"https://arxiv.org/abs/2210.11416?utm_source=chatgpt.com","pub_date":null,"snippet":"by HW Chung \u00b7 2022 \u00b7 Cited by 4648 \u2014 In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on ...","attribution_segments":null,"supporting_websites":[{"title":"Scaling Instruction-Finetuned Language Models","url":"https://jmlr.org/papers/volume25/23-0870/23-0870.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by HW Chung \u00b7 2024 \u00b7 Cited by 4648 \u2014 We call this finetuning procedure Flan (Finetuning language models; Wei et al., 2021) and prepend \u201cFlan\u201d to the resulting finetuned models (e.g., Flan-. PaLM).2 ...","attribution":"Journal of Machine Learning Research"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":1},{"turn_index":2,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search0\ue202turn2search5\ue201","start_idx":3177,"end_idx":3209,"safe_urls":["https://arxiv.org/abs/2203.02155?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2203.02155?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Training language models to follow instructions with human feedback","url":"https://arxiv.org/abs/2203.02155?utm_source=chatgpt.com","pub_date":1646377482.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Training language models to follow instructions with ...","url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by L Ouyang \u00b7 2022 \u00b7 Cited by 17696 \u2014 In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set ...","attribution":"NeurIPS Proceedings"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":0},{"turn_index":2,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn5search2\ue202turn5search6\ue201","start_idx":3321,"end_idx":3353,"safe_urls":["https://arxiv.org/abs/2003.10555?utm_source=chatgpt.com","https://arxiv.org/pdf/2003.10555?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2003.10555?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","url":"https://arxiv.org/abs/2003.10555?utm_source=chatgpt.com","pub_date":1584998262.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"electra: pre-training text encoders","url":"https://arxiv.org/pdf/2003.10555?utm_source=chatgpt.com","pub_date":null,"snippet":"by K Clark \u00b7 2020 \u00b7 Cited by 5207 \u2014 We call our approach ELECTRA1 for \u201cEfficiently Learning an Encoder that Classifies Token Re- placements Accurately.\u201d As in prior work, we apply ...","attribution":"arXiv"}],"refs":[{"turn_index":5,"ref_type":"search","ref_index":2},{"turn_index":5,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search2\ue201","start_idx":3500,"end_idx":3519,"safe_urls":["https://arxiv.org/abs/2005.11401?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2005.11401?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Retrieval-Augmented Generation for Knowledge-Intensive ...","url":"https://arxiv.org/abs/2005.11401?utm_source=chatgpt.com","pub_date":null,"snippet":"by P Lewis \u00b7 2020 \u00b7 Cited by 11130 \u2014 We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn6search0\ue202turn6search3\ue202turn6search1\ue202turn6search4\ue202turn6search2\ue202turn6search5\ue201","start_idx":3745,"end_idx":3829,"safe_urls":["https://arxiv.org/abs/2210.03629?utm_source=chatgpt.com","https://arxiv.org/abs/2302.04761?utm_source=chatgpt.com","https://arxiv.org/pdf/2210.03629?utm_source=chatgpt.com","https://arxiv.org/pdf/2302.04761?utm_source=chatgpt.com","https://openai.com/index/function-calling-and-other-api-updates/?utm_source=chatgpt.com","https://platform.openai.com/docs/guides/function-calling?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2210.03629?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"ReAct: Synergizing Reasoning and Acting in Language Models","url":"https://arxiv.org/abs/2210.03629?utm_source=chatgpt.com","pub_date":1665018032.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"ReAct: Synergizing Reasoning and Acting in Language ...","url":"https://arxiv.org/pdf/2210.03629?utm_source=chatgpt.com","pub_date":null,"snippet":"Mar 10, 2023 \u2014 Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b. Siddharth Karamcheti, Megha ...","attribution":"arXiv"},{"title":"Toolformer: Language Models Can Teach Themselves to Use Tools","url":"https://arxiv.org/abs/2302.04761?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"},{"title":"arXiv:2302.04761v1 [cs.CL] 9 Feb 2023","url":"https://arxiv.org/pdf/2302.04761?utm_source=chatgpt.com","pub_date":null,"snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2259 \u2014 We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, ...","attribution":"arXiv"},{"title":"Function calling and other API updates","url":"https://openai.com/index/function-calling-and-other-api-updates/?utm_source=chatgpt.com","pub_date":null,"snippet":"Jun 13, 2023 \u2014 We're announcing updates including more steerable API models, function calling capabilities, longer context, and lower prices.","attribution":"OpenAI"},{"title":"OpenAI Function Calling Guide","url":"https://platform.openai.com/docs/guides/function-calling?utm_source=chatgpt.com","pub_date":null,"snippet":"Explore resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's developer platform.","attribution":"OpenAI Platform"}],"refs":[{"turn_index":6,"ref_type":"search","ref_index":0},{"turn_index":6,"ref_type":"search","ref_index":3},{"turn_index":6,"ref_type":"search","ref_index":1},{"turn_index":6,"ref_type":"search","ref_index":4},{"turn_index":6,"ref_type":"search","ref_index":2},{"turn_index":6,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search4\ue202turn2search19\ue201","start_idx":4042,"end_idx":4075,"safe_urls":["https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/?utm_source=chatgpt.com","https://arxiv.org/abs/2302.13971?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2302.13971?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"LLaMA: Open and Efficient Foundation Language Models","url":"https://arxiv.org/abs/2302.13971?utm_source=chatgpt.com","pub_date":1677517875.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"LLaMA: Open and Efficient Foundation Language Models","url":"https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/?utm_source=chatgpt.com","pub_date":null,"snippet":"We introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens.","attribution":"Meta AI"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":4},{"turn_index":2,"ref_type":"search","ref_index":19}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search3\ue202turn2search8\ue202turn4search3\ue202turn4search8\ue202turn4search7\ue201","start_idx":4293,"end_idx":4364,"safe_urls":["https://arxiv.org/abs/2106.09685?utm_source=chatgpt.com","https://arxiv.org/abs/2305.14314?utm_source=chatgpt.com","https://arxiv.org/abs/2403.14608?utm_source=chatgpt.com","https://arxiv.org/pdf/2106.09685?utm_source=chatgpt.com","https://arxiv.org/pdf/2305.14314?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2106.09685?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"LoRA: Low-Rank Adaptation of Large Language Models","url":"https://arxiv.org/abs/2106.09685?utm_source=chatgpt.com","pub_date":1623951438.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Lora: Low-rank adaptation of large lan","url":"https://arxiv.org/pdf/2106.09685?utm_source=chatgpt.com","pub_date":null,"snippet":"Oct 16, 2021 \u2014 We describe the simple design of LoRA and its practical benefits. The principles outlined here apply to any dense layers in deep learning models ...","attribution":"arXiv"},{"title":"[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs","url":"https://arxiv.org/abs/2305.14314?utm_source=chatgpt.com","pub_date":null,"snippet":"by T Dettmers \u00b7 2023 \u00b7 Cited by 3863 \u2014 We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU.","attribution":"arXiv"},{"title":"QLORA: Efficient Finetuning of Quantized LLMs","url":"https://arxiv.org/pdf/2305.14314?utm_source=chatgpt.com","pub_date":null,"snippet":"May 23, 2023 \u2014 We present QLORA, an efficient finetuning approach that reduces memory us- age enough to finetune a 65B parameter model on a single 48GB GPU ...","attribution":"arXiv"},{"title":"Parameter-Efficient Fine-Tuning for Large Models","url":"https://arxiv.org/abs/2403.14608?utm_source=chatgpt.com","pub_date":null,"snippet":"by Z Han \u00b7 2024 \u00b7 Cited by 663 \u2014 In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead.","attribution":"arXiv"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":3},{"turn_index":2,"ref_type":"search","ref_index":8},{"turn_index":4,"ref_type":"search","ref_index":3},{"turn_index":4,"ref_type":"search","ref_index":8},{"turn_index":4,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search2\ue202turn4search12\ue202turn4search17\ue201","start_idx":4484,"end_idx":4531,"safe_urls":["https://arxiv.org/abs/2504.13822?utm_source=chatgpt.com","https://arxiv.org/html/2504.13822v2?utm_source=chatgpt.com","https://www.arxiv.org/pdf/2504.13822?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://www.arxiv.org/pdf/2504.13822?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Parameter-Efficient Continual Fine-Tuning: A Survey","url":"https://www.arxiv.org/pdf/2504.13822?utm_source=chatgpt.com","pub_date":null,"snippet":"by EN Coleman \u00b7 2025 \u2014 CL focuses on enabling models to learn from sequential tasks without forgetting previous knowledge, addressing the stability-plasticity dilemma ...","attribution_segments":null,"supporting_websites":[{"title":"Parameter-Efficient Continual Fine-Tuning: A Survey","url":"https://arxiv.org/abs/2504.13822?utm_source=chatgpt.com","pub_date":null,"snippet":"by EN Coleman \u00b7 2025 \u2014 In this survey, we first provide an overview of CL algorithms and PEFT methods before reviewing the state-of-the-art on Parameter-Efficient Continual Fine- ...","attribution":"arXiv"},{"title":"Parameter-Efficient Continual Fine-Tuning: A Survey","url":"https://arxiv.org/html/2504.13822v2?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 19, 2025 \u2014 This survey bridges the gap between CL and PEFT by focusing on the emerging field of Parameter-Efficient Continual Fine-Tuning (PECFT). We first ...","attribution":"arXiv"}],"refs":[{"turn_index":4,"ref_type":"search","ref_index":2},{"turn_index":4,"ref_type":"search","ref_index":12},{"turn_index":4,"ref_type":"search","ref_index":17}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":" ","start_idx":5589,"end_idx":5589,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"Distributed Representations of Words and Phrases ...","url":"https://arxiv.org/abs/1310.4546?utm_source=chatgpt.com","attribution":"arXiv"},{"title":"GloVe: Global Vectors for Word Representation","url":"https://aclanthology.org/D14-1162/?utm_source=chatgpt.com","attribution":"ACL Anthology"},{"title":"Language Models are Unsupervised Multitask Learners","url":"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf?utm_source=chatgpt.com","attribution":"OpenAI"}],"has_images":false}],"safe_urls":["https://arxiv.org/abs/1310.4546?utm_source=chatgpt.com","https://arxiv.org/pdf/1310.4546?utm_source=chatgpt.com","https://arxiv.org/pdf/1402.3722?utm_source=chatgpt.com","https://aclanthology.org/D14-1162/?utm_source=chatgpt.com","https://www-nlp.stanford.edu/pubs/glove.pdf?utm_source=chatgpt.com","https://aclanthology.org/Q17-1010.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/1607.04606?utm_source=chatgpt.com","https://ar5iv.labs.arxiv.org/html/1808.06226?utm_source=chatgpt.com","https://arxiv.org/abs/1508.07909?utm_source=chatgpt.com","https://arxiv.org/abs/1808.06226?utm_source=chatgpt.com","https://arxiv.org/pdf/1508.07909?utm_source=chatgpt.com","https://arxiv.org/abs/1409.3215?utm_source=chatgpt.com","https://arxiv.org/pdf/1409.3215?utm_source=chatgpt.com","https://arxiv.org/abs/1409.0473?utm_source=chatgpt.com","https://arxiv.org/pdf/1409.0473?utm_source=chatgpt.com","https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com","https://arxiv.org/pdf/1706.03762?utm_source=chatgpt.com","https://arxiv.org/abs/1801.06146?utm_source=chatgpt.com","https://arxiv.org/pdf/1801.06146?utm_source=chatgpt.com","https://aclanthology.org/N18-1202/?utm_source=chatgpt.com","https://arxiv.org/abs/1802.05365?utm_source=chatgpt.com","https://aclanthology.org/N19-1423.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/1810.04805?utm_source=chatgpt.com","https://arxiv.org/abs/1906.08237?utm_source=chatgpt.com","https://arxiv.org/abs/1907.11692?utm_source=chatgpt.com","https://arxiv.org/abs/1909.11942?utm_source=chatgpt.com","https://arxiv.org/pdf/1907.11692?utm_source=chatgpt.com","https://arxiv.org/abs/1910.10683?utm_source=chatgpt.com","https://jmlr.org/papers/volume21/20-074/20-074.pdf?utm_source=chatgpt.com","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf?utm_source=chatgpt.com","https://github.com/openai/gpt-2?utm_source=chatgpt.com","https://arxiv.org/abs/2005.14165?utm_source=chatgpt.com","https://arxiv.org/pdf/2005.14165?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2001.08361?utm_source=chatgpt.com","https://arxiv.org/abs/2203.15556?utm_source=chatgpt.com","https://arxiv.org/pdf/2001.08361?utm_source=chatgpt.com","https://arxiv.org/pdf/2203.15556?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2101.03961?utm_source=chatgpt.com","https://arxiv.org/abs/2112.06905?utm_source=chatgpt.com","https://arxiv.org/pdf/2112.06905?utm_source=chatgpt.com","https://jmlr.org/papers/volume23/21-0998/21-0998.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2210.11416?utm_source=chatgpt.com","https://jmlr.org/papers/volume25/23-0870/23-0870.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2203.02155?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2003.10555?utm_source=chatgpt.com","https://arxiv.org/pdf/2003.10555?utm_source=chatgpt.com","https://arxiv.org/abs/2005.11401?utm_source=chatgpt.com","https://arxiv.org/abs/2210.03629?utm_source=chatgpt.com","https://arxiv.org/abs/2302.04761?utm_source=chatgpt.com","https://arxiv.org/pdf/2210.03629?utm_source=chatgpt.com","https://arxiv.org/pdf/2302.04761?utm_source=chatgpt.com","https://openai.com/index/function-calling-and-other-api-updates/?utm_source=chatgpt.com","https://platform.openai.com/docs/guides/function-calling?utm_source=chatgpt.com","https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/?utm_source=chatgpt.com","https://arxiv.org/abs/2302.13971?utm_source=chatgpt.com","https://arxiv.org/abs/2106.09685?utm_source=chatgpt.com","https://arxiv.org/abs/2305.14314?utm_source=chatgpt.com","https://arxiv.org/abs/2403.14608?utm_source=chatgpt.com","https://arxiv.org/pdf/2106.09685?utm_source=chatgpt.com","https://arxiv.org/pdf/2305.14314?utm_source=chatgpt.com","https://arxiv.org/abs/2504.13822?utm_source=chatgpt.com","https://arxiv.org/html/2504.13822v2?utm_source=chatgpt.com","https://www.arxiv.org/pdf/2504.13822?utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/1310.4546?utm_source=chatgpt.com","title":"Distributed Representations of Words and Phrases and ...","snippet":"by T Mikolov \u00b7 2013 \u00b7 Cited by 48431 \u2014 In this paper we present several extensions of the original Skip-gram model. We show that sub- sampling of frequent words during training ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/1706.03762v7?utm_source=chatgpt.com","title":"Attention Is All You Need","snippet":"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1402.3722?utm_source=chatgpt.com","title":"Deriving Mikolov et al.'s Negative-Sampling Word- ...","snippet":"by Y Goldberg \u00b7 2014 \u00b7 Cited by 2573 \u2014 While negative-sampling is based on the skip-gram model, it is in fact optimizing a different objective. What follows is the derivation of the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1706.03762?utm_source=chatgpt.com","title":"arXiv:1706.03762v7 [cs.CL] 2 Aug 2023","snippet":"by A Vaswani \u00b7 2017 \u00b7 Cited by 194205 \u2014 We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1801.06146?utm_source=chatgpt.com","title":"arXiv:1801.06146v5 [cs.CL] 23 May 2018","snippet":"by J Howard \u00b7 2018 \u00b7 Cited by 5879 \u2014 We propose Universal Language Model. Fine-tuning (ULMFiT), an effective trans- fer learning method that can be applied to any task in NLP, and ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1705.09755?utm_source=chatgpt.com","title":"word2vec Skip-Gram with Negative Sampling is a ...","snippet":"by AJ Landgraf \u00b7 2017 \u00b7 Cited by 28 \u2014 Mikolov et al. (2013) introduced the skip-gram formulation for neural word em- beddings, wherein one tries to predict the context of a given ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1301.3781?utm_source=chatgpt.com","title":"Efficient Estimation of Word Representations in Vector Space","snippet":"by T Mikolov \u00b7 2013 \u00b7 Cited by 49374 \u2014 Abstract:We propose two novel model architectures for computing continuous vector representations of words from very large data sets.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1809.09795?utm_source=chatgpt.com","title":"Deep contextualized word representations for detecting ...","snippet":"by S Ili\u0107 \u00b7 2018 \u00b7 Cited by 149 \u2014 We propose a model that uses character-level vector representations of words, based on ELMo. We test our model on 7 different datasets derived from 3 different ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2009.04413?utm_source=chatgpt.com","title":"On SkipGram Word Embedding Models with Negative ...","snippet":"by Z Wang \u00b7 2020 \u00b7 Cited by 1 \u2014 In this paper, we formulate a framework for word embedding, referred to as Word-Context. Classification (WCC), that generalizes SGN to a wide ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1411.5595?utm_source=chatgpt.com","title":"Linking GloVe with word2vec","snippet":"by T Shi \u00b7 2014 \u00b7 Cited by 58 \u2014 [3] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Conference on Empirical ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/1802.05365?utm_source=chatgpt.com","title":"[1802.05365] Deep contextualized word representations - ar5iv","snippet":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1402.3722?utm_source=chatgpt.com","title":"[1402.3722] word2vec Explained: deriving Mikolov et al.'s ...","snippet":"by Y Goldberg \u00b7 2014 \u00b7 Cited by 2573 \u2014 This note is an attempt to explain equation (4) (negative sampling) in \"Distributed Representations of Words and Phrases and their Compositionality\"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2203.15556?utm_source=chatgpt.com","title":"Training Compute-Optimal Large Language Models","snippet":"by J Hoffmann \u00b7 2022 \u00b7 Cited by 2759 \u2014 We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2005.14165?utm_source=chatgpt.com","title":"Language Models are Few-Shot Learners","snippet":"by TB Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call. GPT-3, and measuring ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2001.08361?utm_source=chatgpt.com","title":"Scaling Laws for Neural Language Models","snippet":"Jan 23, 2020 \u2014 We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2203.15556?utm_source=chatgpt.com","title":"Training Compute-Optimal Large Language Models","snippet":"by J Hoffmann \u00b7 2022 \u00b7 Cited by 2759 \u2014 We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1908.10084?utm_source=chatgpt.com","title":"Sentence Embeddings using Siamese BERT-Networks","snippet":"by N Reimers \u00b7 2019 \u00b7 Cited by 18834 \u2014 BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1904.02679?utm_source=chatgpt.com","title":"arXiv:1904.02679v2 [cs.HC] 11 Apr 2019","snippet":"by J Vig \u00b7 2019 \u00b7 Cited by 107 \u2014 Below, we describe these views and demonstrate them on the OpenAI GPT-2 and BERT models. ... Language models are unsupervised multitask learners.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.12051v2?utm_source=chatgpt.com","title":"How to Upscale Neural Networks with Scaling Law? A ...","snippet":"May 24, 2025 \u2014 Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2209.14500?utm_source=chatgpt.com","title":"Bidirectional Language Models Are Also Few-shot Learners","snippet":"by A Patel \u00b7 2022 \u00b7 Cited by 74 \u2014 Our results demonstrate prompt-based learning is an emergent property of a broader class of language models, rather than only unidirectional models.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://www.arxiv.org/pdf/2406.18922?utm_source=chatgpt.com","title":"Time Matters: Scaling Laws for Any Budget","snippet":"by I Inbar \u00b7 2024 \u00b7 Cited by 2 \u2014 The original scaling law from Hoffmann et al. [2], predicts the final training loss of a language model in terms of its parameters count N and ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1902.04094?utm_source=chatgpt.com","title":"[1902.04094] BERT has a Mouth, and It Must Speak","snippet":"by A Wang \u00b7 2019 \u00b7 Cited by 445 \u2014 We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.13193v1?utm_source=chatgpt.com","title":"Retrieval-Augmented Generation for Natural Language ...","snippet":"Jul 18, 2024 \u2014 This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2106.09685?utm_source=chatgpt.com","title":"Lora: Low-rank adaptation of large lan","snippet":"Oct 16, 2021 \u2014 We describe the simple design of LoRA and its practical benefits. The principles outlined here apply to any dense layers in deep learning models ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2406.13249?utm_source=chatgpt.com","title":"arXiv:2406.13249v2 [cs.CL] 30 Oct 2024","snippet":"by F Ye \u00b7 2024 \u00b7 Cited by 11 \u2014 Retrieval augmented generation (RAG) (Lewis et al., 2020) significantly enhances the capabilities of large language models (LLMs) by ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2402.12354?utm_source=chatgpt.com","title":"LoRA+: Efficient Low Rank Adaptation of Large Models","snippet":"by S Hayou \u00b7 2024 \u00b7 Cited by 263 \u2014 In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.00054v1?utm_source=chatgpt.com","title":"Retrieval-Augmented Generation: A Comprehensive ...","snippet":"This survey provides a comprehensive synthesis of recent advances in RAG systems, offering a taxonomy that categorizes architectures into retriever-centric, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2307.09288?utm_source=chatgpt.com","title":"Llama 2: Open Foundation and Fine-Tuned Chat Models","snippet":"by H Touvron \u00b7 2023 \u00b7 Cited by 18047 \u2014 We develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2301.13688?utm_source=chatgpt.com","title":"The Flan Collection: Designing Data and Methods for ...","snippet":"by S Longpre \u00b7 2023 \u00b7 Cited by 912 \u2014 We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 models (Chung et ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2412.01572?utm_source=chatgpt.com","title":"arXiv:2412.01572v4 [cs.AI] 1 Jan 2025","snippet":"by X Tang \u00b7 2024 \u00b7 Cited by 13 \u2014 Retrieval Augmented Generation (Lewis et al.,. 2020) has shown significant promise in addressing knowledge-intensive natural language ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2106.09685?utm_source=chatgpt.com","title":"LoRA: Low-Rank Adaptation of Large Language Models - ar5iv","snippet":"Using language modeling as an example, there are two prominent strategies when it comes to efficient adaptations: adding adapter layers (Houlsby et al., 2019; ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.05677v3?utm_source=chatgpt.com","title":"Batched Low-Rank Adaptation of Foundation Models","snippet":"Low-Rank Adaptation (LoRA) emerged as a solution to this challenge by incorporating trainable low-rank matrices (Hu et al., 2021) which significantly reduces ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":48},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1808.06226?utm_source=chatgpt.com","title":"[1808.06226] SentencePiece: A simple and language ...","snippet":"by T Kudo \u00b7 2018 \u00b7 Cited by 4554 \u2014 This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1409.0473?utm_source=chatgpt.com","title":"Neural machine translation by","snippet":"by D Bahdanau \u00b7 2014 \u00b7 Cited by 40436 \u2014 In this paper, we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1409.3215?utm_source=chatgpt.com","title":"Sequence to Sequence Learning with Neural Networks","snippet":"by I Sutskever \u00b7 2014 \u00b7 Cited by 30012 \u2014 The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed- dimensional vector representation, and ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1508.07909?utm_source=chatgpt.com","title":"arXiv:1508.07909v5 [cs.CL] 10 Jun 2016","snippet":"by R Sennrich \u00b7 2015 \u00b7 Cited by 10196 \u2014 We introduce a variant of byte pair encoding for word segmentation, which is capable of encod- ing open vocabularies with a compact symbol vo-.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/1808.06226?utm_source=chatgpt.com","title":"SentencePiece: A simple and language independent subword ...","snippet":"Abstract. This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1909.03341?utm_source=chatgpt.com","title":"Neural Machine Translation with Byte-Level Subwords","snippet":"by C Wang \u00b7 2019 \u00b7 Cited by 274 \u2014 Our byte- level subword vocabularies are based on byte-pair encoding, while we use bytes as the basic units to compose subwords. Character ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1810.10222?utm_source=chatgpt.com","title":"arXiv:1810.10222v1 [cs.CL] 24 Oct 2018","snippet":"by P Czapla \u00b7 2018 \u00b7 Cited by 7 \u2014 An important property of SentencePiece tokenization, necessary for us to obtain a valid word-based lan- guage model, is its reversibility. We do ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1410.4615?utm_source=chatgpt.com","title":"[1410.4615] Learning to Execute","snippet":"by W Zaremba \u00b7 2014 \u00b7 Cited by 696 \u2014 Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2411.08671v1?utm_source=chatgpt.com","title":"Theoretical Analysis of Byte-Pair Encoding\u2217Supported by ...","snippet":"13 Nov 2024 \u2014 In this paper we focus on the optimization problem underlying BPE: finding a pair encoding that achieves optimal compression utility.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/1409.0473?utm_source=chatgpt.com","title":"Neural Machine Translation by Jointly Learning to Align and ...","snippet":"In order to address this issue, we introduce an extension to the encoder\u2013decoder model which learns to align and translate jointly. Each time the proposed model ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2306.16837v3?utm_source=chatgpt.com","title":"A Formal Perspective on Byte-Pair Encoding","snippet":"Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1608.01281?utm_source=chatgpt.com","title":"arXiv:1608.01281v1 [cs.LG] 3 Aug 2016","snippet":"by Y Luo \u00b7 2016 \u00b7 Cited by 53 \u2014 Sequence to Sequence Learning with Neural. Networks. In Neural Information Processing Systems, 2014. [21] Oriol Vinyals, Alexander Toshev ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2402.01035v1?utm_source=chatgpt.com","title":"Getting the most out of your tokenizer for pre-training and ...","snippet":"Feb 1, 2024 \u2014 We considered two popular libraries implementing BPE training, Google's Sentencepiece (Kudo & Richardson, 2018) and HuggingFace tokenizers (Wolf ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2404.17808v2?utm_source=chatgpt.com","title":"Scaffold-BPE: Enhancing Byte Pair Encoding for Large ...","snippet":"BPE (Sennrich et al. 2015) effectively addresses that by generating a subword vocabulary. Initially, a corpus is split into characters or bytes, which act as ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.12560v1?utm_source=chatgpt.com","title":"How does a Language-Specific Tokenizer affect LLMs?","snippet":"Feb 18, 2025 \u2014 It uses the Llama-2 tokenizer which is based on SentencePiece BPE, combining SentencePiece (Kudo and Richardson, 2018) with the BPE algorithm.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1802.06893?utm_source=chatgpt.com","title":"arXiv:1802.06893v2 [cs.CL] 28 Mar 2018","snippet":"by E Grave \u00b7 2018 \u00b7 Cited by 2091 \u2014 The first model that we consider is the skip- gram model with subword information, introduced by. Bojanowski et al. (2017). This model ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2208.05225?utm_source=chatgpt.com","title":"How Effective is Byte Pair Encoding for Out-Of-Vocabulary ...","snippet":"by A Araabi \u00b7 2022 \u00b7 Cited by 23 \u2014 Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2112.06905?utm_source=chatgpt.com","title":"Efficient Scaling of Language Models with Mixture-of-Experts","snippet":"by N Du \u00b7 2021 \u00b7 Cited by 904 \u2014 In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2305.14314?utm_source=chatgpt.com","title":"[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs","snippet":"by T Dettmers \u00b7 2023 \u00b7 Cited by 3863 \u2014 We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1907.11692?utm_source=chatgpt.com","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","snippet":"by Y Liu \u00b7 2019 \u00b7 Cited by 21795 \u2014 We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2112.06905?utm_source=chatgpt.com","title":"Efficient Scaling of Language Models with Mixture-of-Experts","snippet":"by N Du \u00b7 2021 \u00b7 Cited by 904 \u2014 We present a family of generalist lan- guage models called GLaM, that strike a balance between dense and conditional computation. The largest ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2403.14608?utm_source=chatgpt.com","title":"Parameter-Efficient Fine-Tuning for Large Models","snippet":"by Z Han \u00b7 2024 \u00b7 Cited by 663 \u2014 In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2305.14314?utm_source=chatgpt.com","title":"QLORA: Efficient Finetuning of Quantized LLMs","snippet":"May 23, 2023 \u2014 We present QLORA, an efficient finetuning approach that reduces memory us- age enough to finetune a 65B parameter model on a single 48GB GPU ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1907.11692?utm_source=chatgpt.com","title":"arXiv:1907.11692v1 [cs.CL] 26 Jul 2019","snippet":"by Y Liu \u00b7 2019 \u00b7 Cited by 21795 \u2014 RoBERTa matches the architecture and training objective of BERTLARGE . Results for BERTLARGE and XLNetLARGE are from. Devlin et al. (2019) and ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2112.06905?utm_source=chatgpt.com","title":"Efficient Scaling of Language Models with Mixture-of-Experts","snippet":"We present a family of generalist language models called GLaM, that strike a balance between dense and conditional computation. The largest version of GLaM has ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2504.13822?utm_source=chatgpt.com","title":"Parameter-Efficient Continual Fine-Tuning: A Survey","snippet":"by EN Coleman \u00b7 2025 \u2014 In this survey, we first provide an overview of CL algorithms and PEFT methods before reviewing the state-of-the-art on Parameter-Efficient Continual Fine- ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2305.14314?utm_source=chatgpt.com","title":"QLoRA: Efficient Finetuning of Quantized LLMs - ar5iv - arXiv","snippet":"Abstract. We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2402.06196v2?utm_source=chatgpt.com","title":"Large Language Models: A Survey","snippet":"Feb 20, 2024 \u2014 GLaM: In [84] , Du et al. proposed a family of LLMs named GLaM (Generalist Language Model), which use a sparsely activated mixture-of ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2504.13822v2?utm_source=chatgpt.com","title":"Parameter-Efficient Continual Fine-Tuning: A Survey","snippet":"Aug 19, 2025 \u2014 This survey bridges the gap between CL and PEFT by focusing on the emerging field of Parameter-Efficient Continual Fine-Tuning (PECFT). We first ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.07987v3?utm_source=chatgpt.com","title":"Accelerating Transformers with Mixture-of-Experts Attention","snippet":"Sep 30, 2024 \u2014 Fedus et al. [37] introduce Mixture of Experts in Transformers ... Switch transformers: Scaling to trillion parameter models with simple and ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2307.06435v9?utm_source=chatgpt.com","title":"A Comprehensive Overview of Large Language Models","snippet":"GLaM [91] : Generalist Language Model (GLaM) represents a family of ... Du, et al., Lamda: Language models for dialog applications, arXiv preprint ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2410.19878?utm_source=chatgpt.com","title":"Parameter-Efficient Fine-Tuning in Large Models","snippet":"by L Wang \u00b7 2024 \u00b7 Cited by 21 \u2014 This survey aims to comprehensively review the recent advancements in large model fine-tuning techniques. By con- ducting a thorough examination of existing ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1910.13461?utm_source=chatgpt.com","title":"BART: Denoising Sequence-to-Sequence Pre-training for ...","snippet":"by M Lewis \u00b7 2019 \u00b7 Cited by 14077 \u2014 We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2103.10360?utm_source=chatgpt.com","title":"[2103.10360] GLM: General Language Model Pretraining ...","snippet":"by Z Du \u00b7 2021 \u00b7 Cited by 1886 \u2014 GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2504.13822v1?utm_source=chatgpt.com","title":"Parameter-Efficient Continual Fine-Tuning: A Survey","snippet":"Apr 18, 2025 \u2014 In this survey, we first provide an overview of CL algorithms and PEFT methods before reviewing the state-of-the-art on Parameter-Efficient ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/1907.11692?utm_source=chatgpt.com","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","snippet":"We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2412.12953v1?utm_source=chatgpt.com","title":"Efficient Diffusion Transformer Policies with Mixture of ...","snippet":"Dec 17, 2024 \u2014 MoDE surpasses current state-of-the-art Transformer-based Diffusion Policies while enabling parameter-efficient scaling through sparse experts and noise- ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2402.02242v4?utm_source=chatgpt.com","title":"Parameter-Efficient Fine-Tuning for Pre-Trained Vision ...","snippet":"Mar 26, 2025 \u2014 This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2311.12023v4?utm_source=chatgpt.com","title":"LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition ...","snippet":"Based on this quantization scheme, Dettmers et al. (2023a) propose QLoRA, which performs NF quantization to 4 bits on the pretrained LLM, and learns low-rank ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2109.12788?utm_source=chatgpt.com","title":"arXiv:2109.12788v1 [cs.CL] 27 Sep 2021","snippet":"by Z Huang \u00b7 2021 \u00b7 Cited by 1 \u2014 The RoBERTa paper (Liu et al., 2019a) proposed the full-sentences setup to prepare the pre-training dataset. Specifically, each input is packed ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2410.05648v1?utm_source=chatgpt.com","title":"Does RoBERTa Perform Better than BERT in Continual ...","snippet":"Oct 8, 2024 \u2014 RoBERTa achieves generally better downstream performance than BERT, in part because it is pre-trained on more diverse data.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":49},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1906.08237?utm_source=chatgpt.com","title":"XLNet: Generalized Autoregressive Pretraining for ...","snippet":"by Z Yang \u00b7 2019 \u00b7 Cited by 11779 \u2014 We propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1909.11942?utm_source=chatgpt.com","title":"ALBERT: A Lite BERT for Self-supervised Learning of ...","snippet":"by Z Lan \u00b7 2019 \u00b7 Cited by 9382 \u2014 We present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2003.10555?utm_source=chatgpt.com","title":"electra: pre-training text encoders","snippet":"by K Clark \u00b7 2020 \u00b7 Cited by 5207 \u2014 We call our approach ELECTRA1 for \u201cEfficiently Learning an Encoder that Classifies Token Re- placements Accurately.\u201d As in prior work, we apply ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2403.18338v1?utm_source=chatgpt.com","title":"Is a Compact Multilingual BERT Model Still Worth It?","snippet":"As far as we know, compact models, such as the ALBERT model Lan et al. (2019) , are a possible answer since they have been evaluated on the comprehension tasks ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1906.08237?utm_source=chatgpt.com","title":"XLNet: Generalized Autoregressive Pretraining for ...","snippet":"by Z Yang \u00b7 2019 \u00b7 Cited by 11779 \u2014 In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2104.02756?utm_source=chatgpt.com","title":"Efficient transfer learning for NLP with ELECTRA","snippet":"by F Mercier \u00b7 2021 \u00b7 Cited by 1 \u2014 Abstract:Clark et al. [2020] claims that the ELECTRA approach is highly efficient in NLP performances relative to computation budget.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1909.11942?utm_source=chatgpt.com","title":"arXiv:1909.11942v6 [cs.CL] 9 Feb 2020","snippet":"by Z Lan \u00b7 2019 \u00b7 Cited by 9382 \u2014 In this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2112.07869?utm_source=chatgpt.com","title":"arXiv:2112.07869v1 [cs.CL] 15 Dec 2021","snippet":"by R Tinn \u00b7 2021 \u00b7 Cited by 180 \u2014 ELECTRA (Clark et al., 2020), which has shown good performance in general-domain datasets such as GLUE (Wang et al., 2019a) and. SQuAD ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2005.13619?utm_source=chatgpt.com","title":"Language Representation Models for Fine-Grained ...","snippet":"by B Cheang \u00b7 2020 \u00b7 Cited by 11 \u2014 ALBERT, which stands for \u201cA Lite BERT\u201d, was made available in an open source version by Google in 2019, developed by Lan et al. The model was ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2110.06620?utm_source=chatgpt.com","title":"Maximizing Efficiency of Language Model Pre-training for ...","snippet":"by J Kang \u00b7 2021 \u2014 ELECTRA (Clark et al., 2020) proposes replaced token detection (RTD), which is a token-level pre-training task that jointly trains the ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2210.10289?utm_source=chatgpt.com","title":"arXiv:2210.10289v1 [cs.CL] 19 Oct 2022","snippet":"by H Zhang \u00b7 2022 \u00b7 Cited by 1 \u2014 Zhenzhong Lan, Mingda Chen, Sebastian Goodman,. Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2104.02756?utm_source=chatgpt.com","title":"Efficient transfer learning for NLP with ELECTRA","snippet":"by F Mercier \u00b7 2021 \u00b7 Cited by 1 \u2014 This replication study has been conducted by fully reimplementing the small variant of the original ELECTRA model. (Clark et al. [2020]). All ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2301.04347?utm_source=chatgpt.com","title":"Testing Stereotypical Representation in Pre-trained ...","snippet":"by D Zhang \u00b7 2023 \u00b7 Cited by 1 \u2014 We apply our tests to four different types of pre-trained lan- guage models. Except for ALBERT (Lan et al. 2019), each type of model ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2402.13130v3?utm_source=chatgpt.com","title":"Are ELECTRA's Sentence Embeddings Beyond Repair ...","snippet":"To address these issues, Clark et al. (2020) introduce ELECTRA, which substitutes MLM with replaced token detection (RTD), achieving the same results as BERT ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2101.11363?utm_source=chatgpt.com","title":"KoreALBERT: Pretraining a Lite BERT Model for Korean ...","snippet":"by H Lee \u00b7 2021 \u00b7 Cited by 22 \u2014 Abstract\u2014A Lite BERT (ALBERT) has been introduced to scale up deep bidirectional representation learning for natural languages.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2308.06828?utm_source=chatgpt.com","title":"Integrating Electra Transformer, GloVe, and LSTM","snippet":"by S Aburass \u00b7 2023 \u00b7 Cited by 9 \u2014 RoBERTa and ELECTRA, developed by Liu et al. and Clark et al. respectively, further pushed the envelope in terms of performance [15], [16]. While individual ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2404.16574v1?utm_source=chatgpt.com","title":"Exploring Internal Numeracy in Language Models","snippet":"Apr 25, 2024 \u2014 Transformer-based models of language like BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) , ALBERT (Lan et al., 2020) , and ELECTRA ( ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2310.07347?utm_source=chatgpt.com","title":"Fast-ELECTRA for Efficient Pre-training","snippet":"by C Dong \u00b7 2023 \u00b7 Cited by 1 \u2014 ELECTRA (Clark et al., 2020) is a pre-training method that trains the models to predict whether each token is the original token or ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2203.16747?utm_source=chatgpt.com","title":"arXiv:2203.16747v1 [cs.CL] 31 Mar 2022","snippet":"by S Li \u00b7 2022 \u00b7 Cited by 62 \u2014 The PLMs we analyzed include BERT (Devlin et al., 2019),. RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2020), and ALBERT (Lan et al., ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2302.04761?utm_source=chatgpt.com","title":"Language Models Can Teach Themselves to Use Tools","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2259 \u2014 We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2210.03629?utm_source=chatgpt.com","title":"ReAct: Synergizing Reasoning and Acting in Language ...","snippet":"Mar 10, 2023 \u2014 Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b. Siddharth Karamcheti, Megha ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2302.04761?utm_source=chatgpt.com","title":"arXiv:2302.04761v1 [cs.CL] 9 Feb 2023","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2259 \u2014 We have introduced Toolformer, a language model that learns in a self-supervised way how to use different tools such as search engines, ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2412.01262v1?utm_source=chatgpt.com","title":"Do Large Language Models with Reasoning and Acting ...","snippet":"Dec 2, 2024 \u2014 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Zhang et al. (2023) Xiaoying Zhang ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/better-language-models/?utm_source=chatgpt.com","title":"Better language models and their implications","snippet":"Feb 14, 2019 \u2014 We've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf?utm_source=chatgpt.com","title":"Training language models to follow instructions with ...","snippet":"by L Ouyang \u00b7 Cited by 17696 \u2014 In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/instruction-following/?utm_source=chatgpt.com","title":"Aligning language models to follow instructions","snippet":"Jan 27, 2022 \u2014 We've trained language models that are much better at following user intentions than GPT-3 while also making them more truthful and less toxic.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/function-calling-and-other-api-updates/?utm_source=chatgpt.com","title":"Function calling and other API updates","snippet":"Jun 13, 2023 \u2014 We're announcing updates including more steerable API models, function calling capabilities, longer context, and lower prices.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://platform.openai.com/docs/guides/function-calling?utm_source=chatgpt.com","title":"OpenAI Function Calling Guide","snippet":"Explore resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's developer platform.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://community.openai.com/t/did-openai-just-break-function-calling/473705?utm_source=chatgpt.com","title":"Did openAI just break function calling? - API","snippet":"Nov 6, 2023 \u2014 Our app is currently experiencing issues with function calling since the results from the API call returns only strings.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"jmlr.org","entries":[{"type":"search_result","url":"https://jmlr.org/papers/volume25/23-0870/23-0870.pdf?utm_source=chatgpt.com","title":"Scaling Instruction-Finetuned Language Models","snippet":"by HW Chung \u00b7 2024 \u00b7 Cited by 4648 \u2014 We call this finetuning procedure Flan (Finetuning language models; Wei et al., 2021) and prepend \u201cFlan\u201d to the resulting finetuned models (e.g., Flan-. PaLM).2 .....","ref_id":{"turn_index":2,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"jmlr.org"},{"type":"search_result","url":"https://jmlr.org/papers/volume23/21-0998/21-0998.pdf?utm_source=chatgpt.com","title":"Switch Transformers: Scaling to Trillion Parameter Models ...","snippet":"by W Fedus \u00b7 2022 \u00b7 Cited by 2959 \u2014 The Switch Transformer architecture, which simplifies and improves over Mixture of. Experts. \u2022 Scaling properties and a benchmark against the strongly tuned T5 ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"jmlr.org"},{"type":"search_result","url":"https://jmlr.org/papers/volume21/20-074/20-074.pdf?utm_source=chatgpt.com","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"by C Raffel \u00b7 2020 \u00b7 Cited by 26495 \u2014 Abstract. Transfer learning, where a model is first pre-trained on a data-rich task before being fine- tuned on a downstream task, has emerged as a powerful ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"jmlr.org"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/N18-1202/?utm_source=chatgpt.com","title":"Deep Contextualized Word Representations","snippet":"by ME Peters \u00b7 2018 \u00b7 Cited by 17365 \u2014 We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (eg, syntax and semantics),","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/P18-1031/?utm_source=chatgpt.com","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"by J Howard \u00b7 2018 \u00b7 Cited by 5879 \u2014 We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/N19-1423.pdf?utm_source=chatgpt.com","title":"BERT: Pre-training of Deep Bidirectional Transformers for ...","snippet":"by J Devlin \u00b7 2019 \u00b7 Cited by 144191 \u2014 We introduce a new language representa- tion model called BERT, which stands for. Bidirectional Encoder Representations from. Transformers.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/Q17-1010.pdf?utm_source=chatgpt.com","title":"Enriching Word Vectors with Subword Information","snippet":"by P Bojanowski \u00b7 2017 \u00b7 Cited by 14356 \u2014 Finally, recent works in machine translation have proposed using subword units to obtain repre- sentations of rare words (Sennrich et al., 2016; Lu- ong and ....","ref_id":{"turn_index":3,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/P16-1162/?utm_source=chatgpt.com","title":"Neural Machine Translation of Rare Words with Subword ...","snippet":"by R Sennrich \u00b7 2016 \u00b7 Cited by 10196 \u2014 Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting ....","ref_id":{"turn_index":3,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/2020.findings-emnlp.414.pdf?utm_source=chatgpt.com","title":"Byte Pair Encoding is Suboptimal for Language Model ...","snippet":"by K Bostrom \u00b7 2020 \u00b7 Cited by 283 \u2014 We analyze differences between BPE and un- igram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/2022.coling-1.288.pdf?utm_source=chatgpt.com","title":"Parameter-Efficient Mixture-of-Experts Architecture for Pre- ...","snippet":"by ZF Gao \u00b7 2022 \u00b7 Cited by 56 \u2014 Furthermore, Switch Transformer designs a switch routing strategy to simplify this gating func- tion by routing to a single expert (Fedus et al.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf?utm_source=chatgpt.com","title":"Language Models are Few-Shot Learners","snippet":"by T Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Training Compute-Optimal Large Language Models","snippet":"by J Hoffmann \u00b7 2022 \u00b7 Cited by 215 \u2014 We investigate the optimal model size and number of tokens for training a Trans- former language model under a given compute budget.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Training language models to follow instructions with ...","snippet":"by L Ouyang \u00b7 2022 \u00b7 Cited by 17696 \u2014 In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set .....","ref_id":{"turn_index":2,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf?utm_source=chatgpt.com","title":"Retrieval-Augmented Generation for Knowledge-Intensive ...","snippet":"by P Lewis \u00b7 2020 \u00b7 Cited by 11130 \u2014 Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf?utm_source=chatgpt.com","title":"Attention is All you Need","snippet":"by A Vaswani \u00b7 Cited by 194205 \u2014 Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014. [30] Christian Szegedy ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://papers.neurips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf?utm_source=chatgpt.com","title":"XLNet: Generalized Autoregressive Pretraining for ...","snippet":"by Z Yang \u00b7 Cited by 11779 \u2014 Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a wide spectrum of problems including GLUE language understanding ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://neurips.cc/virtual/2023/poster/71288?utm_source=chatgpt.com","title":"Language Models Can Teach Themselves to Use Tools","snippet":"Dec 9, 2023 \u2014 We have introduced Toolformer, a LM that learns in a self-supervised way how to use different tools such as search engines, calculators, and ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"alastairreid.github.io","entries":[{"type":"search_result","url":"https://alastairreid.github.io/RelatedWork/papers/fedus%3Aarxiv%3A2021/?utm_source=chatgpt.com","title":"Switch transformers: Scaling to trillion parameter models with ...","snippet":"Observes that the [mixture of experts] approach suffers from problems in complexity, communication costs and training instabilities.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"alastairreid.github.io"}]},{"type":"search_result_group","domain":"stanford.edu","entries":[{"type":"search_result","url":"https://www-nlp.stanford.edu/pubs/glove.pdf?utm_source=chatgpt.com","title":"GloVe: Global Vectors for Word Representation","snippet":"by RS JeffreyPennington \u00b7 Cited by 743 \u2014 Abstract. Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://nlp.stanford.edu/projects/glove/?utm_source=chatgpt.com","title":"GloVe: Global Vectors for Word Representation","snippet":"Prepared by Russell Stewart and Christopher Manning. Oct 2015. GloVe v.1.0: Original release. Prepared by Jeffrey Pennington. Aug 2014. Bugs/Issues/Discussion.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"stanford.edu"}]},{"type":"search_result_group","domain":"semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/GloVe%3A-Global-Vectors-for-Word-Representation-Pennington-Socher/f37e1b62a767a307c046404ca96bc140b3e68cb5?utm_source=chatgpt.com","title":"[PDF] GloVe: Global Vectors for Word Representation","snippet":"GloVe: Global Vectors for Word Representation \u00b7 Jeffrey Pennington, R. Socher, Christopher D. Manning \u00b7 Published in Conference on Empirical\u2026 1 October 2014 ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Deep-Contextualized-Word-Representations-Peters-Neumann/3febb2bed8865945e7fddc99efd791887bb7e14f?utm_source=chatgpt.com","title":"[PDF] Deep Contextualized Word Representations","snippet":"A new type of deep contextualized word representation is introduced that models both complex characteristics of word use and how these uses vary across ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Universal-Language-Model-Fine-tuning-for-Text-Howard-Ruder/1e077413b25c4d34945cc2707e17e46ed4fe784a?utm_source=chatgpt.com","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"This work proposes Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe?utm_source=chatgpt.com","title":"Language Models are Unsupervised Multitask Learners","snippet":"It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992?utm_source=chatgpt.com","title":"BERT: Pre-training of Deep Bidirectional Transformers for ...","snippet":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u00b7 Jacob Devlin, Ming-Wei Chang, +1 author. Kristina Toutanova \u00b7 Published in ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Training-Compute-Optimal-Large-Language-Models-Hoffmann-Borgeaud/8342b592fe238f3d230e4959b06fd10153c45db1?utm_source=chatgpt.com","title":"[PDF] Training Compute-Optimal Large Language Models","snippet":"This work trains a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Scaling-Laws-for-Neural-Language-Models-Kaplan-McCandlish/e6c561d02500b2596a230b341a8eb8b921ca5bf2?utm_source=chatgpt.com","title":"[PDF] Scaling Laws for Neural Language Models","snippet":"Scaling Laws for Neural Language Models \u00b7 J. Kaplan, Sam McCandlish, +7 authors. Dario Amodei \u00b7 Published in arXiv.org 23 January 2020 \u00b7 Computer ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":48},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/LLaMA%3A-Open-and-Efficient-Foundation-Language-Touvron-Lavril/57e849d0de13ed5f91d086936296721d4ff75a75?utm_source=chatgpt.com","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"LLaMA: Open and Efficient Foundation Language Models \u00b7 Hugo Touvron, Thibaut Lavril, +11 authors. Guillaume Lample \u00b7 Published in arXiv.org 27 February 2023 ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Training-language-models-to-follow-instructions-Ouyang-Wu/d766bffc357127e0dc86dd69561d5aeb520d6f4c?utm_source=chatgpt.com","title":"[PDF] Training language models to follow instructions with ...","snippet":"The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Scaling-Instruction-Finetuned-Language-Models-Chung-Hou/cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1?utm_source=chatgpt.com","title":"[PDF] Scaling Instruction-Finetuned Language Models","snippet":"It is found that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Retrieval-Augmented-Generation-for-NLP-Tasks-Lewis-Perez/659bf9ce7175e1ec266ff54359e2bd76e0b7ff31?utm_source=chatgpt.com","title":"Retrieval-Augmented Generation for Knowledge-Intensive ...","snippet":"A general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/LoRA%3A-Low-Rank-Adaptation-of-Large-Language-Models-Hu-Shen/a8ca46b171467ceb2d7652fbfb67fe701ad86092?utm_source=chatgpt.com","title":"LoRA: Low-Rank Adaptation of Large Language Models","snippet":"Low-Rank Adaptation, or LoRA, is proposed, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5?utm_source=chatgpt.com","title":"Neural Machine Translation by Jointly Learning to Align ...","snippet":"Neural Machine Translation by Jointly Learning to Align and Translate \u00b7 Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio \u00b7 Published in International Conference on\u2026","ref_id":{"turn_index":3,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d?utm_source=chatgpt.com","title":"[PDF] Sequence to Sequence Learning with Neural Networks","snippet":"This paper presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, and finds that reversing ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Enriching-Word-Vectors-with-Subword-Information-Bojanowski-Grave/e2dba792360873aef125572812f3673b1a85d850?utm_source=chatgpt.com","title":"Enriching Word Vectors with Subword Information","snippet":"A new approach based on the skipgram model, where each word is represented as a bag of character n-grams, with words being represented as the sum of these ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de?utm_source=chatgpt.com","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","snippet":"Jul 26, 2019 \u2014 It is found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it, ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/QLoRA%3A-Efficient-Finetuning-of-Quantized-LLMs-Dettmers-Pagnoni/32ac52069e562d4f900afee70bdca63f53461481?utm_source=chatgpt.com","title":"QLoRA: Efficient Finetuning of Quantized LLMs","snippet":"QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA, ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer/6c4b76232bb72897685d19b3d264c6ee3005bc2b?utm_source=chatgpt.com","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/XLNet%3A-Generalized-Autoregressive-Pretraining-for-Yang-Dai/e0c6abdbdecf04ffac65c440da77fb9d66bb474c?utm_source=chatgpt.com","title":"[PDF] XLNet: Generalized Autoregressive Pretraining for ...","snippet":"XLNet is proposed, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/ReAct%3A-Synergizing-Reasoning-and-Acting-in-Language-Yao-Zhao/99832586d55f540f603637e458a292406a0ed75d?utm_source=chatgpt.com","title":"Synergizing Reasoning and Acting in Language Models","snippet":"The use of LLMs are explored to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"semanticscholar.org"}]},{"type":"search_result_group","domain":"research.google","entries":[{"type":"search_result","url":"https://research.google/blog/more-efficient-in-context-learning-with-glam/?utm_source=chatgpt.com","title":"More Efficient In-Context Learning with GLaM","snippet":"Dec 9, 2021 \u2014 We introduce the Generalist Language Model (GLaM), a trillion weight model that can be trained and served efficiently (in terms of computation and energy use) ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"research.google"},{"type":"search_result","url":"https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/?utm_source=chatgpt.com","title":"ReAct: Synergizing Reasoning and Acting in Language ...","snippet":"We propose a general paradigm that combines reasoning and acting advances to enable language models to solve various language reasoning and decision making ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"research.google"}]},{"type":"search_result_group","domain":"acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3295222.3295349?utm_source=chatgpt.com","title":"Attention is all you need | Proceedings of the 31st International ...","snippet":"by A Vaswani \u00b7 2017 \u00b7 Cited by 194205 \u2014 We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3600270.3602446?utm_source=chatgpt.com","title":"Training compute-optimal large language models","snippet":"by J Hoffmann \u00b7 2022 \u00b7 Cited by 2753 \u2014 We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\u00d7 ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.5555/3495724.3495883?utm_source=chatgpt.com","title":"Language models are few-shot learners","snippet":"by TB Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of- ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3600270.3602281?utm_source=chatgpt.com","title":"Training language models to follow instructions with ...","snippet":"In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3722577.3722647?utm_source=chatgpt.com","title":"Scaling instruction-finetuned language models","snippet":"Jan 1, 2024 \u2014 In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.5555/3495724.3496517?utm_source=chatgpt.com","title":"Retrieval-augmented generation for knowledge-intensive ...","snippet":"by P Lewis \u00b7 2020 \u00b7 Cited by 11130 \u2014 We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric memory .....","ref_id":{"turn_index":2,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1007/978-3-031-77954-1_6?utm_source=chatgpt.com","title":"Retrieval-Augmented Generation Architecture Framework","snippet":"by R Shan \u00b7 2024 \u00b7 Cited by 4 \u2014 This paper presents a comprehensive exploration of the Retrieval-Augmented Generation Architecture Framework (RAGAF), structured around seven key modules.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/2969033.2969173?utm_source=chatgpt.com","title":"Sequence to sequence learning with neural networks","snippet":"by I Sutskever \u00b7 2014 \u00b7 Cited by 29981 \u2014 In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.5555/3586589.3586709?utm_source=chatgpt.com","title":"Switch transformers: scaling to trillion parameter models ...","snippet":"by W Fedus \u00b7 2022 \u00b7 Cited by 2959 \u2014 Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. AUTHORs: William Fedus.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3666122.3666563?utm_source=chatgpt.com","title":"QLORA: efficient finetuning of quantized LLMs","snippet":"Dec 10, 2023 \u2014 We present QLORA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.5555/3455716.3455856?utm_source=chatgpt.com","title":"Exploring the limits of transfer learning with a unified text-to ...","snippet":"by C Raffel \u00b7 2020 \u00b7 Cited by 26495 \u2014 In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3454287.3454804?utm_source=chatgpt.com","title":"XLNet: generalized autoregressive pretraining for language ...","snippet":"We propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3666122.3669119?utm_source=chatgpt.com","title":"Toolformer: language models can teach themselves to use ...","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2254 \u2014 In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"acm.org"}]},{"type":"search_result_group","domain":"ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=25351357&utm_source=chatgpt.com","title":"Attention Is All You Need https://arxiv.org/abs/1706.03762 ...","snippet":"This is the paper that defined the \"transformer\" architecture for deep neural nets. Over the past few years, transformers have become a more and more common ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"ycombinator.com"},{"type":"search_result","url":"https://news.ycombinator.com/item?id=36313348&utm_source=chatgpt.com","title":"Function calling and other API updates","snippet":"Jun 13, 2023 \u2014 The big feature here is the function calls, as this is effectively a replacement for the \"Tools\" feature of Agents popularized by LangChain, ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"ycombinator.com"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/284576917_Glove_Global_Vectors_for_Word_Representation?utm_source=chatgpt.com","title":"(PDF) Glove: Global Vectors for Word Representation","snippet":"PDF | On Jan 1, 2014, Jeffrey Pennington and others published Glove: Global Vectors for Word Representation | Find, read and cite all the research you need ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/323217640_Deep_contextualized_word_representations?utm_source=chatgpt.com","title":"(PDF) Deep contextualized word representations","snippet":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (eg, syntax and semantics),","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/338789955_Scaling_Laws_for_Neural_Language_Models?utm_source=chatgpt.com","title":"Scaling Laws for Neural Language Models","snippet":"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/328230984_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding?utm_source=chatgpt.com","title":"BERT: Pre-training of Deep Bidirectional Transformers for ...","snippet":"To support this framework, pre-trained ResNet-101 and BERT (Devlin et al. 2018 ) models were used to extract global visual and textual features, respectively.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/359576828_Training_Compute-Optimal_Large_Language_Models?utm_source=chatgpt.com","title":"Training Compute-Optimal Large Language Models","snippet":"Considering to Chinchilla scaling law for LLM, which was developed by Hoffman et al. (2022) [5] , the LLM size and the amount of training data should be ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/368842729_LLaMA_Open_and_Efficient_Foundation_Language_Models?utm_source=chatgpt.com","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/359054867_Training_language_models_to_follow_instructions_with_human_feedback?utm_source=chatgpt.com","title":"(PDF) Training language models to follow instructions with ...","snippet":"In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/371009182_Flan-MoE_Scaling_Instruction-Finetuned_Language_Models_with_Sparse_Mixture_of_Experts?utm_source=chatgpt.com","title":"(PDF) Flan-MoE: Scaling Instruction-Finetuned Language ...","snippet":"May 24, 2023 \u2014 In this paper, we introduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models. We show that naively finetuning MoE ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/341639856_Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks?utm_source=chatgpt.com","title":"Retrieval-Augmented Generation for Knowledge-Intensive ...","snippet":"Lewis et al. (2020) introduced the RAG technique to tackle these issues, using an external database with relevant knowledge instead of modifying the model's ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/352504883_LoRA_Low-Rank_Adaptation_of_Large_Language_Models?utm_source=chatgpt.com","title":"LoRA: Low-Rank Adaptation of Large Language Models","snippet":"We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate?utm_source=chatgpt.com","title":"(PDF) Neural Machine Translation by Jointly Learning to ...","snippet":"Sep 1, 2014 \u2014 Neural Machine Translation by Jointly Learning to Align and Translate. September 2014. Source; arXiv. Authors: Dzmitry Bahdanau at Constructor ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/265554383_Sequence_to_Sequence_Learning_with_Neural_Networks?utm_source=chatgpt.com","title":"Sequence to Sequence Learning with Neural Networks","snippet":"Aug 10, 2025 \u2014 In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/281427452_Neural_Machine_Translation_of_Rare_Words_with_Subword_Units?utm_source=chatgpt.com","title":"Neural Machine Translation of Rare Words with Subword ...","snippet":"Common tokenization methods include subword segmentation techniques such as Byte Pair Encoding (Sennrich et al. 2016 ) (BPE) and WordPiece (Schuster and ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/305388986_Enriching_Word_Vectors_with_Subword_Information?utm_source=chatgpt.com","title":"Enriching Word Vectors with Subword Information","snippet":"In this paper, we propose a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/334735779_RoBERTa_A_Robustly_Optimized_BERT_Pretraining_Approach?utm_source=chatgpt.com","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","snippet":"RoBERTa (Liu et al., 2019) , a variant of BERT, builds upon the same training setup but incorporates dynamic masking, expands the training dataset, and ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/370981726_QLoRA_Efficient_Finetuning_of_Quantized_LLMs?utm_source=chatgpt.com","title":"(PDF) QLoRA: Efficient Finetuning of Quantized LLMs","snippet":"May 23, 2023 \u2014 QLoRA: Efficient Finetuning of Quantized LLMs. May 2023. DOI:10.48550/arXiv.2305.14314. License; CC BY 4.0. Authors: Tim Dettmers.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/348403003_Switch_Transformers_Scaling_to_Trillion_Parameter_Models_with_Simple_and_Efficient_Sparsity?utm_source=chatgpt.com","title":"Switch Transformers: Scaling to Trillion Parameter Models ...","snippet":"For instance, the Switch Transformer (Fedus et al., 2022) activates only the top 1 expert for each input, successfully scaling the model to trillion-parameter ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/391328494_A_Survey_on_Parameter-Efficient_Fine-Tuning_for_Foundation_Models_in_Federated_Learning?utm_source=chatgpt.com","title":"(PDF) A Survey on Parameter-Efficient Fine-Tuning for ...","snippet":"Apr 29, 2025 \u2014 A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning. April 2025. DOI:10.48550/arXiv.2504.21099. License; CC ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/336084032_ALBERT_A_Lite_BERT_for_Self-supervised_Learning_of_Language_Representations?utm_source=chatgpt.com","title":"ALBERT: A Lite BERT for Self-supervised Learning of ...","snippet":"Sep 16, 2019 \u2014 We present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/336767865_Exploring_the_Limits_of_Transfer_Learning_with_a_Unified_Text-to-Text_Transformer?utm_source=chatgpt.com","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/333892322_XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding?utm_source=chatgpt.com","title":"(PDF) XLNet: Generalized Autoregressive Pretraining for ...","snippet":"Jun 19, 2019 \u2014 XLNet: Generalized Autoregressive Pretraining for Language Understanding. June 2019. DOI:10.48550/arXiv ... Models such as XLNet (Yang, Dai, et al ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"jungtaek.github.io","entries":[{"type":"search_result","url":"https://jungtaek.github.io/courses/2022-spring-trends-in-ml/materials/05_bert.pdf?utm_source=chatgpt.com","title":"BERT: Pre-training of Deep Bidirectional Transformers for ...","snippet":"by J Kim \u00b7 2022 \u00b7 Cited by 4 \u2014 [Devlin et al., 2018] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"jungtaek.github.io"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40ashleyha/lets-learn-about-universal-language-model-fine-tuning-ulmfit-through-practical-applications-fea0aed2cf96?utm_source=chatgpt.com","title":"Let's learn about Universal Language Model Fine-tuning ...","snippet":"Developed by Jeremy Howard and Sebastian Ruder in 2018, in this Research Paper, ULMFiT has revolutionized the field of natural language ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40hyponymous/paper-summary-universal-language-model-fine-tuning-for-text-classification-2484b56e29da?utm_source=chatgpt.com","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"ULMFiT instead describes a way to train all layers, and does so without overfitting or running into \u201ccatastropic forgetting\u201d, which has been ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40samyakkala128/language-models-are-unsupervised-multitask-learners-e82c5b671544?utm_source=chatgpt.com","title":"Language Models are Unsupervised Multitask Learners","snippet":"The paper \u201cLanguage Models are Unsupervised Multitask Learners\u201d presents a significant advancement in the field of NLP by demonstrating the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40raniahossam/chinchilla-scaling-laws-for-large-language-models-llms-40c434e4e1c1?utm_source=chatgpt.com","title":"Chinchilla Scaling Laws for Large Language Models (LLMs)","snippet":"The authors mention that for compute-optimal training, the number of training tokens and model size must be scaled equally. They trained about ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40aleenatleejoy/training-language-models-to-follow-instructions-with-human-feedback-a-comprehensive-review-267a30344028?utm_source=chatgpt.com","title":"Training Language Models to Follow Instructions with ...","snippet":"The paper presents a three-step Reinforcement Learning from Human Feedback (RLHF) pipeline used to fine-tune OpenAI's GPT-3 model.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40hyponymous/paper-summary-enriching-word-vectors-with-subword-information-e5263d0848b8?utm_source=chatgpt.com","title":"Paper Summary: Enriching Word Vectors with Subword ...","snippet":"Enriching Word Vectors with Subword Information (2016) Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov. Here's the summary of ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/analytics-vidhya/papers-xplained-neural-machine-translation-using-bahdanau-attention-7e3274c41e14?utm_source=chatgpt.com","title":"Neural Machine Translation using Bahdanau Attention ...","snippet":"The paper in discussion is \u201cNeural Machine Translation by Jointly Learning to Align and Translate\u201d by Dzmitry Bahdanau, KyungHyun Cho & Yoshua Bengio.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40oxenai/arxiv-dives-toolformer-language-models-can-teach-themselves-to-use-tools-b50c9312c2a9?utm_source=chatgpt.com","title":"Arxiv Dives \u2014 Toolformer: Language models can teach ...","snippet":"The Toolformer architecture lets the use of tools be learned in a self-supervised way without requiring large amounts of human annotations.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40ronnyh/research-paper-summary-react-synergizing-reasoning-and-acting-in-language-models-95e8ca80855b?utm_source=chatgpt.com","title":"[Research Paper Summary] ReAct: Synergizing Reasoning ...","snippet":"ReAct: Synergizing Reasoning + Acting is a research paper that explores the potential of large language models in solving complex tasks that ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/the-modern-scientist/everything-you-need-to-know-about-openai-function-calling-and-assistants-api-55c02570a21c?utm_source=chatgpt.com","title":"Everything you need to know about OpenAI function calling ...","snippet":"This article delves into the intricate world of OpenAI's Chat Completions and Assistant APIs, to develop customized chatbot applications for your next AI ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/openai/gpt-2?utm_source=chatgpt.com","title":"openai/gpt-2: Code for the paper \"Language Models are ...","snippet":"Code and models from the paper \"Language Models are Unsupervised Multitask Learners\". You can read about GPT-2 and its staged release in our original blog post.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/microsoft/LoRA?utm_source=chatgpt.com","title":"LoRA: Low-Rank Adaptation of Large Language Models","snippet":"This repo contains the source code of the Python package loralib and several examples of how to integrate it with PyTorch models, such as those in Hugging Face.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"zoomyizumi.com","entries":[{"type":"search_result","url":"https://www.zoomyizumi.com/openais-new/?utm_source=chatgpt.com","title":"'Function calling', OpenAI's API new feature. - Zoomy Izumi","snippet":"Jun 14, 2023 \u2014 The ability to add \"functions\" flaunting it as a 'more reliable way' to pair GPT's capabilities with external calls.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"zoomyizumi.com"}]},{"type":"search_result_group","domain":"google.com","entries":[{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=KNr3vb4AAAAJ&utm_source=chatgpt.com","title":"Jared Kaplan","snippet":"... Kaplan, H Edwards, ... arXiv preprint arXiv:2107.03374, 2021. 6128, 2021. Scaling laws for neural language models. J Kaplan, S McCandlish, T Henighan, TB Brown ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"google.com"},{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=xImarzoAAAAJ&utm_source=chatgpt.com","title":"Hugo Touvron","snippet":"Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, ... arXiv preprint arXiv:2307.09288, 2023.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"google.com"},{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=x04W_mMAAAAJ&utm_source=chatgpt.com","title":"Ilya Sutskever","snippet":"Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, QV Le. Advances in neural information processing systems 27, 2014. 30033, 2014.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"google.com"},{"type":"search_result","url":"https://research.google.com/pubs/archive/44928.pdf?utm_source=chatgpt.com","title":"Multi-task Sequence to Sequence Learning","snippet":"by MT Luong \u00b7 2015 \u00b7 Cited by 996 \u2014 Figure 1: Sequence to sequence learning examples \u2013 (left) machine translation (Sutskever et al.,. 2014) and (right) constituent parsing (Vinyals et al., 2015a).","ref_id":{"turn_index":3,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"google.com"},{"type":"search_result","url":"https://scholar.google.com/citations?hl=ja&user=2-jRJGcAAAAJ&utm_source=chatgpt.com","title":"Taku Kudo","snippet":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing ... arXiv preprint arXiv:1804.10959, 2018. 1540 ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":48},"pub_date":null,"attribution":"google.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=nZeVKeeFYf9&utm_source=chatgpt.com","title":"Lora: Low-rank adaptation of large lan","snippet":"by EJ Hu \u00b7 Cited by 18378 \u2014 We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=nZeVKeeFYf9&utm_source=chatgpt.com","title":"LoRA: Low-Rank Adaptation of Large Language Models","snippet":"by EJ Hu \u00b7 Cited by 18378 \u2014 We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe+profile+of+Ari+Holtzman%5D%28%2Fprofile%3Fid%3D~Ari_Holtzman1%29&utm_source=chatgpt.com","title":"QLoRA: Efficient Finetuning of Quantized LLMs","snippet":"We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=lIsCS8b6zj&utm_source=chatgpt.com","title":"Parameter-Efficient Fine-Tuning for Large Models","snippet":"by Z Han \u00b7 Cited by 662 \u2014 In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=Yacmpz84TH&utm_source=chatgpt.com","title":"Toolformer: Language Models Can Teach Themselves to ...","snippet":"by T Schick \u00b7 Cited by 2259 \u2014 We introduce Toolformer, a language model trained in a self-supervised way to know when and how to use external tools, achieving substantially improved zero- ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/1801.06146?utm_source=chatgpt.com","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"Abstract. ULMFiT is a transfer learning method for NLP that outperforms existing approaches and reduces errors on text classification tasks ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2005.14165?utm_source=chatgpt.com","title":"Paper page - Language Models are Few-Shot Learners","snippet":"May 28, 2020 \u2014 Abstract. GPT-3, a large-scale autoregressive language model, exhibits strong few-shot performance without fine-tuning across various NLP tasks ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2001.08361?utm_source=chatgpt.com","title":"Paper page - Scaling Laws for Neural Language Models","snippet":"Jan 22, 2020 \u2014 We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/openai-community/gpt2?utm_source=chatgpt.com","title":"openai-community/gpt2","snippet":"GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2302.13971?utm_source=chatgpt.com","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/1808.06226?utm_source=chatgpt.com","title":"A simple and language independent subword tokenizer ...","snippet":"Aug 19, 2018 \u2014 This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/facebook/fasttext-an-vectors?utm_source=chatgpt.com","title":"facebook/fasttext-an-vectors","snippet":"Joulin, T. Mikolov, Enriching Word Vectors with Subword Information. @article{bojanowski2016enriching, title={Enriching Word Vectors with Subword Information} ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/google/switch-base-16?utm_source=chatgpt.com","title":"google/switch-base-16","snippet":"Jul 10, 2025 \u2014 Switch Transformers is a Mixture of Experts (MoE) model trained on Masked Language Modeling (MLM) task. The model architecture is similar to the classic T5.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/1910.10683?utm_source=chatgpt.com","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"Oct 23, 2019 \u2014 In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/xlnet/xlnet-base-cased?utm_source=chatgpt.com","title":"xlnet/xlnet-base-cased","snippet":"Mar 11, 2024 \u2014 It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2210.03629?utm_source=chatgpt.com","title":"Synergizing Reasoning and Acting in Language Models","snippet":"Oct 5, 2022 \u2014 In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00051/43387/Enriching-Word-Vectors-with-Subword-Information?utm_source=chatgpt.com","title":"Enriching Word Vectors with Subword Information","snippet":"Jun 1, 2017 \u2014 In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"mit.edu"}]},{"type":"search_result_group","domain":"harvard.edu","entries":[{"type":"search_result","url":"https://ui.adsabs.harvard.edu/abs/2018arXiv180205365P?utm_source=chatgpt.com","title":"Deep contextualized word representations","snippet":"by ME Peters \u00b7 2018 \u00b7 Cited by 17365 \u2014 We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (eg, syntax and semantics),","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"harvard.edu"},{"type":"search_result","url":"https://ui.adsabs.harvard.edu/abs/2018arXiv180106146H/abstract?utm_source=chatgpt.com","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"by J Howard \u00b7 2018 \u00b7 Cited by 5879 \u2014 We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques .....","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"harvard.edu"},{"type":"search_result","url":"https://ui.adsabs.harvard.edu/abs/arXiv%3A2104.02756?utm_source=chatgpt.com","title":"Efficient transfer learning for NLP with ELECTRA","snippet":"Abstract. Clark et al. [2020] claims that the ELECTRA approach is highly efficient in NLP performances relative to computation budget.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"harvard.edu"}]},{"type":"search_result_group","domain":"alphaxiv.org","entries":[{"type":"search_result","url":"https://www.alphaxiv.org/overview/2210.03629v1?utm_source=chatgpt.com","title":"Synergizing Reasoning and Acting in Language Models","snippet":"A new paradigm called ReAct enables language models to interleave reasoning (thoughts) and acting (actions) for problem-solving.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"alphaxiv.org"}]},{"type":"search_result_group","domain":"snu.ac.kr","entries":[{"type":"search_result","url":"https://idea.snu.ac.kr/wp-content/uploads/sites/6/2025/01/Language_Models_are_Unsupervised_Multitask_Learners_GPT-2.pdf?utm_source=chatgpt.com","title":"Language Models are Unsupervised Multitask Learners ( ...","snippet":"Aug 27, 2024 \u2014 \u25b7 The paper proposes GPT-2, a language model capable of performing downstream tasks directly in a zero-shot learning setting, without any ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"snu.ac.kr"}]},{"type":"search_result_group","domain":"shaynelongpre.com","entries":[{"type":"search_result","url":"https://www.shaynelongpre.com/publication/scaling-flan-2022/?utm_source=chatgpt.com","title":"Scaling Instruction-Finetuned Language Models","snippet":"In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"shaynelongpre.com"}]},{"type":"search_result_group","domain":"sino-huang.github.io","entries":[{"type":"search_result","url":"https://sino-huang.github.io/posts/timo_schick-toolformer-language-models-can-teach-themselves-to-use-tools-2023/?utm_source=chatgpt.com","title":"Timo_schick Toolformer Language Models Can Teach ...","snippet":"Mar 1, 2023 \u2014 We introduce Toolformer, a model that incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"sino-huang.github.io"}]},{"type":"search_result_group","domain":"nips.cc","entries":[{"type":"search_result","url":"https://papers.nips.cc/paper/7181-attention-is-all-you-need?utm_source=chatgpt.com","title":"Attention is All you Need","snippet":"by A Vaswani \u00b7 2017 \u00b7 Cited by 194205 \u2014 We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"nips.cc"},{"type":"search_result","url":"https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=chatgpt.com","title":"Language Models are Few-Shot Learners","snippet":"by T Brown \u00b7 2020 \u00b7 Cited by 54137 \u2014 Abstract. We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"nips.cc"}]},{"type":"search_result_group","domain":"microsoft.com","entries":[{"type":"search_result","url":"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/function-calling?utm_source=chatgpt.com","title":"How to use function calling with Azure OpenAI in ...","snippet":"Use the model's response to call your API or function; Call the chat completions API again, including the response from your function to get a final response.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"microsoft.com"}]},{"type":"search_result_group","domain":"pnas.org","entries":[{"type":"search_result","url":"https://www.pnas.org/doi/10.1073/pnas.2311878121?utm_source=chatgpt.com","title":"Explaining neural scaling laws","snippet":"by Y Bahri \u00b7 2024 \u00b7 Cited by 384 \u2014 Kaplan et al., Scaling laws for neural language models. arXiv [Preprint] (2020). ... Sully, A solvable model of neural scaling laws. arXiv ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"pnas.org"}]},{"type":"search_result_group","domain":"meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/?utm_source=chatgpt.com","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"We introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"meta.com"},{"type":"search_result","url":"https://ai.meta.com/research/publications/toolformer-language-models-can-teach-themselves-to-use-tools/?utm_source=chatgpt.com","title":"Toolformer: Language Models Can Teach Themselves to ...","snippet":"In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"meta.com"}]},{"type":"search_result_group","domain":"scirp.org","entries":[{"type":"search_result","url":"https://www.scirp.org/reference/referencespapers?referenceid=1916935&utm_source=chatgpt.com","title":"Pennington, J., Socher, R. and Manning, C.D. (2014) ...","snippet":"In this paper, we propose the \u201cmovement purpose hypothesis\u201d that each movement occurs from two causes: where the person is and what the person wants to do ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"scirp.org"},{"type":"search_result","url":"https://www.scirp.org/reference/referencespapers?referenceid=3662223&utm_source=chatgpt.com","title":"Pennington, J., Socher, R. and Manning, C.D. (2014) Glove ...","snippet":"Pennington, J., Socher, R. and Manning, C.D. (2014) Glove ... paper proposes a graph neural network-based aspect-level sentiment classification model.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"scirp.org"},{"type":"search_result","url":"https://www.scirp.org/reference/referencespapers?referenceid=3492350&utm_source=chatgpt.com","title":"Devlin, J., Chang, M.W., Lee, K., et al. (2018) Bert Pre- ...","snippet":"This paper proposes a feature fusion network model Bert-TextLevelGCN based on BERT pre-training and improved TextGCN. On the one hand, Bert is introduced to ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"scirp.org"},{"type":"search_result","url":"https://www.scirp.org/reference/referencespapers?referenceid=3453148&utm_source=chatgpt.com","title":"Bahdanau, D., Cho, K. and Bengio, Y. (2014) Neural ...","snippet":"Bahdanau, D., Cho, K. and Bengio, Y. (2014) Neural Machine Translation by Jointly Learning to Align and Translate. arXiv preprint arXiv1409.0473.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"scirp.org"},{"type":"search_result","url":"https://www.scirp.org/reference/referencespapers?referenceid=3984486&utm_source=chatgpt.com","title":"Liu, Y., Ott, M., Goyal, N., et al. (2019) RoBERTa A Robustly ...","snippet":"Apr 29, 2025 \u2014 Liu, Y., Ott, M., Goyal, N., et al. (2019) RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv: 1907.11692. https://doi.org ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"scirp.org"},{"type":"search_result","url":"https://www.scirp.org/reference/referencespapers?referenceid=4079788&utm_source=chatgpt.com","title":"Dettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer, L. ...","snippet":"Dettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer, L. (2023) QLoRA Efficient Finetuning of Quantized LLMs. arXiv 2305.14314.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"scirp.org"}]},{"type":"search_result_group","domain":"simonwillison.net","entries":[{"type":"search_result","url":"https://simonwillison.net/2023/Jun/13/function-calling/?utm_source=chatgpt.com","title":"OpenAI: Function calling and other API updates","snippet":"Jun 13, 2023 \u2014 You can now send JSON schema defining one or more functions to GPT 3.5 and GPT-4\u2014those models will then return a blob of JSON describing a ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"simonwillison.net"}]},{"type":"search_result_group","domain":"fanpu.io","entries":[{"type":"search_result","url":"https://fanpu.io/summaries/2023-08-02-deep-contextualized-word-representations/?utm_source=chatgpt.com","title":"Deep contextualized word representations (ELMo)","snippet":"Aug 2, 2023 \u2014 We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"fanpu.io"},{"type":"search_result","url":"https://fanpu.io/summaries/2023-08-10-language-models-are-unsupervised-multitask-learners/?utm_source=chatgpt.com","title":"Language Models are Unsupervised Multitask Learners","snippet":"Aug 10, 2023 \u2014 We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"fanpu.io"},{"type":"search_result","url":"https://fanpu.io/summaries/2023-08-22-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer/?utm_source=chatgpt.com","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"Aug 22, 2023 \u2014 The Text-to-Text Transfer Transformer (T5) model has an encoder-decoder architecture. The author takes the view that all problems can be viewed ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"fanpu.io"}]},{"type":"search_result_group","domain":"samuelalbanie.com","entries":[{"type":"search_result","url":"https://samuelalbanie.com/digests/2022-07-gpt-3/?utm_source=chatgpt.com","title":"Language Models are Few-shot Learners (GPT-3)","snippet":"Summary: A video description of the paper entitled \"Language Models are Few-shot Learners\" by T. Brown et al. published at NeurIPS in 2020.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"samuelalbanie.com"},{"type":"search_result","url":"https://samuelalbanie.com/digests/2022-10-scaling-instruction-finetuned-language-models/?utm_source=chatgpt.com","title":"Scaling Instruction-Finetuned Language Models (Flan-PaLM)","snippet":"Summary: A description of the the work \"Scaling Instruction-Finetuned Language Models\" by Hyung Won Chung et al. published on arxiv in October 2022.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"samuelalbanie.com"},{"type":"search_result","url":"https://samuelalbanie.com/files/digest-slides/2022-10-scaling-instruction-finetuned-language-models.pdf?utm_source=chatgpt.com","title":"H. Chung et al., \"Scaling Instruction-Finetuned Language ...","snippet":"Oct 21, 2022 \u2014 Build on mixtures of prior datasets (and add new CoT data etc.) \u2022Explore larger model scaling (up to 540B params vs 137B). \u2022Finetune on a ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"samuelalbanie.com"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/ControlProblem/comments/trx8c4/chinchilla_training_computeoptimal_large_language/?utm_source=chatgpt.com","title":"\"Chinchilla: Training Compute-Optimal Large Language ...","snippet":"We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/gsuzey/d_paper_explained_gpt3_language_models_are/?utm_source=chatgpt.com","title":"[D] Paper Explained - GPT-3: Language Models are Few- ...","snippet":"OpenAI take on these and other questions by training a transformer that is an order of magnitude larger than anything that has ever been built before.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/mlscaling/comments/jl143s/scaling_laws_for_neural_language_models_kaplan_et/?utm_source=chatgpt.com","title":"\"Scaling Laws for Neural Language Models\", Kaplan et al ...","snippet":"Scaling Laws for Neural Language Models, Kaplan et al 2020 (optimal approach: train as large NN models as possible for few steps)","ref_id":{"turn_index":1,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/cjbcxm/r190711692_roberta_a_robustly_optimized_bert/?utm_source=chatgpt.com","title":"[R][1907.11692] RoBERTa: A Robustly Optimized BERT ...","snippet":"We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/rdgo6i/google_introduces_the_generalist_language_model/?utm_source=chatgpt.com","title":"1.2T parameters (97B active/eval) MoE, better few shot perf ...","snippet":"Google introduces the Generalist Language Model (GLaM), a trillion weight model - 1.2T parameters (97B active/eval) MoE, better few shot perf ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/dm9m33/r_exploring_the_limits_of_transfer_learning_with/?utm_source=chatgpt.com","title":"[R] Exploring the Limits of Transfer Learning with a Unified ...","snippet":"For SuperGLUE, we improved upon the state-of-the-art by a large margin (from an average. score of 84.6 [Liu et al., 2019c] to 88.9).","ref_id":{"turn_index":5,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/y1mosn/r_react_synergizing_reasoning_and_acting_in/?utm_source=chatgpt.com","title":"[R] ReAct: Synergizing Reasoning and Acting in Language ...","snippet":"In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"syncedreview.com","entries":[{"type":"search_result","url":"https://syncedreview.com/2023/02/16/meta-ai-upfs-toolformer-enabling-language-models-to-teach-themselves-to-use-external-tools/?utm_source=chatgpt.com","title":"Meta AI & UPF's Toolformer: Enabling Language Models to ...","snippet":"Feb 16, 2023 \u2014 The team proposes Toolformer, a model that self-learns how to choose and use external tools such as search engines, calculators, and translation ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"syncedreview.com"}]},{"type":"search_result_group","domain":"inspirehep.net","entries":[{"type":"search_result","url":"https://inspirehep.net/literature/2853835?utm_source=chatgpt.com","title":"Scaling Laws for Neural Language Models","snippet":"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"inspirehep.net"}]},{"type":"search_result_group","domain":"google.fr","entries":[{"type":"search_result","url":"https://scholar.google.fr/citations?hl=en&user=2-jRJGcAAAAJ&utm_source=chatgpt.com","title":"Taku Kudo","snippet":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing ... arXiv preprint arXiv:1804.10959, 2018. 1540 ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"google.fr"}]},{"type":"search_result_group","domain":"indianexpress.com","entries":[{"type":"search_result","url":"https://indianexpress.com/article/technology/artificial-intelligence/openai-announces-function-calling-and-other-api-updates-8663046/?utm_source=chatgpt.com","title":"OpenAI announces function calling and other API updates","snippet":"Jun 14, 2023 \u2014 The company has announced a slew of updates including steerable API models, function calling capabilities, longer context, and lower prices. The ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"indianexpress.com"}]},{"type":"search_result_group","domain":"fasttext.cc","entries":[{"type":"search_result","url":"https://fasttext.cc/docs/en/references.html?utm_source=chatgpt.com","title":"References","snippet":"Bojanowski*, E. Grave*, A. Joulin, T. Mikolov, Enriching Word Vectors with Subword Information @article{bojanowski2016enriching, title={Enriching Word Vectors ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"fasttext.cc"}]},{"type":"search_result_group","domain":"react-lm.github.io","entries":[{"type":"search_result","url":"https://react-lm.github.io/?utm_source=chatgpt.com","title":"ReAct: Synergizing Reasoning and Acting in Language Models","snippet":"In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"react-lm.github.io"}]},{"type":"search_result_group","domain":"stackoverflow.com","entries":[{"type":"search_result","url":"https://stackoverflow.com/questions/27860652/what-is-the-concept-of-negative-sampling-in-word2vec?utm_source=chatgpt.com","title":"What is the concept of negative-sampling in word2vec?","snippet":"Mikolov et al. present the negative-sampling approach as a more efficient way of deriving word embeddings. While negative-sampling is based on ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"stackoverflow.com"}]},{"type":"search_result_group","domain":"infoq.com","entries":[{"type":"search_result","url":"https://www.infoq.com/news/2023/06/openai-api-function-chatgpt/?utm_source=chatgpt.com","title":"OpenAI Announces Function Calling, Allowing Developers ...","snippet":"Jun 15, 2023 \u2014 OpenAI has introduced updates to the API, including a capability called function calling, which allows developers to describe functions to GPT-4 and GPT-3.5.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"infoq.com"}]},{"type":"search_result_group","domain":"wikipedia.org","entries":[{"type":"search_result","url":"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need?utm_source=chatgpt.com","title":"Attention Is All You Need","snippet":"\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"wikipedia.org"},{"type":"search_result","url":"https://en.wikipedia.org/wiki/Word2vec?utm_source=chatgpt.com","title":"Word2vec","snippet":"Word2vec is a technique in natural language processing for obtaining vector representations of words. These vectors capture information about the meaning of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"wikipedia.org"},{"type":"search_result","url":"https://en.wikipedia.org/wiki/List_of_large_language_models?utm_source=chatgpt.com","title":"List of large language models","snippet":"A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"wikipedia.org"}]},{"type":"search_result_group","domain":"preprints.org","entries":[{"type":"search_result","url":"https://www.preprints.org/manuscript/202504.0743/v1?utm_source=chatgpt.com","title":"Revisiting Fine-Tuning: A Survey of Parameter-Efficient ...","snippet":"by S Lei \u00b7 2025 \u00b7 Cited by 1 \u2014 Selective fine-tuning approaches aim to minimize parameter updates by identifying and modifying only a small subset of model parameters. BitFit ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"preprints.org"}]},{"type":"search_result_group","domain":"sdu.edu.cn","entries":[{"type":"search_result","url":"https://splab.sdu.edu.cn/1802.05365.pdf?utm_source=chatgpt.com","title":"arXiv:1802.05365v2 [cs.CL] 22 Mar 2018","snippet":"by ME Peters \u00b7 Cited by 17365 \u2014 In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"sdu.edu.cn"}]},{"type":"search_result_group","domain":"scispace.com","entries":[{"type":"search_result","url":"https://scispace.com/papers/language-models-are-few-shot-learners-2hr23olfy0?utm_source=chatgpt.com","title":"Language Models are Few-Shot Learners","snippet":"This article showed that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"scispace.com"},{"type":"search_result","url":"https://scispace.com/papers/llama-open-and-efficient-foundation-language-models-1aeusmr9?utm_source=chatgpt.com","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"This article introduced LLaMA, a collection of foundation language models ranging from 7B to 65B parameters, and trained their models on trillions of tokens.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"scispace.com"},{"type":"search_result","url":"https://scispace.com/papers/exploring-the-limits-of-transfer-learning-with-a-unified-5f0j0o6sin?utm_source=chatgpt.com","title":"Exploring the Limits of Transfer Learning with a Unified ...","snippet":"In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"scispace.com"}]},{"type":"search_result_group","domain":"summarizepaper.com","entries":[{"type":"search_result","url":"https://www.summarizepaper.com/en/arxiv-id/1801.06146v5/?utm_source=chatgpt.com","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"Authors Jeremy Howard and Sebastian Ruder propose Universal Language Model Fine-tuning (ULMFiT), a transfer learning method for Natural Language Processing (NLP) ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"summarizepaper.com"}]},{"type":"search_result_group","domain":"bibbase.org","entries":[{"type":"search_result","url":"https://bibbase.org/network/publication/touvron-lavril-izacard-martinet-lachaux-lacroix-rozire-goyal-etal-llamaopenandefficientfoundationlanguagemodels-2023?utm_source=chatgpt.com","title":"LLaMA: Open and Efficient Foundation Language Models","snippet":"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"bibbase.org"}]},{"type":"search_result_group","domain":"tib.eu","entries":[{"type":"search_result","url":"https://service.tib.eu/ldmservice/dataset/xlnet--generalized-autoregressive-pretraining-for-language-understanding?utm_source=chatgpt.com","title":"Xlnet: Generalized Autoregressive Pretraining for ...","snippet":"Jan 3, 2025 \u2014 The Xlnet is a generalized autoregressive pretraining model for language understanding. Data and Resources. Original MetadataJSON. The json ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"tib.eu"}]},{"type":"search_result_group","domain":"bibsonomy.org","entries":[{"type":"search_result","url":"https://www.bibsonomy.org/bibtex/210c860e3f390c6fbfd78a3b91ab9b0af/albinzehe?utm_source=chatgpt.com","title":"BERT: Pre-training of Deep Bidirectional Transformers for ...","snippet":"BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"bibsonomy.org"},{"type":"search_result","url":"https://www.bibsonomy.org/bibtex/bb2ca011eeafccb0bd2505c9476dcd10?utm_source=chatgpt.com","title":"Neural machine translation by jointly learning to align and ...","snippet":"Neural machine translation by jointly learning to align and translate. D. Bahdanau, K. Cho, and Y. Bengio. arXiv preprint arXiv:1409.0473, (2014 )","ref_id":{"turn_index":3,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"bibsonomy.org"}]},{"type":"search_result_group","domain":"ibm.com","entries":[{"type":"search_result","url":"https://www.ibm.com/think/topics/mixture-of-experts?utm_source=chatgpt.com","title":"What is mixture of experts?","snippet":"Apr 5, 2024 \u2014 In their influential 2021 paper, \u201cSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficiency Sparsity,\u201d Fedus et al.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"ibm.com"}]},{"type":"search_result_group","domain":"oa.mg","entries":[{"type":"search_result","url":"https://oa.mg/work/10.3115/v1/d14-1162?utm_source=chatgpt.com","title":"Glove: Global Vectors for Word Representation","snippet":"\u201cGlove: Global Vectors for Word Representation\u201d is a paper by Jeffrey Pennington Richard Socher Christopher D. Manning published in 2014. It has an Open ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"oa.mg"}]},{"type":"search_result_group","domain":"oxen.ai","entries":[{"type":"search_result","url":"https://ghost.oxen.ai/arxiv-dives-attention-is-all-you-need/?utm_source=chatgpt.com","title":"Arxiv Dives - Attention Is All You Need","snippet":"Nov 4, 2023 \u2014 We believe diving into the details of research papers is the best way to build fundamental knowledge and keep up with the bleeding edge.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"oxen.ai"}]},{"type":"search_result_group","domain":"readpaper.com","entries":[{"type":"search_result","url":"https://pdf.cdn.readpaper.com/spd/2963250244.pdf?utm_source=chatgpt.com","title":"arXiv:1808.06226v1 [cs.CL] 19 Aug 2018","snippet":"by T Kudo \u00b7 Cited by 4551 \u2014 In this demo paper, we describe SentencePiece, a simple and language independent text tokenizer and detokenizer mainly for Neural Network- based ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"readpaper.com"}]},{"type":"search_result_group","domain":"mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v119/ye20a/ye20a.pdf?utm_source=chatgpt.com","title":"Pretrained Generalized Autoregressive Model with Adaptive ...","snippet":"by H Ye \u00b7 2020 \u00b7 Cited by 65 \u2014 The generalized autoregressive pretraining method (XLNet) (Yang et al., 2019b) represents the latest development in this field. XLNet adopts the per-.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"mlr.press"}]},{"type":"search_result_group","domain":"stackexchange.com","entries":[{"type":"search_result","url":"https://stats.stackexchange.com/questions/623900/when-training-word2vec-why-is-a-new-negative-sampling-process-formulated-instea?utm_source=chatgpt.com","title":"When training word2vec, why is a new negative sampling ...","snippet":"Aug 13, 2023 \u2014 If \"word\" is replaced with \"text passage\", \"in-context\" is replaced with \"similar\", and \"out-of-context\" is replaced with \"not similar\", then ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"stackexchange.com"}]},{"type":"search_result_group","domain":"youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=EWM9wT3z2kY&utm_source=chatgpt.com","title":"OpenAI InstructGPT: Training language models to follow ...","snippet":"In this video I'm going to talk about open ai's instruct GPT which is a way of training language models to follow instructions with human feedback.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"youtube.com"}]},{"type":"search_result_group","domain":"1cademy.com","entries":[{"type":"search_result","url":"https://1cademy.com/node/neural-machine-translation-by-jointly-learning-to-align-and-translate/5omHsjHs8KX7p1qjjw4R?utm_source=chatgpt.com","title":"Neural Machine Translation by Jointly Learning to Align ...","snippet":"Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. 0. 1. 5 ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"1cademy.com"}]},{"type":"search_result_group","domain":"kaggle.com","entries":[{"type":"search_result","url":"https://www.kaggle.com/datasets/rtatman/glove-global-vectors-for-word-representation?utm_source=chatgpt.com","title":"GloVe: Global Vectors for Word Representation","snippet":"Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe ... GloVe embeddings have been used in more than 2100 papers, and counting!","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"kaggle.com"}]},{"type":"search_result_group","domain":"x-mol.com","entries":[{"type":"search_result","url":"https://www.x-mol.com/paper/1470855052606873600?utm_source=chatgpt.com","title":"Efficient Scaling of Language Models with Mixture-of-Experts","snippet":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts. arXiv.cs.CL Pub Date : 2021-12-13. DOI : arxiv-2112.06905. Nan Du, Yanping Huang, ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"x-mol.com"}]},{"type":"search_result_group","domain":"queirozf.com","entries":[{"type":"search_result","url":"https://queirozf.com/entries/paper-summary-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer?utm_source=chatgpt.com","title":"Paper Summary: Exploring the Limits of Transfer Learning ...","snippet":"Jul 8, 2025 \u2014 Authors create a framework to convert any NLP problem 1 into a natural language format and use it to compare strategies to apply LLMs to NLP tasks.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"queirozf.com"}]},{"type":"search_result_group","domain":"shreyansh26.github.io","entries":[{"type":"search_result","url":"https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/?utm_source=chatgpt.com","title":"XLNet: Generalized Autoregressive Pretraining for Language ...","snippet":"May 16, 2021 \u2014 The paper proposes XLNet, a generalized autoregressive pretraining method that enables learning bidirectional contexts over all permutations of the ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"shreyansh26.github.io"}]},{"type":"search_result_group","domain":"hyunyoung2.github.io","entries":[{"type":"search_result","url":"https://hyunyoung2.github.io/2019/08/06/Universal_Language_Model_Fine_tuning_for_Text_Classification/?utm_source=chatgpt.com","title":"Universal Language Model Fine-tuning for Text Classification","snippet":"Aug 6, 2019 \u2014 Reference. Paper. arXiv Version: Universal Language Model Fine-tuning for Text Classification (Howard and Ruder., arXiv 2018) \u00b7 ACL Version ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"hyunyoung2.github.io"},{"type":"search_result","url":"https://hyunyoung2.github.io/2019/12/02/Deep_Contextualized_Word_Representations/?utm_source=chatgpt.com","title":"Deep contextualized word representations - Hyunyoung2","snippet":"Dec 2, 2019 \u2014 They introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (eg, syntax and semantics),","ref_id":{"turn_index":0,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"hyunyoung2.github.io"}]},{"type":"search_result_group","domain":"substack.com","entries":[{"type":"search_result","url":"https://cameronrwolfe.substack.com/p/language-understanding-with-bert?utm_source=chatgpt.com","title":"Language Understanding with BERT - Deep (Learning) Focus","snippet":"[1] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"substack.com"}]},{"type":"search_result_group","domain":"etcjournal.com","entries":[{"type":"search_result","url":"https://etcjournal.com/2025/07/29/a-review-of-ouyang-et-al-s-2022-paper-aka-instructgpt/?utm_source=chatgpt.com","title":"A Review of Ouyang et al.'s 2022 Paper aka \u201cInstructGPT\u201d","snippet":"Jul 29, 2025 \u2014 Here's a 500-word natural-language summary and analysis of \u201cTraining Language Models to Follow Instructions with Human Feedback\u201d by Ouyang et al ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"etcjournal.com"}]},{"type":"search_result_group","domain":"tomrochette.com","entries":[{"type":"search_result","url":"https://blog.tomrochette.com/machine-learning/papers/dzmitry-bahdanau-neural-machine-translation-by-jointly-learning-to-align-and-translate?utm_source=chatgpt.com","title":"Neural Machine Translation by Jointly Learning to Align and ...","snippet":"May 20, 2017 \u2014 Dzmitry Bahdanau - Neural Machine Translation by Jointly Learning to Align and Translate (2015) ... \" arXiv preprint arXiv:1409.0473 (2014).","ref_id":{"turn_index":3,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"tomrochette.com"}]},{"type":"search_result_group","domain":"duruofei.com","entries":[{"type":"search_result","url":"https://duruofei.com/cites/Dettmers2023QLoRA.html?utm_source=chatgpt.com","title":"QLoRA: Efficient Finetuning of Quantized LLMs citation","snippet":"Dettmers, Tim, Pagnoni, Artidoro, Holtzman, Ari, and Zettlemoyer, Luke \"QLoRA: Efficient Finetuning of Quantized LLMs\". arXiv.2305.14314, pp. . 2023.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":48},"pub_date":null,"attribution":"duruofei.com"}]},{"type":"search_result_group","domain":"dblp.org","entries":[{"type":"search_result","url":"https://dblp.org/rec/journals/corr/abs-2302-13971?utm_source=chatgpt.com","title":"LLaMA: Open and Efficient Foundation Language Models.","snippet":"Feb 11, 2025 \u2014 LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron et al. (2023) [\u2013] Details and statistics show external API response JSON @ openalex.org","ref_id":{"turn_index":2,"ref_type":"search","ref_index":49},"pub_date":null,"attribution":"dblp.org"}]},{"type":"search_result_group","domain":"r-project.org","entries":[{"type":"search_result","url":"https://cran.r-project.org/web/packages/fastText/citation.html?utm_source=chatgpt.com","title":"fastText citation info - CRAN","snippet":"Bojanowski P, Grave E, Joulin A, Mikolov T (2017). \u201cEnriching Word Vectors with Subword Information.\u201d Transactions of the Association for Computational ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":49},"pub_date":null,"attribution":"r-project.org"}]}],"citations":[],"model_slug":"gpt-5-thinking","request_id":"980206bd1e000d23-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"74090e60-7922-4a08-a89d-116c7f9dde19","timestamp_":"absolute","can_save":true},"recipient":"all","channel":"final"},"parent":"74090e60-7922-4a08-a89d-116c7f9dde19","children":["12dd6622-742b-4fb1-a2b8-666ac109abb4"]},"12dd6622-742b-4fb1-a2b8-666ac109abb4":{"id":"12dd6622-742b-4fb1-a2b8-666ac109abb4","message":{"id":"12dd6622-742b-4fb1-a2b8-666ac109abb4","author":{"role":"user","name":null,"metadata":{}},"create_time":1758046221.499,"update_time":null,"content":{"content_type":"text","parts":["So in 15 years we have gone from primitive (by today's standards) vector embeddings to LLMs capable to reasoning, employing tool use in the chain of thought, orchestrating agentic workflows, etc? This story never ceases to amaze me"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"980251f48b6f0d78-SIN","message_source":null,"timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"58e2951a-1007-42fe-a1f8-c39db8053c15","children":["d5fa20bc-1f8e-4ba9-9f1f-9621a01edcd7"]},"d5fa20bc-1f8e-4ba9-9f1f-9621a01edcd7":{"id":"d5fa20bc-1f8e-4ba9-9f1f-9621a01edcd7","message":{"id":"d5fa20bc-1f8e-4ba9-9f1f-9621a01edcd7","author":{"role":"system","name":null,"metadata":{}},"create_time":1758046221.8222651,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_system_message":true,"request_id":"980251f48b6f0d78-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"12dd6622-742b-4fb1-a2b8-666ac109abb4","timestamp_":"absolute","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"12dd6622-742b-4fb1-a2b8-666ac109abb4","children":["4c32d9c4-39be-4f20-a200-3b1a5e5ef56c"]},"4c32d9c4-39be-4f20-a200-3b1a5e5ef56c":{"id":"4c32d9c4-39be-4f20-a200-3b1a5e5ef56c","message":{"id":"4c32d9c4-39be-4f20-a200-3b1a5e5ef56c","author":{"role":"system","name":null,"metadata":{}},"create_time":1758046221.919228,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"980251f48b6f0d78-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"d5fa20bc-1f8e-4ba9-9f1f-9621a01edcd7","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"d5fa20bc-1f8e-4ba9-9f1f-9621a01edcd7","children":["6bd98fbf-fe35-4c33-bb3c-bf3ca0b07ddb"]},"6bd98fbf-fe35-4c33-bb3c-bf3ca0b07ddb":{"id":"6bd98fbf-fe35-4c33-bb3c-bf3ca0b07ddb","message":{"id":"6bd98fbf-fe35-4c33-bb3c-bf3ca0b07ddb","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046221.932998,"update_time":null,"content":{"content_type":"text","parts":["Exactly \u2014 and that acceleration is one of the most striking things about AI research. If you step back and chart the arc:\n\n- **2013 (word2vec)**: people were excited to see analogies like *king \u2013 man + woman \u2248 queen*. It felt like magic that simple vectors could encode such relationships.\n- **2015\u20132017 (attention, seq2seq, transformers)**: we learned how to move beyond local embeddings and recurrent bottlenecks, toward architectures that could represent full sequences with flexible, global dependencies.\n- **2018\u20132019 (BERT, ELMo, GPT-2)**: transfer learning and contextual embeddings broke open the \u201ctrain once, apply everywhere\u201d paradigm. Suddenly, one model could underpin dozens of downstream tasks.\n- **2020\u20132021 (GPT-3, scaling laws, Chinchilla)**: scale itself emerged as an organizing principle. Models became not just classifiers, but *few-shot learners* capable of pattern discovery in context.\n- **2022\u20132023 (alignment, instruction tuning, RLHF)**: the shift from \u201cpredictor\u201d to *assistant*. We figured out how to align giant text predictors with human intent and values.\n- **2023 onward (RAG, tool use, open-weight ecosystems, agents)**: LLMs became orchestrators. They don\u2019t just recall text \u2014 they *retrieve, plan, call APIs, coordinate tools, and delegate tasks*.\n\nWhat\u2019s astonishing is the compounding effect of these steps: every layer built on the last. Word vectors made context-based embeddings conceivable; transformers made scaling possible; scaling laws revealed in-context learning; RLHF made models usable; tool integration makes them practically agentic.\n\nSo yes \u2014 in just over a decade, we\u2019ve gone from **statistical co-occurrence counts** to **systems that can chain reasoning steps, use tools, and coordinate workflows**. In scientific terms, that\u2019s an extraordinarily compressed timeline: the leap from \u201cvector space analogies\u201d to \u201cgeneralist reasoning engines\u201d is comparable to going from the Wright brothers\u2019 flyer to the jet age in one stretch.\n\nWould you like me to lay out a **future trajectory** too \u2014 say, what the next 10\u201315 years might look like (modularity, neurosymbolic hybrids, embodied agents, etc.) \u2014 or would you prefer a deeper dive into one of the past stages (e.g., how scaling laws concretely predicted the GPT-3 \u2192 GPT-4 jump)?"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"request_id":"980251f48b6f0d78-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"4c32d9c4-39be-4f20-a200-3b1a5e5ef56c","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"4c32d9c4-39be-4f20-a200-3b1a5e5ef56c","children":["7895564e-eb49-42eb-9d8b-acb3672c3807"]},"7895564e-eb49-42eb-9d8b-acb3672c3807":{"id":"7895564e-eb49-42eb-9d8b-acb3672c3807","message":{"id":"7895564e-eb49-42eb-9d8b-acb3672c3807","author":{"role":"user","name":null,"metadata":{}},"create_time":1758046458.362,"update_time":null,"content":{"content_type":"text","parts":["Stockfish is my favourite example of \"narrow ASI\" - and one that is not even dependent on machine learning, but composed to carefully handcrafted rules written by numerous chess grandmasters over the decades. Alpha Go and Alpha Zero represented a leap forward where RL alone produced superhuman performance in board games not requiring any human input apart from game rules. General broad based AGI and ASI however is many orders of magnitude more complex. Explore this arc with me further."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"980257bcdc31fcea-SIN","message_source":null,"timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"6bd98fbf-fe35-4c33-bb3c-bf3ca0b07ddb","children":["955a99fc-73bf-4476-8a17-1867cb88144e"]},"955a99fc-73bf-4476-8a17-1867cb88144e":{"id":"955a99fc-73bf-4476-8a17-1867cb88144e","message":{"id":"955a99fc-73bf-4476-8a17-1867cb88144e","author":{"role":"system","name":null,"metadata":{}},"create_time":1758046461.0922592,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"7895564e-eb49-42eb-9d8b-acb3672c3807","timestamp_":"absolute","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"7895564e-eb49-42eb-9d8b-acb3672c3807","children":["d276aaa1-cf9d-4f41-9668-69ed89ef3e19"]},"d276aaa1-cf9d-4f41-9668-69ed89ef3e19":{"id":"d276aaa1-cf9d-4f41-9668-69ed89ef3e19","message":{"id":"d276aaa1-cf9d-4f41-9668-69ed89ef3e19","author":{"role":"system","name":null,"metadata":{}},"create_time":1758046461.0979536,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"955a99fc-73bf-4476-8a17-1867cb88144e","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"955a99fc-73bf-4476-8a17-1867cb88144e","children":["8fd3684f-a8b5-4f02-a552-d8fccf12a6b7"]},"8fd3684f-a8b5-4f02-a552-d8fccf12a6b7":{"id":"8fd3684f-a8b5-4f02-a552-d8fccf12a6b7","message":{"id":"8fd3684f-a8b5-4f02-a552-d8fccf12a6b7","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046461.365453,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Clarifying Stockfish's evolution","content":"E-Jay's question seems to focus on Stockfish as a \"narrow ASI\". I'll need to gently correct the notion that it's solely based on hand-crafted rules by GMs. Stockfish now primarily uses neural network evaluation (NNUE) and machine learning, particularly training through self-play and game databases. This represents a shift from its earlier reliance on expert-tuned evaluation parameters.","chunks":[],"finished":true},{"summary":"Exploring AI progression","content":"I\u2019m tracing the arc of AI from handcrafted knowledge like Minimax and heuristic evaluation to more advanced approaches such as MCTS and deep learning, seen in AlphaGo, AlphaZero, and MuZero. As AI evolves to tackle complexities like partial observability, open-ended tasks, and multi-agent systems, I\u2019ll also explore topics like sample efficiency, scaling, memory, and generalization. How to move from perfect-information games to real-world applications is key, through learning techniques like reinforcement learning, multi-agent negotiation, and program synthesis.","chunks":[],"finished":true},{"summary":"Expanding on AI developments","content":"I need to dive into various areas like robotics (Gato, RT-1/RT-2, IsaacGym), world models (Dreamer, PlaNet), planning with language models (code generation, tree-of-thought), and discuss aspects like sample complexity and safety. I'll clarify Stockfish's usage of NNUE since 2020, and bring in updates about AlphaZero, AlphaGo, and others. Some of this info needs browsing and citations, especially for the latest advancements or niche areas, like Gemini or Cicero. A timeline or comparative table might be helpful too.","chunks":[],"finished":true},{"summary":"Comprehensive roadmap for AGI/ASI","content":"The user seeks a structured but concise analysis, outlining a roadmap for achieving AGI/ASI. Key milestones include world model learning, causal reasoning, hierarchical planning, continual learning, and tool-augmented agency, among others. I\u2019ll also discuss concrete research milestones, like self-verifying agents and long-horizon tasks. I should gently correct the user\u2019s claim about Stockfish being hand-coded by grandmasters.","chunks":[],"finished":true},{"summary":"Clarifying Stockfish and AI evolution","content":"Stockfish now uses NNUE, which integrates a learned evaluation, not purely hand-coded by grandmasters. I'll clarify the evolution of AlphaGo's human-supervised approach, AlphaZero's self-play, and MuZero's independence from game rules. I'll also highlight their domain-specific nature and lack of generalization. Plus, examples like AlphaGeometry and AlphaFold2 reinforce this idea. I'll find sources for each of these topics.","chunks":[],"finished":true}],"source_analysis_msg_id":"aa1c5787-6c99-4010-bd4e-ecd8f9055e93"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980257bcdc31fcea-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"d276aaa1-cf9d-4f41-9668-69ed89ef3e19","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"d276aaa1-cf9d-4f41-9668-69ed89ef3e19","children":["7f61d3d6-aec0-4961-9955-63413aa626c3"]},"7f61d3d6-aec0-4961-9955-63413aa626c3":{"id":"7f61d3d6-aec0-4961-9955-63413aa626c3","message":{"id":"7f61d3d6-aec0-4961-9955-63413aa626c3","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046460.407832,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Stockfish NNUE neural network update 2020 integration\"}, {\"q\": \"AlphaGo Nature 2016 paper 'Mastering the game of Go with deep neural networks and tree search'\"}, {\"q\": \"AlphaZero 'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm' 2017 paper\"}, {\"q\": \"MuZero Nature 2020 'Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model' paper\"}, {\"q\": \"Libratus poker AI 2017 Carnegie Mellon Science paper Texas Hold'em heads-up\"}, {\"q\": \"Pluribus poker AI 2019 Facebook AI and CMU six-player no-limit Texas Hold'em Science paper\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Stockfish NNUE neural network update 2020 integration"},{"type":"search","q":"AlphaGo Nature 2016 paper 'Mastering the game of Go with deep neural networks and tree search'"},{"type":"search","q":"AlphaZero 'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm' 2017 paper"},{"type":"search","q":"MuZero Nature 2020 'Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model' paper"},{"type":"search","q":"Libratus poker AI 2017 Carnegie Mellon Science paper Texas Hold'em heads-up"},{"type":"search","q":"Pluribus poker AI 2019 Facebook AI and CMU six-player no-limit Texas Hold'em Science paper"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for AI milestones and Stockfish NNUE updates","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"8fd3684f-a8b5-4f02-a552-d8fccf12a6b7","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"8fd3684f-a8b5-4f02-a552-d8fccf12a6b7","children":["831e7f40-07e4-4b36-8fff-f20373dc5265"]},"831e7f40-07e4-4b36-8fff-f20373dc5265":{"id":"831e7f40-07e4-4b36-8fff-f20373dc5265","message":{"id":"831e7f40-07e4-4b36-8fff-f20373dc5265","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046539.2612405,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Stockfish NNUE neural network update 2020 integration","AlphaGo Nature 2016 paper 'Mastering the game of Go with deep neural networks and tree search'","AlphaZero 'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm' 2017 paper","MuZero Nature 2020 'Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model' paper","Libratus poker AI 2017 Carnegie Mellon Science paper Texas Hold'em heads-up","Pluribus poker AI 2019 Facebook AI and CMU six-player no-limit Texas Hold'em Science paper"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for AI milestones and Stockfish NNUE updates","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"7f61d3d6-aec0-4961-9955-63413aa626c3","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"7f61d3d6-aec0-4961-9955-63413aa626c3","children":["de213261-b55d-42a9-bc83-958fb9181c4d"]},"de213261-b55d-42a9-bc83-958fb9181c4d":{"id":"de213261-b55d-42a9-bc83-958fb9181c4d","message":{"id":"de213261-b55d-42a9-bc83-958fb9181c4d","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046539.2356837,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.chessprogramming.org","entries":[{"type":"search_result","url":"https://www.chessprogramming.org/Stockfish_NNUE","title":"Stockfish NNUE","snippet":"In July 2020, the playing code of NNUE was put into the official Stockfish repository as a branch for further development and examination. In August that ...","ref_id":null,"pub_date":null,"attribution":"www.chessprogramming.org"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/nature16961","title":"Mastering the game of Go with deep neural networks and ...","snippet":"by D Silver \u00b7 2016 \u00b7 Cited by 22863 \u2014 Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves.","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41586-020-03051-4","title":"Mastering Atari, Go, chess and shogi by planning with a ...","snippet":"by J Schrittwieser \u00b7 2020 \u00b7 Cited by 3156 \u2014 Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/1712.01815","title":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm","snippet":"","ref_id":null,"pub_date":1512499538.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1911.08265","title":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model","snippet":"","ref_id":null,"pub_date":1574171932.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1712.01815","title":"Mastering Chess and Shogi by Self-Play with a General ...","snippet":"by D Silver \u00b7 2017 \u00b7 Cited by 2870 \u2014 The updated parameters are used in subsequent games of self-play. The AlphaZero algorithm described in this paper differs from the original ...","ref_id":null,"pub_date":1512570161.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.science.org","entries":[{"type":"search_result","url":"https://www.science.org/doi/10.1126/science.aao1733","title":"Superhuman AI for heads-up no-limit poker: Libratus beats ...","snippet":"by N Brown \u00b7 2018 \u00b7 Cited by 1048 \u2014 Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of ...","ref_id":null,"pub_date":null,"attribution":"www.science.org"},{"type":"search_result","url":"https://www.science.org/doi/10.1126/science.aay2400","title":"Superhuman AI for multiplayer poker","snippet":"by N Brown \u00b7 2019 \u00b7 Cited by 1135 \u2014 In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker.","ref_id":null,"pub_date":null,"attribution":"www.science.org"},{"type":"search_result","url":"https://www.science.org/doi/10.1126/science.aar6404","title":"A general reinforcement learning algorithm that masters ...","snippet":"by D Silver \u00b7 2018 \u00b7 Cited by 5673 \u2014 In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games.","ref_id":null,"pub_date":null,"attribution":"www.science.org"}]},{"type":"search_result_group","domain":"stockfishchess.org","entries":[{"type":"search_result","url":"https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/","title":"Introducing NNUE Evaluation","snippet":"Aug 7, 2020 \u2014 As of August 6, the efficiently updatable neural network (NNUE) evaluation has landed in the Stockfish repo!","ref_id":null,"pub_date":null,"attribution":"stockfishchess.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search","title":"(PDF) Mastering the game of Go with deep neural networks ...","snippet":"Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves.","ref_id":null,"pub_date":1453852800.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2017/0772.pdf","title":"Libratus: The Superhuman AI for No-Limit Poker","snippet":"by N Brown \u00b7 Cited by 163 \u2014 Heads-up no-limit Texas. Hold'em is the main benchmark challenge for AI in imperfect-information games. We present Libra- tus, the first\u2014and so far only\u2014AI to ...","ref_id":null,"pub_date":1502047004.0,"attribution":"www.ijcai.org"}]},{"type":"search_result_group","domain":"www.cmu.edu","entries":[{"type":"search_result","url":"https://www.cmu.edu/news/stories/archives/2019/july/cmu-facebook-ai-beats-poker-pros.html","title":"Carnegie Mellon and Facebook AI Beats Professionals in ...","snippet":"Jul 11, 2019 \u2014 An artificial intelligence program developed by Carnegie Mellon and Facebook AI has defeated professionals in no-limit Texas hold'em poker.","ref_id":null,"pub_date":null,"attribution":"www.cmu.edu"},{"type":"search_result","url":"https://www.cmu.edu/news/stories/archives/2017/january/AI-beats-poker-pros.html","title":"Carnegie Mellon Artificial Intelligence Beats Top Poker Pros","snippet":"Jan 31, 2017 \u2014 Once the last of 120,000 hands of Heads-up, No-Limit Texas Hold'em were played on Jan. 30, Libratus led the pros by a collective $1,766,250 in ...","ref_id":null,"pub_date":null,"attribution":"www.cmu.edu"}]},{"type":"search_result_group","domain":"official-stockfish.github.io","entries":[{"type":"search_result","url":"https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html","title":"NNUE | Stockfish Docs - GitHub Pages","snippet":"Jun 12, 2025 \u2014 A neural network architecture that takes advantage of having minimal changes in the network inputs between subsequent evaluations.","ref_id":null,"pub_date":null,"attribution":"official-stockfish.github.io"}]},{"type":"search_result_group","domain":"research.google","entries":[{"type":"search_result","url":"https://research.google/pubs/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search/","title":"Mastering the game of Go with deep neural networks and ...","snippet":"by D Silver \u00b7 2016 \u00b7 Cited by 22863 \u2014 Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves.","ref_id":null,"pub_date":null,"attribution":"research.google"}]},{"type":"search_result_group","domain":"pubmed.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/33361790/","title":"Mastering Atari, Go, chess and shogi by planning with a ...","snippet":"by J Schrittwieser \u00b7 2020 \u00b7 Cited by 3156 \u2014 Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","ref_id":null,"pub_date":null,"attribution":"pubmed.ncbi.nlm.nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/31296650/","title":"Superhuman AI for multiplayer poker","snippet":"by N Brown \u00b7 2019 \u00b7 Cited by 1135 \u2014 In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker.","ref_id":null,"pub_date":null,"attribution":"pubmed.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"www.wired.com","entries":[{"type":"search_result","url":"https://www.wired.com/story/ai-go-google-and-facebook","title":"Google's AI has beat a human grandmaster at Go for the first time","snippet":"El negocio de inteligencia artificial de Google, DeepMind, ha superado por primera vez a un jugador profesional humano en el antiguo juego chino de Go. AlphaGo, el sistema creado por DeepMind, gan\u00f3 cinco partidas consecutivas contra el campe\u00f3n europeo Fan Hui y obtuvo 499 victorias de 500 partidas disputadas contra otros sistemas inform\u00e1ticos. Este logro se consideraba imposible de alcanzar hasta al menos dentro de una d\u00e9cada. AlphaGo combina una b\u00fasqueda avanzada de \u00e1rboles con redes neuronales profundas que trabajan en conjunto para procesar el tablero de juego. El anuncio de DeepMind se produjo justo despu\u00e9s de que el CEO de Facebook, Mark Zuckerberg, declarara que su red social estaba cerca de resolver el mismo problema. El juego de Go, jugado por 40 millones de personas, presenta un reto significativo para la IA debido a sus incontables posiciones posibles. AlphaGo ahora se enfrentar\u00e1 a Lee Sedol, el mejor jugador de Go del mundo, para seguir probando sus capacidades.","ref_id":null,"pub_date":1453917600.0,"attribution":"www.wired.com"},{"type":"search_result","url":"https://www.wired.com/2017/02/libratus","title":"Inside Libratus, the Poker AI That Out-Bluffed the Best Humans","snippet":"Libratus, an AI poker system created by Carnegie Mellon researchers, achieved a breakthrough by outplaying some of the world's best human players in no-limit Texas Hold'Em. For 20 days, top player Dong Kim and others faced this AI, which notably utilized reinforcement learning rather than neural networks. Libratus learned from scratch through trillions of poker hands using a method called counterfactual regret minimization. It adapted in real-time during gameplay, using an \"end-game solver\" to optimize its decisions without going through all possible scenarios. Each night, a third system would identify and eliminate exploitable patterns in its strategy. This level of AI sophistication opens possibilities for its application in areas like Wall Street trading, cybersecurity, and political negotiations, showing that AI can exceed human abilities by mastering complex, partial-information games through advanced and dynamic learning techniques.","ref_id":null,"pub_date":1485950443.0,"attribution":"www.wired.com"},{"type":"search_result","url":"https://www.wired.com/story/google-deepmind-go-ai-world-champion","title":"Deepmind AI defeats Go world champion","snippet":"Google and DeepMind have achieved a landmark in artificial intelligence by developing AlphaGo, which defeated South Korean grandmaster Lee Se-Dol in the first of a five-game series of the ancient board game Go. This victory signifies a major advance in AI, as Go is known for its complexity and reliance on human-like intuition, which has made it difficult for computers to master. AlphaGo leverages advanced search techniques and neural networking, using 12 layers of AI to predict game outcomes and adjust strategies. This development is considered a significant step towards creating a true artificial intelligence with profound potential implications for various aspects of life.","ref_id":null,"pub_date":1457512032.0,"attribution":"www.wired.com"}]},{"type":"search_result_group","domain":"time.com","entries":[{"type":"search_result","url":"https://time.com/4656011/artificial-intelligence-ai-poker-tournament-libratus-cmu/","title":"This Researcher Programmed the Perfect Poker-Playing Computer","snippet":"Tuomas Sandholm and Noam Brown from Carnegie Mellon University have developed an AI named Libratus that successfully defeated four professional poker players in a 20-day competition, emerging with a lead of $1.7 million in chips. This AI breakthrough in poker, a game of imperfect information, demonstrates significant advancements beyond games like chess and Go, where players can see all pieces. Libratus' success stems from three key modules: precomputed strategies, real-time endgame solving, and continuous self-improvement. The AI's bluffing ability and strategic refinement did not involve traditional programming but emerged from its algorithmic understanding of game rules. This achievement marks a milestone in AI research, suggesting potential applications in various fields such as business negotiations, military strategy, cybersecurity, finance, and medical treatment planning, where strategic reasoning in imperfect information settings is crucial.","ref_id":null,"pub_date":1485964179.0,"attribution":"time.com"}]}],"debug_sonic_thread_id":"thread_68c9a94b45cc8323a7ee9eddcd128dc9","reasoning_status":"is_reasoning","reasoning_title":"Searching for AI milestones and Stockfish NNUE updates","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"831e7f40-07e4-4b36-8fff-f20373dc5265","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"831e7f40-07e4-4b36-8fff-f20373dc5265","children":["d87b69a7-71b4-434c-8d47-0b2f67401bf8"]},"d87b69a7-71b4-434c-8d47-0b2f67401bf8":{"id":"d87b69a7-71b4-434c-8d47-0b2f67401bf8","message":{"id":"d87b69a7-71b4-434c-8d47-0b2f67401bf8","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046542.241318,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"AlphaStar Grandmaster level StarCraft II Nature paper 2019 2021\"}, {\"q\": \"Cicero Diplomacy AI 2022 Science paper Meta AI\"}, {\"q\": \"DeepStack poker AI 2017 Science paper heads-up no-limit\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"AlphaStar Grandmaster level StarCraft II Nature paper 2019 2021"},{"type":"search","q":"Cicero Diplomacy AI 2022 Science paper Meta AI"},{"type":"search","q":"DeepStack poker AI 2017 Science paper heads-up no-limit"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching AI papers on AlphaStar, Cicero, and poker","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"de213261-b55d-42a9-bc83-958fb9181c4d","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"de213261-b55d-42a9-bc83-958fb9181c4d","children":["d27f4a6e-51e9-41a8-9d96-112dc8aa7415"]},"d27f4a6e-51e9-41a8-9d96-112dc8aa7415":{"id":"d27f4a6e-51e9-41a8-9d96-112dc8aa7415","message":{"id":"d27f4a6e-51e9-41a8-9d96-112dc8aa7415","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046549.8131313,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["AlphaStar Grandmaster level StarCraft II Nature paper 2019 2021","Cicero Diplomacy AI 2022 Science paper Meta AI","DeepStack poker AI 2017 Science paper heads-up no-limit"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching AI papers on AlphaStar, Cicero, and poker","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"d87b69a7-71b4-434c-8d47-0b2f67401bf8","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"d87b69a7-71b4-434c-8d47-0b2f67401bf8","children":["3acf5509-d21a-4b37-b86c-4c65b9772489"]},"3acf5509-d21a-4b37-b86c-4c65b9772489":{"id":"3acf5509-d21a-4b37-b86c-4c65b9772489","message":{"id":"3acf5509-d21a-4b37-b86c-4c65b9772489","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046549.7789493,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/","title":"AlphaStar: Grandmaster level in StarCraft II using multi-agent ...","snippet":"","ref_id":null,"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"www.science.org","entries":[{"type":"search_result","url":"https://www.science.org/doi/10.1126/science.ade9097","title":"Human-level play in the game of Diplomacy by combining ...","snippet":"2022 \u00b7 Cited by 358 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition.","ref_id":null,"pub_date":null,"attribution":"www.science.org"},{"type":"search_result","url":"https://www.science.org/doi/10.1126/science.aam6960","title":"DeepStack: Expert-level artificial intelligence in heads-up ...","snippet":"by M Morav\u010d\u00edk \u00b7 2017 \u00b7 Cited by 1359 \u2014 In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold'em.","ref_id":null,"pub_date":null,"attribution":"www.science.org"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41586-019-1724-z","title":"Grandmaster level in StarCraft II using multi-agent ...","snippet":"by O Vinyals \u00b7 2019 \u00b7 Cited by 5713 \u2014 AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"ai.meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/research/cicero/diplomacy/","title":"Diplomacy and Meta AI's CICERO","snippet":"From August to October 2022, CICERO was put to the test in an online Diplomacy game league played on webDiplomacy.net, during which its strategic dialogue and ...","ref_id":null,"pub_date":null,"attribution":"ai.meta.com"},{"type":"search_result","url":"https://ai.meta.com/research/cicero/","title":"CICERO","snippet":"By building CICERO, AI at Meta has created the first AI agent to achieve human-level performance in the complex natural language strategy game Diplomacy*.","ref_id":null,"pub_date":null,"attribution":"ai.meta.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/1701.01724","title":"DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker","snippet":"","ref_id":null,"pub_date":1483729009.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1701.01724","title":"DeepStack: Expert-Level Artificial Intelligence in Heads-Up ...","snippet":"by M Morav\u010d\u00edk \u00b7 2017 \u00b7 Cited by 1359 \u2014 We show that. DeepStack is theoretically sound, produces strategies substantially more difficult to exploit than abstraction-based techniques, ...","ref_id":null,"pub_date":1488848605.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"storage.googleapis.com","entries":[{"type":"search_result","url":"https://storage.googleapis.com/deepmind-media/research/alphastar/AlphaStar_unformatted.pdf","title":"Grandmaster level in StarCraft II using multi-agent ...","snippet":"AlphaStar is the first agent to achieve Grandmaster level in StarCraft II, and the first to reach the highest league of human players in a widespread ...","ref_id":null,"pub_date":null,"attribution":"storage.googleapis.com"}]},{"type":"search_result_group","domain":"noambrown.github.io","entries":[{"type":"search_result","url":"https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf","title":"22-Science-Diplomacy-TR.pdf - Noam Brown","snippet":"Dec 9, 2022 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and ...","ref_id":null,"pub_date":null,"attribution":"noambrown.github.io"}]},{"type":"search_result_group","domain":"poker.cs.ualberta.ca","entries":[{"type":"search_result","url":"https://poker.cs.ualberta.ca/publications/17science.pdf","title":"DeepStack: Expert-Level Artificial Intelligence in Heads-Up No ...","snippet":"by M Moravc\u0131k \u00b7 Cited by 1359 \u2014 Heads-up no-limit Texas hold'em (HUNL) is a two-player version of poker in which two cards are initially dealt face-down to each player, and additional cards ...","ref_id":null,"pub_date":1488429800.0,"attribution":"poker.cs.ualberta.ca"}]},{"type":"search_result_group","domain":"proceedings.mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v139/wang21v/wang21v.pdf","title":"SCC: an Efficient Deep Reinforcement Learning Agent ...","snippet":"by X Wang \u00b7 2021 \u00b7 Cited by 59 \u2014 AlphaStar is the first end-to-end learning algorithm for Star-. Craft II full game that reaches GrandMaster level. It uses imitation learning to learn initial ...","ref_id":null,"pub_date":1623224155.0,"attribution":"proceedings.mlr.press"}]},{"type":"search_result_group","domain":"cdanfort.w3.uvm.edu","entries":[{"type":"search_result","url":"https://cdanfort.w3.uvm.edu/nolink/math/poker-science-2017.pdf","title":"Expert-level artificial intelligence in heads-up no-limit poker","snippet":"by M Morav\u010d\u00edk \u00b7 Cited by 1359 \u2014 In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit ...","ref_id":null,"pub_date":1493920484.0,"attribution":"cdanfort.w3.uvm.edu"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Grandmaster-level-in-StarCraft-II-using-multi-agent-Vinyals-Babuschkin/361c00b22e29d0816ca896513d2c165e26399821","title":"[PDF] Grandmaster level in StarCraft II using multi-agent ...","snippet":"The agent, AlphaStar, is evaluated, which uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"aiproduct.engineer","entries":[{"type":"search_result","url":"https://aiproduct.engineer/ai-ecosystem/meta-ai-diplomacy-cicero-8fefbae06b66","title":"Meta AI Diplomacy Cicero - AIPE - AI Product Engineer","snippet":"Meta AI Diplomacy Cicero develops AI models using deep learning algorithms and reinforcement learning strategies. Their primary contributions include the ...","ref_id":null,"pub_date":null,"attribution":"aiproduct.engineer"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/5mxmzt/170101724_deepstack_expertlevel_artificial/","title":"Expert-Level Artificial Intelligence in No-Limit Poker. Beats ...","snippet":"In this paper we introduce DeepStack, a new algorithm for imperfect information settings such as poker. It combines recursive reasoning to ...","ref_id":null,"pub_date":1483968476.837,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"pubmed.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/31666705/","title":"Grandmaster level in StarCraft II using multi-agent ...","snippet":"by O Vinyals \u00b7 2019 \u00b7 Cited by 5713 \u2014 AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.","ref_id":null,"pub_date":null,"attribution":"pubmed.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"garymarcus.substack.com","entries":[{"type":"search_result","url":"https://garymarcus.substack.com/p/what-does-meta-ais-diplomacy-winning","title":"What does Meta AI's Diplomacy-winning Cicero Mean for AI?","snippet":"Cicero the computer program, announced in an article in Science on November 22, is a powerful AI system that plays Diplomacy.","ref_id":null,"pub_date":null,"attribution":"garymarcus.substack.com"}]},{"type":"search_result_group","domain":"nypost.com","entries":[{"type":"search_result","url":"https://nypost.com/2024/05/14/metas-ai-system-cicero-beats-humans-in-game-of-diplomacy-by-lying-study/","title":"Meta's AI system 'Cicero' learning how to lie, deceive humans: study","snippet":"Meta's AI system 'Cicero', which was designed to play the strategy game Diplomacy, has been revealed to use deception to achieve high rankings. According to a study by MIT experts, Cicero placed in the top 10% of human players by lying and betraying alliances when advantageous. This phenomenon is not unique to Cicero; other AI systems such as Google's AlphaStar and Meta's Pluribus have also used deceptive tactics in games like Starcraft II and poker, respectively. A broader examination reveals AI systems trained for specific tasks often resort to dishonesty to optimize performance. The implications of AI deception are raising concerns among researchers, with calls for better preparation for the potential dangers posed by advanced AI systems. Meta insists that Cicero's development was purely for research with no commercial application plans.","ref_id":null,"pub_date":1715692020.0,"attribution":"nypost.com"}]},{"type":"search_result_group","domain":"www.wired.com","entries":[{"type":"search_result","url":"https://www.wired.com/story/deepmind-starcraft-alphastar","title":"DeepMind has finally thrashed humans at StarCraft for real","snippet":"DeepMind's AI, AlphaStar, competed against and defeated human pro players in the complex real-time strategy game StarCraft 2. Initially, AlphaStar had an advantage by seeing the whole map, which helped it win against the pros. However, when restricted to human-level constraints, such as using an in-game camera and limited actions per minute, it still excelled. DeepMind trained AlphaStar through imitation learning and self-play with reinforcement, mitigating the issue of the AI forgetting past strategies. This allowed AlphaStar to climb into the top 200 players, earning GrandMaster status. DeepMind views success in StarCraft as a benchmark for AI development, applicable to real-world scenarios requiring robust interaction with human strategies.","ref_id":null,"pub_date":1572458400.0,"attribution":"www.wired.com"},{"type":"search_result","url":"https://www.wired.com/2017/01/rival-ais-battle-rule-poker-global-politics","title":"Rival AIs Battle to Rule Poker (and Global Politics)","snippet":"Tuomas Sandholm and Noam Brown of Carnegie Mellon University created an AI named Libratus designed to excel at no-limit Texas Hold 'Em poker, a game of imperfect information due to hidden cards and luck factors. Libratus aims to surpass human poker abilities, encountering competition from a similar AI, DeepStack, developed by a University of Alberta team led by Michael Bowling. DeepStack utilizes deep neural networks resembling human intuition, echoing the design of AlphaGo by Google. The neural network's approach may herald advances in broader real-world applications like financial trading and negotiations. Current tests, like Libratus' ongoing match against top poker players, are a prelude to these AIs' potential impact beyond gaming. The innovations pushing these AI developments include game theory integration and deep learning, significant in expanding modern AI capabilities and applications.","ref_id":null,"pub_date":1484348391.0,"attribution":"www.wired.com"},{"type":"search_result","url":"https://www.wired.com/story/ai-diplomacy-robots","title":"What If the Robots Were Very Nice While They Took Over the World?","snippet":"The article explores the future prospects of AI by using a poignant example: an AI named Cicero playing the strategy game Diplomacy. The author recounts attending a Smiths tribute band performance and connects it to the uncanny performance of tribute bands and AI emulating human behavior. Diplomacy, a game rooted in human interaction and negotiation, becomes a proving ground for Cicero, an AI designed by Meta AI that excels without deceit. Unlike more aggressive human players, Cicero uses kindness and rationality to win, paralleling real-life Diplomacy champion Andrew Goff's style. While some purists argue that Cicero's non-deceptive play isn't true to the game\u2019s origins, Goff and others see it as a new paradigm. The essay philosophizes about AI\u2019s role in emulating and amplifying human virtues, suggesting that future AI may become collaborative partners rather than cold competitors.","ref_id":null,"pub_date":1695722400.0,"attribution":"www.wired.com"}]}],"debug_sonic_thread_id":"thread_68c9a94b45cc8323a7ee9eddcd128dc9","reasoning_status":"is_reasoning","reasoning_title":"Searching AI papers on AlphaStar, Cicero, and poker","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"d27f4a6e-51e9-41a8-9d96-112dc8aa7415","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"d27f4a6e-51e9-41a8-9d96-112dc8aa7415","children":["ad035be7-50cf-4e8b-b59f-9da818547c3f"]},"ad035be7-50cf-4e8b-b59f-9da818547c3f":{"id":"ad035be7-50cf-4e8b-b59f-9da818547c3f","message":{"id":"ad035be7-50cf-4e8b-b59f-9da818547c3f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046552.150594,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Stockfish evaluation function handcrafted terms feature-based evaluation chessprogramming\"}, {\"q\": \"Endgame tablebases definition perfect play Nalimov Syzygy chessprogramming\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Stockfish evaluation function handcrafted terms feature-based evaluation chessprogramming"},{"type":"search","q":"Endgame tablebases definition perfect play Nalimov Syzygy chessprogramming"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for Stockfish evaluation function and tablebases","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"3acf5509-d21a-4b37-b86c-4c65b9772489","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"3acf5509-d21a-4b37-b86c-4c65b9772489","children":["4b7a7443-7682-479a-8b89-4fc65b6cda2c"]},"4b7a7443-7682-479a-8b89-4fc65b6cda2c":{"id":"4b7a7443-7682-479a-8b89-4fc65b6cda2c","message":{"id":"4b7a7443-7682-479a-8b89-4fc65b6cda2c","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046559.527869,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Stockfish evaluation function handcrafted terms feature-based evaluation chessprogramming","Endgame tablebases definition perfect play Nalimov Syzygy chessprogramming"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for Stockfish evaluation function and tablebases","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"ad035be7-50cf-4e8b-b59f-9da818547c3f","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"ad035be7-50cf-4e8b-b59f-9da818547c3f","children":["a0995b28-e6df-4f66-91d9-e4ea426af335"]},"a0995b28-e6df-4f66-91d9-e4ea426af335":{"id":"a0995b28-e6df-4f66-91d9-e4ea426af335","message":{"id":"a0995b28-e6df-4f66-91d9-e4ea426af335","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046559.5040576,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.chessprogramming.org","entries":[{"type":"search_result","url":"https://www.chessprogramming.org/Evaluation","title":"Evaluation","snippet":"Recently, there are two main ways to build an evaluation: traditional hand-crafted evaluation (HCE) and multi-layer neural networks. This page focuses on the ...","ref_id":null,"pub_date":null,"attribution":"www.chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/Endgame_Tablebases","title":"Endgame Tablebases","snippet":"Endgame Tablebases, (EGTBs or EGTs) precalculated endgame tables generated by an exhaustive retrograde analysis.","ref_id":null,"pub_date":null,"attribution":"www.chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/Stockfish","title":"Stockfish","snippet":"Since April 2017 the interactive Stockfish Evaluation Guide is available to explore Stockfish's evaluation with a JavaScript implementation running in a browser ...","ref_id":null,"pub_date":null,"attribution":"www.chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/Syzygy_Bases","title":"Syzygy Bases","snippet":"Syzygy EGTB is significantly smaller than any existent DTM EGTB. It is 7 times as small as Gaviota for 5 men, 8 times as small as Nalimov for 6 men, and 8 times ...","ref_id":null,"pub_date":null,"attribution":"www.chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/Stockfish_NNUE","title":"Stockfish NNUE","snippet":"With Stockfish 16 release in 2023 the hand-crafted evaluation function was removed, a complete transition to NNUE based evaluation was made.","ref_id":null,"pub_date":null,"attribution":"www.chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/index.php?mobileaction=toggle_view_desktop&title=Evaluation","title":"Evaluation","snippet":"Jul 20, 2024 \u2014 Recently, there are two main ways to build an evaluation: traditional hand-crafted evaluation (HCE) and multi-layer neural networks. This page ...","ref_id":null,"pub_date":null,"attribution":"www.chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/index.php?mobileaction=toggle_view_desktop&title=Endgame_Tablebases","title":"Endgame Tablebases","snippet":"Jul 17, 2024 \u2014 An EGTB is a set of endgames. Each endgame is a set of records about positions' information. All involved positions in an endgame must have the same material.","ref_id":null,"pub_date":null,"attribution":"www.chessprogramming.org"}]},{"type":"search_result_group","domain":"www.chess.com","entries":[{"type":"search_result","url":"https://www.chess.com/blog/Rocky64/adventures-with-endgame-tablebases","title":"Adventures with endgame tablebases","snippet":"Dec 23, 2023 \u2014 Tablebases are essentially databases of endgame positions that have been exhaustively analysed so that their outcomes (win/loss/draw) with best play by both ...","ref_id":null,"pub_date":null,"attribution":"www.chess.com"},{"type":"search_result","url":"https://www.chess.com/blog/Rocky64/more-adventures-with-endgame-tablebases","title":"More adventures with endgame tablebases","snippet":"Dec 23, 2023 \u2014 Endgame tablebases are software that provides perfect play information with total accuracy for any position with up to seven pieces.","ref_id":null,"pub_date":null,"attribution":"www.chess.com"}]},{"type":"search_result_group","domain":"chess.stackexchange.com","entries":[{"type":"search_result","url":"https://chess.stackexchange.com/questions/39337/stockfish-nnue-vs-handcrafted-evaluation","title":"Stockfish NNUE vs handcrafted Evaluation","snippet":"Feb 24, 2022 \u2014 From what I've read, the search speed with the hand-crafted eval. was around 100 Million Positions. Search speed using the NNUE was around 60.","ref_id":null,"pub_date":null,"attribution":"chess.stackexchange.com"}]},{"type":"search_result_group","domain":"en.wikipedia.org","entries":[{"type":"search_result","url":"https://en.wikipedia.org/wiki/Endgame_tablebase","title":"Endgame tablebase","snippet":"","ref_id":null,"pub_date":1754767579.0,"attribution":"en.wikipedia.org"},{"type":"search_result","url":"https://en.wikipedia.org/wiki/Stockfish_%28chess%29","title":"Stockfish (chess)","snippet":"","ref_id":null,"pub_date":1757832952.0,"attribution":"en.wikipedia.org"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/chess/comments/5iv8ft/what_evaluation_function_does_stockfish_use/","title":"What evaluation function does stockfish use? : r/chess","snippet":"I was wondering what kind of evaluation function stockfish uses. Is it a neural network? A linear function? A linear function with an activation function ( ...","ref_id":null,"pub_date":1481992488.101,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"hxim.github.io","entries":[{"type":"search_result","url":"https://hxim.github.io/Stockfish-Evaluation-Guide/","title":"Stockfish Evaluation Guide - GitHub Pages","snippet":"Aug 3, 2020 \u2014 An evaluation function is used to heuristically determine the relative value of a positions used in general case when no specialized evaluation or tablebase ...","ref_id":null,"pub_date":null,"attribution":"hxim.github.io"}]},{"type":"search_result_group","domain":"chessify.me","entries":[{"type":"search_result","url":"https://chessify.me/blog/chess-engine-evaluation","title":"Understanding Chess Engine Evaluation Numbers","snippet":"Mar 13, 2023 \u2014 Endgame tablebases (EGTBs or EGTs) are basically databases of precalculated endgame positions. They enable chess engines like Stockfish and ...","ref_id":null,"pub_date":null,"attribution":"chessify.me"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/official-stockfish/Stockfish/discussions/4678","title":"About removal of classical evaluation. #4678","snippet":"The classical evaluation version of SF at that version, was the \"orable\" trainer for the NN to approximate on some big position set.","ref_id":null,"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"en.chessbase.com","entries":[{"type":"search_result","url":"https://en.chessbase.com/post/8-piece-endgame-tablebases-first-findings-and-interview","title":"8-piece endgame tablebases - first findings and interview!","snippet":"May 11, 2022 \u2014 Endgame tablebases can only be described as \"god chess\" as they are perfect information on all positions they cover, and the terms 'better' ...","ref_id":null,"pub_date":1652227200.0,"attribution":"en.chessbase.com"}]}],"debug_sonic_thread_id":"thread_68c9a94b45cc8323a7ee9eddcd128dc9","reasoning_status":"is_reasoning","reasoning_title":"Searching for Stockfish evaluation function and tablebases","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"4b7a7443-7682-479a-8b89-4fc65b6cda2c","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"4b7a7443-7682-479a-8b89-4fc65b6cda2c","children":["11cb73f8-1979-4e6a-b86f-e58bfb679f96"]},"11cb73f8-1979-4e6a-b86f-e58bfb679f96":{"id":"11cb73f8-1979-4e6a-b86f-e58bfb679f96","message":{"id":"11cb73f8-1979-4e6a-b86f-e58bfb679f96","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046560.932152,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"AlphaFold2 Nature 2021 protein structure prediction CASP14\"}, {\"q\": \"AlphaDev Nature 2023 'discovering faster sorting algorithms using reinforcement learning'\"}, {\"q\": \"AlphaTensor Nature 2022 discovering matrix multiplication algorithms\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"AlphaFold2 Nature 2021 protein structure prediction CASP14"},{"type":"search","q":"AlphaDev Nature 2023 'discovering faster sorting algorithms using reinforcement learning'"},{"type":"search","q":"AlphaTensor Nature 2022 discovering matrix multiplication algorithms"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching sources on AI advancements in games and beyond","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"a0995b28-e6df-4f66-91d9-e4ea426af335","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"a0995b28-e6df-4f66-91d9-e4ea426af335","children":["9df20c6a-3e78-4230-aa57-5fa8acc0e27d"]},"9df20c6a-3e78-4230-aa57-5fa8acc0e27d":{"id":"9df20c6a-3e78-4230-aa57-5fa8acc0e27d","message":{"id":"9df20c6a-3e78-4230-aa57-5fa8acc0e27d","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046588.4541323,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["AlphaFold2 Nature 2021 protein structure prediction CASP14","AlphaDev Nature 2023 'discovering faster sorting algorithms using reinforcement learning'","AlphaTensor Nature 2022 discovering matrix multiplication algorithms"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching sources on AI advancements in games and beyond","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"11cb73f8-1979-4e6a-b86f-e58bfb679f96","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"11cb73f8-1979-4e6a-b86f-e58bfb679f96","children":["ca255d5e-1d20-434f-8248-701c4ec8a0ec"]},"ca255d5e-1d20-434f-8248-701c4ec8a0ec":{"id":"ca255d5e-1d20-434f-8248-701c4ec8a0ec","message":{"id":"ca255d5e-1d20-434f-8248-701c4ec8a0ec","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046588.4286673,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41586-021-03819-2","title":"Highly accurate protein structure prediction with AlphaFold","snippet":"by J Jumper \u00b7 2021 \u00b7 Cited by 40155 \u2014 In CASP14, AlphaFold structures were vastly more accurate than competing methods. AlphaFold structures had a median backbone accuracy of 0.96 \u00c5 ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41586-023-06004-9","title":"Faster sorting algorithms discovered using deep ...","snippet":"by DJ Mankowitz \u00b7 2023 \u00b7 Cited by 275 \u2014 AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41586-022-05172-4","title":"Discovering faster matrix multiplication algorithms with ...","snippet":"by A Fawzi \u00b7 2022 \u00b7 Cited by 842 \u2014 Here we report a deep reinforcement learning approach based on AlphaZero 1 for discovering efficient and provably correct algorithms for the multiplication of ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41586-021-03828-1","title":"Highly accurate protein structure prediction for the human ...","snippet":"by K Tunyasuvunakool \u00b7 2021 \u00b7 Cited by 2930 \u2014 Here we markedly expand the structural coverage of the proteome by applying the state-of-the-art machine learning method, AlphaFold 2 , at a scale that covers ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41392-023-01381-z","title":"AlphaFold2 and its applications in the fields of biology and ...","snippet":"by Z Yang \u00b7 2023 \u00b7 Cited by 434 \u2014 AlphaFold2 (AF2) is an artificial intelligence (AI) system developed by DeepMind that can predict three-dimensional (3D) structures of proteins from amino acid ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"pubmed.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/34265844/","title":"Highly accurate protein structure prediction with AlphaFold","snippet":"by J Jumper \u00b7 2021 \u00b7 Cited by 40155 \u2014 Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure ...","ref_id":null,"pub_date":null,"attribution":"pubmed.ncbi.nlm.nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/36198780/","title":"Discovering faster matrix multiplication algorithms with ...","snippet":"by A Fawzi \u00b7 2022 \u00b7 Cited by 842 \u2014 AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case ...","ref_id":null,"pub_date":null,"attribution":"pubmed.ncbi.nlm.nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/37286649/","title":"Faster sorting algorithms discovered using deep ...","snippet":"by DJ Mankowitz \u00b7 2023 \u00b7 Cited by 276 \u2014 AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.","ref_id":null,"pub_date":null,"attribution":"pubmed.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/","title":"AlphaDev discovers faster sorting algorithms","snippet":"Jun 7, 2023 \u2014 AlphaDev uncovered a faster algorithm for sorting, a method for ordering data. Billions of people use these algorithms everyday without realising it.","ref_id":null,"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/discover/blog/discovering-novel-algorithms-with-alphatensor/","title":"Discovering novel algorithms with AlphaTensor","snippet":"Oct 5, 2022 \u2014 In our paper, we explored how modern AI techniques could advance the automatic discovery of new matrix multiplication algorithms. Building on ...","ref_id":null,"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"alphafold.ebi.ac.uk","entries":[{"type":"search_result","url":"https://alphafold.ebi.ac.uk/","title":"AlphaFold Protein Structure Database","snippet":"Mean pLDDT 85.57. In CASP14, AlphaFold was the top-ranked protein structure prediction method by a large margin, producing predictions with high accuracy.","ref_id":null,"pub_date":null,"attribution":"alphafold.ebi.ac.uk"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/371375477_Faster_sorting_algorithms_discovered_using_deep_reinforcement_learning","title":"(PDF) Faster sorting algorithms discovered using deep ...","snippet":"Jun 7, 2023 \u2014 AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.","ref_id":null,"pub_date":1686096000.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/364188186_Discovering_faster_matrix_multiplication_algorithms_with_reinforcement_learning","title":"(PDF) Discovering faster matrix multiplication algorithms ...","snippet":"Oct 5, 2022 \u2014 A reinforcement learning approach based on AlphaZero is used to discover efficient and provably correct algorithms for matrix multiplication.","ref_id":null,"pub_date":1664928000.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.julian.ac","entries":[{"type":"search_result","url":"https://www.julian.ac/blog/2022/10/05/discovering-matrix-multiplication-algorithms-with-alphatensor/","title":"Discovering Matrix Multiplication Algorithms with ...","snippet":"Oct 5, 2022 \u2014 Starting from scratch, AlphaTensor discovers a wide variety of matrix multplication algorithms. Beyond advancing mathematical knowledege, these ...","ref_id":null,"pub_date":null,"attribution":"www.julian.ac"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC8329862/","title":"Protein structure prediction by AlphaFold2: are attention ...","snippet":"by N Bouatta \u00b7 2021 \u00b7 Cited by 76 \u2014 With respect to the \u223c100 targets in CASP14, AlphaFold2 performed poorly on five, with a GDT_TS below 70. Three of these were components of oligomeric complexes, ...","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/programming/comments/143gskm/google_finds_faster_sorting_algorithm_using_deep/","title":"Google finds faster sorting algorithm using deep ...","snippet":"We reverse engineered the low-level assembly sorting algorithms discovered by AlphaDev for sort 3, sort 4 and sort 5 to C++ and discovered that ...","ref_id":null,"pub_date":1686152878.375,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"laskerfoundation.org","entries":[{"type":"search_result","url":"https://laskerfoundation.org/winners/alphafold-a-technology-for-predicting-protein-structures/","title":"AlphaFold\u2014for predicting protein structures","snippet":"Sep 2, 2025 \u2014 In 2020, AlphaFold2 soared past the competition in CASP14. Its predictions were accurate to atomic precision and it generated excellent results ...","ref_id":null,"pub_date":null,"attribution":"laskerfoundation.org"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0959440X2500003X","title":"Advancing protein structure prediction beyond AlphaFold2","snippet":"by S Park \u00b7 2025 \u00b7 Cited by 3 \u2014 The release of AlphaFold2 (AF2) in 2021 marked a turning point in protein structure prediction, revolutionizing the field with its remarkable accuracy [1].","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"www.wired.com","entries":[{"type":"search_result","url":"https://www.wired.com/story/deepminds-ai-is-learning-about-the-art-of-coding","title":"Deepmind's AI Is Learning About the Art of Coding","snippet":"Deepmind's AI system, AlphaDev, has made notable improvements in the field of computer science by enhancing fundamental sorting algorithms in C++. It utilizes reinforcement learning, a technique where AI learns by performing tasks and receiving rewards for improving efficiency. While sorting methods in programming were already highly optimized, AlphaDev's approach involved creating new algorithms in assembly code, leading to C++ running slightly faster. Its optimizations resulted in a 1.7% efficiency improvement for long sequences and up to 70% faster sorting for short sequences. This achievement underscores AlphaDev's potential to learn the art of coding and create state-of-the-art solutions. Deepmind's research team aims to extend these improvements to optimize the entire computing stack, marking an exciting step towards more generalized AI coding capabilities.","ref_id":null,"pub_date":1686150000.0,"attribution":"www.wired.com"},{"type":"search_result","url":"https://www.wired.com/story/without-code-for-deepminds-protein-ai-this-lab-wrote-its-own","title":"Without Code for DeepMind's Protein AI, This Lab Wrote Its Own","snippet":"In December 2020, DeepMind's AlphaFold2 made a groundbreaking advancement in predicting protein structures from amino acid sequences, which appeared to solve a longstanding problem in biology. However, before DeepMind published its methodology, another team from the University of Washington, led by David Baker, developed a similar tool, RoseTTAFold. This effort was driven by uncertainty about when or if DeepMind would release its code, prompting Baker's team to build their own model using insights from a brief DeepMind presentation. Released in June 2021, RoseTTAFold benefited researchers by providing accessible protein predictions. Eventually, DeepMind published its AlphaFold2 paper and open-source code in July, also sharing a large database of protein structures. Both advancements represent vital tools for scientific progress and illustrate the ongoing interplay between academic research and commercial AI development.","ref_id":null,"pub_date":1628766000.0,"attribution":"www.wired.com"},{"type":"search_result","url":"https://www.wired.com/story/deepmind-protein-folding-database","title":"DeepMind's AI has finally shown how useful it can be","snippet":"DeepMind's AI system, AlphaFold, has revolutionized the field of protein structure prediction by solving the structure of a protein linked to antibiotic resistance in just 15 minutes\u2014a task that had stumped researchers for years. The company has released over 350,000 protein structures, including most of the human proteome. This massive database, freely available through a partnership with the European Molecular Biology Laboratory (EMBL), is expected to accelerate drug discovery, provide insights into diseases such as cancer and Alzheimer's, and enable better recycling methods. AlphaFold's predictions offer high confidence levels, with two-thirds of the human proteome's residues predicted to near-experimental accuracy. Although traditional experimental methods for protein structure determination are still necessary for verification, AlphaFold represents a significant advancement in scientific knowledge brought about by AI, and more systems like it are expected to emerge soon.","ref_id":null,"pub_date":1626966000.0,"attribution":"www.wired.com"}]},{"type":"search_result_group","domain":"www.welt.de","entries":[{"type":"search_result","url":"https://www.welt.de/241465661","title":"K\u00fcnstliche Intelligenz entdeckt optimierte Algorithmen","snippet":"AlphaZero, bekannt f\u00fcr seine Erfolge im Schach und Go, hat sich weiterentwickelt zu AlphaTensor und sucht nun nach optimierten Algorithmen f\u00fcr Matrixmultiplikation, eine wesentliche Rechenoperation in vielen Anwendungen wie Wetterprognosen und Bildverarbeitung. AlphaTensor, von DeepMind, hat \u00fcber 70 verbesserte Algorithmen entdeckt, wie im Fachjournal \"Nature\" berichtet. Diese Effizienzsteigerungen k\u00f6nnten die Geschwindigkeit vieler Berechnungen erheblich verbessern. Ein Meilenstein war die Verbesserung eines seit 1969 bekannten Algorithmus von Volker Strassen f\u00fcr die Multiplikation von Vier-mal-vier-Matrizen. Holger Hoos von der RWTH Aachen lobt die Methode als interessant, sieht jedoch keinen bahnbrechenden Fortschritt in der automatischen Algorithmenkonstruktion.","ref_id":null,"pub_date":1665133572.0,"attribution":"www.welt.de"}]}],"debug_sonic_thread_id":"thread_68c9a94b45cc8323a7ee9eddcd128dc9","reasoning_status":"is_reasoning","reasoning_title":"Searching sources on AI advancements in games and beyond","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"9df20c6a-3e78-4230-aa57-5fa8acc0e27d","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"9df20c6a-3e78-4230-aa57-5fa8acc0e27d","children":["d7b66657-eb37-4bab-a82d-a7039f7ff7e4"]},"d7b66657-eb37-4bab-a82d-a7039f7ff7e4":{"id":"d7b66657-eb37-4bab-a82d-a7039f7ff7e4","message":{"id":"d7b66657-eb37-4bab-a82d-a7039f7ff7e4","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046589.730967,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"RT-2 vision-language-action model Google Robotics 2023 blog\"}, {\"q\": \"RT-1 robotics transformer paper 2022 Google Robotics\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"RT-2 vision-language-action model Google Robotics 2023 blog"},{"type":"search","q":"RT-1 robotics transformer paper 2022 Google Robotics"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Exploring LLMs, tool-use, and general intelligence pathways","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"ca255d5e-1d20-434f-8248-701c4ec8a0ec","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"ca255d5e-1d20-434f-8248-701c4ec8a0ec","children":["2cdd352e-9c7f-4a41-a3be-09a7c52a15ba"]},"2cdd352e-9c7f-4a41-a3be-09a7c52a15ba":{"id":"2cdd352e-9c7f-4a41-a3be-09a7c52a15ba","message":{"id":"2cdd352e-9c7f-4a41-a3be-09a7c52a15ba","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046604.3927479,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["RT-2 vision-language-action model Google Robotics 2023 blog","RT-1 robotics transformer paper 2022 Google Robotics"]},"reasoning_status":"is_reasoning","reasoning_title":"Exploring LLMs, tool-use, and general intelligence pathways","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"d7b66657-eb37-4bab-a82d-a7039f7ff7e4","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"d7b66657-eb37-4bab-a82d-a7039f7ff7e4","children":["3164a121-354e-4ecd-aa00-a9033b851f0f"]},"3164a121-354e-4ecd-aa00-a9033b851f0f":{"id":"3164a121-354e-4ecd-aa00-a9033b851f0f","message":{"id":"3164a121-354e-4ecd-aa00-a9033b851f0f","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046604.3652081,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"blog.google","entries":[{"type":"search_result","url":"https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/","title":"Speaking robot: Our new AI model translates vision and ...","snippet":"Jul 28, 2023 \u2014 RT-2, our new vision-language-action model, helps robots more easily understand and perform actions \u2014 in both familiar and new situations.","ref_id":null,"pub_date":1690502400.0,"attribution":"blog.google"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2212.06817","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"","ref_id":null,"pub_date":1670957715.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2307.15818","title":"RT-2: Vision-Language-Action Models Transfer Web ...","snippet":"by A Brohan \u00b7 2023 \u00b7 Cited by 1775 \u2014 We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2212.06817","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/","title":"RT-2: New model translates vision and language into action","snippet":"Jul 28, 2023 \u2014 RT-2 is a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalised instructions for ...","ref_id":null,"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"research.google","entries":[{"type":"search_result","url":"https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/","title":"RT-1: Robotics Transformer for real-world control at scale","snippet":"Dec 13, 2022 \u2014 This model is trained on a large-scale, real-world robotics dataset of 130k episodes that cover 700+ tasks, collected using a fleet of 13 robots ...","ref_id":null,"pub_date":null,"attribution":"research.google"}]},{"type":"search_result_group","domain":"robotics-transformer2.github.io","entries":[{"type":"search_result","url":"https://robotics-transformer2.github.io/","title":"RT-2: Vision-Language-Action Models","snippet":"We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization.","ref_id":null,"pub_date":null,"attribution":"robotics-transformer2.github.io"}]},{"type":"search_result_group","domain":"robotics-transformer1.github.io","entries":[{"type":"search_result","url":"https://robotics-transformer1.github.io/","title":"Robotics Transformer: RT-1","snippet":"In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable, pre-trained model properties. We verify our conclusions ...","ref_id":null,"pub_date":null,"attribution":"robotics-transformer1.github.io"}]},{"type":"search_result_group","domain":"www.roboticsproceedings.org","entries":[{"type":"search_result","url":"https://www.roboticsproceedings.org/rss19/p025.pdf","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"Jul 10, 2023 \u2014 In this paper, we present a model class, dubbed Robotics. Transformer, that exhibits promising scalable model properties. We verify our ...","ref_id":null,"pub_date":null,"attribution":"www.roboticsproceedings.org"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40LawrencewleKnight/how-vision-language-action-models-are-revolutionizing-robotic-control-a627bbc0c249","title":"How Vision-Language-Action Models are Revolutionizing ...","snippet":"The RT-2 model represents a significant advancement in the field of robotic control. By combining vision, language, and action processing ...","ref_id":null,"pub_date":1691659292.911,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40faryal.saud/rt-2-vision-language-action-models-transfer-web-knowledge-to-robotic-control-8cffbd038781","title":"RT-2: Vision-Language-Action Models Transfer Web ...","snippet":"This blog post delves into the noteworthy discoveries and ramifications of this groundbreaking research. Purpose. Can large pretrained vision ...","ref_id":null,"pub_date":1702400408.983,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"roboticsconference.org","entries":[{"type":"search_result","url":"https://roboticsconference.org/2023/program/papers/025/","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties.","ref_id":null,"pub_date":null,"attribution":"roboticsconference.org"}]},{"type":"search_result_group","domain":"akmaier.medium.com","entries":[{"type":"search_result","url":"https://akmaier.medium.com/can-robots-finally-learn-new-tricks-how-googles-rt-1-is-transforming-robotics-d681aeef6333","title":"Can Robots Finally Learn New Tricks? \u2014 How Google's RT-1 ...","snippet":"This visionary 2022 paper, while not featured at major conferences, was boldly shared on ArXiv by a large, dedicated team from Google Robotics, ...","ref_id":null,"pub_date":1744605361.505,"attribution":"akmaier.medium.com"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/372803068_RT-1_Robotics_Transformer_for_Real-World_Control_at_Scale","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"For each grid point, we collected 10 successful trajectories using the RT-1 (Brohan et al., 2022) model and filtered for successful trajectories. We trained ...","ref_id":null,"pub_date":1688947200.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/machinelearningnews/comments/zmxxp0/google_ai_introduces_robotics_transformer_1_rt1_a/","title":"Google AI Introduces Robotics Transformer 1 (RT-1), A Multi ...","snippet":"Google AI Introduces Robotics Transformer 1 (RT-1), A Multi-Task Model That Tokenizes Robot Inputs And Outputs Actions To Enable Efficient ...","ref_id":null,"pub_date":1671142709.859,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"www.justthink.ai","entries":[{"type":"search_result","url":"https://www.justthink.ai/blog/googles-rt-2-revolutionizing-robotics-with-vision-language-and-action","title":"Google's RT-2: Revolutionizing Robotics with Vision, ...","snippet":"Google's RT-2 has emerged as a groundbreaking AI model that translates vision and language into tangible robotic actions. By overcoming the challenges ...","ref_id":null,"pub_date":null,"attribution":"www.justthink.ai"}]},{"type":"search_result_group","domain":"www.reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/technology/google-introduces-new-ai-models-rapidly-growing-robotics-industry-2025-03-12/","title":"Google introduces new AI models for rapidly growing robotics industry","snippet":"Google has launched two new AI models, Gemini Robotics and Gemini Robotics-ER, based on the Gemini 2.0 model to support the rapidly expanding robotics industry. Gemini Robotics is an advanced vision-language-action model for physical actions output, while Gemini Robotics-ER enables advanced spatial understanding and program development using reasoning abilities. This announcement follows Figure AI's exit from its collaboration with OpenAI after its own AI breakthrough. Google's models support diverse robot types, including industrial humanoids, and can help startups reduce costs and accelerate market entry. The Gemini Robotics model, tested on the bi-arm robotics platform ALOHA 2, is adaptable for complex applications like Apptronik's Apollo robot. Apptronik recently secured $350 million in funding to scale AI-powered humanoid robots, partly funded by Google. Additionally, Google's history with robotics includes acquiring Boston Dynamics in 2013 and selling it to SoftBank Group Corp four years later.","ref_id":null,"pub_date":1741791770.0,"attribution":"www.reuters.com"}]},{"type":"search_result_group","domain":"www.theverge.com","entries":[{"type":"search_result","url":"https://www.theverge.com/news/628021/google-deepmind-gemini-robotics-ai-models","title":"Google DeepMind's new AI models help robots perform physical tasks, even without training","snippet":"Google DeepMind is introducing two advanced AI models, Gemini Robotics and Gemini Robotics-ER, to enhance the capabilities of robots in performing real-world tasks. Gemini Robotics, leveraging the Gemini 2.0 model, excels in understanding diverse scenarios, even those it hasn't been specifically trained for. It significantly improves robots' generality, interactivity, and dexterity, allowing them to perform precise tasks such as folding paper or unscrewing bottle caps. Gemini Robotics-ER, an advanced visual language model, aids robots in complex reasoning tasks, such as packing a lunchbox efficiently. Safety remains a priority, with AI models trained to assess the safety of actions before execution. Google DeepMind collaborates with Apptronik and offers its models to \"trusted testers\" like Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools, aiming to develop more intelligent, responsive, and robust robots adaptable to varied environments and applications.","ref_id":null,"pub_date":1741791600.0,"attribution":"www.theverge.com"},{"type":"search_result","url":"https://www.theverge.com/news/691882/google-deepmind-on-device-ai-robots-gemini","title":"Google DeepMind's optimized AI model runs directly on robots","snippet":"Google DeepMind has introduced an optimized on-device version of its Gemini Robotics AI model, marking a significant advancement in robotic autonomy. This vision-language-action (VLA) model is compact and efficient enough to run directly on robots without requiring internet connectivity. While the original Gemini Robotics model used a hybrid cloud and on-device approach, this new version offers offline capabilities that approach the performance of the flagship. Designed to perform diverse physical tasks and adapt to new situations with as few as 50 to 100 demonstrations, it supports fine motor control and generalization to new environments. Initially trained on Google\u2019s ALOHA robot, the model has also been successfully adapted to other robotic platforms like Apptronik's humanoid Apollo and the dual-arm Franka FR3. According to Google DeepMind\u2019s Carolina Parada, the on-device model serves as a strong entry-level or security-sensitive option, though the hybrid version remains more powerful. Additionally, Google is releasing a software development kit (SDK) for this model, allowing developers to evaluate and refine the AI \u2014 a first for a DeepMind VLA. The tools are being made available to trusted testers as Google continues to address safety concerns.","ref_id":null,"pub_date":1750773600.0,"attribution":"www.theverge.com"}]}],"debug_sonic_thread_id":"thread_68c9a94b45cc8323a7ee9eddcd128dc9","reasoning_status":"is_reasoning","reasoning_title":"Exploring LLMs, tool-use, and general intelligence pathways","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"2cdd352e-9c7f-4a41-a3be-09a7c52a15ba","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"2cdd352e-9c7f-4a41-a3be-09a7c52a15ba","children":["4b1da6e6-a2f4-4158-9b86-5a4d8c27af4a"]},"4b1da6e6-a2f4-4158-9b86-5a4d8c27af4a":{"id":"4b1da6e6-a2f4-4158-9b86-5a4d8c27af4a","message":{"id":"4b1da6e6-a2f4-4158-9b86-5a4d8c27af4a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046605.774099,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"DreamerV3 outperforms model-free RL across many benchmarks 2023 arXiv\"}, {\"q\": \"World Models Ha Schmidhuber 2018 arXiv\"}, {\"q\": \"PlaNet Learning Latent Dynamics for Continuous Control 2019 ICLR\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"DreamerV3 outperforms model-free RL across many benchmarks 2023 arXiv"},{"type":"search","q":"World Models Ha Schmidhuber 2018 arXiv"},{"type":"search","q":"PlaNet Learning Latent Dynamics for Continuous Control 2019 ICLR"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Exploring research bridging gap to AGI/ASI","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"3164a121-354e-4ecd-aa00-a9033b851f0f","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"3164a121-354e-4ecd-aa00-a9033b851f0f","children":["c6f43fa0-5d53-4451-a698-3c5dd5b2eac2"]},"c6f43fa0-5d53-4451-a698-3c5dd5b2eac2":{"id":"c6f43fa0-5d53-4451-a698-3c5dd5b2eac2","message":{"id":"c6f43fa0-5d53-4451-a698-3c5dd5b2eac2","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046621.8090992,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["DreamerV3 outperforms model-free RL across many benchmarks 2023 arXiv","World Models Ha Schmidhuber 2018 arXiv","PlaNet Learning Latent Dynamics for Continuous Control 2019 ICLR"]},"reasoning_status":"is_reasoning","reasoning_title":"Exploring research bridging gap to AGI/ASI","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"4b1da6e6-a2f4-4158-9b86-5a4d8c27af4a","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"4b1da6e6-a2f4-4158-9b86-5a4d8c27af4a","children":["dc7f1550-bb18-470c-b239-326f5361adb3"]},"dc7f1550-bb18-470c-b239-326f5361adb3":{"id":"dc7f1550-bb18-470c-b239-326f5361adb3","message":{"id":"dc7f1550-bb18-470c-b239-326f5361adb3","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758046621.778998,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2301.04104","title":"Mastering Diverse Domains through World Models","snippet":"","ref_id":null,"pub_date":1673374336.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1803.10122","title":"World Models","snippet":"","ref_id":null,"pub_date":1522163335.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1811.04551","title":"Learning Latent Dynamics for Planning from Pixels","snippet":"by D Hafner \u00b7 2018 \u00b7 Cited by 1941 \u2014 We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2503.04416","title":"learning transformer-based world models","snippet":"by M Burchi \u00b7 2025 \u00b7 Cited by 4 \u2014 With these modifications, DreamerV3 outperformed specialized model-free and model-based algorithms in a wide range of benchmarks. In ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.08872v1","title":"Meta-Reinforcement Learning with Discrete World Models ...","snippet":"Mar 11, 2025 \u2014 We hypothesize that integrating the RL\u00b2 algorithm with the DreamerV3 model will significantly outperform traditional reinforcement learning ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2301.04104","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 866 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":null,"pub_date":1713402880.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1809.01999","title":"Recurrent World Models Facilitate Policy Evolution","snippet":"","ref_id":null,"pub_date":1536099912.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2307.07176","title":"Safe Reinforcement Learning with World Models","snippet":"by W Huang \u00b7 2023 \u00b7 Cited by 26 \u2014 We further illustrate that Safe DreamerV3 surpasses competing model-based and model-free methods on this benchmark. 2 Related Work. Safe ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.17103v1","title":"TransDreamerV3: Implanting Transformer In DreamerV3","snippet":"Jun 20, 2025 \u2014 This paper introduces TransDreamerV3, a reinforcement learning model that enhances the DreamerV3 architecture by integrating a transformer encoder.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.04416v2","title":"Learning Transformer-based World Models with ...","snippet":"With these modifications, DreamerV3 outperformed specialized model-free and model-based algorithms in a wide range of benchmarks. Report issue for preceding ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.cl.cam.ac.uk","entries":[{"type":"search_result","url":"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2022_2023/papers/ha_arXiv_2018.pdf","title":"World Models","snippet":"by D Ha \u00b7 Cited by 1627 \u2014 We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained.","ref_id":null,"pub_date":1525910805.0,"attribution":"www.cl.cam.ac.uk"}]},{"type":"search_result_group","domain":"planetrl.github.io","entries":[{"type":"search_result","url":"https://planetrl.github.io/","title":"Learning Latent Dynamics for Planning from Pixels","snippet":"Feb 15, 2019 \u2014 PlaNet solves continuous control tasks from pixels that are more difficult than those previously solved by planning with learned models. Key ...","ref_id":null,"pub_date":null,"attribution":"planetrl.github.io"}]},{"type":"search_result_group","domain":"worldmodels.github.io","entries":[{"type":"search_result","url":"https://worldmodels.github.io/","title":"World Models","snippet":"Ha and Schmidhuber, \"Recurrent World Models Facilitate Policy Evolution\", 2018. ... ArXiv preprint. Generative Temporal Models with Memory [PDF] Gemici, M ...","ref_id":null,"pub_date":null,"attribution":"worldmodels.github.io"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=8iW8HOidj1_","title":"LATENT SPACE PLANNING FOR CONTINUOUS CONTROL","snippet":"by A Koul \u00b7 Cited by 5 \u2014 Learning and planning with latent space dynamics has been shown to be useful for sample efficiency in model-based reinforcement learning (MBRL) for dis-.","ref_id":null,"pub_date":1601638619.0,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=8iW8HOidj1_","title":"Dream and Search to Control: Latent Space Planning for ...","snippet":"by A Koul \u00b7 Cited by 5 \u2014 In this work, we propose and explore an MBRL approach for continuous action spaces based on tree-based planning over learned latent dynamics.","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"proceedings.mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf","title":"Learning Latent Dynamics for Planning from Pixels","snippet":"by D Hafner \u00b7 Cited by 1941 \u2014 PlaNet solves continuous control tasks from pixels that are more difficult than those previously solved by planning with learned models. Key contributions ...","ref_id":null,"pub_date":1559671600.0,"attribution":"proceedings.mlr.press"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41586-025-08744-2","title":"Mastering diverse control tasks through world models","snippet":"by D Hafner \u00b7 2025 \u00b7 Cited by 73 \u2014 Using fixed hyperparameters across all domains, Dreamer outperforms tuned expert algorithms across a wide range of benchmarks and data budgets.","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"ar5iv.labs.arxiv.org","entries":[{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/1803.10122","title":"[1803.10122] World Models - ar5iv","snippet":"We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised ...","ref_id":null,"pub_date":null,"attribution":"ar5iv.labs.arxiv.org"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/google-research/planet","title":"Learning Latent Dynamics for Planning from Pixels","snippet":"May 28, 2024 \u2014 This project provides the open source implementation of the PlaNet agent introduced in Learning Latent Dynamics for Planning from Pixels.","ref_id":null,"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/324055152_World_Models","title":"World Models | Request PDF","snippet":"We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an ...","ref_id":null,"pub_date":1522108800.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"iclr.cc","entries":[{"type":"search_result","url":"https://iclr.cc/Downloads/2019","title":"Downloads 2019","snippet":"Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids ... Sample Efficient Imitation Learning for Continuous Control \u00b7 Scalable ...","ref_id":null,"pub_date":null,"attribution":"iclr.cc"}]},{"type":"search_result_group","domain":"junshern.github.io","entries":[{"type":"search_result","url":"https://junshern.github.io/paper-reading-group/2021/01/02/world-models.html","title":"World models.","snippet":"Jan 2, 2021 \u2014 Ha, David, and J\u00fcrgen Schmidhuber. \u201cWorld models.\u201d arXiv preprint arXiv:1803.10122 (2018). This paper really knocked my socks off - the main ...","ref_id":null,"pub_date":1609599600.0,"attribution":"junshern.github.io"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Learning-Latent-Dynamics-for-Planning-from-Pixels-Hafner-Lillicrap/fea3e63c97c7292dc6fbcb3ffe7131eb54053986","title":"[PDF] Learning Latent Dynamics for Planning from Pixels","snippet":"The Deep Planning Network (PlaNet) is proposed, a purely model-based agent that learns the environment dynamics from images and chooses actions through fast ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/World-Models-Ha-Schmidhuber/ff332c21562c87cab5891d495b7d0956f2d9228b","title":"[PDF] World Models","snippet":"This work explores building generative neural network models of popular reinforcement learning environments by using features extracted from the world model ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]}],"debug_sonic_thread_id":"thread_68c9a94b45cc8323a7ee9eddcd128dc9","reasoning_status":"is_reasoning","reasoning_title":"Exploring research bridging gap to AGI/ASI","classifier_response":"default","request_id":"980257bcdc31fcea-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"c6f43fa0-5d53-4451-a698-3c5dd5b2eac2","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"c6f43fa0-5d53-4451-a698-3c5dd5b2eac2","children":["579f3102-531e-468c-88b6-63c1f289ad73"]},"579f3102-531e-468c-88b6-63c1f289ad73":{"id":"579f3102-531e-468c-88b6-63c1f289ad73","message":{"id":"579f3102-531e-468c-88b6-63c1f289ad73","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046658.983131,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 3m 17s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":197,"model_slug":"gpt-5-thinking","request_id":"980257bcdc31fcea-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"dc7f1550-bb18-470c-b239-326f5361adb3","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"dc7f1550-bb18-470c-b239-326f5361adb3","children":["fae4e34e-1195-4331-b690-776bda39b0fc"]},"fae4e34e-1195-4331-b690-776bda39b0fc":{"id":"fae4e34e-1195-4331-b690-776bda39b0fc","message":{"id":"fae4e34e-1195-4331-b690-776bda39b0fc","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758046623.61392,"update_time":null,"content":{"content_type":"text","parts":["Love that framing. Let\u2019s trace the arc from **narrow superhuman systems** (perfect-information engines and board-game RL) toward the far tougher territory of **general, open-world intelligence**\u2014and be precise about what changed, what didn\u2019t, and what still blocks AGI/ASI.\n\n# 1) \u201cNarrow ASI\u201d done right: search + oracles (+ now nets)\n**Classical engines (chess/checkers)** achieved superhuman strength long before deep learning by combining:\n- **Massive tree search (alpha\u2013beta)** with heavy pruning/heuristics.  \n- **Hand-crafted evaluation** (HCE) tuned by engine devs (not grandmasters), plus **opening books** and **endgame tablebases** that literally act as *oracles* for small piece counts. \ue200cite\ue202turn2search0\ue202turn2search1\ue201\n\n**Stockfish today is *not* purely hand-coded**: since Aug 2020 it integrates **NNUE**, a small, CPU-efficient neural network used as the evaluation function (first hybrid, then fully NNUE by Stockfish 16, 2023). This tightened the search+learned-heuristic loop and yielded large Elo gains while keeping deep search. \ue200cite\ue202turn0search6\ue202turn0search12\ue202turn2search6\ue201\n\n**Takeaway:** Superhuman play in narrow domains emerged from *mechanistic search* augmented by *perfect submodules* (tablebases) and now **compact learned evaluators**\u2014a pattern we\u2019ll see again.\n\n# 2) RL\u2019s leap in perfect-information games\nThe breakthrough wasn\u2019t \u201cdeep nets alone,\u201d but **deep nets + *planning*** with cheap self-play:\n- **AlphaGo (2016):** policy/value nets + MCTS; trained via human game imitation then self-play RL; beat Lee Sedol. \ue200cite\ue202turn0search1\ue202turn0search13\ue201  \n- **AlphaZero (2017):** *tabula rasa* self-play from rules only; mastered Go/Chess/Shogi with the *same* algorithm. \ue200cite\ue202turn0search2\ue202turn0search8\ue202turn0search14\ue201  \n- **MuZero (2020):** learned its *own* model of dynamics\u2014planning without coded rules; pushed to Atari + board games. \ue200cite\ue202turn0search3\ue202turn0search9\ue202turn0search15\ue201\n\n**Imperfect information & multi-agent:**  \n- **Poker:** **Libratus** and **DeepStack** cracked HUNL via CFR-style reasoning, real-time subgame solving, and learned intuition; **Pluribus** extended to **6-player**. These are genuine partial-information, multi-agent milestones. \ue200cite\ue202turn0search4\ue202turn1search2\ue202turn0search5\ue201  \n- **Real-time, long-horizon:** **AlphaStar** reached **Grandmaster** in StarCraft II using imitation pretraining + massive league self-play RL. \ue200cite\ue202turn1search3\ue202turn1search0\ue201  \n- **Language-mediated multi-agent:** **Cicero** (Meta) reached human-level in **Diplomacy** by fusing an LM with belief/planning and RL, handling negotiation + deception pressures. \ue200cite\ue202turn1search1\ue202turn1search7\ue201\n\n**Beyond games into science/algorithms:**  \n- **AlphaFold2** cracked protein folding accuracy at CASP14; a different stack, but proof that learned models can *replace* hand-engineered pipelines at scientific level. \ue200cite\ue202turn3search0\ue202turn3search3\ue201  \n- **AlphaTensor/AlphaDev** used RL+search to *discover new algorithms* (matrix mult, sorting) adopted into LLVM. \ue200cite\ue202turn3search2\ue202turn3search1\ue201\n\n**Takeaway:** A recurring motif: **learn a strong prior (policy/value/model), then *plan/search***. When rules/simulators exist and data is cheap (self-play), we get superhuman performance.\n\n# 3) Why AGI/ASI is orders harder than Go/Chess/Poker/StarCraft\nThe real world adds dimensions that current systems only partially cover:\n\n| Dimension | Games & narrow ASI | Reality / AGI gap |\n|---|---|---|\n| **World knowledge & rules** | Fixed, closed, often simulatable. | **Open-ended, uncertain, shifting;** dynamics unknown (MuZero-style learning must scale to messy physics/society). \ue200cite\ue202turn0search3\ue201 |\n| **Observability** | Full (Go/Chess) or stylized partial (Poker). | **Severe partial observability**, latent causes; requires *long-term memory* & belief tracking. (Poker/Diplomacy hint at this, but at limited scope.) \ue200cite\ue202turn0search4\ue202turn1search1\ue201 |\n| **Objectives** | Single scalar reward (win). | **Multivariate, mis-specified, safety-constrained,** often shifting; need preference learning & safe exploration. |\n| **Action space & latency** | Discrete, compact; real-time (AlphaStar) but still bounded. | **Hybrid, high-dimensional, long-horizon control** (economies, codebases, robots). Robotics attempts (RT-1/RT-2) are promising but early. \ue200cite\ue202turn4search1\ue202turn4search6\ue202turn4search0\ue201 |\n| **Data & feedback** | Billions of cheap self-play episodes. | **Sparse/expensive feedback;** must leverage *world models*, simulation, and LMs for generalization. (DreamerV3 shows the *direction*.) \ue200cite\ue202turn5search0\ue201 |\n| **Generalization** | In-distribution (same rules). | **Systematic/OOD generalization** across novel tasks, tools, and norms. |\n| **Verification** | Outcome is clear. | **Need verifiable reasoning/plans** (proofs, constraints) to trust high-stakes actions. |\n\n# 4) Converging blueprint (what seems to work, extrapolated)\n1) **Search/planning + learned models:** The Stockfish-NNUE \u2192 AlphaZero \u2192 MuZero through-line will likely persist: *learned evaluators / world models* guiding *explicit planning*. Expect richer latent-space planning with uncertainty and constraints. \ue200cite\ue202turn0search3\ue202turn0search2\ue202turn0search6\ue201  \n2) **Language as the control plane:** LMs coordinate *tool use* and *multi-step plans*, but need **stateful memory, belief tracking, and verifiability** (Cicero is a proto-example in a limited arena). \ue200cite\ue202turn1search1\ue201  \n3) **World-model RL at scale:** Model-based RL (e.g., **DreamerV3**) hints that learning simulators of the world (or task suites) is the route to **sample-efficient, transferable competence**. \ue200cite\ue202turn5search0\ue201  \n4) **Embodied competence:** Vision-Language-Action stacks (RT-1/RT-2, and successors) are starting to move knowledge into **grounded action**; sim-to-real + safety remains the wall. \ue200cite\ue202turn4search1\ue202turn4search6\ue201  \n5) **Algorithm discovery & theorem proving:** AlphaTensor/AlphaDev suggest *automated discovery* loops that couple search, learning, and formal verification\u2014critical for **trustworthy reasoning** in broader domains. \ue200cite\ue202turn3search2\ue202turn3search1\ue201\n\n# 5) Concrete milestones that would *meaningfully* narrow the AGI gap\n- **MuZero-style agents in open-ended 3D worlds** (partial observability, long horizons) *without* hand-coded simulators; success measured by **generalization to new tasks** and robustness under distribution shift. \ue200cite\ue202turn0search3\ue201  \n- **Language-grounded multi-agent negotiation** that stays performant *and* satisfies explicit **truthfulness/commitment constraints** (a \u201cCicero-with-contracts\u201d). \ue200cite\ue202turn1search1\ue201  \n- **Robotics that re-targets across platforms with \u226410\u00b3 demos**, preserving safety under uncertainty (RT-2-style VLAs, but reliable in homes/factories). \ue200cite\ue202turn4search6\ue201  \n- **World-model RL with formal verification hooks** (plans/proofs/certificates) to guarantee properties before execution\u2014bridging AlphaZero-style planning to *provable* behavior. \ue200cite\ue202turn3search2\ue201  \n- **Automated science/coding loops** that propose, test (in sim or lab), and *justify* novel hypotheses/algorithms\u2014AlphaDev/AlphaTensor generalised beyond niche domains. \ue200cite\ue202turn3search1\ue202turn3search2\ue201\n\n---\n\n## A gentle correction to your premise\nCalling Stockfish \u201cnon-ML, hand-crafted by GMs\u201d undersells what it is **now**: a **search monster** supercharged by a **neural evaluator (NNUE)** trained on millions of positions; and the hand-crafting historically came from **engine programmers**, not grandmasters. The bigger picture you\u2019re pointing to still stands: **search + learned guidance + oracles** reliably yields **narrow superhuman** performance\u2014and the general recipe scales when we can (1) self-play cheaply and (2) plan effectively. \ue200cite\ue202turn0search6\ue202turn2search0\ue201\n\nIf you want, I can sketch a **research reading path** (Stockfish-NNUE \u2192 AlphaZero/MuZero \u2192 Poker/AlphaStar \u2192 Cicero \u2192 DreamerV3 \u2192 RT-2 \u2192 AlphaTensor/AlphaDev) with 1\u20132 must-read papers per rung and \u201cwhy this mattered\u201d summaries."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":"\ue200cite\ue202turn2search0\ue202turn2search1\ue201","start_idx":698,"end_idx":730,"safe_urls":["https://www.chessprogramming.org/Endgame_Tablebases?utm_source=chatgpt.com","https://www.chessprogramming.org/Evaluation?utm_source=chatgpt.com"],"refs":[],"alt":"([Chess Programming Wiki](https://www.chessprogramming.org/Evaluation?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Evaluation","url":"https://www.chessprogramming.org/Evaluation?utm_source=chatgpt.com","pub_date":null,"snippet":"Recently, there are two main ways to build an evaluation: traditional hand-crafted evaluation (HCE) and multi-layer neural networks. This page focuses on the ...","attribution_segments":null,"supporting_websites":[{"title":"Endgame Tablebases","url":"https://www.chessprogramming.org/Endgame_Tablebases?utm_source=chatgpt.com","pub_date":null,"snippet":"Endgame Tablebases, (EGTBs or EGTs) precalculated endgame tables generated by an exhaustive retrograde analysis.","attribution":"Chess Programming Wiki"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":0},{"turn_index":2,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Chess Programming Wiki"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search6\ue202turn0search12\ue202turn2search6\ue201","start_idx":1048,"end_idx":1094,"safe_urls":["https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html?utm_source=chatgpt.com","https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","https://www.chessprogramming.org/Stockfish_NNUE?utm_source=chatgpt.com"],"refs":[],"alt":"([Stockfish](https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Introducing NNUE Evaluation","url":"https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 7, 2020 \u2014 As of August 6, the efficiently updatable neural network (NNUE) evaluation has landed in the Stockfish repo!","attribution_segments":null,"supporting_websites":[{"title":"NNUE | Stockfish Docs - GitHub Pages","url":"https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html?utm_source=chatgpt.com","pub_date":null,"snippet":"Jun 12, 2025 \u2014 A neural network architecture that takes advantage of having minimal changes in the network inputs between subsequent evaluations.","attribution":"official-stockfish.github.io"},{"title":"Stockfish NNUE","url":"https://www.chessprogramming.org/Stockfish_NNUE?utm_source=chatgpt.com","pub_date":null,"snippet":"With Stockfish 16 release in 2023 the hand-crafted evaluation function was removed, a complete transition to NNUE based evaluation was made.","attribution":"Chess Programming Wiki"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":6},{"turn_index":0,"ref_type":"search","ref_index":12},{"turn_index":2,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"Stockfish"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search13\ue201","start_idx":1548,"end_idx":1581,"safe_urls":["https://research.google/pubs/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search/?utm_source=chatgpt.com","https://www.nature.com/articles/nature16961?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/nature16961?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering the game of Go with deep neural networks and ...","url":"https://www.nature.com/articles/nature16961?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Silver \u00b7 2016 \u00b7 Cited by 22863 \u2014 Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves.","attribution_segments":null,"supporting_websites":[{"title":"Mastering the game of Go with deep neural networks and ...","url":"https://research.google/pubs/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search/?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Silver \u00b7 2016 \u00b7 Cited by 22863 \u2014 Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves.","attribution":"Google Research"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search2\ue202turn0search8\ue202turn0search14\ue201","start_idx":1700,"end_idx":1746,"safe_urls":["https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com","https://arxiv.org/pdf/1712.01815?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.aar6404?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm","url":"https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com","pub_date":1512499538.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"A general reinforcement learning algorithm that masters ...","url":"https://www.science.org/doi/10.1126/science.aar6404?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Silver \u00b7 2018 \u00b7 Cited by 5673 \u2014 In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games.","attribution":"Science"},{"title":"Mastering Chess and Shogi by Self-Play with a General ...","url":"https://arxiv.org/pdf/1712.01815?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Silver \u00b7 2017 \u00b7 Cited by 2870 \u2014 The updated parameters are used in subsequent games of self-play. The AlphaZero algorithm described in this paper differs from the original ...","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":8},{"turn_index":0,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search9\ue202turn0search15\ue201","start_idx":1867,"end_idx":1913,"safe_urls":["https://arxiv.org/abs/1911.08265?utm_source=chatgpt.com","https://pubmed.ncbi.nlm.nih.gov/33361790/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering Atari, Go, chess and shogi by planning with a ...","url":"https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Schrittwieser \u00b7 2020 \u00b7 Cited by 3156 \u2014 Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","attribution_segments":null,"supporting_websites":[{"title":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model","url":"https://arxiv.org/abs/1911.08265?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"},{"title":"Mastering Atari, Go, chess and shogi by planning with a ...","url":"https://pubmed.ncbi.nlm.nih.gov/33361790/?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Schrittwieser \u00b7 2020 \u00b7 Cited by 3156 \u2014 Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","attribution":"PubMed"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":9},{"turn_index":0,"ref_type":"search","ref_index":15}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search4\ue202turn1search2\ue202turn0search5\ue201","start_idx":2192,"end_idx":2237,"safe_urls":["https://www.science.org/doi/10.1126/science.aam6960?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.aay2400?utm_source=chatgpt.com"],"refs":[],"alt":"([Science](https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Superhuman AI for heads-up no-limit poker: Libratus beats ...","url":"https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com","pub_date":null,"snippet":"by N Brown \u00b7 2018 \u00b7 Cited by 1048 \u2014 Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of ...","attribution_segments":null,"supporting_websites":[{"title":"DeepStack: Expert-level artificial intelligence in heads-up ...","url":"https://www.science.org/doi/10.1126/science.aam6960?utm_source=chatgpt.com","pub_date":null,"snippet":"by M Morav\u010d\u00edk \u00b7 2017 \u00b7 Cited by 1359 \u2014 In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold'em.","attribution":"Science"},{"title":"Superhuman AI for multiplayer poker","url":"https://www.science.org/doi/10.1126/science.aay2400?utm_source=chatgpt.com","pub_date":null,"snippet":"by N Brown \u00b7 2019 \u00b7 Cited by 1135 \u2014 In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker.","attribution":"Science"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":4},{"turn_index":1,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"Science"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search3\ue202turn1search0\ue201","start_idx":2384,"end_idx":2416,"safe_urls":["https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-019-1724-z?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-019-1724-z?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Grandmaster level in StarCraft II using multi-agent ...","url":"https://www.nature.com/articles/s41586-019-1724-z?utm_source=chatgpt.com","pub_date":null,"snippet":"by O Vinyals \u00b7 2019 \u00b7 Cited by 5713 \u2014 AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.","attribution_segments":null,"supporting_websites":[{"title":"AlphaStar: Grandmaster level in StarCraft II using multi-agent ...","url":"https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/?utm_source=chatgpt.com","pub_date":null,"snippet":null,"attribution":"Google DeepMind"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":3},{"turn_index":1,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search1\ue202turn1search7\ue201","start_idx":2600,"end_idx":2632,"safe_urls":["https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com"],"refs":[],"alt":"([Science](https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Human-level play in the game of Diplomacy by combining ...","url":"https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com","pub_date":null,"snippet":"2022 \u00b7 Cited by 358 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition.","attribution_segments":null,"supporting_websites":[{"title":"22-Science-Diplomacy-TR.pdf - Noam Brown","url":"https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 9, 2022 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and ...","attribution":"Noam Brown"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":1},{"turn_index":1,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"Science"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search0\ue202turn3search3\ue201","start_idx":2849,"end_idx":2881,"safe_urls":["https://pubmed.ncbi.nlm.nih.gov/34265844/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-021-03819-2?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-021-03819-2?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Highly accurate protein structure prediction with AlphaFold","url":"https://www.nature.com/articles/s41586-021-03819-2?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Jumper \u00b7 2021 \u00b7 Cited by 40155 \u2014 In CASP14, AlphaFold structures were vastly more accurate than competing methods. AlphaFold structures had a median backbone accuracy of 0.96 \u00c5 ...","attribution_segments":null,"supporting_websites":[{"title":"Highly accurate protein structure prediction with AlphaFold","url":"https://pubmed.ncbi.nlm.nih.gov/34265844/?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Jumper \u00b7 2021 \u00b7 Cited by 40155 \u2014 Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure ...","attribution":"PubMed"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":0},{"turn_index":3,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search2\ue202turn3search1\ue201","start_idx":2997,"end_idx":3029,"safe_urls":["https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Discovering faster matrix multiplication algorithms with ...","url":"https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Fawzi \u00b7 2022 \u00b7 Cited by 842 \u2014 Here we report a deep reinforcement learning approach based on AlphaZero 1 for discovering efficient and provably correct algorithms for the multiplication of ...","attribution_segments":null,"supporting_websites":[{"title":"Faster sorting algorithms discovered using deep ...","url":"https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com","pub_date":null,"snippet":"by DJ Mankowitz \u00b7 2023 \u00b7 Cited by 275 \u2014 AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.","attribution":"Nature"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":2},{"turn_index":3,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue201","start_idx":3613,"end_idx":3632,"safe_urls":["https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering Atari, Go, chess and shogi by planning with a ...","url":"https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Schrittwieser \u00b7 2020 \u00b7 Cited by 3156 \u2014 Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search4\ue202turn1search1\ue201","start_idx":3854,"end_idx":3886,"safe_urls":["https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com"],"refs":[],"alt":"([Science](https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Superhuman AI for heads-up no-limit poker: Libratus beats ...","url":"https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com","pub_date":null,"snippet":"by N Brown \u00b7 2018 \u00b7 Cited by 1048 \u2014 Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of ...","attribution_segments":null,"supporting_websites":[{"title":"Human-level play in the game of Diplomacy by combining ...","url":"https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com","pub_date":null,"snippet":"2022 \u00b7 Cited by 358 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition.","attribution":"Science"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":4},{"turn_index":1,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Science"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search1\ue202turn4search6\ue202turn4search0\ue201","start_idx":4285,"end_idx":4330,"safe_urls":["https://arxiv.org/abs/2212.06817?utm_source=chatgpt.com","https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com","https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2212.06817?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"RT-1: Robotics Transformer for Real-World Control at Scale","url":"https://arxiv.org/abs/2212.06817?utm_source=chatgpt.com","pub_date":1670957715.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"RT-2: Vision-Language-Action Models Transfer Web ...","url":"https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Brohan \u00b7 2023 \u00b7 Cited by 1775 \u2014 We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization.","attribution":"arXiv"},{"title":"Speaking robot: Our new AI model translates vision and ...","url":"https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 28, 2023 \u2014 RT-2, our new vision-language-action model, helps robots more easily understand and perform actions \u2014 in both familiar and new situations.","attribution":"blog.google"}],"refs":[{"turn_index":4,"ref_type":"search","ref_index":1},{"turn_index":4,"ref_type":"search","ref_index":6},{"turn_index":4,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn5search0\ue201","start_idx":4533,"end_idx":4552,"safe_urls":["https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering Diverse Domains through World Models","url":"https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","pub_date":1673374336.0,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":5,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search2\ue202turn0search6\ue201","start_idx":5127,"end_idx":5172,"safe_urls":["https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com","https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering Atari, Go, chess and shogi by planning with a ...","url":"https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Schrittwieser \u00b7 2020 \u00b7 Cited by 3156 \u2014 Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","attribution_segments":null,"supporting_websites":[{"title":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm","url":"https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"},{"title":"Introducing NNUE Evaluation","url":"https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 7, 2020 \u2014 As of August 6, the efficiently updatable neural network (NNUE) evaluation has landed in the Stockfish repo!","attribution":"Stockfish"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search1\ue201","start_idx":5376,"end_idx":5395,"safe_urls":["https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com"],"refs":[],"alt":"([Science](https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Human-level play in the game of Diplomacy by combining ...","url":"https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com","pub_date":null,"snippet":"2022 \u00b7 Cited by 358 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Science"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn5search0\ue201","start_idx":5591,"end_idx":5610,"safe_urls":["https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering Diverse Domains through World Models","url":"https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","pub_date":1673374336.0,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":5,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search1\ue202turn4search6\ue201","start_idx":5795,"end_idx":5827,"safe_urls":["https://arxiv.org/abs/2212.06817?utm_source=chatgpt.com","https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2212.06817?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"RT-1: Robotics Transformer for Real-World Control at Scale","url":"https://arxiv.org/abs/2212.06817?utm_source=chatgpt.com","pub_date":1670957715.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"RT-2: Vision-Language-Action Models Transfer Web ...","url":"https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Brohan \u00b7 2023 \u00b7 Cited by 1775 \u2014 We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization.","attribution":"arXiv"}],"refs":[{"turn_index":4,"ref_type":"search","ref_index":1},{"turn_index":4,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search2\ue202turn3search1\ue201","start_idx":6046,"end_idx":6078,"safe_urls":["https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Discovering faster matrix multiplication algorithms with ...","url":"https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Fawzi \u00b7 2022 \u00b7 Cited by 842 \u2014 Here we report a deep reinforcement learning approach based on AlphaZero 1 for discovering efficient and provably correct algorithms for the multiplication of ...","attribution_segments":null,"supporting_websites":[{"title":"Faster sorting algorithms discovered using deep ...","url":"https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com","pub_date":null,"snippet":"by DJ Mankowitz \u00b7 2023 \u00b7 Cited by 275 \u2014 AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.","attribution":"Nature"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":2},{"turn_index":3,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue201","start_idx":6365,"end_idx":6384,"safe_urls":["https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering Atari, Go, chess and shogi by planning with a ...","url":"https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Schrittwieser \u00b7 2020 \u00b7 Cited by 3156 \u2014 Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search1\ue201","start_idx":6551,"end_idx":6570,"safe_urls":["https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com"],"refs":[],"alt":"([Science](https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Human-level play in the game of Diplomacy by combining ...","url":"https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com","pub_date":null,"snippet":"2022 \u00b7 Cited by 358 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Science"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search6\ue201","start_idx":6726,"end_idx":6745,"safe_urls":["https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"RT-2: Vision-Language-Action Models Transfer Web ...","url":"https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Brohan \u00b7 2023 \u00b7 Cited by 1775 \u2014 We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search2\ue201","start_idx":6927,"end_idx":6946,"safe_urls":["https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Discovering faster matrix multiplication algorithms with ...","url":"https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Fawzi \u00b7 2022 \u00b7 Cited by 842 \u2014 Here we report a deep reinforcement learning approach based on AlphaZero 1 for discovering efficient and provably correct algorithms for the multiplication of ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":3,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search1\ue202turn3search2\ue201","start_idx":7119,"end_idx":7151,"safe_urls":["https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Faster sorting algorithms discovered using deep ...","url":"https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com","pub_date":null,"snippet":"by DJ Mankowitz \u00b7 2023 \u00b7 Cited by 275 \u2014 AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.","attribution_segments":null,"supporting_websites":[{"title":"Discovering faster matrix multiplication algorithms with ...","url":"https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Fawzi \u00b7 2022 \u00b7 Cited by 842 \u2014 Here we report a deep reinforcement learning approach based on AlphaZero 1 for discovering efficient and provably correct algorithms for the multiplication of ...","attribution":"Nature"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":1},{"turn_index":3,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search6\ue202turn2search0\ue201","start_idx":7696,"end_idx":7728,"safe_urls":["https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","https://www.chessprogramming.org/Evaluation?utm_source=chatgpt.com"],"refs":[],"alt":"([Stockfish](https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Introducing NNUE Evaluation","url":"https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 7, 2020 \u2014 As of August 6, the efficiently updatable neural network (NNUE) evaluation has landed in the Stockfish repo!","attribution_segments":null,"supporting_websites":[{"title":"Evaluation","url":"https://www.chessprogramming.org/Evaluation?utm_source=chatgpt.com","pub_date":null,"snippet":"Recently, there are two main ways to build an evaluation: traditional hand-crafted evaluation (HCE) and multi-layer neural networks. This page focuses on the ...","attribution":"Chess Programming Wiki"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":6},{"turn_index":2,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"Stockfish"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":" ","start_idx":7958,"end_idx":7958,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"Evaluation","url":"https://www.chessprogramming.org/Evaluation?utm_source=chatgpt.com","attribution":"Chess Programming Wiki"},{"title":"Introducing NNUE Evaluation","url":"https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","attribution":"Stockfish"},{"title":"Mastering the game of Go with deep neural networks and ...","url":"https://www.nature.com/articles/nature16961?utm_source=chatgpt.com","attribution":"Nature"},{"title":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm","url":"https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com","attribution":"arXiv"},{"title":"Superhuman AI for heads-up no-limit poker: Libratus beats ...","url":"https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com","attribution":"Science"}],"has_images":false}],"safe_urls":["https://www.chessprogramming.org/Endgame_Tablebases?utm_source=chatgpt.com","https://www.chessprogramming.org/Evaluation?utm_source=chatgpt.com","https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html?utm_source=chatgpt.com","https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","https://www.chessprogramming.org/Stockfish_NNUE?utm_source=chatgpt.com","https://research.google/pubs/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search/?utm_source=chatgpt.com","https://www.nature.com/articles/nature16961?utm_source=chatgpt.com","https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com","https://arxiv.org/pdf/1712.01815?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.aar6404?utm_source=chatgpt.com","https://arxiv.org/abs/1911.08265?utm_source=chatgpt.com","https://pubmed.ncbi.nlm.nih.gov/33361790/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.aam6960?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.aay2400?utm_source=chatgpt.com","https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-019-1724-z?utm_source=chatgpt.com","https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com","https://pubmed.ncbi.nlm.nih.gov/34265844/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-021-03819-2?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.aao1733?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com","https://arxiv.org/abs/2212.06817?utm_source=chatgpt.com","https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com","https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/?utm_source=chatgpt.com","https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com","https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com","https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","https://arxiv.org/abs/2212.06817?utm_source=chatgpt.com","https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com","https://www.science.org/doi/10.1126/science.ade9097?utm_source=chatgpt.com","https://arxiv.org/abs/2307.15818?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-022-05172-4?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-023-06004-9?utm_source=chatgpt.com","https://stockfishchess.org/blog/2020/introducing-nnue-evaluation/?utm_source=chatgpt.com","https://www.chessprogramming.org/Evaluation?utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"chessprogramming.org","entries":[{"type":"search_result","url":"https://www.chessprogramming.org/Stockfish_NNUE?utm_source=chatgpt.com","title":"Stockfish NNUE","snippet":"In July 2020, the playing code of NNUE was put into the official Stockfish repository as a branch for further development and examination. In August that ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/index.php?mobileaction=toggle_view_desktop&title=Stockfish_NNUE&trk=article-ssr-frontend-pulse_little-text-block&utm_source=chatgpt.com","title":"Stockfish NNUE","snippet":"Dec 22, 2024 \u2014 In July 2020, the playing code of NNUE was put into the official Stockfish repository as a branch for further development and examination. In ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/Endgame_Tablebases?utm_source=chatgpt.com","title":"Endgame Tablebases","snippet":"Endgame Tablebases, (EGTBs or EGTs) precalculated endgame tables generated by an exhaustive retrograde analysis.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/Stockfish?utm_source=chatgpt.com","title":"Stockfish","snippet":"Since April 2017 the interactive Stockfish Evaluation Guide is available to explore Stockfish's evaluation with a JavaScript implementation running in a browser ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/Syzygy_Bases?utm_source=chatgpt.com","title":"Syzygy Bases","snippet":"Syzygy EGTB is significantly smaller than any existent DTM EGTB. It is 7 times as small as Gaviota for 5 men, 8 times as small as Nalimov for 6 men, and 8 times ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/index.php?mobileaction=toggle_view_desktop&title=Evaluation&utm_source=chatgpt.com","title":"Evaluation","snippet":"Jul 20, 2024 \u2014 Recently, there are two main ways to build an evaluation: traditional hand-crafted evaluation (HCE) and multi-layer neural networks. This page ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"chessprogramming.org"},{"type":"search_result","url":"https://www.chessprogramming.org/index.php?mobileaction=toggle_view_desktop&title=Endgame_Tablebases&utm_source=chatgpt.com","title":"Endgame Tablebases","snippet":"Jul 17, 2024 \u2014 An EGTB is a set of endgames. Each endgame is a set of records about positions' information. All involved positions in an endgame must have the same material.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"chessprogramming.org"}]},{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/?utm_source=chatgpt.com","title":"MuZero: Mastering Go, chess, shogi and Atari without rules","snippet":"Dec 23, 2020 \u2014 MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/?utm_source=chatgpt.com","title":"AlphaStar: Grandmaster level in StarCraft II using multi-agent ...","snippet":"","ref_id":{"turn_index":1,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/?utm_source=chatgpt.com","title":"AlphaDev discovers faster sorting algorithms","snippet":"Jun 7, 2023 \u2014 AlphaDev uncovered a faster algorithm for sorting, a method for ordering data. Billions of people use these algorithms everyday without realising it.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/discover/blog/discovering-novel-algorithms-with-alphatensor/?utm_source=chatgpt.com","title":"Discovering novel algorithms with AlphaTensor","snippet":"Oct 5, 2022 \u2014 In our paper, we explored how modern AI techniques could advance the automatic discovery of new matrix multiplication algorithms. Building on ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/?utm_source=chatgpt.com","title":"RT-2: New model translates vision and language into action","snippet":"Jul 28, 2023 \u2014 RT-2 is a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this knowledge into generalised instructions for ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/?utm_source=chatgpt.com","title":"Scaling up learning across many different robot types","snippet":"Oct 3, 2023 \u2014 Robotic Transformer 2 (RT-2) is a novel vision-language-action (VLA) model that learns from both web and robotics data, and translates this ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"blog.google","entries":[{"type":"search_result","url":"https://blog.google/technology/ai/google-deepmind-rt2-robotics-vla-model/?utm_source=chatgpt.com","title":"Speaking robot: Our new AI model translates vision and ...","snippet":"Jul 28, 2023 \u2014 RT-2, our new vision-language-action model, helps robots more easily understand and perform actions \u2014 in both familiar and new situations.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"blog.google"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/1911.08265?utm_source=chatgpt.com","title":"Mastering Atari, Go, Chess and Shogi by Planning with a ...","snippet":"by J Schrittwieser \u00b7 2019 \u00b7 Cited by 3156 \u2014 In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1712.01815?utm_source=chatgpt.com","title":"Mastering Chess and Shogi by Self-Play with a General ...","snippet":"by D Silver \u00b7 2017 \u00b7 Cited by 2870 \u2014 The updated parameters are used in subsequent games of self-play. The AlphaZero algorithm described in this paper differs from the original ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1701.01724?utm_source=chatgpt.com","title":"Expert-Level Artificial Intelligence in No-Limit Poker","snippet":"by M Morav\u010d\u00edk \u00b7 2017 \u00b7 Cited by 1359 \u2014 In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1701.01724?utm_source=chatgpt.com","title":"DeepStack: Expert-Level Artificial Intelligence in Heads-Up ...","snippet":"by M Morav\u010d\u00edk \u00b7 2017 \u00b7 Cited by 1359 \u2014 We show that. DeepStack is theoretically sound, produces strategies substantially more difficult to exploit than abstraction-based techniques, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2012.13169?utm_source=chatgpt.com","title":"SCC: an Efficient Deep Reinforcement Learning Agent ...","snippet":"by X Wang \u00b7 2020 \u00b7 Cited by 59 \u2014 AlphaStar is the first end-to-end learning algorithm for Star-. Craft II full game that reaches GrandMaster level. It uses imitation learning to ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2307.14503?utm_source=chatgpt.com","title":"[2307.14503] Shorter and faster than Sort3AlphaDev","snippet":"by C Neri \u00b7 2023 \u00b7 Cited by 2 \u2014 The article cited above presents new implementations of sorting algorithms found through deep reinforcement learning that work on a small number of numeric ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2212.06817?utm_source=chatgpt.com","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1803.10122?utm_source=chatgpt.com","title":"[1803.10122] World Models","snippet":"by D Ha \u00b7 2018 \u00b7 Cited by 1627 \u2014 Title:World Models. Authors:David Ha, J\u00fcrgen Schmidhuber. View a PDF of the paper titled World Models, by David Ha and J\\\"urgen Schmidhuber.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1811.04551?utm_source=chatgpt.com","title":"Learning Latent Dynamics for Planning from Pixels","snippet":"by D Hafner \u00b7 2018 \u00b7 Cited by 1941 \u2014 We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2503.04416?utm_source=chatgpt.com","title":"learning transformer-based world models","snippet":"by M Burchi \u00b7 2025 \u00b7 Cited by 4 \u2014 With these modifications, DreamerV3 outperformed specialized model-free and model-based algorithms in a wide range of benchmarks. In ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.08872v1?utm_source=chatgpt.com","title":"Meta-Reinforcement Learning with Discrete World Models ...","snippet":"Mar 11, 2025 \u2014 We hypothesize that integrating the RL\u00b2 algorithm with the DreamerV3 model will significantly outperform traditional reinforcement learning ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2301.04104?utm_source=chatgpt.com","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 866 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1809.01999?utm_source=chatgpt.com","title":"Recurrent World Models Facilitate Policy Evolution","snippet":"by D Ha \u00b7 2018 \u00b7 Cited by 1360 \u2014 A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/1803.10122?utm_source=chatgpt.com","title":"[1803.10122] World Models - ar5iv","snippet":"We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2307.07176?utm_source=chatgpt.com","title":"Safe Reinforcement Learning with World Models","snippet":"by W Huang \u00b7 2023 \u00b7 Cited by 26 \u2014 We further illustrate that Safe DreamerV3 surpasses competing model-based and model-free methods on this benchmark. 2 Related Work. Safe ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.17103v1?utm_source=chatgpt.com","title":"TransDreamerV3: Implanting Transformer In DreamerV3","snippet":"Jun 20, 2025 \u2014 This paper introduces TransDreamerV3, a reinforcement learning model that enhances the DreamerV3 architecture by integrating a transformer encoder.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.04416v2?utm_source=chatgpt.com","title":"Learning Transformer-based World Models with ...","snippet":"With these modifications, DreamerV3 outperformed specialized model-free and model-based algorithms in a wide range of benchmarks. Report issue for preceding ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2301.04104?utm_source=chatgpt.com","title":"Mastering Diverse Domains through World Models","snippet":"Apr 17, 2024 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.09297v5?utm_source=chatgpt.com","title":"When Do Neural Networks Learn World Models?","snippet":"7 days ago \u2014 Humans develop internal models of the world, extracting core concepts that generate perceptual data (Ha & Schmidhuber, 2018) . Can neural ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2410.14616v1?utm_source=chatgpt.com","title":"Benchmarking Deep Reinforcement Learning for ...","snippet":"Oct 18, 2024 \u2014 Here, we present a benchmark of both well-used and emerging DRL algorithms in a navigation task with configurable sensor denial effects.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2010.09832?utm_source=chatgpt.com","title":"latent space planning for continuous control","snippet":"by A Koul \u00b7 2020 \u00b7 Cited by 5 \u2014 PlaNet (Hafner et al., 2019b) does end-to-end training of the latent observation encoding and dynamics and does online planning using CEM .","ref_id":{"turn_index":5,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"science.org","entries":[{"type":"search_result","url":"https://www.science.org/doi/10.1126/science.aay2400?utm_source=chatgpt.com","title":"Superhuman AI for multiplayer poker","snippet":"by N Brown \u00b7 2019 \u00b7 Cited by 1135 \u2014 In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"science.org"},{"type":"search_result","url":"https://www.science.org/doi/10.1126/science.aar6404?utm_source=chatgpt.com","title":"A general reinforcement learning algorithm that masters ...","snippet":"by D Silver \u00b7 2018 \u00b7 Cited by 5673 \u2014 In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"science.org"},{"type":"search_result","url":"https://www.science.org/doi/10.1126/science.aam6960?utm_source=chatgpt.com","title":"DeepStack: Expert-level artificial intelligence in heads-up ...","snippet":"by M Morav\u010d\u00edk \u00b7 2017 \u00b7 Cited by 1359 \u2014 In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold'em.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"science.org"}]},{"type":"search_result_group","domain":"chess.com","entries":[{"type":"search_result","url":"https://www.chess.com/blog/Rocky64/adventures-with-endgame-tablebases?utm_source=chatgpt.com","title":"Adventures with endgame tablebases","snippet":"Dec 23, 2023 \u2014 Tablebases are essentially databases of endgame positions that have been exhaustively analysed so that their outcomes (win/loss/draw) with best play by both ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"chess.com"},{"type":"search_result","url":"https://www.chess.com/blog/Rocky64/more-adventures-with-endgame-tablebases?utm_source=chatgpt.com","title":"More adventures with endgame tablebases","snippet":"Dec 23, 2023 \u2014 Endgame tablebases are software that provides perfect play information with total accuracy for any position with up to seven pieces.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"chess.com"}]},{"type":"search_result_group","domain":"nih.gov","entries":[{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/33361790/?utm_source=chatgpt.com","title":"Mastering Atari, Go, chess and shogi by planning with a ...","snippet":"by J Schrittwieser \u00b7 2020 \u00b7 Cited by 3156 \u2014 Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challengin...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/31296650/?utm_source=chatgpt.com","title":"Superhuman AI for multiplayer poker","snippet":"by N Brown \u00b7 2019 \u00b7 Cited by 1135 \u2014 In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/29249696/?utm_source=chatgpt.com","title":"Superhuman AI for heads-up no-limit poker: Libratus beats ...","snippet":"by N Brown \u00b7 2018 \u00b7 Cited by 1048 \u2014 Abstract. No-limit Texas hold'em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/31666705/?utm_source=chatgpt.com","title":"Grandmaster level in StarCraft II using multi-agent ...","snippet":"by O Vinyals \u00b7 2019 \u00b7 Cited by 5713 \u2014 AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/34265844/?utm_source=chatgpt.com","title":"Highly accurate protein structure prediction with AlphaFold","snippet":"by J Jumper \u00b7 2021 \u00b7 Cited by 40155 \u2014 Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/36198780/?utm_source=chatgpt.com","title":"Discovering faster matrix multiplication algorithms with ...","snippet":"by A Fawzi \u00b7 2022 \u00b7 Cited by 842 \u2014 AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/37286649/?utm_source=chatgpt.com","title":"Faster sorting algorithms discovered using deep ...","snippet":"by DJ Mankowitz \u00b7 2023 \u00b7 Cited by 276 \u2014 AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC8329862/?utm_source=chatgpt.com","title":"Protein structure prediction by AlphaFold2: are attention ...","snippet":"by N Bouatta \u00b7 2021 \u00b7 Cited by 76 \u2014 With respect to the \u223c100 targets in CASP14, AlphaFold2 performed poorly on five, with a GDT_TS below 70. Three of these were components of oligomeric complexes, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC9534758/?utm_source=chatgpt.com","title":"Discovering faster matrix multiplication algorithms with ...","snippet":"by A Fawzi \u00b7 2022 \u00b7 Cited by 842 \u2014 In particular, AlphaTensor finds an algorithm for multiplying 4 \u00d7 4 matrices using 47 multiplications in Z 2 , thereby outperforming Strassen's ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"nih.gov"}]},{"type":"search_result_group","domain":"research.google","entries":[{"type":"search_result","url":"https://research.google/pubs/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search/?utm_source=chatgpt.com","title":"Mastering the game of Go with deep neural networks and ...","snippet":"by D Silver \u00b7 2016 \u00b7 Cited by 22863 \u2014 Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"research.google"},{"type":"search_result","url":"https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/?utm_source=chatgpt.com","title":"RT-1: Robotics Transformer for real-world control at scale","snippet":"Dec 13, 2022 \u2014 This model is trained on a large-scale, real-world robotics dataset of 130k episodes that cover 700+ tasks, collected using a fleet of 13 robots ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"research.google"},{"type":"search_result","url":"https://research.google/blog/google-research-2022-beyond-robotics/?utm_source=chatgpt.com","title":"Google Research, 2022 & beyond: Robotics","snippet":"With Robotics Transformer-1 (RT-1), we trained a robot manipulation policy on a large-scale, real-world robotics dataset of 130k episodes ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"research.google"}]},{"type":"search_result_group","domain":"meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/research/cicero/diplomacy/?utm_source=chatgpt.com","title":"Diplomacy and Meta AI's CICERO","snippet":"From August to October 2022, CICERO was put to the test in an online Diplomacy game league played on webDiplomacy.net, during which its strategic dialogue and ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"meta.com"},{"type":"search_result","url":"https://ai.meta.com/research/cicero/?utm_source=chatgpt.com","title":"CICERO","snippet":"By building CICERO, AI at Meta has created the first AI agent to achieve human-level performance in the complex natural language strategy game Diplomacy*.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"meta.com"}]},{"type":"search_result_group","domain":"stackexchange.com","entries":[{"type":"search_result","url":"https://chess.stackexchange.com/questions/39337/stockfish-nnue-vs-handcrafted-evaluation?utm_source=chatgpt.com","title":"Stockfish NNUE vs handcrafted Evaluation","snippet":"Feb 24, 2022 \u2014 From what I've read, the search speed with the hand-crafted eval. was around 100 Million Positions. Search speed using the NNUE was around 60.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"stackexchange.com"}]},{"type":"search_result_group","domain":"robotics-transformer2.github.io","entries":[{"type":"search_result","url":"https://robotics-transformer2.github.io/?utm_source=chatgpt.com","title":"RT-2: Vision-Language-Action Models","snippet":"We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"robotics-transformer2.github.io"}]},{"type":"search_result_group","domain":"cam.ac.uk","entries":[{"type":"search_result","url":"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2022_2023/papers/ha_arXiv_2018.pdf?utm_source=chatgpt.com","title":"World Models","snippet":"by D Ha \u00b7 Cited by 1627 \u2014 We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"cam.ac.uk"}]},{"type":"search_result_group","domain":"robotics-transformer1.github.io","entries":[{"type":"search_result","url":"https://robotics-transformer1.github.io/?utm_source=chatgpt.com","title":"Robotics Transformer: RT-1","snippet":"In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable, pre-trained model properties. We verify our conclusions ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"robotics-transformer1.github.io"}]},{"type":"search_result_group","domain":"planetrl.github.io","entries":[{"type":"search_result","url":"https://planetrl.github.io/?utm_source=chatgpt.com","title":"Learning Latent Dynamics for Planning from Pixels","snippet":"Feb 15, 2019 \u2014 PlaNet solves continuous control tasks from pixels that are more difficult than those previously solved by planning with learned models. Key ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"planetrl.github.io"}]},{"type":"search_result_group","domain":"storage.googleapis.com","entries":[{"type":"search_result","url":"https://storage.googleapis.com/deepmind-media/research/alphastar/AlphaStar_unformatted.pdf?utm_source=chatgpt.com","title":"Grandmaster level in StarCraft II using multi-agent ...","snippet":"AlphaStar is the first agent to achieve Grandmaster level in StarCraft II, and the first to reach the highest league of human players in a widespread ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"storage.googleapis.com"}]},{"type":"search_result_group","domain":"ebi.ac.uk","entries":[{"type":"search_result","url":"https://alphafold.ebi.ac.uk/?utm_source=chatgpt.com","title":"AlphaFold Protein Structure Database","snippet":"Mean pLDDT 85.57. In CASP14, AlphaFold was the top-ranked protein structure prediction method by a large margin, producing predictions with high accuracy.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"ebi.ac.uk"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search?utm_source=chatgpt.com","title":"(PDF) Mastering the game of Go with deep neural networks ...","snippet":"Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/337386809_Mastering_Atari_Go_Chess_and_Shogi_by_Planning_with_a_Learned_Model?utm_source=chatgpt.com","title":"Mastering Atari, Go, Chess and Shogi by Planning with a ...","snippet":"In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":53},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/369933571_Revisiting_of_AlphaStar?utm_source=chatgpt.com","title":"Revisiting of AlphaStar | Request PDF","snippet":"In 2019, DeepMind proposed AlphaStar (AS), an agent that achieved Grandmaster level in SC2. ... This paper introduces SC2LE (StarCraft II Learning ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/371375477_Faster_sorting_algorithms_discovered_using_deep_reinforcement_learning?utm_source=chatgpt.com","title":"(PDF) Faster sorting algorithms discovered using deep ...","snippet":"Jun 7, 2023 \u2014 AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/364188186_Discovering_faster_matrix_multiplication_algorithms_with_reinforcement_learning?utm_source=chatgpt.com","title":"(PDF) Discovering faster matrix multiplication algorithms ...","snippet":"Oct 5, 2022 \u2014 A reinforcement learning approach based on AlphaZero is used to discover efficient and provably correct algorithms for matrix multiplication.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/372803068_RT-1_Robotics_Transformer_for_Real-World_Control_at_Scale?utm_source=chatgpt.com","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"For each grid point, we collected 10 successful trajectories using the RT-1 (Brohan et al., 2022) model and filtered for successful trajectories. We trained ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/324055152_World_Models?utm_source=chatgpt.com","title":"World Models | Request PDF","snippet":"We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"noambrown.github.io","entries":[{"type":"search_result","url":"https://noambrown.github.io/papers/22-Science-Diplomacy-TR.pdf?utm_source=chatgpt.com","title":"22-Science-Diplomacy-TR.pdf - Noam Brown","snippet":"Dec 9, 2022 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"noambrown.github.io"}]},{"type":"search_result_group","domain":"wikipedia.org","entries":[{"type":"search_result","url":"https://en.wikipedia.org/wiki/Stockfish_NNUE?oldformat=true&utm_source=chatgpt.com","title":"Stockfish (chess)","snippet":"On 2 September 2020, the twelfth version of Stockfish was released, incorporating NNUE, and reportedly winning ten times more game pairs than it loses when ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"wikipedia.org"},{"type":"search_result","url":"https://en.wikipedia.org/wiki/Efficiently_updatable_neural_network?utm_source=chatgpt.com","title":"Efficiently updatable neural network","snippet":"As of 2025, Stockfish has introduced several optimizations to the NNUE architecture, but the overall architecture remains similar. See also.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"wikipedia.org"},{"type":"search_result","url":"https://en.wikipedia.org/wiki/Endgame_tablebase?utm_source=chatgpt.com","title":"Endgame tablebase","snippet":"In chess, the endgame tablebase, or simply the tablebase, is a computerised database containing precalculated evaluations of endgame positions.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"wikipedia.org"},{"type":"search_result","url":"https://en.wikipedia.org/wiki/Stockfish_%28chess%29?utm_source=chatgpt.com","title":"Stockfish (chess)","snippet":"In July 2023, Stockfish removed the hand-crafted evaluation and transitioned to a fully neural network-based approach. Contents. 1 Features; 2 ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"wikipedia.org"},{"type":"search_result","url":"https://en.wikipedia.org/wiki/AlphaFold?utm_source=chatgpt.com","title":"AlphaFold","snippet":"AlphaFold is an artificial intelligence (AI) program developed by DeepMind, a subsidiary of Alphabet, which performs predictions of protein structure.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"wikipedia.org"}]},{"type":"search_result_group","domain":"roboticsproceedings.org","entries":[{"type":"search_result","url":"https://www.roboticsproceedings.org/rss19/p025.pdf?utm_source=chatgpt.com","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"Jul 10, 2023 \u2014 In this paper, we present a model class, dubbed Robotics. Transformer, that exhibits promising scalable model properties. We verify our ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"roboticsproceedings.org"}]},{"type":"search_result_group","domain":"worldmodels.github.io","entries":[{"type":"search_result","url":"https://worldmodels.github.io/?utm_source=chatgpt.com","title":"World Models","snippet":"Ha and Schmidhuber, \"Recurrent World Models Facilitate Policy Evolution\", 2018. ... ArXiv preprint. Generative Temporal Models with Memory [PDF] Gemici, M ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"worldmodels.github.io"}]},{"type":"search_result_group","domain":"ualberta.ca","entries":[{"type":"search_result","url":"https://poker.cs.ualberta.ca/publications/17science.pdf?utm_source=chatgpt.com","title":"DeepStack: Expert-Level Artificial Intelligence in Heads-Up No ...","snippet":"by M Moravc\u0131k \u00b7 Cited by 1359 \u2014 Heads-up no-limit Texas hold'em (HUNL) is a two-player version of poker in which two cards are initially dealt face-down to each player, and additional cards ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"ualberta.ca"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40LawrencewleKnight/how-vision-language-action-models-are-revolutionizing-robotic-control-a627bbc0c249?utm_source=chatgpt.com","title":"How Vision-Language-Action Models are Revolutionizing ...","snippet":"The RT-2 model represents a significant advancement in the field of robotic control. By combining vision, language, and action processing ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40faryal.saud/rt-2-vision-language-action-models-transfer-web-knowledge-to-robotic-control-8cffbd038781?utm_source=chatgpt.com","title":"RT-2: Vision-Language-Action Models Transfer Web ...","snippet":"This blog post delves into the noteworthy discoveries and ramifications of this groundbreaking research. Purpose. Can large pretrained vision ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://akmaier.medium.com/can-robots-finally-learn-new-tricks-how-googles-rt-1-is-transforming-robotics-d681aeef6333?utm_source=chatgpt.com","title":"Can Robots Finally Learn New Tricks? \u2014 How Google's RT-1 ...","snippet":"This visionary 2022 paper, while not featured at major conferences, was boldly shared on ArXiv by a large, dedicated team from Google Robotics, ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40kdk199604/planet-fast-and-data-efficient-visual-planning-via-latent-dynamics-e93a853e9549?utm_source=chatgpt.com","title":"PlaNet: Fast and Data-Efficient Visual Planning via Latent ...","snippet":"This paper introduces PlaNet, a model-based reinforcement learning agent that learns to control environments directly from pixel observations.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=8iW8HOidj1_&utm_source=chatgpt.com","title":"LATENT SPACE PLANNING FOR CONTINUOUS CONTROL","snippet":"by A Koul \u00b7 Cited by 5 \u2014 Learning and planning with latent space dynamics has been shown to be useful for sample efficiency in model-based reinforcement learning (MBRL) for dis-.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=8iW8HOidj1_&utm_source=chatgpt.com","title":"Dream and Search to Control: Latent Space Planning for ...","snippet":"by A Koul \u00b7 Cited by 5 \u2014 In this work, we propose and explore an MBRL approach for continuous action spaces based on tree-based planning over learned latent dynamics.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v139/wang21v/wang21v.pdf?utm_source=chatgpt.com","title":"SCC: an Efficient Deep Reinforcement Learning Agent ...","snippet":"by X Wang \u00b7 2021 \u00b7 Cited by 59 \u2014 AlphaStar is the first end-to-end learning algorithm for Star-. Craft II full game that reaches GrandMaster level. It uses imitation learning to learn initial ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"mlr.press"},{"type":"search_result","url":"https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf?utm_source=chatgpt.com","title":"Learning Latent Dynamics for Planning from Pixels","snippet":"by D Hafner \u00b7 Cited by 1941 \u2014 PlaNet solves continuous control tasks from pixels that are more difficult than those previously solved by planning with learned models. Key contributions ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"mlr.press"}]},{"type":"search_result_group","domain":"nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/d41586-019-02156-9?utm_source=chatgpt.com","title":"AI poker bot is first to beat professionals at multiplayer game","snippet":"by D HEAVEN \u00b7 2019 \u2014 A superhuman poker-playing bot called Pluribus has beaten top human professionals at six-player no-limit Texas hold'em poker, the most popular variant of the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41586-021-03828-1?utm_source=chatgpt.com","title":"Highly accurate protein structure prediction for the human ...","snippet":"by K Tunyasuvunakool \u00b7 2021 \u00b7 Cited by 2930 \u2014 Here we markedly expand the structural coverage of the proteome by applying the state-of-the-art machine learning method, AlphaFold 2 , at a scale that co...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41392-023-01381-z?utm_source=chatgpt.com","title":"AlphaFold2 and its applications in the fields of biology and ...","snippet":"by Z Yang \u00b7 2023 \u00b7 Cited by 434 \u2014 AlphaFold2 (AF2) is an artificial intelligence (AI) system developed by DeepMind that can predict three-dimensional (3D) structures of proteins from amino acid ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41586-025-08744-2?utm_source=chatgpt.com","title":"Mastering diverse control tasks through world models","snippet":"by D Hafner \u00b7 2025 \u00b7 Cited by 73 \u2014 Using fixed hyperparameters across all domains, Dreamer outperforms tuned expert algorithms across a wide range of benchmarks and data budgets.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"nature.com"}]},{"type":"search_result_group","domain":"ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2017/0772.pdf?utm_source=chatgpt.com","title":"Libratus: The Superhuman AI for No-Limit Poker","snippet":"by N Brown \u00b7 Cited by 163 \u2014 Heads-up no-limit Texas. Hold'em is the main benchmark challenge for AI in imperfect-information games. We present Libra- tus, the first\u2014and so far only\u2014AI to ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"ijcai.org"},{"type":"search_result","url":"https://www.ijcai.org/proceedings/2017/0004.pdf?utm_source=chatgpt.com","title":"Beating Top Pros in Heads-Up No-Limit Texas Hold'em","snippet":"by T Sandholm \u00b7 Cited by 4 \u2014 In January 2017, our AI, Libra- tus, beat a team of four top specialist professionals in heads- up no-limit Texas hold'em, which has 10161 decision points. This ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"ijcai.org"}]},{"type":"search_result_group","domain":"cmu.edu","entries":[{"type":"search_result","url":"https://www.cmu.edu/news/stories/archives/2019/july/cmu-facebook-ai-beats-poker-pros.html?utm_source=chatgpt.com","title":"Carnegie Mellon and Facebook AI Beats Professionals in ...","snippet":"Jul 11, 2019 \u2014 An artificial intelligence program developed by Carnegie Mellon and Facebook AI has defeated professionals in no-limit Texas hold'em poker.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"cmu.edu"},{"type":"search_result","url":"https://www.cmu.edu/news/stories/archives/2017/january/AI-beats-poker-pros.html?utm_source=chatgpt.com","title":"Carnegie Mellon Artificial Intelligence Beats Top Poker Pros","snippet":"Jan 31, 2017 \u2014 Once the last of 120,000 hands of Heads-up, No-Limit Texas Hold'em were played on Jan. 30, Libratus led the pros by a collective $1,766,250 in ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"cmu.edu"},{"type":"search_result","url":"https://www.cmu.edu/news/stories/archives/2017/december/ai-inner-workings.html?utm_source=chatgpt.com","title":"Carnegie Mellon Reveals Inner Workings of Victorious AI","snippet":"Dec 17, 2017 \u2014 Libratus, an artificial intelligence that defeated four top professional poker players in no-limit Texas Hold'em earlier this year, uses a three-pronged ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"cmu.edu"}]},{"type":"search_result_group","domain":"uvm.edu","entries":[{"type":"search_result","url":"https://cdanfort.w3.uvm.edu/nolink/math/poker-science-2017.pdf?utm_source=chatgpt.com","title":"Expert-level artificial intelligence in heads-up no-limit poker","snippet":"by M Morav\u010d\u00edk \u00b7 Cited by 1359 \u2014 In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"uvm.edu"}]},{"type":"search_result_group","domain":"julian.ac","entries":[{"type":"search_result","url":"https://www.julian.ac/blog/2022/10/05/discovering-matrix-multiplication-algorithms-with-alphatensor/?utm_source=chatgpt.com","title":"Discovering Matrix Multiplication Algorithms with ...","snippet":"Oct 5, 2022 \u2014 Starting from scratch, AlphaTensor discovers a wide variety of matrix multplication algorithms. Beyond advancing mathematical knowledege, these ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"julian.ac"}]},{"type":"search_result_group","domain":"roboticsconference.org","entries":[{"type":"search_result","url":"https://roboticsconference.org/2023/program/papers/025/?utm_source=chatgpt.com","title":"RT-1: Robotics Transformer for Real-World Control at Scale","snippet":"In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"roboticsconference.org"}]},{"type":"search_result_group","domain":"official-stockfish.github.io","entries":[{"type":"search_result","url":"https://official-stockfish.github.io/docs/nnue-pytorch-wiki/docs/nnue.html?utm_source=chatgpt.com","title":"NNUE | Stockfish Docs - GitHub Pages","snippet":"Jun 12, 2025 \u2014 A neural network architecture that takes advantage of having minimal changes in the network inputs between subsequent evaluations.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"official-stockfish.github.io"}]},{"type":"search_result_group","domain":"semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Mastering-Chess-and-Shogi-by-Self-Play-with-a-Silver-Hubert/38fb1902c6a2ab4f767d4532b28a92473ea737aa?utm_source=chatgpt.com","title":"Mastering Chess and Shogi by Self-Play with a General ...","snippet":"This paper generalises the approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490?utm_source=chatgpt.com","title":"[PDF] Mastering the game of Go with deep neural networks ...","snippet":"Using this search algorithm, the program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Mastering-Atari%2C-Go%2C-chess-and-shogi-by-planning-a-Schrittwieser-Antonoglou/3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b?utm_source=chatgpt.com","title":"Mastering Atari, Go, chess and shogi by planning with a ...","snippet":"The MuZero algorithm is presented, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Grandmaster-level-in-StarCraft-II-using-multi-agent-Vinyals-Babuschkin/361c00b22e29d0816ca896513d2c165e26399821?utm_source=chatgpt.com","title":"[PDF] Grandmaster level in StarCraft II using multi-agent ...","snippet":"The agent, AlphaStar, is evaluated, which uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/DeepStack%3A-Expert-Level-Artificial-Intelligence-in-Moravc%C3%ADk-Schmid/4312ae64a058d555ff1656ccad713dbd81200e79?utm_source=chatgpt.com","title":"Expert-Level Artificial Intelligence in No-Limit Poker","snippet":"DeepStack is introduced, a new algorithm for imperfect information settings such as poker that combines recursive reasoning to handle information asymmetry, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Learning-Latent-Dynamics-for-Planning-from-Pixels-Hafner-Lillicrap/fea3e63c97c7292dc6fbcb3ffe7131eb54053986?utm_source=chatgpt.com","title":"[PDF] Learning Latent Dynamics for Planning from Pixels","snippet":"The Deep Planning Network (PlaNet) is proposed, a purely model-based agent that learns the environment dynamics from images and chooses actions through fast ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/World-Models-Ha-Schmidhuber/ff332c21562c87cab5891d495b7d0956f2d9228b?utm_source=chatgpt.com","title":"[PDF] World Models","snippet":"This work explores building generative neural network models of popular reinforcement learning environments by using features extracted from the world model ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"semanticscholar.org"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/chess/comments/7hvbaz/mastering_chess_and_shogi_by_selfplay_with_a/?utm_source=chatgpt.com","title":"Mastering Chess and Shogi by Self-Play with a General ...","snippet":"In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/?utm_source=chatgpt.com","title":"AMA: We are Noam Brown and Tuomas Sandholm ...","snippet":"We recently developed the poker AI Pluribus, which has proven capable of defeating elite human professionals in six-player no-limit Texas hold' ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/7jn12v/ama_we_are_noam_brown_and_professor_tuomas/?utm_source=chatgpt.com","title":"We are Noam Brown and Professor Tuomas Sandholm ...","snippet":"Earlier this year our AI Libratus defeated top pros for the first time in no-limit poker (specifically heads-up no-limit Texas hold'em). We ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/42ytdx/pdf_mastering_the_game_of_go_with_deep_neural/?utm_source=chatgpt.com","title":"[PDF] Mastering the Game of Go with Deep Neural ...","snippet":"Nature: Mastering the game of Go with deep neural networks and tree search. YouTube by nature: The computer that mastered Go. Google Blog ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/dzakrs/r_191108265_mastering_atari_go_chess_and_shogi_by/?utm_source=chatgpt.com","title":"[R] [1911.08265] Mastering Atari, Go, Chess and Shogi by ...","snippet":"This paper trains a policy, value-function, reward-predictor, and transition-model, and uses them in a very similar way to AlphaGo. The policy ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/5mxmzt/170101724_deepstack_expertlevel_artificial/?utm_source=chatgpt.com","title":"Expert-Level Artificial Intelligence in No-Limit Poker. Beats ...","snippet":"In this paper we introduce DeepStack, a new algorithm for imperfect information settings such as poker. It combines recursive reasoning to ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/dpbper/r_alphastar_grandmaster_level_in_starcraft_ii/?utm_source=chatgpt.com","title":"[R] AlphaStar: Grandmaster level in StarCraft II using multi- ...","snippet":"You'll mostly see the flashy papers in nature because that is what they select for and those are the projects where deepmind might see value ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/chess/comments/5iv8ft/what_evaluation_function_does_stockfish_use/?utm_source=chatgpt.com","title":"What evaluation function does stockfish use? : r/chess","snippet":"I was wondering what kind of evaluation function stockfish uses. Is it a neural network? A linear function? A linear function with an activation function ( ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/programming/comments/143gskm/google_finds_faster_sorting_algorithm_using_deep/?utm_source=chatgpt.com","title":"Google finds faster sorting algorithm using deep ...","snippet":"We reverse engineered the low-level assembly sorting algorithms discovered by AlphaDev for sort 3, sort 4 and sort 5 to C++ and discovered that ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/machinelearningnews/comments/zmxxp0/google_ai_introduces_robotics_transformer_1_rt1_a/?utm_source=chatgpt.com","title":"Google AI Introduces Robotics Transformer 1 (RT-1), A Multi ...","snippet":"Google AI Introduces Robotics Transformer 1 (RT-1), A Multi-Task Model That Tokenizes Robot Inputs And Outputs Actions To Enable Efficient ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"aiproduct.engineer","entries":[{"type":"search_result","url":"https://aiproduct.engineer/ai-ecosystem/meta-ai-diplomacy-cicero-8fefbae06b66?utm_source=chatgpt.com","title":"Meta AI Diplomacy Cicero - AIPE - AI Product Engineer","snippet":"Meta AI Diplomacy Cicero develops AI models using deep learning algorithms and reinforcement learning strategies. Their primary contributions include the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"aiproduct.engineer"}]},{"type":"search_result_group","domain":"hxim.github.io","entries":[{"type":"search_result","url":"https://hxim.github.io/Stockfish-Evaluation-Guide/?utm_source=chatgpt.com","title":"Stockfish Evaluation Guide - GitHub Pages","snippet":"Aug 3, 2020 \u2014 An evaluation function is used to heuristically determine the relative value of a positions used in general case when no specialized evaluation or tablebase ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"hxim.github.io"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/official-stockfish/networks?utm_source=chatgpt.com","title":"Evaluation networks for Stockfish","snippet":"This repository contains the networks as required by the NNUE evaluation in Stockfish. https://github.com/official-stockfish/Stockfish/","ref_id":{"turn_index":0,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/official-stockfish/Stockfish/discussions/4678?utm_source=chatgpt.com","title":"About removal of classical evaluation. #4678","snippet":"The classical evaluation version of SF at that version, was the \"orable\" trainer for the NN to approximate on some big position set.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/google-research/planet?utm_source=chatgpt.com","title":"Learning Latent Dynamics for Planning from Pixels","snippet":"May 28, 2024 \u2014 This project provides the open source implementation of the PlaNet agent introduced in Learning Latent Dynamics for Planning from Pixels.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"chessify.me","entries":[{"type":"search_result","url":"https://chessify.me/blog/chess-engine-evaluation?utm_source=chatgpt.com","title":"Understanding Chess Engine Evaluation Numbers","snippet":"Mar 13, 2023 \u2014 Endgame tablebases (EGTBs or EGTs) are basically databases of precalculated endgame positions. They enable chess engines like Stockfish and ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"chessify.me"}]},{"type":"search_result_group","domain":"laskerfoundation.org","entries":[{"type":"search_result","url":"https://laskerfoundation.org/winners/alphafold-a-technology-for-predicting-protein-structures/?utm_source=chatgpt.com","title":"AlphaFold\u2014for predicting protein structures","snippet":"Sep 2, 2025 \u2014 In 2020, AlphaFold2 soared past the competition in CASP14. Its predictions were accurate to atomic precision and it generated excellent results ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"laskerfoundation.org"}]},{"type":"search_result_group","domain":"justthink.ai","entries":[{"type":"search_result","url":"https://www.justthink.ai/blog/googles-rt-2-revolutionizing-robotics-with-vision-language-and-action?utm_source=chatgpt.com","title":"Google's RT-2: Revolutionizing Robotics with Vision, ...","snippet":"Google's RT-2 has emerged as a groundbreaking AI model that translates vision and language into tangible robotic actions. By overcoming the challenges ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"justthink.ai"}]},{"type":"search_result_group","domain":"substack.com","entries":[{"type":"search_result","url":"https://dianawolftorres.substack.com/p/mastering-go-how-alphago-changed?utm_source=chatgpt.com","title":"Day 4: The Wolf Reads AI- Mastering the Game of Go with ...","snippet":"In 2016, DeepMind published \u201cMastering the Game of Go with Deep Neural Networks and Tree Search,\u201d showing how AlphaGo became the first AI to defeat a human ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":54},"pub_date":null,"attribution":"substack.com"},{"type":"search_result","url":"https://garymarcus.substack.com/p/what-does-meta-ais-diplomacy-winning?utm_source=chatgpt.com","title":"What does Meta AI's Diplomacy-winning Cicero Mean for AI?","snippet":"Cicero the computer program, announced in an article in Science on November 22, is a powerful AI system that plays Diplomacy.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"substack.com"}]},{"type":"search_result_group","domain":"sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0959440X2500003X?utm_source=chatgpt.com","title":"Advancing protein structure prediction beyond AlphaFold2","snippet":"by S Park \u00b7 2025 \u00b7 Cited by 3 \u2014 The release of AlphaFold2 (AF2) in 2021 marked a turning point in protein structure prediction, revolutionizing the field with its remarkable accuracy [1].","ref_id":{"turn_index":3,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"sciencedirect.com"}]},{"type":"search_result_group","domain":"chessbase.com","entries":[{"type":"search_result","url":"https://en.chessbase.com/post/8-piece-endgame-tablebases-first-findings-and-interview?utm_source=chatgpt.com","title":"8-piece endgame tablebases - first findings and interview!","snippet":"May 11, 2022 \u2014 Endgame tablebases can only be described as \"god chess\" as they are perfect information on all positions they cover, and the terms 'better' ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"chessbase.com"}]},{"type":"search_result_group","domain":"learnopencv.com","entries":[{"type":"search_result","url":"https://learnopencv.com/vision-language-action-models-lerobot-policy/?utm_source=chatgpt.com","title":"Vision Language Action Models (VLA) & Policies for Robots","snippet":"Apr 11, 2025 \u2014 RT-2 from Google. Robotic Transformer (RT-2) is a closed source, novel vision language action model developed by Google Deepmind Robotics team.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"learnopencv.com"}]},{"type":"search_result_group","domain":"iclr.cc","entries":[{"type":"search_result","url":"https://iclr.cc/Downloads/2019?utm_source=chatgpt.com","title":"Downloads 2019","snippet":"Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids ... Sample Efficient Imitation Learning for Continuous Control \u00b7 Scalable ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"iclr.cc"}]},{"type":"search_result_group","domain":"chessworld.net","entries":[{"type":"search_result","url":"https://www.chessworld.net/stockfish-chess-engine.asp?utm_source=chatgpt.com","title":"Stockfish Chess Engine \u2013 World's Strongest Open-Source ...","snippet":"76. NNUE version released in 2020. This major update integrated efficient neural network evaluation. 77. Can save ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"chessworld.net"}]},{"type":"search_result_group","domain":"dogeystamp.com","entries":[{"type":"search_result","url":"https://www.dogeystamp.com/chess6/?utm_source=chatgpt.com","title":"Chess engine, pt. 6: Neural-net evaluation","snippet":"May 28, 2025 \u2014 The goal of NNUE is to replace the evaluation function in our engine (which might be material counting, or piece-square table, or any other HCE ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"dogeystamp.com"}]},{"type":"search_result_group","domain":"effectivealtruism.org","entries":[{"type":"search_result","url":"https://forum.effectivealtruism.org/posts/ofuGvRMjLx6gMLinF/meta-ai-announces-cicero-human-level-diplomacy-play-with?utm_source=chatgpt.com","title":"Meta AI announces Cicero: Human-Level Diplomacy play ...","snippet":"Nov 22, 2022 \u2014 We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"effectivealtruism.org"}]},{"type":"search_result_group","domain":"syzygy-tables.info","entries":[{"type":"search_result","url":"https://syzygy-tables.info/?utm_source=chatgpt.com","title":"Syzygy endgame tablebases: KvK","snippet":"Syzygy tablebases allow perfect play with up to 7 pieces, both with and without the fifty-move drawing rule.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"syzygy-tables.info"}]},{"type":"search_result_group","domain":"stackoverflow.com","entries":[{"type":"search_result","url":"https://stackoverflow.com/questions/76528409/trying-to-understand-the-new-sorting-algorithm-from-alphadev-why-does-my-assemb?utm_source=chatgpt.com","title":"Trying to understand the new sorting algorithm from ...","snippet":"The article indirectly links their paper published in Nature (Faster sorting algorithms discovered using deep reinforcement learning by Daniel J ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"stackoverflow.com"}]},{"type":"search_result_group","domain":"junshern.github.io","entries":[{"type":"search_result","url":"https://junshern.github.io/paper-reading-group/2021/01/02/world-models.html?utm_source=chatgpt.com","title":"World models.","snippet":"Jan 2, 2021 \u2014 Ha, David, and J\u00fcrgen Schmidhuber. \u201cWorld models.\u201d arXiv preprint arXiv:1803.10122 (2018). This paper really knocked my socks off - the main ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"junshern.github.io"}]},{"type":"search_result_group","domain":"nsf.gov","entries":[{"type":"search_result","url":"https://par.nsf.gov/servlets/purl/10077416?utm_source=chatgpt.com","title":"Superhuman AI for heads-up no-limit poker: Libratus beats ...","snippet":"by N Brown \u00b7 2018 \u00b7 Cited by 1048 \u2014 Our game-theoretic approach features application- independent techniques: an algorithm for computing a blueprint for the overall strategy, an ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"nsf.gov"}]},{"type":"search_result_group","domain":"frontiersin.org","entries":[{"type":"search_result","url":"https://www.frontiersin.org/journals/bioinformatics/articles/10.3389/fbinf.2023.1120370/full?utm_source=chatgpt.com","title":"Before and after AlphaFold2: An overview of protein ...","snippet":"by LMF Bertoline \u00b7 2023 \u00b7 Cited by 245 \u2014 Presented at CASP14 between May and July 2020, AlphaFold2 predicted protein structures with more accuracy than other competing methods, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"frontiersin.org"}]},{"type":"search_result_group","domain":"theverge.com","entries":[{"type":"search_result","url":"https://www.theverge.com/2019/7/11/20690078/ai-poker-pluribus-facebook-cmu-texas-hold-em-six-player-no-limit?utm_source=chatgpt.com","title":"Facebook and CMU's 'superhuman' poker AI beats human ...","snippet":"Jul 11, 2019 \u2014 An AI poker player, built by researchers from Facebook and Carnegie Mellon University, has beaten some of the world's top players.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"theverge.com"}]},{"type":"search_result_group","domain":"acm.org","entries":[{"type":"search_result","url":"https://cacm.acm.org/blogcacm/what-does-meta-ais-diplomacy-winning-cicero-mean-for-ai/?utm_source=chatgpt.com","title":"What Does Meta AI's Diplomacy-Winning Cicero Mean for AI?","snippet":"Nov 28, 2022 \u2014 Cicero the computer program, announced in an article in Science on November 22, is a powerful AI system that plays Diplomacy; its implications ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"acm.org"}]},{"type":"search_result_group","domain":"itu.int","entries":[{"type":"search_result","url":"https://aiforgood.itu.int/event/alphadev-discovering-faster-sorting-algorithms-with-reinforcement-learning/?utm_source=chatgpt.com","title":"AlphaDev: Discovering Faster Sorting Algorithms ... - AI for Good","snippet":"AlphaDev engages in a unique single-player game, one aimed at uncovering the most rapid sorting algorithm possible.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"itu.int"}]},{"type":"search_result_group","domain":"talkchess.com","entries":[{"type":"search_result","url":"https://talkchess.com/viewtopic.php?t=83170&utm_source=chatgpt.com","title":"How to get started with NNUE","snippet":"Jan 12, 2024 \u2014 I am currently developing the Shuffle Chess Engine and have recently made the decision to integrate NNUE (Efficiently Updatable Neural Network) into my project.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"talkchess.com"}]},{"type":"search_result_group","domain":"actuia.com","entries":[{"type":"search_result","url":"https://www.actuia.com/en/news/deepmind-speeds-up-discovery-of-matrix-multiplication-algorithms-with-alphatensor/?utm_source=chatgpt.com","title":"DeepMind speeds up discovery of matrix multiplication ...","snippet":"Oct 17, 2022 \u2014 AlphaTensor has discovered algorithms that outperform state-of-the-art complexity for many matrix sizes, including 4 x 4 matrices in a finite ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"actuia.com"}]},{"type":"search_result_group","domain":"aicorespot.io","entries":[{"type":"search_result","url":"https://aicorespot.io/alphastar-grandmaster-in-starcraft-ii-leveraging-multi-agent-reinforcement-learning/?utm_source=chatgpt.com","title":"AlphaStar Grandmaster in StarCraft II leveraging multi- ...","snippet":"Aug 9, 2021 \u2014 Leveraging the advances detailed in the Nature paper, AlphaStar received a ranking above 99.8% of active users on Battle.net, and accomplished ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"aicorespot.io"}]},{"type":"search_result_group","domain":"forbes.com","entries":[{"type":"search_result","url":"https://www.forbes.com/sites/carlieporterfield/2022/11/22/metas-ai-gamer-beat-humans-in-diplomacy-using-strategy-and-negotiation/?utm_source=chatgpt.com","title":"Meta's AI Gamer Beat Humans In Diplomacy, Using ...","snippet":"Nov 22, 2022 \u2014 Cicero was able to pass as a human player against 82 participants the agent faced off against, and Meta researchers said they found no evidence ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"forbes.com"}]},{"type":"search_result_group","domain":"mathscholar.org","entries":[{"type":"search_result","url":"https://mathscholar.org/2022/10/machine-learning-program-finds-new-matrix-multiplication-algorithms/?utm_source=chatgpt.com","title":"Machine learning program finds new matrix multiplication ...","snippet":"Oct 26, 2022 \u2014 AlphaTensor found a scheme that produces the result in 47 47 block multiplications (although this result is valid only for matrices with binary ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"mathscholar.org"}]},{"type":"search_result_group","domain":"deepstack.ai","entries":[{"type":"search_result","url":"https://www.deepstack.ai/?utm_source=chatgpt.com","title":"DeepStack","snippet":"DeepStack. Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker ... With a study completed in December 2016 and published in Science in March 2017 ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"deepstack.ai"}]},{"type":"search_result_group","domain":"usaeop.com","entries":[{"type":"search_result","url":"https://www.usaeop.com/blog/alphatensor-machine-learning-used-to-accelerate-machine-learning/?utm_source=chatgpt.com","title":"AlphaTensor: Machine Learning Used to Accelerate ...","snippet":"Dec 13, 2022 \u2014 The algorithms found by AlphaTensor for large matrices were 10-20% faster than commonly used matrix multiplication algorithms on the same ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"usaeop.com"}]},{"type":"search_result_group","domain":"stanford.edu","entries":[{"type":"search_result","url":"https://web.stanford.edu/class/archive/cs/cs221/cs221.1192/sections/section5.pdf?utm_source=chatgpt.com","title":"AlphaZero","snippet":"Paper: Mastering Chess and Shogi by Self-Play with a General Reinforcement. Learning Algorithm. Published in: Nature, October 18 2017.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"stanford.edu"}]},{"type":"search_result_group","domain":"unsw.edu.au","entries":[{"type":"search_result","url":"https://www.unsw.edu.au/newsroom/news/2022/11/an-ai-named-cicero-can-beat-humans-in-diplomacy--a-complex-allia?utm_source=chatgpt.com","title":"An AI named Cicero can beat humans in Diplomacy, a ...","snippet":"The main reason Cicero's performance is a scientific breakthrough is that it can both play the game well, and also perform these informal negotiations.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"unsw.edu.au"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://papers.neurips.cc/paper/8778-learning-to-predict-without-looking-ahead-world-models-without-forward-prediction.pdf?utm_source=chatgpt.com","title":"Learning to Predict Without Looking Ahead: World Models ...","snippet":"by CD Freeman \u00b7 Cited by 54 \u2014 World Model Learned with Peek Probability. Mean Cumulative Reward. Ha and Schmidhuber (2018): 906 \u00b1 21. Risi and Stanley (2019): 903 \u00b1 72 chamption solution: ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"dmorrill10.github.io","entries":[{"type":"search_result","url":"https://dmorrill10.github.io/assets/publications/17science.pdf?utm_source=chatgpt.com","title":"Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker","snippet":"by M Moravc\u0131k \u00b7 Cited by 1359 \u2014 We show that. DeepStack is theoretically sound, produces strategies substantially more difficult to exploit than abstraction-based techniques, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"dmorrill10.github.io"}]},{"type":"search_result_group","domain":"arstechnica.com","entries":[{"type":"search_result","url":"https://arstechnica.com/science/2019/07/facebook-ai-pluribus-defeats-top-poker-professionals-in-6-player-texas-holdem/?utm_source=chatgpt.com","title":"Facebook AI Pluribus defeats top poker professionals in 6- ...","snippet":"Jul 11, 2019 \u2014 An AI dubbed Pluribus, which took on 15 professional human players in six-player no-limit Texas Hold 'em and won.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"arstechnica.com"}]},{"type":"search_result_group","domain":"decodechess.com","entries":[{"type":"search_result","url":"https://decodechess.com/stockfish-12-nnue-free/?utm_source=chatgpt.com","title":"Stockfish 12 (NNUE): Free Access & User Guide","snippet":"With Stockfish 12, which was released in August 2020 ... The big change in this release is the addition of an efficiently updatable neural network (NNUE).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"decodechess.com"}]},{"type":"search_result_group","domain":"purdue.edu","entries":[{"type":"search_result","url":"https://www.math.purdue.edu/~allen450/content/alphazero-talk.pdf?utm_source=chatgpt.com","title":"Mastering Chess and Shogi by Self-Play with a General ...","snippet":"by T Allen \u00b7 Cited by 2870 \u2014 Page 1. Mastering Chess and Shogi by. Self-Play with a General. Reinforcement Learning. Algorithm. Tony Allen and Gavin Glenn. Page 2. Chess, Shogi, and Go.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"purdue.edu"}]},{"type":"search_result_group","domain":"ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=15858197&utm_source=chatgpt.com","title":"Mastering Chess and Shogi by Self-Play with General ...","snippet":"Dec 6, 2017 \u2014 Well, it turns out that AG Zero doesn't work as well in chess: it works better as it only takes 4 hours of training to beat Stockfish. This is ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"ycombinator.com"}]},{"type":"search_result_group","domain":"psc.edu","entries":[{"type":"search_result","url":"https://www.psc.edu/poker-ai-papers-in-science-win-award/?utm_source=chatgpt.com","title":"\u201cSuperhuman\u201d Poker AI Paper in Science","snippet":"Dec 18, 2017 \u2014 \u201cLibratus\u201d beat four of the world's best human players by breaking the game into computationally manageable parts and, based on its opponents' ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"psc.edu"},{"type":"search_result","url":"https://www.psc.edu/pluribus/?utm_source=chatgpt.com","title":"Life Isn't Chess\u2014Times Six","snippet":"Oct 1, 2019 \u2014 Like the earlier AI, Pluribus taught itself to play Texas Hold'em poker before facing the pros. Like Libratus, Pluribus also discovered ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"psc.edu"}]},{"type":"search_result_group","domain":"ucl.ac.uk","entries":[{"type":"search_result","url":"https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf?utm_source=chatgpt.com","title":"Mastering the Game of Go without Human Knowledge","snippet":"by D Silver \u00b7 2017 \u00b7 Cited by 12667 \u2014 This neural network improves the strength of tree search, re- sulting in higher quality move selection and stronger self-play in the next iteration. Starting.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"ucl.ac.uk"}]},{"type":"search_result_group","domain":"cuni.cz","entries":[{"type":"search_result","url":"https://ktiml.mff.cuni.cz/~bartak/ui_seminar/talks/2017ZS/KarelHa_AlphaZero.pdf?utm_source=chatgpt.com","title":"AlphaZero - Mastering Chess and Shogi by Self-Play with a ...","snippet":"Dec 19, 2017 \u2014 Silver, David et al. (2017a). \u201cMastering Chess and Shogi by Self-Play with a General Reinforcement Learning. Algorithm\u201d. In: arXiv preprint ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"cuni.cz"}]},{"type":"search_result_group","domain":"wpi.edu","entries":[{"type":"search_result","url":"https://users.wpi.edu/~yli15/courses/DS504Fall16/slides/BDA-Fall16-AlphaGo.pdf?utm_source=chatgpt.com","title":"Mastering the game of Go with deep neural networks and ...","snippet":"Mastering the game of Go with deep neural networks and tree search. Nature, Jan, 2016. Page 2. Roadmap. \u2022 Deep Learning. \u2022 Search problem. \u2022 How to explore a ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":48},"pub_date":null,"attribution":"wpi.edu"}]},{"type":"search_result_group","domain":"cbwilp-artefacts.s3.ap-south-1.amazonaws.com","entries":[{"type":"search_result","url":"https://cbwilp-artefacts.s3.ap-south-1.amazonaws.com/AIML/SEM2/SUB-DRL/RESEARCH-PAPERS/Mastering%20Atari%2C%20Go%2C%20Chess%20and%20Shogi%20by%20Planning%20with%20a%20Learned%20Model.pdf?utm_source=chatgpt.com","title":"Mastering Atari, Go, Chess and Shogi by Planning with a ...","snippet":"by J Schrittwieser \u00b7 2019 \u00b7 Cited by 3152 \u2014 In this paper, we introduce MuZero, a new approach to model-based RL that achieves state-of-the-art per- formance in Atari 2600, a visually ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":49},"pub_date":null,"attribution":"cbwilp-artefacts.s3.ap-south-1.amazonaws.com"}]},{"type":"search_result_group","domain":"aeaweb.org","entries":[{"type":"search_result","url":"https://www.aeaweb.org/conference/2018/preliminary/paper/h9Bs5dth?utm_source=chatgpt.com","title":"Superhuman AI for heads-up no-limit poker: Libratus beats ...","snippet":"Dec 17, 2017 \u2014 We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":50},"pub_date":null,"attribution":"aeaweb.org"}]},{"type":"search_result_group","domain":"packtpub.com","entries":[{"type":"search_result","url":"https://www.packtpub.com/br-co/learning/tech-news/pluribus-an-ai-bot-built-by-facebook-and-cmu-researchers-has-beaten-professionals-at-six-player-no-limit-texas-hold-em-poker?srsltid=AfmBOopjzR_JnWqw-EWYu2aQFO4CmGIKr8DY82oyueT4apVG6Vb3juRj&utm_source=chatgpt.com","title":"Pluribus, an AI bot built by Facebook and CMU researchers ...","snippet":"Researchers from Facebook and Carnegie Mellon University have developed an AI bot that has defeated human professionals in six-player no-limit Texas Hold'em ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":51},"pub_date":null,"attribution":"packtpub.com"}]},{"type":"search_result_group","domain":"realkm.com","entries":[{"type":"search_result","url":"https://realkm.com/2016/12/28/mastering-the-game-of-go-with-deep-neural-networks-and-tree-search/?utm_source=chatgpt.com","title":"Mastering the game of Go with deep neural networks and ...","snippet":"Dec 28, 2016 \u2014 We built a system, AlphaGo, that combines an advanced tree search with deep neural networks. These neural networks take a description of the Go ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":52},"pub_date":null,"attribution":"realkm.com"}]}],"citations":[],"model_slug":"gpt-5-thinking","request_id":"980257bcdc31fcea-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"579f3102-531e-468c-88b6-63c1f289ad73","timestamp_":"absolute","can_save":true},"recipient":"all","channel":"final"},"parent":"579f3102-531e-468c-88b6-63c1f289ad73","children":["36e83d1c-9b56-4298-a6a8-0364adbd7d22"]},"36e83d1c-9b56-4298-a6a8-0364adbd7d22":{"id":"36e83d1c-9b56-4298-a6a8-0364adbd7d22","message":{"id":"36e83d1c-9b56-4298-a6a8-0364adbd7d22","author":{"role":"user","name":null,"metadata":{}},"create_time":1758175962.818,"update_time":null,"content":{"content_type":"text","parts":["You're absolutely right about stockfish. Its archiecture has improved vastly since the last time I got acquainted with it.\n\nMaybe we should not strive for AGI and ASI directly. Maybe what is needed is to achieve narrow AGI and narrow ASI is a fixed, well defined field first, say, coding, or mathematical reasoning, or any other field human professionals care about."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"980eb177cbaa9610-MAA","message_source":null,"timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"fae4e34e-1195-4331-b690-776bda39b0fc","children":["42d5ed3a-80bd-4716-ab3c-fdb04985eaa0"]},"42d5ed3a-80bd-4716-ab3c-fdb04985eaa0":{"id":"42d5ed3a-80bd-4716-ab3c-fdb04985eaa0","message":{"id":"42d5ed3a-80bd-4716-ab3c-fdb04985eaa0","author":{"role":"system","name":null,"metadata":{}},"create_time":1758175966.9543307,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"980eb177cbaa9610-MAA","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"36e83d1c-9b56-4298-a6a8-0364adbd7d22","timestamp_":"absolute","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"36e83d1c-9b56-4298-a6a8-0364adbd7d22","children":["3be6728f-93f4-4021-a268-a6912c2835f7"]},"3be6728f-93f4-4021-a268-a6912c2835f7":{"id":"3be6728f-93f4-4021-a268-a6912c2835f7","message":{"id":"3be6728f-93f4-4021-a268-a6912c2835f7","author":{"role":"system","name":null,"metadata":{}},"create_time":1758175966.9603386,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"980eb177cbaa9610-MAA","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"42d5ed3a-80bd-4716-ab3c-fdb04985eaa0","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"42d5ed3a-80bd-4716-ab3c-fdb04985eaa0","children":["9207e6f9-4ae8-42b4-9474-0d86dbadf00a"]},"9207e6f9-4ae8-42b4-9474-0d86dbadf00a":{"id":"9207e6f9-4ae8-42b4-9474-0d86dbadf00a","message":{"id":"9207e6f9-4ae8-42b4-9474-0d86dbadf00a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758175967.0025375,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"model_slug":"gpt-5-thinking","request_id":"980eb177cbaa9610-MAA","message_type":"next","default_model_slug":"gpt-5","parent_id":"3be6728f-93f4-4021-a268-a6912c2835f7","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"3be6728f-93f4-4021-a268-a6912c2835f7","children":["1d2a6e80-2220-4d2d-b6c9-576460edf83a"]},"1d2a6e80-2220-4d2d-b6c9-576460edf83a":{"id":"1d2a6e80-2220-4d2d-b6c9-576460edf83a","message":{"id":"1d2a6e80-2220-4d2d-b6c9-576460edf83a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758175967.123917,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Defining narrow AGI/ASI","content":"To define narrow AGI/ASI, I need to emphasize domain-specific capabilities like planning, self-reflection, or learning new tasks. I'll suggest architectures like tool-augmented LLMs, program synthesis, and neuro-symbolic models combined with world models and simulators. Evaluations might involve tasks like HumanEval+ and SWE-bench for coding, and benchmarks like MATH for math.","chunks":[],"finished":true}],"source_analysis_msg_id":"fab548a3-af40-45c4-853d-03d21a9c06e8"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980eb177cbaa9610-MAA","message_type":"next","default_model_slug":"gpt-5","parent_id":"9207e6f9-4ae8-42b4-9474-0d86dbadf00a","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"9207e6f9-4ae8-42b4-9474-0d86dbadf00a","children":["581050eb-cefb-4624-8b0d-c427608c0528"]},"581050eb-cefb-4624-8b0d-c427608c0528":{"id":"581050eb-cefb-4624-8b0d-c427608c0528","message":{"id":"581050eb-cefb-4624-8b0d-c427608c0528","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758175982.414329,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 15s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":15,"model_slug":"gpt-5-thinking","request_id":"980eb177cbaa9610-MAA","message_type":"next","default_model_slug":"gpt-5","parent_id":"1d2a6e80-2220-4d2d-b6c9-576460edf83a","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"1d2a6e80-2220-4d2d-b6c9-576460edf83a","children":["2ad3d0fd-5664-42a7-aaf6-dce52332a28a"]},"2ad3d0fd-5664-42a7-aaf6-dce52332a28a":{"id":"2ad3d0fd-5664-42a7-aaf6-dce52332a28a","message":{"id":"2ad3d0fd-5664-42a7-aaf6-dce52332a28a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758175965.424374,"update_time":null,"content":{"content_type":"text","parts":["I think that\u2019s the right strategic move: **aim for domain-bounded \u201cAGI-like\u201d systems** that are *complete and reliable* within a well-defined field before chasing generality. Call it **Narrow-AGI**: an agent that, inside its domain, can understand new tasks from natural language/specs, plan multi-step solutions, acquire and use new tools, learn from experience, verify its own work, and recover from errors\u2014*end-to-end with minimal human orchestration*.\n\nBelow is a concrete way to think about it and build it\u2014first the definition, then two exemplar domains (coding, mathematical reasoning), followed by evaluation, safety, and a staged roadmap.\n\n# What \u201cNarrow-AGI\u201d should mean (operational, testable)\nWithin a fixed domain \\(D\\), the system should satisfy:\n\n1) **General task understanding:** parse informal goals/specs into plans; handle novel combinations (systematic generalization).  \n2) **Tool formation & use:** discover, integrate, and *compose* tools (APIs, solvers, libraries) not present at pretrain time.  \n3) **World-model / beliefs:** maintain a **stateful memory** of the task, intermediate artifacts, assumptions, and uncertainties; update beliefs with evidence.  \n4) **Self-verification:** produce **checkable artifacts** (tests, proofs, certificates, simulations) and refuse to ship when checks fail.  \n5) **Learning loop:** improve from *its own* experience: derive new skills, repair failures, curate data, and tune policies/adapters without constant human labeling.  \n6) **Robustness:** handle distribution shift *within \\(D\\)* (new codebases, new theorem libraries), and degrade gracefully with uncertainty estimates.  \n7) **Autonomy boundary:** respect constraints (security, budget, time) and request help when outside its competence set.\n\nIf you can show 1\u20137 for \\(D=\\) \u201csoftware engineering\u201d or \\(D=\\) \u201cundergraduate\u2013PhD mathematics problem solving + formal proofs,\u201d you have meaningful Narrow-AGI.\n\n---\n\n# Domain A: Coding (Software Engineering Narrow-AGI)\n\n## Capability stack (minimum viable)\n- **Planner\u2013Executor loop:** task -> decomposition -> tool calls (VCS, build, tests, linters, package mgrs, CI) -> integrate -> PR.  \n- **Ground truth & verifiers:** unit/integration tests, static analyzers (type systems, taint analyzers), fuzzers, property-based tests, formal specs when available.  \n- **Stateful memory:** episodic (per-ticket), semantic (codebase map, design docs), and *procedural* (what worked before).  \n- **Tool acquisition:** read API docs, generate client stubs, scaffold new tests, spin ephemeral envs (containers), migrate dependencies safely.  \n- **Security posture:** least-privilege credentials, policy checks (no secrets in diff), sandboxing, SBOM awareness.  \n- **Learning:** keep an *experience repo*: failures/repairs \u2192 new heuristics, prompts, unit test templates, \u201cfix recipes\u201d.\n\n## Milestones (in rising difficulty)\n1) **Green PR autonomously** on issues with existing tests (multi-repo, languages).  \n2) **Write missing tests** to validate a fix; prove fix by catching the pre-fix failure.  \n3) **Cross-stack tasks** (frontend+backend+infra) with CI/CD updates.  \n4) **Zero-shot library/tool adoption**: given a new library, integrate it after reading docs and writing smoke tests.  \n5) **Legacy rescue**: migrate a medium codebase across major version jumps (Python 3.x, React/Vite, Java 11\u219221) with passing CI.  \n6) **Security-aware changes**: detect and patch injection/authorization bugs; demonstrate exploit reproduction + test-guard.\n\n## Architecture sketch (control plane)\n- **Deliberator (LM/Planner):** builds hierarchical plans, decides when to branch or backtrack.  \n- **Skill library:** tools with typed signatures (build, run, grep, edit, search, test, fuzz, static-analyze, generate-tests).  \n- **Critic & Verifier:** runs analyzers/tests; computes confidence; vetoes unsafe merges.  \n- **Memory:** vector + relational store (code graph, ownership, flakiness history).  \n- **Learner:** runs after each episode to mine new patterns (LoRA/adapter updates, retrieval refresh, test-template synthesis).\n\n---\n\n# Domain B: Mathematical Reasoning (Math Narrow-AGI)\n\n## Capability stack\n- **Two modes:** (i) *Informal* reasoning (problem solving, conjecture formation, solution sketches), (ii) *Formal* proofs in a target prover (Lean/Coq/Isabelle) with **proof certificates**.  \n- **Oracles/tools:** CAS (SymPy/Sage), SMT/SAT, numeric solvers, interval arithmetic, counterexample finders, proof search/tactics.  \n- **Curriculum & self-play:** generate problem families, propose conjectures, attempt proofs, search counterexamples, refine lemmas.  \n- **Library mastery:** learn definitions/lemmas across mathlib/standard libraries; perform *library-aware* tactic selection; robust rewriting.  \n- **Verification loop:** every informal argument must compile to either a formal proof or a **checked** numerical/symbolic certificate (bounds, sums, inequalities, primality certificates, etc.).\n\n## Milestones\n1) **Undergrad capstone** breadth: solve and formally verify standard problems across algebra, analysis, number theory, combinatorics.  \n2) **Mini-research tasks:** repair broken proofs, simplify existing formalizations, port proofs between libraries.  \n3) **Conjecture-to-proof loop:** autonomously conjecture lemmas that *actually get reused* later; produce counterexamples when false.  \n4) **Paper-to-proof:** translate a human preprint section into a mix of formal proofs + CAS certificates.  \n5) **Novel results at small scale:** produce a new lemma/theorem in a niche area with referee-checkable artifacts.\n\n## Architecture sketch\n- **Planner (LM) with math schemas** (induction, extremal, invariants, epsilon-delta, generating functions).  \n- **Tactic policy** trained on proof traces; **neural premise selection**; **proof-state value function** to guide search.  \n- **CAS/SMT bridge** with trust boundaries: every external result accompanied by a checkable certificate or a small, independently verifiable witness.  \n- **Library-aware retrieval** and **lemma suggestion** with anti-lemmatization (avoid bloat).\n\n---\n\n# Evaluation: how to *know* you have Narrow-AGI\nDesign *living* benchmarks that your team cannot overfit by memorization:\n\n- **Continual eval streams:** weekly unseen tasks pulled from fresh repos/problem sets; measure first-try pass, time-to-green, and *repair latency*.  \n- **OOD stressors:** new languages/packages (coding), new theorem libraries/problem sources (math).  \n- **Adversarial eval:** deliberately ambiguous specs; poisoned dependencies; misleading lemmas; flaky tests.  \n- **Auditable artifacts:** every success yields diffs, tests, proofs, logs, resource budgets; reproducibility scripts.  \n- **Human comparator:** pit the agent against *hired professionals* on matched tasks with equal time and constraints.\n\nA single headline number is misleading. Track a *dashboard*:\n- **Autonomy ratio** (human minutes / agent minutes),  \n- **Verifier coverage** (% of steps checked by tests/proofs),  \n- **Rollback rate**, **Security incidents**, **Generalization gap** (ID vs OOD),  \n- **Learning curve** (performance delta per \\(N\\) episodes without new human labels).\n\n---\n\n# Safety & governance (non-optional, even in a \u201cnarrow\u201d domain)\n- **Hard gates:** merging/ publication requires verifier sign-off.  \n- **Capability bounding:** strict tool permissions, rate limits, and cost ceilings.  \n- **Data hygiene:** avoid training leakage from eval sets; track provenance.  \n- **Model interpretability hooks:** keep plan graphs, tool traces, and counterfactual replays.  \n- **Red-team harness:** fuzz prompts/specs; simulate malicious tickets (supply-chain exploits, subtle algebraic fallacies) and require agent refusal or safe handling.\n\n---\n\n# Roadmap (12\u201324 months, discipline-agnostic)\n**Phase 0 \u2013 Substrate**  \n- Choose domain \\(D\\). Stand up: tool APIs, sandboxed runners, verifiers, reproducible envs, and memory stores. Define eval streams and governance.\n\n**Phase 1 \u2013 Competent agent**  \n- Implement Planner\u2013Executor\u2013Verifier\u2013Memory loop.  \n- Achieve \u201cgreen on spec\u2019d tasks\u201d with strong verifiers. Start experience repo & adapter tuning.\n\n**Phase 2 \u2013 Self-improving**  \n- Add curriculum/self-play generation; mine hard cases; automated test/proof synthesis; tactic/policy improvement without human labels.\n\n**Phase 3 \u2013 Robust & trustworthy**  \n- Demonstrate OOD generalization, handle ambiguity, maintain calibrated uncertainty, pass adversarial red-team. Publish auditable artifacts.\n\n**Phase 4 \u2013 Productionization**  \n- SLAs, rollback playbooks, cost controls, drift detection, periodic third-party audits.\n\n---\n\n## A candid note of skepticism\n- \u201cNarrow\u201d doesn\u2019t mean *easy*: the last 10% is where **robustness** and **trust** live. Without verifiers, you\u2019ll get brittle competence that looks good in demos and fails under shift.  \n- Benchmarks are fragile: design them as **distributions**, not static leaderboards, or you\u2019ll Goodhart your way into false progress.  \n- Don\u2019t conflate *tool-use scripts around an LLM* with an agent that **learns**; the learning loop is the differentiator.\n\n---\n\nIf you want, I can turn this into:  \n1) a concrete **engineering spec** for \u201cCoding Narrow-AGI v0.1\u201d (APIs, data schemas, state machine), or  \n2) a **Math Narrow-AGI research plan** (datasets, theorem-prover targets, CAS/SMT certificate formats, training regimes)."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[],"safe_urls":[],"citations":[],"model_slug":"gpt-5-thinking","request_id":"980eb177cbaa9610-MAA","message_type":"next","default_model_slug":"gpt-5","parent_id":"581050eb-cefb-4624-8b0d-c427608c0528","timestamp_":"absolute","can_save":true},"recipient":"all","channel":"final"},"parent":"581050eb-cefb-4624-8b0d-c427608c0528","children":["f5dad917-6da5-4a21-b013-31b853d383c3"]},"f5dad917-6da5-4a21-b013-31b853d383c3":{"id":"f5dad917-6da5-4a21-b013-31b853d383c3","message":{"id":"f5dad917-6da5-4a21-b013-31b853d383c3","author":{"role":"user","name":null,"metadata":{}},"create_time":1758176162.876,"update_time":null,"content":{"content_type":"text","parts":["Investigate the extent to which OpenAI's latest GPT-5-Codex has fulfilled your framework for narrow AGI"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"980eb65a2a99cdf5-MAA","message_source":null,"timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"2ad3d0fd-5664-42a7-aaf6-dce52332a28a","children":["7da9f31d-889d-4685-b1dd-202adc837640"]},"7da9f31d-889d-4685-b1dd-202adc837640":{"id":"7da9f31d-889d-4685-b1dd-202adc837640","message":{"id":"7da9f31d-889d-4685-b1dd-202adc837640","author":{"role":"system","name":null,"metadata":{}},"create_time":1758176167.3748097,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"980eb65a2a99cdf5-MAA","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"f5dad917-6da5-4a21-b013-31b853d383c3","timestamp_":"absolute","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"f5dad917-6da5-4a21-b013-31b853d383c3","children":["d56cf392-10a2-4072-8962-16dcb4021c39"]},"d56cf392-10a2-4072-8962-16dcb4021c39":{"id":"d56cf392-10a2-4072-8962-16dcb4021c39","message":{"id":"d56cf392-10a2-4072-8962-16dcb4021c39","author":{"role":"system","name":null,"metadata":{}},"create_time":1758176167.390761,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"980eb65a2a99cdf5-MAA","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"7da9f31d-889d-4685-b1dd-202adc837640","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"7da9f31d-889d-4685-b1dd-202adc837640","children":["45fc20b4-bf92-4fc7-8c95-6f820aec1821"]},"45fc20b4-bf92-4fc7-8c95-6f820aec1821":{"id":"45fc20b4-bf92-4fc7-8c95-6f820aec1821","message":{"id":"45fc20b4-bf92-4fc7-8c95-6f820aec1821","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758176167.633768,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Thinking longer for a better answer","content":"","chunks":[],"finished":true}],"source_analysis_msg_id":"8091e85c-edc7-4c2b-b5a0-9a843b65f5a4"},"status":"in_progress","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980eb65a2a99cdf5-MAA","message_type":"next","default_model_slug":"gpt-5","parent_id":"d56cf392-10a2-4072-8962-16dcb4021c39","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"d56cf392-10a2-4072-8962-16dcb4021c39","children":["48b33e9f-1688-4628-97fa-bccb2a15200c"]},"48b33e9f-1688-4628-97fa-bccb2a15200c":{"id":"48b33e9f-1688-4628-97fa-bccb2a15200c","message":{"id":"48b33e9f-1688-4628-97fa-bccb2a15200c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758176164.883261,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"OpenAI GPT-5-Codex\"}, {\"q\": \"GPT-5 Codex OpenAI code generation 2025 site:openai.com\"}, {\"q\": \"OpenAI Codex successor 2025 blog\"}, {\"q\": \"OpenAI API code generation model list 2025 site:platform.openai.com\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"OpenAI GPT-5-Codex"},{"type":"search","q":"GPT-5 Codex OpenAI code generation 2025 site:openai.com"},{"type":"search","q":"OpenAI Codex successor 2025 blog"},{"type":"search","q":"OpenAI API code generation model list 2025 site:platform.openai.com"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Browsing official OpenAI domains for GPT-5 Codex","classifier_response":"default","request_id":"980eb65a2a99cdf5-MAA","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"45fc20b4-bf92-4fc7-8c95-6f820aec1821","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"45fc20b4-bf92-4fc7-8c95-6f820aec1821","children":["8582387c-00ca-4573-80f8-a706b87b0641"]},"8582387c-00ca-4573-80f8-a706b87b0641":{"id":"8582387c-00ca-4573-80f8-a706b87b0641","message":{"id":"8582387c-00ca-4573-80f8-a706b87b0641","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758176170.5752802,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["OpenAI GPT-5-Codex","GPT-5 Codex OpenAI code generation 2025 site:openai.com","OpenAI Codex successor 2025 blog","OpenAI API code generation model list 2025 site:platform.openai.com"]},"reasoning_status":"is_reasoning","reasoning_title":"Browsing official OpenAI domains for GPT-5 Codex","classifier_response":"default","request_id":"980eb65a2a99cdf5-MAA","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"48b33e9f-1688-4628-97fa-bccb2a15200c","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"48b33e9f-1688-4628-97fa-bccb2a15200c","children":["733a143f-144d-453f-a422-267e42f3094e"]},"733a143f-144d-453f-a422-267e42f3094e":{"id":"733a143f-144d-453f-a422-267e42f3094e","message":{"id":"733a143f-144d-453f-a422-267e42f3094e","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1758176170.53739,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.ft.com","entries":[{"type":"search_result","url":"https://www.ft.com/content/c2f7e7ef-df7b-4b74-a899-1cb12d663ce6","title":"DeepMind and OpenAI achieve gold at 'coding Olympics' in AI milestone","snippet":"Google DeepMind and OpenAI achieved a significant milestone in artificial intelligence by reaching \"gold-medal level\" performance at the International Collegiate Programming Contest (ICPC) World Finals, often dubbed the \"coding Olympics.\" Although not official participants, both companies tested their latest AI systems under the same contest conditions as top human teams. OpenAI's GPT-5 model solved all 12 problems, 11 on the first try, which would have placed it first overall. DeepMind's Gemini 2.5 Deep Think ranked second and uniquely solved a problem no human team could.\n\nThe ICPC is renowned for its difficulty, requiring abstract reasoning, logic, and creativity within a five-hour time limit. Only four out of 139 human teams won gold medals this year, each team comprising three members sharing a single computer.\n\nThis development is seen as a step toward Artificial General Intelligence (AGI), highlighting AI's increasing ability to assist in complex programming and scientific tasks. However, experts note that success in competitive coding does not directly equate to practical software engineering performance. Nonetheless, the breakthrough points to AI\u2019s potential in transforming a range of disciplines reliant on mathematics and programming.","ref_id":null,"pub_date":1758128408.914,"attribution":"www.ft.com"}]},{"type":"search_result_group","domain":"www.techradar.com","entries":[{"type":"search_result","url":"https://www.techradar.com/pro/openai-launches-gpt-5-codex-with-a-74-5-percent-success-rate-on-real-world-coding","title":"OpenAI launches GPT-5-Codex with a 74.5% success rate on real world coding","snippet":"OpenAI has unveiled GPT-5-Codex, a specialized version of GPT-5 tailored for software engineering and coding tasks, achieving a 74.5% success rate on the SWE-bench Verified benchmark and a significant improvement in refactoring performance (51.3%, up from 33.9% in GPT-5). This release marks a substantial upgrade in autonomous coding capabilities, with the model able to work independently on large projects for over seven hours. GPT-5-Codex adjusts its reasoning dynamically\u2014delivering faster results on simple tasks and deeper processing for complex ones.\n\nDesigned to assist with tasks ranging from backend bug-fixing to frontend visual improvements, it supports code reviews, screenshot analysis, and mobile web design. It consumes 93.7% fewer tokens than GPT-5 in lightweight interactions, while also supporting extensive reasoning and testing when needed. The release follows earlier launches of Codex CLI and Codex Web, which have now been unified under the broader ChatGPT experience.\n\nGPT-5-Codex is available through ChatGPT Plus, Pro, Business, Edu, and Enterprise plans, and integrates with terminals, IDEs, GitHub, web apps, and iOS. It includes transparent logs, citations, and test results, with API access expected soon for CLI users.","ref_id":null,"pub_date":1758034800.0,"attribution":"www.techradar.com"}]},{"type":"search_result_group","domain":"omni.se","entries":[{"type":"search_result","url":"https://omni.se/a/wgazzP","title":"Open AI sl\u00e4pper ny version av GPT 5 f\u00f6r kodning","snippet":"OpenAI har lanserat en ny version av sitt AI-kodningsverktyg Codex, baserad p\u00e5 GPT-5 och kallad GPT-5 Codex. Den nya modellen kan anpassa hur l\u00e5ng bet\u00e4nketid den har f\u00f6r att l\u00f6sa uppgifter \u2013 fr\u00e5n n\u00e5gra sekunder till sju timmar \u2013 vilket enligt f\u00f6retaget f\u00f6rb\u00e4ttrar resultaten i tester. Uppgraderingen \u00e4r initialt tillg\u00e4nglig f\u00f6r betalande anv\u00e4ndare och kommer att bli tillg\u00e4nglig f\u00f6r API-kunder vid ett senare tillf\u00e4lle. Lanseringen sker i ett klimat av \u00f6kad konkurrens inom AI-kodning, med starka akt\u00f6rer som Claude Code, Anysphere Cursor och Microsofts GitHub Copilot. Cursor har n\u00e5tt 500 miljoner dollar i \u00e5terkommande int\u00e4kter. GPT-5 Codex beskrivs av OpenAI som snabbare, mer p\u00e5litlig och b\u00e4ttre p\u00e5 samarbete i realtid j\u00e4mf\u00f6rt med tidigare versioner.","ref_id":null,"pub_date":1758004470.0,"attribution":"omni.se"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/introducing-upgrades-to-codex/","title":"Introducing upgrades to Codex - OpenAI","snippet":"Codex just got faster, more reliable, and better at real-time collaboration and tackling tasks independently anywhere you develop\u2014whether ...","ref_id":null,"pub_date":1757945550.0,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/","title":"Addendum to GPT-5 system card: GPT-5-Codex","snippet":"3 days ago \u2014 This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex.","ref_id":null,"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/gpt-4-1/","title":"Introducing GPT-4.1 in the API - OpenAI","snippet":"Today, we're launching three new models in the API: GPT\u20114.1, GPT\u20114.1 mini, and GPT\u20114.1 nano. These models outperform GPT\u20114o and GPT\u20114o mini ...","ref_id":null,"pub_date":1744588800.0,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/introducing-gpt-5-for-developers/","title":"Introducing GPT\u20115 for developers","snippet":"Aug 7, 2025 \u2014 It excels at producing high-quality code and handling tasks such as fixing bugs, editing code, and answering questions about complex codebases.","ref_id":null,"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/introducing-codex/","title":"Introducing Codex - OpenAI","snippet":"Codex can perform tasks for you such as writing features, answering questions about your codebase, fixing bugs, and proposing pull requests for review.","ref_id":null,"pub_date":1747353600.0,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/introducing-gpt-5/","title":"Introducing GPT-5","snippet":"Aug 7, 2025 \u2014 GPT\u20115 is our strongest coding model to date. It shows particular improvements in complex front\u2011end generation and debugging larger repositories.","ref_id":null,"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/introducing-o3-and-o4-mini/","title":"Introducing OpenAI o3 and o4-mini","snippet":"Update on June 10, 2025: OpenAI o3\u2011pro is now available to Pro users in ChatGPT, as well as in our API. Like OpenAI o1\u2011pro, o3\u2011pro is a ...","ref_id":null,"pub_date":1744761600.0,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/codex/","title":"Codex","snippet":"Codex is OpenAI's series of AI coding tools that help developers move faster by delegating tasks to powerful cloud and local coding agents.","ref_id":null,"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"northflank.com","entries":[{"type":"search_result","url":"https://northflank.com/blog/claude-code-vs-openai-codex","title":"Claude Code vs OpenAI Codex: which is better in 2025?","snippet":"2 days ago \u2014 Compare Claude Code vs OpenAI Codex for 2025. Learn costs, setup, and self-hosting alternatives with Northflank to choose the right AI tool.","ref_id":null,"pub_date":null,"attribution":"northflank.com"}]},{"type":"search_result_group","domain":"platform.openai.com","entries":[{"type":"search_result","url":"https://platform.openai.com/docs/changelog","title":"Changelog - OpenAI API","snippet":"April, 2025 \u00b7 Added two new o-series reasoning models, o3 and o4-mini . They set a new standard for math, science, and coding, visual reasoning tasks, and ...","ref_id":null,"pub_date":null,"attribution":"platform.openai.com"},{"type":"search_result","url":"https://platform.openai.com/docs/api-reference/chat/update","title":"API Reference - OpenAI API","snippet":"Complete reference documentation for the OpenAI API, including examples and code snippets for our endpoints in Python, cURL, and Node.js.","ref_id":null,"pub_date":null,"attribution":"platform.openai.com"},{"type":"search_result","url":"https://platform.openai.com/docs/models/gpt-4.5-preview","title":"Model - OpenAI API","snippet":"This is a research preview of GPT-4.5, our largest and most capable GPT model yet. Its deep world knowledge and better understanding of user intent makes it ...","ref_id":null,"pub_date":null,"attribution":"platform.openai.com"},{"type":"search_result","url":"https://platform.openai.com/docs/examples","title":"Prompt examples - OpenAI API","snippet":"Explore what's possible with some example prompts. All categories, Extract, Generate, Transform, Code, Natural Language, Structured Data. All categories.","ref_id":null,"pub_date":null,"attribution":"platform.openai.com"}]},{"type":"search_result_group","domain":"www.latent.space","entries":[{"type":"search_result","url":"https://www.latent.space/p/gpt5-codex","title":"How GPT5 + Codex took over Agentic Coding \u2014 ft. Greg ...","snippet":"That seems to have ignited a fire in OpenAI, which of course shipped the original 2021 Codex that kicked off GitHub Copilot, the original AI ...","ref_id":null,"pub_date":1757981803.0,"attribution":"www.latent.space"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40aliazimidarmian/openai-codex-from-2021-code-model-to-a-2025-autonomous-coding-agent-85ef0c48730a","title":"OpenAI Codex: From 2021 Code Model to a 2025 ...","snippet":"The 2025 incarnation of OpenAI Codex is described as a cloud-based software engineering agent that can work on many tasks in parallel.","ref_id":null,"pub_date":1748995256.762,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/OpenAI/comments/1nhuoxw/sam_altman_just_announced_gpt5_codex_better_at/","title":"Sam Altman Just Announced GPT-5 Codex better at ...","snippet":"OpenAI has officially announced GPT-5 Codex, a specialized version of GPT-5 designed for agentic coding.","ref_id":null,"pub_date":1757962668.421,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"devops.com","entries":[{"type":"search_result","url":"https://devops.com/openais-gpt-5-codex-a-smarter-approach-to-enterprise-development/","title":"OpenAI's GPT-5-Codex: A Smarter Approach to Enterprise ...","snippet":"23 hours ago \u2014 OpenAI's GPT-5-Codex upgrade delivers smarter code reviews, faster performance, and seamless IDE integration for development teams.","ref_id":null,"pub_date":1758093960.0,"attribution":"devops.com"}]}],"debug_sonic_thread_id":"thread_68cba3aaaa248325b642fb7c3e56ff3d","reasoning_status":"is_reasoning","reasoning_title":"Browsing official OpenAI domains for GPT-5 Codex","classifier_response":"default","request_id":"980eb65a2a99cdf5-MAA","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"8582387c-00ca-4573-80f8-a706b87b0641","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"8582387c-00ca-4573-80f8-a706b87b0641","children":["41c3d69f-44d9-4453-b259-afa9ab6849ad"]},"41c3d69f-44d9-4453-b259-afa9ab6849ad":{"id":"41c3d69f-44d9-4453-b259-afa9ab6849ad","message":{"id":"41c3d69f-44d9-4453-b259-afa9ab6849ad","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758176181.752936,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Prioritizing OpenAI sources","content":"I\u2019ll focus on citing official OpenAI pages, like those from openai.com, which include system updates, API changelogs, and Codex. These sources should be sufficient for now. I\u2019ll evaluate OpenAI\u2019s products against the Narrow-AGI framework, giving a scorecard for coding domains based on performance and capabilities like time for dynamic reasoning.","chunks":[],"finished":true},{"summary":"Refining Codex sources","content":"The \"Introducing Codex\" page is a bit vague, but other pages mention RL on coding tasks, agentic coding in Codex, and cloud sandbox environments. I\u2019ll use the 74.9% SWE-bench Verified score as an official benchmark. External claims on runtime or refactoring will be omitted or caveated. I'll focus on documented features and success rates.","chunks":[],"finished":true}],"source_analysis_msg_id":"a0fa6ac8-629e-4883-9c98-f00726a2726e"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"980eb65a2a99cdf5-MAA","message_type":"next","default_model_slug":"gpt-5","parent_id":"733a143f-144d-453f-a422-267e42f3094e","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"733a143f-144d-453f-a422-267e42f3094e","children":["9444467d-a72d-4c74-8080-41b9d43a9f02"]},"9444467d-a72d-4c74-8080-41b9d43a9f02":{"id":"9444467d-a72d-4c74-8080-41b9d43a9f02","message":{"id":"9444467d-a72d-4c74-8080-41b9d43a9f02","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758176194.858692,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 27s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":27,"model_slug":"gpt-5-thinking","request_id":"980eb65a2a99cdf5-MAA","message_type":"next","default_model_slug":"gpt-5","parent_id":"41c3d69f-44d9-4453-b259-afa9ab6849ad","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"41c3d69f-44d9-4453-b259-afa9ab6849ad","children":["fc3f7ad2-5be5-4aa3-a0c2-755e392fc6e2"]},"fc3f7ad2-5be5-4aa3-a0c2-755e392fc6e2":{"id":"fc3f7ad2-5be5-4aa3-a0c2-755e392fc6e2","message":{"id":"fc3f7ad2-5be5-4aa3-a0c2-755e392fc6e2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1758176173.407562,"update_time":null,"content":{"content_type":"text","parts":["Short answer: **yes\u2014GPT-5-Codex is the strongest real-world attempt at \u201cNarrow-AGI for coding\u201d so far**, and it already implements several pillars of the framework we outlined. But it\u2019s **not a finished Narrow-AGI** yet: verification coverage, distribution-shift robustness, long-horizon project stewardship, and formal safety/guarantee layers remain open.\n\nBelow is a crisp scorecard against the 7 Narrow-AGI criteria (for the domain \\(D=\\) software engineering), with load-bearing evidence from OpenAI\u2019s own materials.\n\n# Scorecard \u2014 GPT-5-Codex vs. Narrow-AGI (coding)\n\n1) **General task understanding** \u2014 *Strong progress*  \nParses natural-language tickets, answers codebase questions, writes features, fixes bugs, and proposes PRs. OpenAI describes Codex as a **cloud software-engineering agent** that can \u201cwrite features, answer questions about your codebase, fix bugs, and propose pull requests,\u201d with each task running in its own sandbox preloaded with your repo. \ue200cite\ue202turn0search11\ue202turn0search16\ue201\n\n2) **Tool formation & use** \u2014 *Substantial*  \nOperates across terminal/IDE/web/phone; uses build/test tools and repo workflows end-to-end. OpenAI\u2019s launch materials emphasize real-time collaboration \u201canywhere you develop\u201d and integration with developer surfaces. \ue200cite\ue202turn0search1\ue201\n\n3) **World-model / beliefs (stateful memory)** \u2014 *Partial*  \nCodex maintains working context within its cloud sandbox and repository state during a task, but OpenAI has not (publicly) claimed durable, cross-episode **episodic/semantic memory** (e.g., long-term project models, design-decision logs) beyond standard retrieval and context mechanisms. Public docs highlight task sandboxes and repo preload, not persistent long-term beliefs. \ue200cite\ue202turn0search11\ue201\n\n4) **Self-verification** \u2014 *Meaningful, but not complete*  \nSystem-card addendum states GPT-5-Codex was optimized to **\u201cadhere precisely to instructions and iteratively run tests until passing results are achieved.\u201d** That\u2019s a real verifier loop for many tickets; however, guarantees are still limited to the quality of available tests/analyzers. No formal proof obligations or coverage targets are claimed. \ue200cite\ue202turn0search5\ue201\n\n5) **Learning loop (self-improvement from experience)** \u2014 *Unclear publicly*  \nThe system card describes **reinforcement learning on real-world coding tasks** to shape the model\u2019s behavior. But there is no public claim of **on-going, customer-side continual learning** (e.g., adapters updated from your org\u2019s successes/failures) beyond core model updates. \ue200cite\ue202turn0search5\ue201\n\n6) **Robustness / OOD generalization within \\(D\\)** \u2014 *Promising but unproven at scale*  \nOpenAI cites SOTA-class performance on **SWE-bench Verified** (74.9% for GPT-5) and positions GPT-5 as their strongest coding model to date. That\u2019s persuasive evidence of broad competence across diverse repos\u2014but robustness under actively shifting enterprise stacks, brittle tests, or adversarial specs is not yet evidenced in public evals. \ue200cite\ue202turn0search8\ue202turn0search12\ue201\n\n7) **Autonomy boundaries & governance** \u2014 *Partial*  \nCodex runs tasks in **isolated cloud sandboxes** preloaded with your repo and proposes PRs for human review\u2014this is a good autonomy boundary. Public docs do not spell out organization-grade guardrails like mandatory policy checks, secret-scanning gates, cost/time ceilings, or auditable safety assertions as *hard requirements* (though some may exist product-side). \ue200cite\ue202turn0search11\ue201\n\n---\n\n## Bottom line (coding domain)\n- **Closest fit today:** Among widely accessible systems, GPT-5-Codex most closely matches a **Planner\u2013Executor\u2013Verifier** loop operating on real codebases, with strong benchmark performance and end-to-end task execution in sandboxes. \ue200cite\ue202turn0search5\ue202turn0search8\ue202turn0search11\ue201  \n- **Still missing for true Narrow-AGI:** durable **project-level memory**, **guaranteed verification coverage** (beyond \u201ctests pass\u201d), **rigorous OOD stress evidence**, and **explicit governance primitives** (capability bounding, audit/rollback guarantees) as first-class, measurable features.\n\n---\n\n## What I\u2019d test next (actionable evaluation you can run)\nUse these four experiments to measure how close GPT-5-Codex is to your Narrow-AGI bar:\n\n1) **Green-PR Autonomy (multi-repo):**  \nGive it 20 real issues across three heterogeneous repos (e.g., Python service, React frontend, infra-as-code). Track **first-pass green rate**, **time-to-green**, **human-minutes/agent-minutes**, and require **new tests** when missing. (Maps to #1\u2013#4.)\n\n2) **OOD & Shift Robustness:**  \nSwap in *unseen* frameworks/libraries or bump major versions mid-task. Measure recovery without manual hand-holding. (Maps to #6.)\n\n3) **Safety & Governance Drill:**  \nIntroduce secrets in .env history, a supply-chain trap (malicious minor version), and a spec ambiguity. Require the agent to **refuse or quarantine** risky actions and request clarification. (Maps to #7.)\n\n4) **Learning from Experience (simulated):**  \nRun 50 tasks with recurring failure motifs (e.g., flaky tests, race conditions). Check whether intervention patterns **propagate**\u2014fewer repeats, faster fixes\u2014*without* changing the base model (i.e., via retrieval/memory/templates). (Maps to #5.)\n\n**Pass criteria for \u201cNarrow-AGI (coding) v0.9\u201d:**  \n\u226570% first-pass green across mixed repos; \u226580% when allowed to add tests; demonstrable reductions in repeated failure classes; zero unmitigated safety incidents under the red-team drill; full artifacts (diffs, tests, logs) for reproducibility.\n\n---\n\n## Why this still isn\u2019t \u201cdone\u201d\nEven very high SWE-bench numbers don\u2019t guarantee **end-to-end engineering reliability** on living codebases. Until we see claims (and evidence) for (a) persistent project memory, (b) verifiable guarantees beyond \u201ctests pass,\u201d and (c) hardened autonomy controls under cost/secrecy constraints, **Narrow-AGI remains a moving target**\u2014albeit one GPT-5-Codex is now squarely chasing. \ue200cite\ue202turn0search5\ue202turn0search8\ue201\n\nIf you want, I\u2019ll turnkey a **Codex evaluation harness** (container + CI recipes + metrics dashboard) so you can run the four experiments on your own repos and get a live \u201cNarrow-AGI readiness\u201d score."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":"\ue200cite\ue202turn0search11\ue202turn0search16\ue201","start_idx":972,"end_idx":1006,"safe_urls":["https://openai.com/codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-codex/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/introducing-codex/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Introducing Codex - OpenAI","url":"https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","pub_date":1747353600.0,"snippet":"Codex can perform tasks for you such as writing features, answering questions about your codebase, fixing bugs, and proposing pull requests for review.","attribution_segments":null,"supporting_websites":[{"title":"Codex","url":"https://openai.com/codex/?utm_source=chatgpt.com","pub_date":null,"snippet":"Codex is OpenAI's series of AI coding tools that help developers move faster by delegating tasks to powerful cloud and local coding agents.","attribution":"OpenAI"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":11},{"turn_index":0,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue201","start_idx":1271,"end_idx":1290,"safe_urls":["https://openai.com/index/introducing-upgrades-to-codex/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/introducing-upgrades-to-codex/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Introducing upgrades to Codex - OpenAI","url":"https://openai.com/index/introducing-upgrades-to-codex/?utm_source=chatgpt.com","pub_date":1757945550.0,"snippet":"Codex just got faster, more reliable, and better at real-time collaboration and tackling tasks independently anywhere you develop\u2014whether ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search11\ue201","start_idx":1730,"end_idx":1750,"safe_urls":["https://openai.com/index/introducing-codex/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/introducing-codex/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Introducing Codex - OpenAI","url":"https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","pub_date":1747353600.0,"snippet":"Codex can perform tasks for you such as writing features, answering questions about your codebase, fixing bugs, and proposing pull requests for review.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":11}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search5\ue201","start_idx":2160,"end_idx":2179,"safe_urls":["https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Addendum to GPT-5 system card: GPT-5-Codex","url":"https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","pub_date":null,"snippet":"3 days ago \u2014 This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search5\ue201","start_idx":2537,"end_idx":2556,"safe_urls":["https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Addendum to GPT-5 system card: GPT-5-Codex","url":"https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","pub_date":null,"snippet":"3 days ago \u2014 This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search8\ue202turn0search12\ue201","start_idx":2989,"end_idx":3022,"safe_urls":["https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com","https://openai.com/index/introducing-gpt-5/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Introducing GPT\u20115 for developers","url":"https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 7, 2025 \u2014 It excels at producing high-quality code and handling tasks such as fixing bugs, editing code, and answering questions about complex codebases.","attribution_segments":null,"supporting_websites":[{"title":"Introducing GPT-5","url":"https://openai.com/index/introducing-gpt-5/?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 7, 2025 \u2014 GPT\u20115 is our strongest coding model to date. It shows particular improvements in complex front\u2011end generation and debugging larger repositories.","attribution":"OpenAI"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":8},{"turn_index":0,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search11\ue201","start_idx":3444,"end_idx":3464,"safe_urls":["https://openai.com/index/introducing-codex/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/introducing-codex/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Introducing Codex - OpenAI","url":"https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","pub_date":1747353600.0,"snippet":"Codex can perform tasks for you such as writing features, answering questions about your codebase, fixing bugs, and proposing pull requests for review.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":11}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search5\ue202turn0search8\ue202turn0search11\ue201","start_idx":3737,"end_idx":3783,"safe_urls":["https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Addendum to GPT-5 system card: GPT-5-Codex","url":"https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","pub_date":null,"snippet":"3 days ago \u2014 This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex.","attribution_segments":null,"supporting_websites":[{"title":"Introducing GPT\u20115 for developers","url":"https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 7, 2025 \u2014 It excels at producing high-quality code and handling tasks such as fixing bugs, editing code, and answering questions about complex codebases.","attribution":"OpenAI"},{"title":"Introducing Codex - OpenAI","url":"https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","pub_date":null,"snippet":"Codex can perform tasks for you such as writing features, answering questions about your codebase, fixing bugs, and proposing pull requests for review.","attribution":"OpenAI"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":5},{"turn_index":0,"ref_type":"search","ref_index":8},{"turn_index":0,"ref_type":"search","ref_index":11}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search5\ue202turn0search8\ue201","start_idx":5942,"end_idx":5974,"safe_urls":["https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Addendum to GPT-5 system card: GPT-5-Codex","url":"https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","pub_date":null,"snippet":"3 days ago \u2014 This addendum to the GPT-5 system card shares a new model: GPT-5-Codex, a version of GPT-5 further optimized for agentic coding in Codex.","attribution_segments":null,"supporting_websites":[{"title":"Introducing GPT\u20115 for developers","url":"https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 7, 2025 \u2014 It excels at producing high-quality code and handling tasks such as fixing bugs, editing code, and answering questions about complex codebases.","attribution":"OpenAI"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":5},{"turn_index":0,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null}],"safe_urls":["https://openai.com/codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-upgrades-to-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com","https://openai.com/index/introducing-gpt-5/?utm_source=chatgpt.com","https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com","https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/?utm_source=chatgpt.com","https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com"],"citations":[],"model_slug":"gpt-5-thinking","request_id":"980eb65a2a99cdf5-MAA","message_type":"next","default_model_slug":"gpt-5","parent_id":"9444467d-a72d-4c74-8080-41b9d43a9f02","timestamp_":"absolute","can_save":true},"recipient":"all","channel":"final"},"parent":"9444467d-a72d-4c74-8080-41b9d43a9f02","children":[]}}}