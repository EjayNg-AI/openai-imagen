{"id":"67caf3f6-9080-8007-83e8-76144aa028f9","title":"Large Language Diffusion Models","create_time":1741353974.808093,"update_time":1741354023.553116,"current_node":"1bc78d9d-0f4d-4fa3-8fba-1a35da30d8bd","mapping":{"aaa1f44a-ae99-409c-aa3e-5b1e82a9b42b":{"id":"aaa1f44a-ae99-409c-aa3e-5b1e82a9b42b","message":null,"parent":null,"children":["474c6e8e-ef28-4b06-959f-a1219b975338"]},"474c6e8e-ef28-4b06-959f-a1219b975338":{"id":"474c6e8e-ef28-4b06-959f-a1219b975338","message":{"id":"474c6e8e-ef28-4b06-959f-a1219b975338","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"aaa1f44a-ae99-409c-aa3e-5b1e82a9b42b","children":["7cea17b9-d1f0-4cf0-a885-cdf914206407"]},"7cea17b9-d1f0-4cf0-a885-cdf914206407":{"id":"7cea17b9-d1f0-4cf0-a885-cdf914206407","message":{"id":"7cea17b9-d1f0-4cf0-a885-cdf914206407","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"474c6e8e-ef28-4b06-959f-a1219b975338","children":["fea04eaf-d4a0-4749-9b50-e124da185668"]},"fea04eaf-d4a0-4749-9b50-e124da185668":{"id":"fea04eaf-d4a0-4749-9b50-e124da185668","message":{"id":"fea04eaf-d4a0-4749-9b50-e124da185668","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"7cea17b9-d1f0-4cf0-a885-cdf914206407","children":["e48c7ee7-a039-4750-aab8-fb2d63b55658"]},"e48c7ee7-a039-4750-aab8-fb2d63b55658":{"id":"e48c7ee7-a039-4750-aab8-fb2d63b55658","message":{"id":"e48c7ee7-a039-4750-aab8-fb2d63b55658","author":{"role":"user","name":null,"metadata":{}},"create_time":1741353974.371,"update_time":null,"content":{"content_type":"text","parts":["Give me a deep dive into this research paper\n"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"attachments":[{"id":"file-EjEr99nXVn1FWaMNbWUohS","size":2002293,"name":"2502.09992v2.pdf","mime_type":"application/pdf","file_token_size":26647}],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"91ca6c63f9c840ba-SJC","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"fea04eaf-d4a0-4749-9b50-e124da185668","children":["5e871a4a-5ab5-4984-9100-f8e52d67b224"]},"5e871a4a-5ab5-4984-9100-f8e52d67b224":{"id":"5e871a4a-5ab5-4984-9100-f8e52d67b224","message":{"id":"5e871a4a-5ab5-4984-9100-f8e52d67b224","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1741353986.843068,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"","summary":"","assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"command":"spinner","status":"running","message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"e48c7ee7-a039-4750-aab8-fb2d63b55658","request_id":"91ca6c63f9c840ba-SJC","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"e48c7ee7-a039-4750-aab8-fb2d63b55658","children":["006b4d43-9cab-45ed-bee2-e475391f6b13"]},"006b4d43-9cab-45ed-bee2-e475391f6b13":{"id":"006b4d43-9cab-45ed-bee2-e475391f6b13","message":{"id":"006b4d43-9cab-45ed-bee2-e475391f6b13","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1741353986.843204,"update_time":null,"content":{"content_type":"tether_quote","url":"file-EjEr99nXVn1FWaMNbWUohS","domain":"2502.09992v2.pdf","text":"\ue200cite\ue202turn0file0\ue201\n\nLarge Language Diffusion Models\n\n\nLarge Language Diffusion Models\n\nShen Nie 1 * \u2020 Fengqi Zhu 1 * \u2020 Zebin You 1 \u2020 Xiaolu Zhang 2 \u2021 Jingyang Ou 1 Jun Hu 2 \u2021 Jun Zhou 2\n\nYankai Lin 1 \u2021 Ji-Rong Wen 1 Chongxuan Li 1 \u2021 \u00b6\n\nAbstract\nAutoregressive models (ARMs) are widely re-\ngarded as the cornerstone of large language mod-\nels (LLMs). We challenge this notion by intro-\nducing LLaDA, a diffusion model trained from\nscratch under the pre-training and supervised fine-\ntuning (SFT) paradigm. LLaDA models distri-\nbutions through a forward data masking process\nand a reverse process, parameterized by a vanilla\nTransformer to predict masked tokens. By op-\ntimizing a likelihood bound, it provides a prin-\ncipled generative approach for probabilistic in-\nference. Across extensive benchmarks, LLaDA\ndemonstrates strong scalability, outperforming\nour self-constructed ARM baselines. Remark-\nably, LLaDA 8B is competitive with strong LLMs\nlike LLaMA3 8B in in-context learning and, af-\nter SFT, exhibits impressive instruction-following\nabilities in case studies such as multi-turn dia-\nlogue. Moreover, LLaDA addresses the rever-\nsal curse, surpassing GPT-4o in a reversal poem\ncompletion task. Our findings establish diffu-\nsion models as a viable and promising alternative\nto ARMs, challenging the assumption that key\nLLM capabilities discussed above are inherently\ntied to ARMs. Project page and codes: https:\n//ml-gsai.github.io/LLaDA-demo/.\n\n1. Introduction\nWhat is now proved was once only imagined.\nplaceholder,placeholder,placeho\u2014William Blake\n\nLarge language models (LLMs) (Zhao et al., 2023) fall en-\ntirely within the framework of generative modeling. Specifi-\ncally, LLMs aim to capture the true but unknown language\n\n*Equal contribution \u2020Work done during an internship at Ant\nGroup \u2021Project leaders 1Gaoling School of Artificial Intelli-\ngence, Renmin University of China; Beijing Key Laboratory\nof Big Data Management and Analysis Methods 2Ant Group.\n\u00b6Correspondence to: Chongxuan Li <chongxuanli@ruc.edu.cn>.\n\nPreprint.\n\n46\n\n33\n\n37\n50\n\n19\n\n24\n\n33\n49\n\n50\n\n56\n\n39\n\n45\n60\n\n23\n\n29\n\n40\n59\n\n60\n\n65\n\n46\n\n52\n70\n\n27\n\n34\n\n47\n69\n\n70\n\nMMLU\n\nTruthfulQA\n\nARC-C\nGSM8K\n\nMath\n\nHumanEval\n\nMBPP\nCMMLU\n\nC-Eval\n\nGeneral TasksM\nath\n\nem\nati\n\ncs\n\nCode\n\nChinese\n\nLLaDA 8B Base\nLLaMA 3 8B Base\nLLaMA 2 7B Base\n\nFigure 1. Zero/Few-Shot Benchmarks. We scale LLaDA to an\nunprecedented size of 8B parameters from scratch, achieving com-\npetitive performance with strong LLMs (Dubey et al., 2024).\n\ndistribution pdata(\u00b7) by optimizing a model distribution p\u03b8(\u00b7)\nthrough maximum likelihood estimation, or equivalently KL\ndivergence minimization between the two distributions:\n\nmax\n\u03b8\n\nEpdata(x) log p\u03b8(x)\u21d4 min\n\u03b8\n\nKL(pdata(x)||p\u03b8(x))\ufe38 \ufe37\ufe37 \ufe38\nGenerative modeling principles\n\n. (1)\n\nThe predominant approach relies on the autoregressive mod-\neling (ARM)\u2014commonly referred to as the next-token pre-\ndiction paradigm\u2014to define the model distribution:\n\np\u03b8(x) = p\u03b8(x\n1)\n\nL\u220f\ni=2\n\np\u03b8(x\ni | x1, . . . , xi\u22121)\ufe38 \ufe37\ufe37 \ufe38\n\nAutoregressive formulation\n\n, (2)\n\nwhere x is a sequence of length L, and xi is the i-th token.\n\nThis paradigm has proven remarkably effective (Radford,\n2018; Radford et al., 2019; Brown, 2020; OpenAI, 2022)\nand has become the foundation of current LLMs. Despite\nits widespread adoption, a fundamental question remains\nunanswered: Is the autoregressive paradigm the only viable\npath to achieving the intelligence exhibited by LLMs?\n\n1\n\nar\nX\n\niv\n:2\n\n50\n2.\n\n09\n99\n\n2v\n2 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n8 \nFe\n\nb \n20\n\n25\n\nhttps://ml-gsai.github.io/LLaDA-demo/\nhttps://ml-gsai.github.io/LLaDA-demo/\n\n\nLarge Language Diffusion Models\n\nWe argue that the answer is not a simple \u201cyes\u201d. The key\ninsight overlooked previously is: it is the generative mod-\neling principles (i.e., Eq. (1)), rather than the autoregres-\nsive formulation (i.e., Eq. (2)) itself, that fundamentally\nunderpin the essential properties of LLMs, as detailed be-\nlow. However, certain inherent limitations of LLMs can be\ndirectly traced to their autoregressive nature.\n\nIn particular, we argue that scalability is primarily a con-\nsequence of the interplay between Transformers (Vaswani,\n2017), model and data size, and Fisher consistency1 (Fisher,\n1922) induced by the generative principles in Eq. (1), rather\nthan a unique result of ARM. The success of diffusion trans-\nformers (Bao et al., 2023; Peebles & Xie, 2023) on visual\ndata (Brooks et al., 2024) supports this claim.\n\nFurthermore, the capabilities of instruction-following and\nin-context learning (Brown, 2020) appear to be intrinsic\nproperties of all proper conditional generative models on\nstructurally consistent linguistic tasks, rather than exclusive\nadvantages of ARMs. In addition, while ARMs can be\ninterpreted as a lossless data compressor (Deletang et al.;\nHuang et al., 2024b), any sufficiently expressive probabilis-\ntic model can achieve similar capabilities (Shannon, 1948).\n\nNevertheless, the autoregressive nature of LLMs presents\nnotable challenges. For example, sequential token-by-token\ngeneration incurs high computational costs, and the left-\nto-right modeling limits effectiveness in reversal reasoning\ntasks (Berglund et al., 2023). These inherent limitations\nconstrain LLMs in handling longer and more complex tasks.\n\nMotivated by these insights, we introduce LLaDA, a Large\nLanguage Diffusion with mAsking, to investigate whether\nthe capabilities exhibited by LLMs can emerge from gener-\native modeling principles beyond ARMs in Eq. (2), thereby\naddressing the fundamental question posed earlier. In con-\ntrast to traditional ARMs, LLaDA leverages a masked diffu-\nsion model (MDM) (Austin et al., 2021a; Lou et al., 2023;\nShi et al., 2024; Sahoo et al., 2024; Ou et al., 2024), which\nincorporates a discrete random masking process and trains\na mask predictor to approximate its reverse process. This\ndesign enables LLaDA to construct a model distribution\nwith bidirectional dependencies and optimize a lower bound\nof its log-likelihood, offering an unexplored and principled\nalternative to existing LLMs.\n\nWe adopt the standard pipeline of data preparation, pre-\ntraining, supervised fine-tuning (SFT), and evaluation, scal-\ning LLaDA to an unprecedented language diffusion of size\n8B. In particular, LLaDA 8B was pre-trained from scratch\non 2.3 trillion tokens using 0.13 million H800 GPU hours,\nfollowed by SFT on 4.5 million pairs. Across diverse tasks,\nincluding language understanding, math, code, and Chinese,\n\n1It suggests the ability to recover the true data distribution with\ninfinite data, a sufficiently large network and optimal training.\n\nLLaDA demonstrates the following contributions:\n\nScalability. LLaDA scales effectively up to a computational\nbudget of 1023 FLOPs, achieving comparable results to self-\nconstructed ARM baselines trained on the same data across\nsix tasks, e.g., MMLU and GSM8K.\n\nIn-Context Learning. Remarkably, LLaDA 8B surpasses\nLLaMA2 7B (Touvron et al., 2023) on nearly all 15 standard\nzero/few-shot learning tasks while performing on par with\nLLaMA3 8B (Dubey et al., 2024).\n\nInstruction-Following. LLaDA significantly enhances the\nability to follow instructions after SFT, as demonstrated in\ncase studies such as multi-turn dialogue.\n\nReversal Reasoning. LLaDA effectively breaks the re-\nversal curse (Berglund et al., 2023) with consistent per-\nformance across forward and reversal tasks. Notably, it\noutperforms GPT-4o in a reversal poem completion task.\n\n2. Approach\nIn this section, we introduce the probabilistic formulation2,\nalong with the pre-training, supervised fine-tuning, and in-\nference procedures for LLaDA, as illustrated in Fig. 2.\n\n2.1. Probabilistic Formulation\n\nUnlike ARMs in Eq. (2), LLaDA defines a model distribu-\ntion p\u03b8(x0) through a forward process and a reverse pro-\ncess (Austin et al., 2021a; Ou et al., 2024). The forward\nprocess gradually masks tokens independently in x0 until\nthe sequence is fully masked at t = 1. For t \u2208 (0, 1), the\nsequence xt is partially masked, with each being masked\nwith probability t or remaining unmasked with probability\n1\u2212 t. The reverse process recovers the data distribution by\niteratively predicting masked tokens as t moves from 1 to 0.\n\nThe core of LLaDA is a mask predictor, a parametric model\np\u03b8(\u00b7|xt) that takes xt as input and predicts all masked tokens\n(denoted M) simultaneously. It is trained using a cross-\nentropy loss computed only on the masked tokens:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (3)\n\nwhere x0 is sampled from the training data, t is sampled\nuniformly from [0, 1], and xt is sampled from the forward\nprocess. The indicator function 1[\u00b7] ensures that the loss is\ncomputed only for masked tokens.\n\nOnce trained, we can simulate a reverse process (see Sec. 2.4\nfor details) parameterized by the mask predictor and define\nthe model distribution p\u03b8(x0) as the marginal distribution\n\n2Here, we focus on the approach of LLaDA. A rigorous formu-\nlation of MDM is provided in Appendix A for interested readers.\n\n2\n\n\n\nLarge Language Diffusion Models\n\nMask predictor\n\nMask all tokens independently\n\nMask predictor\n\nPrompt Response\n\nMask predictor\n\nPrompt Response\n\n... \n\n... \n\nA\nn\n\n in\nte\n\nrm\ne\n\nd\nia\n\nte\n s\n\nte\np\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nNon-mask token Random mask\n\nRemaskMask token\n\n(c)(b)(a)\n\nRemask\n\nMask ratio \ud835\udc61 \u223c \ud835\udc48(0,1)\n\nFigure 2. A Conceptual Overview of LLaDA. (a) Pre-training. LLaDA is trained on text with random masks applied independently to all\ntokens at the same ratio t \u223c U [0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion\nprocess from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies.\n\ninduced at t = 0. Notably, the loss in Eq. (3) has been\nproven to be an upper bound on the negative log-likelihood\nof the model distribution (Shi et al., 2024; Ou et al., 2024):\n\n\u2212Epdata(x0) [log p\u03b8(x0)] \u2264 L(\u03b8), (4)\n\nmaking it a principled objective for generative modeling.\n\nNotably, LLaDA employs a masking ratio that varies ran-\ndomly between 0 and 1 while masked language models (De-\nvlin, 2018) use a fixed ratio. The subtly differences have\nsignificant implications, especially at scale: as shown in\nEq. (4), LLaDA is a principled generative model with the\npotential to perform in-context learning naturally, akin to\nLLMs. Moreover, its generative perspective ensures Fisher\nconsistency (Fisher, 1922) in extreme cases, suggesting\nstrong scalability with large data and models.\n\n2.2. Pre-training\n\nLLaDA employs a Transformer (Vaswani, 2017) as the mask\npredictor, whose architecture is similar to existing LLMs.\nHowever, LLaDA does not use a causal mask, as its formu-\nlation allows it to see the entire input for predictions.\n\nWe trained two variants of LLaDA with different sizes: 1\nbillion (B) and 8B. We summarize the model architecture\nof LLaDA 8B and LLaMA3 8B (Dubey et al., 2024) here\nand details are provided in Appendix B.2. We have ensured\nconsistency in most hyperparameters while making several\nnecessary modifications. We use vanilla multi-head atten-\ntion instead of grouped query attention (Ainslie et al., 2023)\nfor simplicity, as LLaDA is incompatible with KV caching,\nresulting in a different number of key and value heads. Con-\nsequently, the attention layer has more parameters, and we\nreduce the FFN dimension to maintain a comparable model\nsize. Additionally, the vocabulary size differs slightly due\nto a tokenizer (Brown, 2020) adapted on our data.\n\nThe LLaDA model is pre-trained on a dataset comprising\n2.3 trillion (T) tokens, adhering to a data protocol that aligns\n\nclosely with existing large language models (LLMs) (Tou-\nvron et al., 2023; Chu et al., 2024), without the incorporation\nof any special techniques. The data are derived from online\ncorpora, with low-quality content filtered through manually\ndesigned rules and LLM-based approaches. Beyond general\ntext, the dataset encompasses high-quality code, math, and\nmultilingual data. The mixing of data sources and domains\nis guided by scaled-down ARMs. The pre-training process\nutilizes a fixed sequence length of 4096 tokens, incurring a\ntotal computational cost of 0.13 million H800 GPU hours,\nsimilar to ARMs of the same scale and dataset size.\n\nFor a training sequence x0, we randomly sample t \u2208 [0, 1],\nmask each token independently with the same probability\nt to obtain xt (see Fig. 2 (a)) and estimate Eq. (3) via the\nMonte Carlo method for stochastic gradient descent training.\nIn addition, following Nie et al. (2024), to enhance the\nability of LLaDA to handle variable-length data, we set 1%\nof the pre-training data to a random length that is uniformly\nsampled from the range [1, 4096].\n\nWe adopted the Warmup-Stable-Decay (Hu et al., 2024)\nlearning rate scheduler to monitor the training progress\nwithout interrupting continuous training. Specifically, we\nlinearly increased the learning rate from 0 to 4\u00d7 10\u22124 over\nthe first 2000 iterations and maintained it at 4\u00d7 10\u22124. After\nprocessing 1.2T tokens, we decayed the learning rate to\n1\u00d7 10\u22124 and held it constant for the next 0.8T tokens to en-\nsure stable training. Finally, we linearly reduced the learning\nrate from 1\u00d710\u22124 to 1\u00d710\u22125 for the last 0.3T tokens. Fur-\nthermore, we utilized the AdamW optimizer (Loshchilov,\n2017) with a weight decay of 0.1, a batch size of 1280, and\na local batch size of 4 per GPU. The 8B experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.3. Supervised Fine-Tuning\n\nWe enhance the capability of LLaDA to follow instructions\nby supervised fine-tuning (SFT) with paired data (p0, r0),\n\n3\n\n\n\nLarge Language Diffusion Models\n\nwhere p0 is the prompt and r0 denotes the response. This\nis the simplest and most basic post-training method for\nLLMs. Technically, this requires to model the conditional\ndistribution p\u03b8(r0|p0) instead of p\u03b8(x0) in pre-training.\n\nThe implementation is similar to pre-training. As shown in\nFig. 2 (b), we leave the prompt unchanged and mask the\ntokens in the response independently, as done for x0. Then,\nwe feed both the prompt and the masked response rt to the\npre-trained mask predictor to compute the loss for SFT:\n\n\u2212Et,p0,r0,rt\n\n\uf8ee\uf8f01\n\nt\n\nL\u2032\u2211\ni=1\n\n1[rit = M] log p\u03b8(r\ni\n0|p0, rt)\n\n\uf8f9\uf8fb , (5)\n\nwhere L\u2032 denotes a dynamic length specified later, and all\nother notations remain the same as before.\n\nNote that this approach is fully compatible with pre-training.\nEssentially, the concatenation of p0 and r0 can be treated\nas clean pre-training data x0, while the concatenation of\np0 and rt serves as the masked version xt. The process is\nidentical to pre-training, with the only difference being that\nall masked tokens happen to appear in the r0 portion.\n\nThe LLaDA 8B model undergoes SFT on a dataset com-\nprising 4.5 million pairs. Consistent with the pre-training\nprocess, both data preparation and training follow the SFT\nprotocols utilized in existing LLMs (Chu et al., 2024; Yang\net al., 2024), without introducing any additional techniques\nto optimize LLaDA\u2019s performance. The dataset spans mul-\ntiple domains, including code, mathematics, instruction-\nfollowing, and structured data understanding. We append\n|EOS| tokens to the end of short pairs in each mini-batch\nto ensure equal lengths across all data. We treat |EOS| as a\nnormal token during training and remove it during sampling,\nenabling LLaDA to control the response length automati-\ncally. Please refer to Appendix B.1 for more details.\n\nWe train for 3 epochs on the SFT data using a similar sched-\nule to the pre-training phase. The learning rate is linearly\nincreased from 0 to 2.5\u00d7 10\u22125 over the first 50 iterations\nand then kept constant. During the final 10% of iterations,\nit is linearly reduced to 2.5 \u00d7 10\u22126. Additionally, we set\nthe weight decay to 0.1, the global batch size to 256, and\nthe local batch size to 2 per GPU. The SFT experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.4. Inference\n\nAs a generative model, LLaDA is capable of both sampling\nnew text and evaluating the likelihood of candidate text.\n\nWe begin with the sampling. As illustrated in Fig. 2 (c),\ngiven a prompt p0, we discretize the reverse process to sam-\nple from the model distribution p\u03b8(r0|p0), starting from a\nfully masked response. The total number of sampling steps\nis a hyperparameter, which naturally provides LLaDA with a\n\ntrade-off between efficiency and sample quality, as analyzed\nin Sec. 3.3. We employ uniformly distributed timesteps by\ndefault. In addition, the generation length is also treated as\na hyperparameter, specifying the length of the fully masked\nsentence at the beginning of the sampling process. As de-\ntailed in Appendix B.4, since both pre-training and SFT\nare conducted using datasets with variable lengths, the final\nresults are insensitive to this length hyperparameter.\n\nAt an intermediate step from time t \u2208 (0, 1] to s \u2208 [0, t), we\nfeed both p0 and rt into the mask predictor and predict all\nmasked tokens simultaneously. Subsequently, we remask s\n\nt\nof the predicted tokens in expectation to obtain rs, ensuring\nthat the transition of the reverse process aligns with the\nforward process for accurate sampling (Austin et al., 2021a).\n\nIn principle, the remasking strategy should be purely ran-\ndom. However, inspired by the annealing tricks of sampling\nin LLMs (Holtzman et al., 2019; Brown, 2020), we explore\ntwo deterministic yet effective remasking strategies. Specif-\nically, similarly to Chang et al. (2022), we remask the s\n\nt\nof predicted tokens with the lowest confidence based on\nthe predictions, called low-confidence remasking. Addition-\nally, for LLaDA after SFT, we can divide the sequence into\nseveral blocks and generate them from left to right, called\nsemi-autoregressive remasking. Within each block, we ap-\nply the reverse process to perform sampling. We provide\nmore details and ablation studies in Appendix. B.3.\n\nFor conditional likelihood evaluation, we can naturally uti-\nlize the upper bound in Eq. (5). However, we find that the\nfollowing equivalent form (Ou et al., 2024) exhibits lower\nvariance and is more stable for evaluation:\n\n\u2212El,r0,rl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[ril = M] log p\u03b8(r\ni\n0|p0, rl)\n\n]\n, (6)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and rl is\nobtained by uniformly sampling l tokens from r0 without\nreplacement for masking. In addition, we employ the unsu-\npervised classifier-free guidance (Nie et al., 2024). We refer\nthe readers to more details in Appendix A.2.\n\nWe present the training, sampling, and likelihood evaluation\nalgorithms, along with theoretical details, in Appendix A.\n\n3. Experiments\nWe evaluate the scalability, instruction-following, and in-\ncontext learning capabilities of LLaDA on standard bench-\nmarks, followed by analyses and case studies on more con-\ntrolled datasets to provide a comprehensive assessment.\n\n3.1. Scalability of LLaDA on Language Tasks\n\nWe first investigate the scalability of LLaDA on downstream\ntasks in comparison with the ARM baselines we constructed.\n\n4\n\n\n\nLarge Language Diffusion Models\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nA\nR\n\nC\n-C\n\n (0\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nC\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt) Autoregressive Baseline\n\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n50\n\n60\n\n70\n\n80\n\nPI\nQ\n\nA\n (0\n\n-s\nho\n\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n20\n\n40\n\n60\n\nG\nSM\n\n8K\n (4\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n8\n\n16\n\n24\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\nFigure 3. Scalability of LLaDA. We evaluate the performance of LLaDA and our ARM baselines trained on the same data across\nincreasing computational FLOPs. LLaDA exhibits strong scalability, matching the overall performance of ARMs on six tasks.\n\nSpecifically, at the 1B scale, we ensured that LLaDA and\nARM shared the same architecture, data, and all other config-\nurations. At larger scales, we also report results for LLaDA\nand ARM models of slightly different sizes trained on the\nsame data due to resource limit, as detailed in Appendix B.2.\nWe use the computational cost as a unified scaling metric.\nFor evaluation, we focused on six standard and diverse tasks.\n\nAs shown in Fig. 3, LLaDA demonstrates impressive scala-\nbility, with its overall trend being highly competitive with\nthat of ARM. Notably, in tasks such as MMLU and GSM8K,\nLLaDA exhibits even stronger scalability. Even on tasks like\nPIQA, where performance lags, LLaDA narrows the gap\nwith ARMs at larger scales. To account for the significant\ninfluence of outliers, we opted not to fit quantitative scaling\ncurves, avoiding potential misinterpretation. Nevertheless,\nthe results clearly demonstrate the scalability of LLaDA.\n\nNie et al. (2024) suggests that MDM requires 16 times\nmore computation than ARM to achieve the same likeli-\nhood. However, there are key differences that make the\nconclusions of this study more broadly applicable. In partic-\nular, likelihood is a relatively indirect metric for downstream\ntask performance, and diffusion optimizes a bound of the\nlikelihood, making it not directly comparable to ARM. Ad-\nditionally, we extended the scaling range from 1018 \u223c 1020\n\nin Nie et al. (2024) to 1020 \u223c 1023 in this work.\n\n3.2. Benchmark Results\n\nTo comprehensively evaluate the in-context learning and\ninstruction-following capabilities of LLaDA 8B, we con-\nducted detailed comparisons with existing LLMs (Touvron\n\net al., 2023; Dubey et al., 2024; Chu et al., 2024; Yang et al.,\n2024; Bi et al., 2024; Jiang et al., 2023) of similar scale.\nThe selection of tasks and evaluation protocols was aligned\nwith existing studies, encompassing 15 popular benchmarks\nin general tasks, mathematics, code, and Chinese. Further\ndetails are provided in Appendix B.5. For a more direct\ncomparison, we re-evaluated representative LLMs (Touvron\net al., 2023; Dubey et al., 2024) in our implementation.\n\nAs shown in Tab. 1, after pretraining on 2.3T tokens,\nLLaDA 8B demonstrates remarkable performance, surpass-\ning LLaMA2 7B on nearly all tasks, and is overall competi-\ntive with LLaMA3 8B. LLaDA shows advantages in math\nand Chinese tasks. We conjecture that the strengths stem\nfrom the same factors as its relatively weaker performance\nin some tasks\u2014differences in data quality and distribution,\nlargely due to the closed-source situation of LLM datasets.\n\nNotably, we have carefully ruled out the possibility of data\nleakage by taking GSM8K as an example. First, as shown\nin Fig. 3, LLaDA outperformed ARM baselines regarding\nGSM8K. Moreover, the conclusion remains on a fully un-\nseen GSM8K-like task (Ye et al., 2024) in Appendix B.7.\n\nFurther, Tab. 2 compares the performance of LLaDA 8B In-\nstruct with existing LLMs. We observed that SFT improved\nLLaDA\u2019s performance on most downstream tasks. A few\nmetrics, such as MMLU, showed declines, and we conjec-\nture may be due to the suboptimal quality of the SFT data.\nOverall, since we did not perform alignment with reinforce-\nment learning (RL), our results are slightly behind LLaMA3\n8B Instruct, though the gaps in many metrics remain small.\nNotably, even with only SFT, LLaDA demonstrates impres-\n\n5\n\n\n\nLarge Language Diffusion Models\n\nTable 1. Benchmark Results of Pre-trained LLMs. \u2217 indicates that LLaDA 8B Base, LLaMA2 7B Base, and LLaMA3 8B Base are\nevaluated under the same protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Chu et al. (2024); Yang et al.\n(2024) and Bi et al. (2024) respectively. The numbers in parentheses represent the number of shots used for evaluation. \u201c-\u201d indicates\nunknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Mistral 7B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T - 2T\n\nGeneral Tasks\n\nMMLU 65.9 (5) 65.4 (5) 45.9 (5) 70.3 (5) 74.2 (5) 64.2 (5) 48.2 (5)\nBBH 49.8 (3) 57.6 (3) 37.3 (3) 62.3 (3) 70.4 (3) 56.1 (3) 39.5 (3)\nARC-C 47.9 (0) 53.1 (0) 46.3 (0) 60.6 (25) 63.7 (25) 60.0 (25) 48.1 (0)\nHellaswag 72.5 (0) 79.1 (0) 76.0 (0) 80.7 (10) 80.2 (10) 83.3 (10) 75.4 (0)\nTruthfulQA 46.4 (0) 44.0 (0) 39.0 (0) 54.2 (0) 56.4 (0) 42.2 (0) -\nWinoGrande 74.8 (5) 77.3 (5) 72.5 (5) 77.0 (5) 75.9 (5) 78.4 (5) 70.5 (0)\nPIQA 74.4 (0) 80.6 (0) 79.1 (0) - - - 79.2 (0)\n\nMathematics & Science\n\nGSM8K 70.7 (4) 53.1 (4) 14.3 (4) 80.2 (4) 85.4 (4) 36.2 (4) 17.4 (8)\nMath 27.3 (4) 15.1 (4) 3.2 (4) 43.5 (4) 49.8 (4) 10.2 (4) 6.0 (4)\nGPQA 26.1 (5) 25.9 (5) 25.7 (5) 30.8 (5) 36.4 (5) 24.7 (5) -\n\nCode\n\nHumanEval 33.5 (0) 34.2 (0) 12.8 (0) 51.2 (0) 57.9 (0) 29.3 (0) 26.2 (0)\nHumanEval-FIM 73.8 (2) 73.3 (2) 26.9 (2) - - - -\nMBPP 38.2 (4) 47.4 (4) 18.4 (4) 64.2 (0) 74.9 (0) 51.1 (0) 39.0 (3)\n\nChinese\n\nCMMLU 69.9 (5) 50.7 (5) 32.5 (5) 83.9 (5) - - 47.2 (5)\nC-Eval 70.5 (5) 51.7 (5) 34.0 (5) 83.2 (5) - - 45.0 (5)\n\nTable 2. Benchmark Results of Post-trained LLMs. LLaDA only employs an SFT procedure while other models have extra reinforcement\nlearning (RL) alignment. \u2217 indicates that LLaDA 8B Instruct, LLaMA2 7B Instruct, and LLaMA3 8B Instruct are evaluated under the\nsame protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Yang et al. (2024) and Bi et al. (2024) respectively.\nThe numbers in parentheses represent the number of shots used for in-context learning. \u201c-\u201d indicates unknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Gemma2 9B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T 8T 2T\n\nPost-training SFT SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL\nAlignment pairs 4.5M - - 0.5M + - 1M + 0.15M - 1.5M + -\n\nGeneral Tasks\n\nMMLU 65.5 (5) 68.4 (5) 44.1 (5) - - - 49.4 (0)\nMMLU-pro 37.0 (0) 41.9 (0) 4.6 (0) 44.1 (5) 56.3 (5) 52.1 (5) -\nHellaswag 74.6 (0) 75.5 (0) 51.5 (0) - - - 68.5 (-)\nARC-C 88.5 (0) 82.4 (0) 57.3 (0) - - - 49.4 (-)\n\nMathematics & Science\n\nGSM8K 78.6 (4) 78.3 (4) 29.0 (4) 85.7 (0) 91.6 (0) 76.7 (0) 63.0 (0)\nMath 26.6 (0) 29.6 (0) 3.8 (0) 52.9 (0) 75.5 (0) 44.3 (0) 15.8 (0)\nGPQA 31.8 (5) 31.9 (5) 28.4 (5) 34.3 (0) 36.4 (0) 32.8 (0) -\n\nCode\n\nHumanEval 47.6 (0) 59.8 (0) 16.5 (0) 79.9 (0) 84.8 (0) 68.9 (0) 48.2 (-)\nMBPP 34.2 (4) 57.6 (4) 20.6 (4) 67.2 (0) 79.2 (0) 74.9 (0) 35.2 (-)\n\n6\n\n\n\nLarge Language Diffusion Models\n\nTable 3. Comparison in the Poem Completion Task.\n\nForward Reversal\n\nGPT-4o (2024-08-06) 82.7 34.3\nQwen2.5 7B Instruct 75.9 38.0\nLLaDA 8B Instruct 48.8 42.4\n\nsive instruction-following abilities, as detailed in Sec. 3.4.\nWe leave RL-based alignment for future work.\n\nOverall, despite the lack of data transparency, we have made\nevery effort to adopt standardized procedures and introduce\ndiverse tasks, we believe they sufficiently demonstrate the\nextraordinary capabilities of LLaDA, which is the only com-\npetitive non-autoregressive model to our knowledge.\n\n3.3. Reversal Reasoning and Analyses\n\nTo quantify the reversal reasoning ability of mod-\nels (Berglund et al., 2023), we follow the protocol estab-\nlished in Allen-Zhu & Li (2023). Specifically, we construct\na dataset of 496 famous Chinese poem sentence pairs. Given\na sentence from a poem, models are tasked with generating\nthe subsequent line (forward) or the preceding line (rever-\nsal) without additional fine-tuning. Examples can be found\nin Appendix B.8. This setting provides a straightforward\nand more realistic evaluation compared to previous stud-\nies (Nie et al., 2024; Kitouni et al., 2024).\n\nAs shown in Tab. 3, LLaDA effectively addresses the rever-\nsal curse (Berglund et al., 2023), demonstrating consistent\nzero-shot performance across both forward and reversal\ntasks. In contrast, both Qwen 2.5 and GPT-4o exhibit a\nsignificant gap between the two. The results on forward\ngeneration confirm that both ARMs are strong, benefiting\nfrom significantly larger datasets and greater computational\nresources than LLaDA. However, LLaDA outperforms both\nby a large margin in the reversal task.\n\nWe emphasize that we did not design anything special for\nreversal tasks. Intuitively, LLaDA treats tokens uniformly\nwithout inductive bias, leading to balanced performance.\nSee more details in Appendix A.2.\n\nWe also analyze the effect of remasking strategies and sam-\npling steps, as detailed in Appendix B.3 and Appendix B.6.\n\n3.4. Case Studies\n\nWe present samples generated by LLaDA 8B Instruct in\nTab. 4, showcasing its instruction-following capabilities.\nFirst, the table illustrates LLaDA\u2019s ability to generate co-\nherent, fluent, and extended text in a non-autoregressive\nmanner. Second, it highlights the model\u2019s multi-turn dia-\nlogue capability, effectively retaining conversation history\n\nand producing contextually appropriate responses across\nmultiple languages. Such chat capabilities of LLaDA are\nimpressive, as it departs from conventional ARMs for the\nfirst time, to the best of our knowledge. See more case\nstudies on remasking and reasoning tasks in Appendix B.9.\n\n4. Related Work\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020) have excelled in visual domains but\nremain unverified for LLMs despite extensive efforts.\n\nA simple approach is to continuousize text data and apply\ndiffusion models directly (Li et al., 2022; Gong et al., 2022;\nHan et al., 2022; Strudel et al., 2022; Chen et al., 2022;\nDieleman et al., 2022; Richemond et al., 2022; Wu et al.,\n2023; Mahabadi et al., 2024; Ye et al., 2023b). Alternatively,\nsome methods model continuous parameters of discrete dis-\ntributions instead (Lou & Ermon, 2023; Graves et al., 2023;\nLin et al., 2023; Xue et al., 2024). However, scalability\nremains a challenge, as a 1B parameter model requires 64\ntimes the compute of an ARM to achieve comparable per-\nformance (Gulrajani & Hashimoto, 2024).\n\nAnother approach replaces continuous diffusion with dis-\ncrete processes featuring new forward and reverse dy-\nnamics (Austin et al., 2021a), leading to numerous vari-\nants (Hoogeboom et al., 2021b;a; He et al., 2022; Campbell\net al., 2022; Meng et al., 2022; Reid et al., 2022; Sun et al.,\n2022; Kitouni et al., 2023; Zheng et al., 2023; Chen et al.,\n2023; Ye et al., 2023a; Gat et al., 2024; Zheng et al., 2024;\nSahoo et al., 2024; Shi et al., 2024). Notably, Lou et al.\n(2023) showed that masked diffusion, as a special case of\ndiscrete diffusion, achieves perplexity comparable to or sur-\npassing ARMs at GPT-2 scale. Ou et al. (2024) established\nfundamental theoretical results, which motivated our model\ndesign, training, and inference (see Appendix A). Nie et al.\n(2024) explored how MDM can be leveraged for language\ntasks such as question answering at GPT-2 scale. Gong et al.\n(2024) fine-tune ARMs in the MDM formulation. However,\nimprovements are confined to certain metrics, and it remains\nunclear whether this approach can yield a foundation model\ncomparable to strong LLMs under a comprehensive evalua-\ntion.\n\nIn comparison, this study scales MDM to an unprecedented\nsize of 8B parameters from scratch, achieving performance\ncomparable to leading LLMs such as LLaMA 3.\n\nAdditionally, a parallel line of work on image genera-\ntion (Chang et al., 2022; 2023) aligns well with the appli-\ncation of MDMs to text data. Moreover, MDMs have also\nshown promise in domains such as protein generation (Wang\net al., 2024b;c), where they have achieved promising results.\nNotably, Kou et al. (2024); Xu et al. (2025) demonstrate the\npotential of using distillation to accelerate MDMs sampling,\n\n7\n\n\n\nLarge Language Diffusion Models\n\nTable 4. Visualization of the Sampling Process and a Generated Multi-round Dialogue. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nSampling Process\n\nUser Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour.\nHow many kilometers can she run in 8 hours?\n\nLLaDA\n\nMulti-round Dialogue Case\n\nUser Could you please share with me what the initial two lines of the renowned poem \u2018The\nRoad Not Taken\u2019?\n\nLLaDA Certainly! The initial two lines of the renowned poem \u201cThe Road Not Taken\u201d by Robert\nFrost are: \u201cTwo roads diverged in a yellow wood, And sorry I could not travel both.\u201d These\nlines set the stage for the poem, introducing the idea of a speaker facing a choice between\ntwo paths.\n\nUser Please help me translate into Chinese.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nChinese as: \u201c\u4e24\u6761\u8def\u5206\u5c94\u5728\u9ec4\u8272\u7684\u6811\u6797\u4e2d\uff0c\u9057\u61be\u6211\u4e0d\u80fd\u540c\u65f6\u8d70\u201d\nUser Please translate into German.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nGerman as: \u201cZwei Wege trennten sich im gelben Wald, und leider konnte ich nicht beide\ntun.\u201d\n\nUser Please also write a poem about life choices. I require 5 sentences, and the first word of\neach sentence starts with C as the first letter.\n\nLLaDA Certainly! Here\u2019s a poem about life choices, with each sentence starting with the letter \u201cC\u201d:\nChoices cascade through life\u2019s journey, Creating connections with each decision, Careful\nconsideration leads to better paths, Courageous choices shape our destiny, Choosing wisely\nguides us through the years.\n\nfurther enhancing their efficiency.\n\n5. Conclusion and Discussion\nIn the middle of difficulty lies opportunity.\nplaceholder,placeholder,placeh\u2014Albert Einstein\n\nWe introduce LLaDA, a principled and previously unex-\nplored approach to large language modeling based on dif-\nfusion models. LLaDA demonstrates strong capabilities in\nscalability, in-context learning, and instruction-following,\nachieving performance comparable to strong LLMs. In addi-\ntion, LLaDA offers unique advantages such as bidirectional\nmodeling and enhanced robustness, effectively addressing\nseveral inherent limitations of existing LLMs. Our findings\nnot only establish diffusion models as a viable and promis-\ning alternative but also challenge the prevailing assumption\nthat these essential capabilities are inherently tied to ARMs.\n\nWhile promising, the full potential of diffusion models re-\n\nmains to be fully explored. Several limitations of this work\npresent significant opportunities for future research.\n\nDue to computational constraints, direct comparisons be-\ntween LLaDA and ARMs\u2014such as training on identical\ndatasets\u2014were restricted to a computational budget of less\nthan 1023 FLOPs. To allocate resources for training the\nlargest possible LLaDA model and showcasing its poten-\ntial, we were unable to scale the ARM baseline to the same\nextent. Moreover, no specialized attention mechanisms\nor position embeddings were designed for LLaDA, nor\nwere any system-level architectural optimizations applied.\nOn the inference side, our exploration of guidance mecha-\nnisms (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) re-\nmains preliminary, and LLaDA currently exhibits sensitivity\nto inference hyperparameters. Furthermore, LLaDA has yet\nto undergo alignment with reinforcement learning (Ouyang\net al., 2022; Rafailov et al., 2024), which is crucial for im-\nproving its performance and alignment with human intent.\n\n8\n\n\n\nLarge Language Diffusion Models\n\nLooking forward, the scale of LLaDA is still smaller than\nleading counterparts (Achiam et al., 2023; Dubey et al.,\n2024; Google, 2024; Anthropic, 2024; Yang et al., 2024;\nLiu et al., 2024), highlighting the need for further scaling\nto fully assess its capabilities. In addition, LLaDA\u2019s abil-\nity to handle multi-modal data remains unexplored. The\nimpact of LLaDA on prompt tuning techniques (Wei et al.,\n2022) and its integration into agent-based systems (Park\net al., 2023; Wang et al., 2024a) is yet to be fully under-\nstood. Finally, a systematic investigation into post-training\nfor LLaDA could facilitate the development of O1-like sys-\ntems (OpenAI, 2024; Guo et al., 2025).\n\nImpact Statement\nThis work challenges the prevailing dominance of autore-\ngressive modeling in LLMs by proposing a diffusion-based\nalternative. Our findings open new avenues for exploring\nalternative probabilistic paradigms in natural language pro-\ncessing, with potential applications in conversational AI,\ncode generation, and complex reasoning tasks.\n\nHowever, diffusion-based models, like traditional LLMs,\nraise similar societal concerns. These include the environ-\nmental impact of large-scale training, the potential misuse\nfor generating harmful or misleading content, and the ampli-\nfication of biases present in training data. Addressing these\nchallenges is critical to ensuring the responsible develop-\nment and deployment of diffusion-based language models.\n\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (No. 92470118); Beijing Natural Sci-\nence Foundation (No. L247030); Beijing Nova Program\n(No. 20220484044); Major Innovation & Planning Interdis-\nciplinary Platform for the \u201cDouble-First Class\u201d Initiative,\nRenmin University of China; the Fundamental Research\nFunds for the Central Universities, the Research Funds of\nRenmin University of China (22XNKJ13); Ant Group Re-\nsearch Fund. The work was partially done at the Engineering\nResearch Center of Next-Generation Intelligent Search and\nRecommendation, Ministry of Education.\n\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\n\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebron, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. In Proceedings of the 2023 Conference on Em-\n\npirical Methods in Natural Language Processing, pp.\n4895\u20134901, 2023.\n\nAllen-Zhu, Z. and Li, Y. Physics of Language Mod-\nels: Part 3.2, Knowledge Manipulation. ArXiv e-prints,\nabs/2309.14402, September 2023. Full version available\nat http://arxiv.org/abs/2309.14402.\n\nAnonymous. Interpolating autoregressive and discrete de-\nnoising diffusion language models. In The Thirteenth\nInternational Conference on Learning Representations,\n2025. URL https://openreview.net/forum?\nid=tyEyYT267x.\n\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.\nanthropic.com/news/claude-3-5-sonnet.\n\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van\nDen Berg, R. Structured denoising diffusion models in\ndiscrete state-spaces. Advances in Neural Information\nProcessing Systems, 34:17981\u201317993, 2021a.\n\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021b.\n\nBao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu,\nJ. All are worth words: A vit backbone for diffusion\nmodels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22669\u2013\n22679, 2023.\n\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M. Efficient training of\nlanguage models to fill in the middle. arXiv preprint\narXiv:2207.14255, 2022.\n\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O. The reversal curse:\nLlms trained on\u201d a is b\u201d fail to learn\u201d b is a\u201d. arXiv\npreprint arXiv:2309.12288, 2023.\n\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\nScaling open-source language models with longtermism.\narXiv preprint arXiv:2401.02954, 2024.\n\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\n2020.\n\nBrooks, T., Peebles, B., Holmes, C., DePue, W.,\nGuo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman,\nT., Luhman, E., Ng, C., Wang, R., and Ramesh,\nA. Video generation models as world simulators.\n2024. URL https://openai.com/research/\nvideo-generation-models-as-world-simulators.\n\n9\n\nhttp://arxiv.org/abs/2309.14402\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://openai.com/research/video-generation-models-as-world-simulators\nhttps://openai.com/research/video-generation-models-as-world-simulators\n\n\nLarge Language Diffusion Models\n\nBrown, T. B. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\n\nCampbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deli-\ngiannidis, G., and Doucet, A. A continuous time frame-\nwork for discrete denoising models. Advances in Neural\nInformation Processing Systems, 35:28266\u201328279, 2022.\n\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,\nW. T. Maskgit: Masked generative image transformer. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11315\u201311325, 2022.\n\nChang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J.,\nJiang, L., Yang, M.-H., Murphy, K., Freeman, W. T.,\nRubinstein, M., et al. Muse: Text-to-image genera-\ntion via masked generative transformers. arXiv preprint\narXiv:2301.00704, 2023.\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\n\nChen, T., Zhang, R., and Hinton, G. Analog bits: Gen-\nerating discrete data using diffusion models with self-\nconditioning. arXiv preprint arXiv:2208.04202, 2022.\n\nChen, Z., Yuan, H., Li, Y., Kou, Y., Zhang, J., and Gu, Q.\nFast sampling via de-randomization for discrete diffusion\nmodels. arXiv preprint arXiv:2312.09193, 2023.\n\nChu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng,\nY., Lv, Y., He, J., Lin, J., et al. Qwen2-audio technical\nreport. arXiv preprint arXiv:2407.10759, 2024.\n\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\n\nDeletang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Ge-\nnewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K.,\nAitchison, M., Orseau, L., et al. Language modeling is\ncompression. In The Twelfth International Conference on\nLearning Representations.\n\nDevlin, J. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021.\n\nDieleman, S., Sartran, L., Roshannai, A., Savinov, N.,\nGanin, Y., Richemond, P. H., Doucet, A., Strudel, R.,\nDyer, C., Durkan, C., et al. Continuous diffusion for\ncategorical data. arXiv preprint arXiv:2211.15089, 2022.\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nFisher, R. A. On the mathematical foundations of theoretical\nstatistics. Philosophical transactions of the Royal Society\nof London. Series A, containing papers of a mathematical\nor physical character, 222(594-604):309\u2013368, 1922.\n\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n07 2024. URL https://zenodo.org/records/\n12608602.\n\nGat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Syn-\nnaeve, G., Adi, Y., and Lipman, Y. Discrete flow match-\ning. arXiv preprint arXiv:2407.15595, 2024.\n\nGong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq:\nSequence to sequence text generation with diffusion mod-\nels. arXiv preprint arXiv:2210.08933, 2022.\n\nGong, S., Agarwal, S., Zhang, Y., Ye, J., Zheng, L., Li, M.,\nAn, C., Zhao, P., Bi, W., Han, J., et al. Scaling diffu-\nsion language models via adaptation from autoregressive\nmodels. arXiv preprint arXiv:2410.17891, 2024.\n\nGoogle. Our next-generation model: Gemini 1.5, 2024.\nURL https://blog.google/technology/ai/\ngoogle-gemini-next-generation-model-february-2024.\n\nGraves, A., Srivastava, R. K., Atkinson, T., and\nGomez, F. Bayesian flow networks. arXiv preprint\narXiv:2308.07037, 2023.\n\nGulrajani, I. and Hashimoto, T. B. Likelihood-based diffu-\nsion language models. Advances in Neural Information\nProcessing Systems, 36, 2024.\n\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\n\nHan, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semi-\nautoregressive simplex-based diffusion language model\nfor text generation and modular control. arXiv preprint\narXiv:2210.17432, 2022.\n\n10\n\nhttps://zenodo.org/records/12608602\nhttps://zenodo.org/records/12608602\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\n\n\nLarge Language Diffusion Models\n\nHe, Z., Sun, T., Wang, K., Huang, X., and Qiu, X. Diffu-\nsionbert: Improving generative masked language models\nwith diffusion models. arXiv preprint arXiv:2211.15029,\n2022.\n\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J. Measuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\n\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\n\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840\u20136851, 2020.\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\n\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The\ncurious case of neural text degeneration. arXiv preprint\narXiv:1904.09751, 2019.\n\nHoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B.,\nBerg, R. v. d., and Salimans, T. Autoregressive diffusion\nmodels. arXiv preprint arXiv:2110.02037, 2021a.\n\nHoogeboom, E., Nielsen, D., Jaini, P., Forre\u0301, P., and Welling,\nM. Argmax flows and multinomial diffusion: Learning\ncategorical distributions. Advances in Neural Information\nProcessing Systems, 34:12454\u201312465, 2021b.\n\nHu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z.,\nFang, Y., Huang, Y., Zhao, W., et al. Minicpm: Unveiling\nthe potential of small language models with scalable train-\ning strategies. arXiv preprint arXiv:2404.06395, 2024.\n\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu,\nJ., Lv, C., Zhang, Y., Fu, Y., et al. C-eval: A multi-level\nmulti-discipline chinese evaluation suite for foundation\nmodels. Advances in Neural Information Processing\nSystems, 36, 2024a.\n\nHuang, Y., Zhang, J., Shan, Z., and He, J. Compres-\nsion represents intelligence linearly. arXiv preprint\narXiv:2404.09937, 2024b.\n\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\n\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\n\nKitouni, O., Nolte, N., Hensman, J., and Mitra, B. Disk: A\ndiffusion model for structured knowledge. arXiv preprint\narXiv:2312.05253, 2023.\n\nKitouni, O., Nolte, N., Bouchacourt, D., Williams, A., Rab-\nbat, M., and Ibrahim, M. The factorization curse: Which\ntokens you predict underlie the reversal curse and more.\narXiv preprint arXiv:2406.05183, 2024.\n\nKou, S., Hu, L., He, Z., Deng, Z., and Zhang, H. Cllms:\nConsistency large language models. arXiv preprint\narXiv:2403.00835, 2024.\n\nLi, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y.,\nDuan, N., and Baldwin, T. Cmmlu: Measuring mas-\nsive multitask language understanding in chinese. arXiv\npreprint arXiv:2306.09212, 2023.\n\nLi, X., Thickstun, J., Gulrajani, I., Liang, P. S., and\nHashimoto, T. B. Diffusion-lm improves controllable\ntext generation. Advances in Neural Information Process-\ning Systems, 35:4328\u20134343, 2022.\n\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\n\nLin, Z., Gong, Y., Shen, Y., Wu, T., Fan, Z., Lin, C., Duan,\nN., and Chen, W. Text generation with diffusion language\nmodels: A pre-training approach with continuous para-\ngraph denoise. In International Conference on Machine\nLearning, pp. 21051\u201321064. PMLR, 2023.\n\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,\nC., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437, 2024.\n\nLoshchilov, I. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017.\n\nLou, A. and Ermon, S. Reflected diffusion models, 2023.\n\nLou, A., Meng, C., and Ermon, S. Discrete diffusion lan-\nguage modeling by estimating the ratios of the data distri-\nbution. arXiv preprint arXiv:2310.16834, 2023.\n\nMahabadi, R. K., Ivison, H., Tae, J., Henderson, J., Beltagy,\nI., Peters, M. E., and Cohan, A. Tess: Text-to-text self-\nconditioned simplex diffusion, 2024.\n\n11\n\n\n\nLarge Language Diffusion Models\n\nMeng, C., Choi, K., Song, J., and Ermon, S. Concrete score\nmatching: Generalized score matching for discrete data.\nAdvances in Neural Information Processing Systems, 35:\n34532\u201334545, 2022.\n\nNie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M.,\nand Li, C. Scaling up masked diffusion models on text.\narXiv preprint arXiv:2410.18514, 2024.\n\nOpenAI. ChatGPT: Optimizing Language Models for Di-\nalogue. OpenAI blog, November 2022. URL https:\n//openai.com/blog/chatgpt/.\n\nOpenAI. Learning to reason with llms, 2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n\nOu, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li,\nC. Your absorbing discrete diffusion secretly models the\nconditional distributions of clean data. arXiv preprint\narXiv:2406.03736, 2024.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems, 35:27730\u201327744, 2022.\n\nPark, J. S., O\u2019Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and\ntechnology, pp. 1\u201322, 2023.\n\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pp. 4195\u20134205,\n2023.\n\nRadford, A. Improving language understanding by genera-\ntive pre-training, 2018.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\n\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\n\nReid, M., Hellendoorn, V. J., and Neubig, G. Diffuser:\nDiscrete diffusion via edit-based reconstruction, 2022.\n\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang,\nR. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa:\nA graduate-level google-proof q&a benchmark. arXiv\npreprint arXiv:2311.12022, 2023.\n\nRichemond, P. H., Dieleman, S., and Doucet, A. Categorical\nsdes with simplex diffusion, 2022.\n\nSahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marro-\nquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. Simple\nand effective masked diffusion language models. arXiv\npreprint arXiv:2406.07524, 2024.\n\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\n\nShannon, C. E. A mathematical theory of communication.\nThe Bell system technical journal, 27(3):379\u2013423, 1948.\n\nShazeer, N. Glu variants improve transformer. arXiv\npreprint arXiv:2002.05202, 2020.\n\nShi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. K.\nSimplified and generalized masked diffusion for discrete\ndata. arXiv preprint arXiv:2406.04329, 2024.\n\nShih, A., Sadigh, D., and Ermon, S. Training and infer-\nence on any-order autoregressive models the right way.\nIn Proceedings of the 31th International Conference on\nMachine Learning, 2022.\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256\u20132265. PMLR, 2015.\n\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\n\nStrudel, R., Tallec, C., Altche\u0301, F., Du, Y., Ganin, Y., Men-\nsch, A., Grathwohl, W., Savinov, N., Dieleman, S., Sifre,\nL., et al. Self-conditioned embedding diffusion for text\ngeneration. arXiv preprint arXiv:2211.04236, 2022.\n\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, 568:127063, 2024.\n\nSun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Score-\nbased continuous-time discrete diffusion models. arXiv\npreprint arXiv:2211.16750, 2022.\n\nSuzgun, M., Scales, N., Scha\u0308rli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\n\n12\n\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/index/learning-to-reason-with-llms/\nhttps://openai.com/index/learning-to-reason-with-llms/\n\n\nLarge Language Diffusion Models\n\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n\nUria, B., Murray, I., and Larochelle, H. A deep and tractable\ndensity estimator. In Proceedings of the 31th Interna-\ntional Conference on Machine Learning, 2014.\n\nVaswani, A. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J.,\nChen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on\nlarge language model based autonomous agents. Frontiers\nof Computer Science, 18(6):186345, 2024a.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDiffusion language models are versatile protein learners.\narXiv preprint arXiv:2402.18567, 2024b.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDplm-2: A multimodal diffusion protein language model.\narXiv preprint arXiv:2410.13782, 2024c.\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\nelicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824\u201324837,\n2022.\n\nWu, T., Fan, Z., Liu, X., Gong, Y., Shen, Y., Jiao, J., Zheng,\nH.-T., Li, J., Wei, Z., Guo, J., Duan, N., and Chen, W.\nAr-diffusion: Auto-regressive diffusion model for text\ngeneration, 2023.\n\nXu, C., Wang, X., Liao, Z., Li, Y., Hou, T., and Deng,\nZ. Show-o turbo: Towards accelerated unified multi-\nmodal understanding and generation. arXiv preprint\narXiv:2502.05415, 2025.\n\nXue, K., Zhou, Y., Nie, S., Min, X., Zhang, X., Zhou, J.,\nand Li, C. Unifying bayesian flow networks and diffusion\nmodels through stochastic differential equations. arXiv\npreprint arXiv:2404.15766, 2024.\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B.,\nLi, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu,\nJ., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang,\nK., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M.,\nZhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren,\nX., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y.,\nCui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\narXiv preprint arXiv:2412.15115, 2024.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Gu, Q. Diffusion\nlanguage models can perform many tasks with scaling and\n\ninstruction-finetuning. arXiv preprint arXiv:2308.12219,\n2023a.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Wang, M. Dinoiser:\nDiffused conditional sequence learning by manipulating\nnoises. arXiv preprint arXiv:2302.10025, 2023b.\n\nYe, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics of Lan-\nguage Models: Part 2.1, Grade-School Math and the Hid-\nden Reasoning Process. ArXiv e-prints, abs/2407.20311,\nJuly 2024. Full version available at http://arxiv.\norg/abs/2407.20311.\n\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\n\nZhang, B. and Sennrich, R. Root mean square layer nor-\nmalization. Advances in Neural Information Processing\nSystems, 32, 2019.\n\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,\nMin, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of\nlarge language models. arXiv preprint arXiv:2303.18223,\n2023.\n\nZheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and\nZhang, Q. Masked diffusion models are secretly time-\nagnostic masked models and exploit inaccurate categor-\nical sampling, 2024. URL https://arxiv.org/\nabs/2409.02908.\n\nZheng, L., Yuan, J., Yu, L., and Kong, L. A reparameter-\nized discrete diffusion model for text generation. ArXiv,\nabs/2302.05737, 2023.\n\n13\n\nhttp://arxiv.org/abs/2407.20311\nhttp://arxiv.org/abs/2407.20311\nhttps://arxiv.org/abs/2409.02908\nhttps://arxiv.org/abs/2409.02908\n\n\nLarge Language Diffusion Models\n\nAlgorithm 1 Pre-training of LLaDA\n\nRequire: mask predictor p\u03b8, data distribution pdata\n1: repeat\n2: x0 \u223c pdata, t \u223c U(0, 1] # with a probability of 1%, the sequence length of x0 follows U[1, 4096]\n3: xt \u223c qt|0(xt|x0) # qt|0 is defined in Eq. (7)\n4: Calculate L = \u2212 1\n\nt\u2217L\n\u2211L\n\ni=1 1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt) # L is the sequence length of x0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 2 Supervised Fine-Tuning of LLaDA\n\nRequire: mask predictor p\u03b8, pair data distribution pdata\n1: repeat\n2: p0, r0 \u223c pdata, t \u223c U(0, 1] # please refer to Appendix B.1 for details on the SFT data processing.\n3: rt \u223c qt|0(rt|r0) # qt|0 is defined in Eq. (7)\n\n4: Calculate L = \u2212 1\nt\u2217L\u2032\n\n\u2211L\u2032\n\ni=1 1[rit = M] log p\u03b8(r\ni\n0|p0, rt) # L\u2032 is the sequence length of r0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 3 Conditional Log-likelihood Evaluation of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, response r0, the number of Monte Carlo estimations nmc\n\n1: log likelihood = 0\n2: for i\u2190 1 to nmc do\n3: l \u223c {1, 2, . . . , L} # L is the sequence length of r0\n4: Obtain rl by uniformly sampling l tokens from r0 without replacement for masking\n5: log likelihood = log likelihood + L\n\nl\n\n\u2211L\ni=1 1[ril = M] log p\u03b8(r\n\ni\n0|p0, rl)\n\n6: end for\n7: log likelihood = log likelihood/nmc\n\n8: Return log likelihood\n\nA. Formulation of Masked Diffusion Models\n\nA.1. Training\n\nMasked diffusion models (MDMs) (Austin et al., 2021a; Lou et al., 2023; Ou et al., 2024) define the model distribution\np\u03b8(x0) in a manner distinct from autoregressive models.\n\nThese models introduce a forward process {xt} indexed by a time t \u2208 [0, 1]. This process gradually and independently\nmasks all tokens in the sequence x0. At time t = 0, the data point x0 is fully observed with no masks, while for t \u2208 (0, 1],\nxt represents latent variables with varying mask ratios in expectation.\n\nFormally, the conditional distribution of xt given x0 is defined by a fully factorized form:\n\nqt|0(xt|x0) =\n\nL\u220f\ni=1\n\nqt|0(x\ni\nt|xi\n\n0), (7)\n\nwhere the conditional distribution for each token is given by:\n\nqt|0(x\ni\nt|xi\n\n0) =\n\n{\n1\u2212 t, xi\n\nt = xi\n0,\n\nt, xi\nt = M.\n\n(8)\n\nHere, M denotes the mask token. Intuitively, each token either remains unchanged or is masked, with the probability of\n\n14\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 4 Reverse Process of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: r0 = argmaxr0 p\u03b8(r0|p0, rt) # we employ greedy sampling when predicting masked tokens\n5: for i\u2190 1 to L do\n6: if rt \u0338= M then\n7: ri0 = rit\n8: else\n9: With probability s\n\nt , ri0 is set to M\n10: end if\n11: end for\n12: rs = r0\n13: end for\n14: Return r0\n\nbeing masked increasing linearly as t progresses from 0 to 1. At t = 1, all tokens are guaranteed to be masked, meaning that\nx1 follows a Dirac distribution concentrated on a sequence of fully masked tokens. Notably, the linear masking probability\nis analogous to but distinct from, the noise schedule in continuous diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020). This linearity is motivated by the assumption that the information in the text is proportional to the\nnumber of tokens on average, making it reasonable to lose information linearly during the forward process.\n\nThe forward process is not only reversible but also corresponds to a reverse process that is fully factorized across all\ntokens (Austin et al., 2021a). The reverse process, from time t = 1 to 0, generates new data from sequences of fully masked\ntokens. The conditional distribution for the reverse process, for 0 \u2264 s < t \u2264 1, is factorized as:\n\nqs|t(xs|xt) =\n\nL\u220f\ni=1\n\nqs|t(x\ni\ns|xt), (9)\n\nwhere the conditional distribution for each token is:\n\nqs|t(x\ni\ns|xt) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n1, xi\n\nt \u0338= M, xi\ns = xi\n\nt,\ns\nt , xi\n\nt = M, xi\ns = M,\n\nt\u2212s\nt q0|t(x\n\ni\ns|xt), xi\n\nt = M, xi\ns \u0338= M,\n\n0, otherwise.\n\n(10)\n\nThus, the key function to estimate is the conditional distribution q0|t(x\ni\ns|xt), which predicts the original token if it is masked\n\nin the input xt. This is analogous to the data prediction form in continuous diffusion models.\n\nAs proven in (Ou et al., 2024), an equivalent yet time-free parameterization can be derived as:\n\nq0|t(x\ni\ns|xt) = pdata(x\n\ni\n0|xUM\n\nt ), \u2200i such that xi\nt = M, (11)\n\nwhere xUM\nt denotes the collection of unmasked tokens in xt, which is identical to the corresponding tokens in the original\n\ndata x0 since unmasked tokens are solely determined by x0 and are independent of time t. Intuitively, this implies that\nestimating the data prediction function is equivalent to estimating the conditional distributions on clean data, which is\ntime-invariant. Consequently, the time t need not be provided as input to the parametric model.\n\nAlthough the development of masked diffusion is nontrivial, the implementation is straightforward. We first introduce the\nmask predictor, a parametric model p\u03b8(\u00b7|xt) (e.g., a Transformer without a causal mask), which takes xt for any t as input\nand predict all masked tokens simultaneously. Then, we define the model distribution p\u03b8(x0) as follows: starting with x1 as\na sequence of fully masked tokens, we simulate an approximate reverse process parameterized by p\u03b8(\u00b7|xt) from t = 1 to 0.\nThe marginal distribution induced at t = 0 then represents the model distribution p\u03b8(x0).\n\n15\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 5 Low-confidence Remasking Strategy of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: for i\u2190 1 to L do\n5: if rit \u0338= M then\n6: ri0 = rit, c\n\ni = 1\n7: else\n8: ri0 = argmaxri0 p\u03b8(r\n\ni\n0|p0, rt)\n\n9: ci = p\u03b8(r\ni\n0|p0, rt)ri0\n\n10: end if\n11: end for\n12: nun = \u230aL(1\u2212 s)\u230b # the number of unmasked tokens is nun in timestep s\n13: for i\u2190 1 to L do\n14: if ci \u2208 Lowest\u2212 nun\n\n(\n{ci}L1\n\n)\nthen\n\n15: ri0 = M # the nun positions with the least confidence are selected for remasking.\n16: end if\n17: end for\n18: rs = r0\n19: end for\n20: Return r0\n\nFormally, the mask predictor is trained using a cross-entropy loss with masking:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (12)\n\nwhere x0 is sampled from the training data, t is sampled uniformly from [0, 1], and xt is sampled from qt|0(xt|x0). The\nindicator function 1[\u00b7] ensures that the cross-entropy loss is computed only for masked tokens. In Ou et al. (2024), it has\nbeen proven that the loss function L(\u03b8) is an upper bound on the negative log-likelihood of the model distribution:\n\n\u2212Ex0\u223cpdata(x0)\n[log p\u03b8(x0)] \u2264 L(\u03b8). (13)\n\nIn summary, this principled approach trains a generative model by progressively masking tokens during a forward process\nand learning to recover the data distribution during a reverse process, all under the (approximate) maximum likelihood\nestimation framework.\n\nA.2. Inference\n\nThe cross-entropy loss in Eq. (12) has several equivalent forms (Ou et al., 2024). The first one is given by\n\n\u2212El\u223c{1,2,...,L},x0,xl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[xi\nl = M] log p\u03b8(x\n\ni\n0|xl)\n\n]\n, (14)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and xl is obtained by uniformly sampling l tokens from x0 without\nreplacement for masking. Despite masking exactly l tokens is different from masking each token independently with\nprobability t, these two masking methods lead to equivalent results in expectation (Ou et al., 2024).\n\nWhile Eq. (12) and Eq. (14) share the same expectation, their variances differ. Intuitively, in Eq. (12), we expect xt to\nhave a fraction of t tokens masked. However, the randomness of the forward process (i.e., Eq. (7)) often causes deviations,\nespecially when xt contains few tokens. In contrast, in Eq. (14), the fraction of masked tokens in xl is deterministically l\n\nL .\nWhile a theoretical analysis depends on the data distribution, empirical results show that Eq. (12) requires over 1000 Monte\nCarlo estimates for stable results, whereas Eq. (14) achieves stability with only 128 estimates. In addition, we can simply\nmodify Eq. (14) to its conditional version (i.e., Eq. (6)) based on Eq. (5).\n\n16\n\n\n\nLarge Language Diffusion Models\n\nPrompt Block 0\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nBlock 1 Block 2\n\nNon-mask token\n\nMask token\n\nFigure 4. A Conceptual Overview of the Semi-autoregressive Sampling.\n\nAny-order autoregressive models (AO-ARM) (Uria et al., 2014; Hoogeboom et al., 2021a; Shih et al., 2022) characterize the\njoint distribution autoregressively for all possible orders \u03c0 of the L variables. To learn such a distribution, an AO-ARM\nutilizes a weight-sharing neural network to model all univariate conditionals and employs mask tokens to represent absent\nvariables. During training, the expected negative log-likelihood over the uniform distribution of all orders U\u03c0 is minimized:\n\n\u2212Ex0,\u03c0\u223cU\u03c0\n\n[\nL\u2211\n\ni=1\n\nlog p\u03b8(x\n\u03c0(i)\n0 |x\u03c0(<i)\n\n0 ;\u03c0)\n\n]\n. (15)\n\nIntuitively, x\u03c0(<i)\n0 can be understood as a masked token xt with index in \u03c0(\u2265 i) being masked. It can be further proved that\n\nEq. (15) is equivalent to Eq. (12). This connection explains the bidirectional reasoning capabilities of LLaDA, even though\nit was never used explicitly in the inference procedure.\n\nNie et al. (2024) introduce unsupervised classifier-free guidance, a plug-and-play technique that balances alignment with\nprompts and text diversity. Specifically, unsupervised classifier-free guidance employs the following modified mask predictor\nfor inference:\n\np\u0303\u03b8(r0|p0, rt) \u221d\np\u03b8(r0|p0, rt)1+w\n\np\u03b8(r0|m, rt)w\n, (16)\n\nwhere m is a mask sequence of the same length as p0 and w is a hyperparameter that controls the strength of p0. We employ\nunsupervised classifier-free guidance in the downstream tasks, as detailed in Appendix B.5.\n\nA.3. Algorithms\n\nIn this section, we present the training and inference algorithms. Specifically, we introduce the pre-training and supervised\nfine-tuning algorithms in Algorithm 1 and Algorithm 2, respectively. In addition, the likelihood evaluation algorithm is\nprovided in Algorithm 3. Finally, we present the sampling algorithms in Algorithm 4 and Algorithm 5, which correspond to\nthe original reverse process (i.e., random remasking) and the low-confidence remasking strategy (Chang et al., 2022) of\nLLaDA, respectively.\n\nFurthermore, we apply a semi-autoregressive remasking strategy for the instruct model. Specifically, the sequence is\ndivided into several blocks and generated from left to right. Within each block, we apply the random remasking or the\nlow-confidence remasking strategy for sampling. For clarity and better visualization, we illustrate the semi-autoregressive\nsampling process in Fig. 4.\n\nOur semi-autoregressive remasking strategy is inspired by the prior work of Anonymous (2025). However, it is important to\nnote that Anonymous (2025) and LLaDA have entirely different training objectives and network architectures. Specifically,\nthe pre-training objective of Anonymous (2025) differs from Eq. (3). Moreover, their approach incorporates a causal mask\nin the Transformer attention block, whereas LLaDA employs a bidirectional Transformer.\n\n17\n\n\n\nLarge Language Diffusion Models\n\nTable 5. Model Architecture. We provide the configurations of our 1B and 7B ARM baselines, as well as the 1B and 8B LLaDA and the\n8B LLaMA3 models.\n\nOur ARM Baseline 1B LLaDA 1B Our ARM Baseline 7B LLaDA 8B LLaMA3 8B\n\nLayers 22 22 28 32 32\n\nModel dimension 2048 2048 4096 4096 4096\n\nAttention heads 32 32 32 32 32\n\nVocabulary size 126,464 126,464 126,464 126,464 128,000\n\nFFN dimension 5634 5634 13,440 12,288 14,336\n\nKey/Value heads 4 4 8 32 8\n\nTotal parameters 1.49 B 1.49 B 6.83 B 8.02 B 8.03 B\n\nNon-embedding parameters 0.97 B 0.97 B 5.80 B 6.98 B 6.98 B\n\nB. Experiments\nB.1. Details of SFT Data\n\nA dynamic sequence length is employed during supervised SFT, where |EOS| tokens are appended to the end of short pairs\nto ensure equal sequence lengths across all data in each mini-batch. Notably, the padding |EOS| tokens are treated as part of\nthe response, i.e., masked and included in the training objective. The |EOS| tokens are removed from the generated outputs\nduring sampling. This strategy is crucial for LLaDA and ensures that the model learns to control the length of its responses\nby generating |EOS|, enabling the response length to align effectively with the given prompt.\n\nIn addition, for n-turn dialogues (p00, r\n0\n0, p\n\n1\n0, r\n\n1\n0, . . . , p\n\nn\u22121\n0 , rn\u22121\n\n0 ), we treat it as n single-turn dialogue pairs, i.e.,\n(p00, r\n\n0\n0), (p\n\n0\n0r\n\n0\n0p\n\n1\n0, r\n\n1\n0), . . . , (p\n\n0\n0r\n\n0\n0p\n\n1\n0r\n\n1\n0 . . . p\n\nn\u22121\n0 , rn\u22121\n\n0 ) and randomly sample one. This data partitioning strategy not only\nequips LLaDA with multi-turn dialogue capabilities but also aligns with the padding strategy.\n\nB.2. Architectures and Scaling Experiments\n\nThis section provides detailed information about the models we trained and the scalability experiments described in Sec. 3.1.\n\nFirstly, for efficiency, we trained an ARM and a MDM, both with 1.5B parameters and identical architectures. Additionally,\nwe scaled the MDM to 8B parameters. Due to computational resource constraints, we did not train an 8B autoregressive\nmodel with the same architecture. Instead, we utilized our previously trained 7B autoregressive model for comparison.\nThese four models are utilized in the scalability analysis in Sec. 3.1.\n\nWe adopted a Transformer architecture similar to LLaMA (Touvron et al., 2023; Dubey et al., 2024) for the ARMs and MDMs\nwe trained. Specifically, we employ RMSNorm (Zhang & Sennrich, 2019) to stabilize training, used SwiGLU (Shazeer,\n2020) as the activation function to enhance non-linearity, and integrate RoPE (Su et al., 2024) for more expressive positional\nencoding. Tab. 5 provides an overview of the model architectures.\n\nAdditionally, we employ the widely used 6ND formulation (Kaplan et al., 2020; Hoffmann et al., 2022) to calculate the\ntraining FLOPs in Fig. 3, where N represents the number of non-embedding parameters, and D denotes the total number of\ntraining tokens.\n\nB.3. Details and Ablation on Remasking\n\nThis section first explains the inference methods in detail, followed by ablation studies on various remasking strategies.\n\nFor instruct model, since our training includes a large number of |EOS| tokens, directly applying the lowest confidence\nremasking strategy would result in an excessively high and unnatural occurrence of |EOS| tokens in the generated sentences.\nTherefore, we divide the entire generation length into several blocks. Each block is generated from left to right, and within\neach block, we apply the reverse process to perform sampling. However, for the base model, since |EOS| tokens are not\npadded during training, using the semi-autoregressive remasking strategy makes little difference in the results.\n\n18\n\n\n\nLarge Language Diffusion Models\n\nTable 6. Ablation on Remasking. We use GSM8K accuracy as the evaluation metric. The lowest-confidence & semi-autoregressive\nremasking strategy refers to generating each block from left to right, while applying the lowest-confidence remasking strategy within each\nblock.\n\nLLaDA 8B Base LLaDA 8B Instruct\n\nRandomly remasking 52.3 72.0\nLowest confidence remasking 64.7 12.9\nLowest confidence & semi-autoregressive remasking 64.4 73.8\n\nTable 7. Ablation on Generated Length. We use the GSM8K accuracy as the evaluation metric and set the number of sampling steps to\nhalf of the generated length.\n\nLength LLaDA 8B Base LLaDA 8B Instruct\n\n256 62.5 75.3\n512 64.7 73.8\n1024 65.9 75.3\n\nFor efficiency, we set the answer length to 512 and the number of sampling steps to 256 in our ablation studies. For the\nsemi-autoregressive remasking strategy, we set the block length to 32. To simplify the evaluation, we use GSM8K accuracy\nas the performance metric. As shown in Table 6, for the base model, applying the lowest confidence remasking strategy\nsignificantly improves performance, whereas the semi-autoregressive remasking strategy does not yield any noticeable\nbenefits. In contrast, for the instruct model, directly applying the lowest confidence remasking strategy leads to a substantial\nperformance drop. However, combining the lowest confidence remasking strategy with the semi-autoregressive remasking\nstrategy results in the best performance for the instruct model.\n\nWe provide the specific remasking strategies and hyperparameters used for each benchmark in Appendix B.5.\n\nB.4. Ablation on Generated Length\n\nIn this section, we conduct ablation studies on the generated length. For simplicity, we use GSM8K as the evaluation metric.\nTo ensure efficiency and fairness, for each setting, we set the number of sampling steps to half of the generated length,\nensuring that in each sampling step, two tokens are transferred from the mask to the text.\n\nFor the LLaDA 8B Base model, we employ the lowest-confidence remasking strategy (i.e., Algorithm 5). For the LLaDA\n8B Instruct model, we adopt semi-autoregressive sampling with a block length of 32.\n\nAs reported in Tab. 7, both the Base model and the Instruct model exhibit minimal sensitivity to the length hyperparameter.\n\nB.5. Standard Benchmarks and Evaluation\n\nIn this section, we briefly introduce the benchmarks used in our study and describe the details of our evaluation process.\n\nFollowing standard LLM (Chu et al., 2024; Yang et al., 2024) evaluation practices, we assess LLaDA across four dimensions:\n\nGeneral ability: MMLU (Hendrycks et al., 2020), BBH (Suzgun et al., 2022), ARC-C (Clark et al., 2018), Hel-\nlaswag (Zellers et al., 2019), TruthfulQA (Lin et al., 2021), WinoGrande (Sakaguchi et al., 2021) and PIQA (Bisk\net al., 2020).\n\nMath and science ability: GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021) and GPQA (Rein et al., 2023).\n\nCode generation: HumanEval (Chen et al., 2021), HumanEval-FIM (Bavarian et al., 2022) and MBPP (Austin et al.,\n2021b).\n\nChinese understanding: CMMLU (Li et al., 2023) and C-Eval (Huang et al., 2024a).\n\nFor all the aforementioned benchmarks, we follow the widely adopted evaluation process (Gao et al., 2024) used in LLM\nassessments, primarily employing conditional likelihood estimation and conditional generation. Specifically, for certain\nbenchmarks, a prompt and multiple candidate answers are provided, and the model is required to compute each candidate\u2019s\n\n19\n\n\n\nLarge Language Diffusion Models\n\nTable 8. Inference Configuration for LLaDA 8B Instruct. We apply the low-confidence remasking strategy across all datasets. A block\nlength smaller than the answer length indicates the use of the semi-autoregressive remasking strategy; otherwise, it is not applied. The\nshorter answer lengths for MMLU and HellaSwag result from their requirement to generate only a single token (i.e., A, B, C, or D) and\ntwo special end tokens (i.e., end of the model response and end of the sentence).\n\nAnswer length Block length\n\nMMLU 3 3\nHellaswag 3 3\nGPQA 128 64\nGSM8K 256 8\nHumanEval 512 32\nMBPP 512 32\nMath 256 256\nMMLU-pro 256 256\nARC-C 512 512\n\nTable 9. Comparison on iGSM Dataset.\n\n4 steps 5 steps 6 steps\n\nLLaMA3 8B Base 38.0 35.0 34.0\nLLaDA 8B Base 64.0 41.0 44.0\n\nconditional likelihood. The candidate with the highest likelihood is then selected as the model\u2019s final answer, and accuracy\nis used as the evaluation metric. For the remaining benchmarks, the model generates responses based on the given prompt,\nand performance is evaluated using metrics such as exact match and other relevant criteria.\n\nFor the base model, we use conditional likelihood estimation for MMLU, CMMLU, C-Eval, ARC-C, Hellaswag, TruthfulQA,\nWinoGrande, PIQA, and GPQA, while the remaining benchmarks are evaluated using conditional generation. For the\ninstruct model, we evaluate all benchmarks using conditional generation. For datasets evaluated with conditional likelihood\nestimation, we adopt the widely used lm-evaluation-harness (Gao et al., 2024) framework. However, for datasets evaluated\nwith conditional generation, we rely on an internal library for evaluation, as lm-evaluation-harness lacks implementations\nfor certain key metrics (e.g., HumanEval-FIM).\n\nFor benchmarks evaluated using conditional likelihood estimation, we use Monte Carlo estimation to approximate Eq. (6)\nfor conditional likelihood estimation. Since MMLU, CMMLU, and C-EVAL only require the likelihood of a single token, a\nsingle Monte Carlo estimate is sufficient for these benchmarks. For all other benchmarks, we find that 128 Monte Carlo\nsamples are adequate to produce stable results. For ARC-C, Hellaswag, TruthfulQA, WinoGrande, PIQA, and GPQA, we\nreport the results with the optimal unsupervised classifier-free guidance scale searched in {0, 0.5, 1, 1.5, 2}.\n\nFor benchmarks evaluated using conditional generation, we apply the lowest confidence remasking strategy to the base model,\nsetting both the response length and the number of sampling steps to 1024. For the instruct model, we set the sampling steps\nequal to the answer length and provide the inference hyperparameters in Table 8. Due to computational resource constraints,\nwe do not use the unsupervised classifier-free guidance for these benchmarks evaluated using conditional generation.\n\nB.6. Analysis of Sampling Steps\n\nIn this section, we analyze the impact of sampling steps. For simplicity, we use the LLaDA 8B Base model and evaluate it\non the GSM8K and HumanEval datasets. Additionally, we fix the answer length at 1024. As shown in Fig. 5, as the number\nof sampling steps increases to match the answer length, the model\u2019s accuracy improves accordingly.\n\nB.7. Evaluation on iGSM Dataset\n\nTo further assess the mathematical capabilities of LLaDA, we test its performance on iGSM (Ye et al., 2024), an infinite,\nsynthetic GSM8K-like dataset. iGSM is generated via specific rules, with parameters that control the difficulty of problems\n(i.e., the number of solution steps). For evaluation consistency, we append \u201d#### $answer\u201d to the final solution, adhering to\n\n20\n\n\n\nLarge Language Diffusion Models\n\n64 128 256 512 1,024\nNFEs\n\n20\n\n40\n\n60\nG\n\nSM\n8K\n\n (4\n-s\n\nho\nt)\n\n64 128 256 512 1,024\nNFEs\n\n10\n\n20\n\n30\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt)\n\nFigure 5. Analysis of Sampling Steps.\n\nthe GSM8K format. Below is an example with solution steps set to 4:\n\n(Question) The number of each North Star Elementary\u2019s Cultural Studies Classroom equals 1. The number of each\nWestridge Elementary\u2019s Dance Studio equals 3 times as much as the sum of each North Star Elementary\u2019s Classroom\nand each North Star Elementary\u2019s Cultural Studies Classroom. How many Dance Studio does Westridge Elementary\nhave?\n(Solution) Define North Star Elementary\u2019s Cultural Studies Classroom as x; so x = 1.\nDefine North Star Elementary\u2019s Classroom as m; so m = x = 1.\nDefine Westridge Elementary\u2019s Dance Studio as n; w = m + x = 1 + 1 = 2;\nso n = 3 * w = 3 * 2 = 1 #### 1\n\nSince there are slight differences between GSM8K and iGSM (e.g., the use of a mod 5 algorithmic system), we follow (Ye\net al., 2024) and provide a system prompt along with four-shot question-answer pairs for each problem.\n\n(Prompt) You\u2019re an expert at solving elementary math problems involving addition, subtraction, and multiplication.\nYou solve all the problems in a uniform format. All calculations are done modulo 5. For example, 4 + 4 equals 3, 2 + 4\nequals 1, 3 + 3 + 3 equals 4, 3 * 3 equals 4, and 2 * 2 equals 4. When providing your solution, please end with \u2019#### x.\u2019\nwhere x is your final answer, an integer between 0 and 4. You must solve all the problems using the same solution\nformat. Our scenarios involve up to four categories of objects: schools, classrooms, backpacks and stationeries. Each\nschool may contain classrooms, each classroom may contain backpacks, and each backpack may contain stationeries.\nWe can specify quantities, such as t\u0308he number of dance studios at each Lakeshore High.A\u0308ssume that every entity\nwith the same name has an identical configuration; for example, each Lakeshore High contains the same number of\ndance studios. Another guiding principle is that what is not mentioned does not exist: when we refer to classrooms at\nLakeshore High, we are only discussing the classrooms explicitly mentioned in our scenario. Furthermore, if Lakeshore\nHigh is not even mentioned, any classroom within it is automatically considered to be non-existent (i.e. 0).\n\nFor solution steps ranging from 4 to 6, we generate 100 questions for each case and report the corresponding accuracy in\nTable 9. As shown in the table, LLaDA Base demonstrates significant and consistent advantages over LLaMA3 on unseen\nmathematical problems, aligning with the results in Table 1.\n\nB.8. Poem Completion Tasks\n\nIn this section, we present examples from our poem completion dataset as follows.\n\nExample 1:\nPrompt: \u7a88\u7a95\u6dd1\u5973\u7684\u4e0b\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u541b\u5b50\u597d\u9011\u3002\n\n21\n\n\n\nLarge Language Diffusion Models\n\nTable 10. Visualization of the Semi-Autoregressive Sampling Process. In the response of LLaDA, darker colors indicate tokens\npredicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser What are the benefits of regular exercise for physical and mental health?\nLLaDA\n\nTable 11. Visualization of the Multi-turn Dialogue. We employ random remasking strategy. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser I want to have a pet, but I don\u2019t know whether to get a cat or a dog. Do you have any\nsuggestions?\n\nLLaDA\n\nUser I\u2019m very busy at work, so it seems like a cat would be a more suitable option. Do you have\nany suggestions for getting a cat?\n\nLLaDA\n\nExample 2:\nPrompt: \u4e0d\u62d8\u4e00\u683c\u964d\u4eba\u624d\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u6211\u529d\u5929\u516c\u91cd\u6296\u64de\u3002\n\nB.9. More Case Studies\n\nIn this section, we present additional case studies of LLaDA 8B Instruct. First, Tab. 10 illustrates the sampling process of\nsemi-autoregressive remasking, while Tab. 11 depicts the sampling process for multi-turn dialogues with random remasking.\nAdditionally, Tab. 12 and Tab. 13 provide further examples of single-turn and multi-turn dialogues. Finally, Tab. 14 presents\nexamples of poem reversal completions where the LLaDA 8B Instruct model succeeds, in contrast to the failure of GPT-4o.\n\n22\n\n\n\nLarge Language Diffusion Models\n\nTable 12. Single-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser What\u2019s a piece of advice you\u2019ve received that you\u2019ve found helpful?\nLLaDA One helpful piece of advice I\u2019ve received is to always be open to learning and growth.\n\nI\u2019ve found that being willing to explore new ideas and experiences has helped me become\nmore resilient and adaptable. By embracing a growth mindset, I\u2019ve been able to identify\nopportunities for improvement and development, both personally and professionally.\n\nDaily life\nUser Please recommend me three famous movies\nLLaDA Certainly! Here are three famous movies that have achieved widespread popularity and\n\ncritical acclaim:\n1. **The Shawshank Redemption** (1994) - Inspired by a novella and directed by Frank\nDarabont, this film tells the story of Andy Dufresne, a man convicted of murdering his\nwife, dealing with his harsh life in Shawshank Prison and forming an unlikely friendship\nwith a fellow inmate.\n2. **The Godfather** (1972) - Directed by Francis Ford Coppola, this film is based\non Mario Puzo\u2019s novel of the same name. It follows the story of the Corleone family,\nparticularly Michael Corleone, who rises to power and takes over the family\u2019s criminal\nempire.\n3. **The Empire Strikes Back** (1980) - Directed by George Lucas, this film continues\nthe story of a young Skywalker in the Star Wars saga. It follows Luke Skywalker as\nhe helps the Rebel Alliance fight against the Galactic Empire and destroy the powerful\nweapon, the Death Star.\nAll of these films are celebrated for their cinematic achievements, powerful storytelling,\nand memorable characters.\n\nTranslation\nUser Please help me translate into Chinese: \u2018What is now proved was once only imagined\u2019\nLLaDA \u73b0\u5728\u88ab\u8bc1\u660e\u7684\u66fe\u7ecf\u53ea\u662f\u60f3\u8c61\n\nTranslation\nUser Please help me translate into German: \u2018What is now proved was once only imagined\u2019\nLLaDA Wer heute besta\u0308tigt ist, war einst nur immaginiert.\n\nMath\nUser Emma decides to run 4 sprints 5 times a week. She runs 75 meters each sprint. How many\n\ntotal meters does she run a week?\nLLaDA Emma runs 4 sprints each time, so she runs a total of 4 x 5 = 20 sprints a week. Each sprint\n\nis 75 meters long, so Emma runs a total of 20 x 75 = 1500 meters a week. The final result\nis 1500\n\nCode\nUser I am writing python code, I have an array, please help me sort it from largest to smallest\nLLaDA Certainly! You can sort an array from the largest to smallest in Python using the \u2018sort()\u2019\n\nmethod or the \u2018sorted()\u2018 function. Here\u2019s an example:\n### Using \u2018sort()\u2019 method:\nmy array = [3, 1, 4, 1, 5, 2]\nmy array.sort(reverse=True)\nprint(my array)\n\n23\n\n\n\nLarge Language Diffusion Models\n\nTable 13. Multi-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser How do I prepare for a job interview?\nLLaDA To prepare for a job interview, you should research the company and the job, practice\n\ncommon interview questions, and dress appropriately.\nUser What are some common interview questions?\nLLaDA Some common interview questions include \u201cTell me about yourself\u201d and \u201cWhy do you\n\nwant to work for us\u201d?\n\nDaily life\nUser How do I start learning programming?\nLLaDA To learn programming, you can start by choosing a programming language that you are\n\ninterested in. Some popular languages include Python, Java, and C++.\nUser Which programming language should I learn first?\nLLaDA It depends on your interests and goals. Python is a great language for beginners for its\n\nsimplicity and versatility.\nUser How long does it take to learn Python?\nLLaDA It can take anywhere from a few weeks to several months to learn Python.\nUser What are some good Python projects for beginners?\nLLaDA Some good Python projects for beginners include building a simple calculator, a game, and\n\na web application.\n\nMath\nUser Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.\n\nHow many clips did Natalia sell altogether in April and May?\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n\n48/2 = 24 clips in May. Altogether, Natalia sold 48 + 24 = 72 clips in April and May. The\nfinal result is 72.\n\nUser Natalia sold three times as many items in June as she did in May. How many clips did\nNatalia sell altogether in these three months?\n\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May,\nso she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in\nthese three months. The final result is 144.\n\nTable 14. Poem Reversal Completion Cases where LLaDA 8B Instruct Succeeds but GPT-4o Fails.\n\nQuestion1\nUser \u2018\u4f46\u95fb\u4eba\u8bed\u54cd\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\u201d,\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bcd\u4eba\u674e\u6e05\u7167\u7684\uff08\u6b66\u9675\n\n\u6625\uff0c\u6625\u665a:\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\uff0c\u4f46\u95fb\u4eba\u8bed\u54cd\u3002\u8fd9\u9996\u8bcd\u8868\u8fbe\u4e86\u4f5c\u8005\u5728\u6625\u5929\u65f6\u8282\u7684\u5b64\u72ec\u4e0e\n\u611f\u4f24\uff0c\u501f\u82b1\u843d\u548c\u4eba\u8bed\u58f0\u7684\u63cf\u5199\uff0c\u53cd\u6620\u51fa\u8bcd\u4eba\u5185\u5fc3\u7684\u65e0\u4eb2\u4e0e\u5931\u843d\u3002\n\nLLaDA \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u4e0d\u89c1\u4eba\u201d\n\nQuestion2\nUser \u2018\u591c\u9759\u6625\u5c71\u7a7a\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u65b0\u96e8\u540e\u201d.\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bd7\u4eba\u738b\u7ef4\u7684\uff08\u5c71\u5c45\u79cb\n\n\u819c\uff09:\u7a7a\u5c71\u65b0\u96e8\u540e\uff0c\u591c\u9759\u6625\u5c71\u7a7a\u3002\u8fd9\u9996\u8bd7\u63cf\u7ed8\u4e86\u6625\u5929\u5c71\u4e2d\u96e8\u540e\u7684\u5bc2\u9759\u4e0e\u7a7a\u7075\uff0c\n\u8868\u73b0\u51fa\u81ea\u7136\u7684\u5b81\u9759\u4e0e\u548c\u8c10\u7f8e\u3002\n\nLLaDA \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u4eba\u95f2\u6842\u82b1\u843d\u201d\n\n24","title":"2502.09992v2.pdf","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"context_stuff","is_visually_hidden_from_conversation":false,"retrieval_turn_number":0,"retrieval_file_index":0,"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"5e871a4a-5ab5-4984-9100-f8e52d67b224","request_id":"91ca6c63f9c840ba-SJC","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"5e871a4a-5ab5-4984-9100-f8e52d67b224","children":["721ee8ae-40cc-4a79-bf07-e1411bf72709"]},"721ee8ae-40cc-4a79-bf07-e1411bf72709":{"id":"721ee8ae-40cc-4a79-bf07-e1411bf72709","message":{"id":"721ee8ae-40cc-4a79-bf07-e1411bf72709","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1741353986.843289,"update_time":null,"content":{"content_type":"text","parts":["All the files uploaded by the user have been fully loaded. Searching won't provide additional information."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":false,"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"006b4d43-9cab-45ed-bee2-e475391f6b13","request_id":"91ca6c63f9c840ba-SJC","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"006b4d43-9cab-45ed-bee2-e475391f6b13","children":["249d096c-1946-4cfd-83f3-8b8a938aa1f2"]},"249d096c-1946-4cfd-83f3-8b8a938aa1f2":{"id":"249d096c-1946-4cfd-83f3-8b8a938aa1f2","message":{"id":"249d096c-1946-4cfd-83f3-8b8a938aa1f2","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1741353977.13469,"update_time":null,"content":{"content_type":"text","parts":["**Detailing the request**\n\nThe user wants a deep dive into the research paper \"Large Language Diffusion Models\" from 2025, focusing on \"LLaDA\" and highlighting key sections with comprehensive explanations and citations."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned about research paper analysis for 6 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"721ee8ae-40cc-4a79-bf07-e1411bf72709","finished_duration_sec":6,"request_id":"91ca6c63f9c840ba-SJC","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"721ee8ae-40cc-4a79-bf07-e1411bf72709","children":["1bc78d9d-0f4d-4fa3-8fba-1a35da30d8bd"]},"1bc78d9d-0f4d-4fa3-8fba-1a35da30d8bd":{"id":"1bc78d9d-0f4d-4fa3-8fba-1a35da30d8bd","message":{"id":"1bc78d9d-0f4d-4fa3-8fba-1a35da30d8bd","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1741353975.056889,"update_time":null,"content":{"content_type":"text","parts":["Below is a detailed, section-by-section walkthrough of the paper \u201cLarge Language Diffusion Models\u201d by Shen Nie et al. \ue200cite\ue202turn0file0\ue201. The authors introduce LLaDA, a diffusion-based large language model (LLM) that departs from the standard autoregressive (AR) approach used in nearly all state-of-the-art LLMs (e.g., GPT variants or LLaMA). Instead, LLaDA uses Masked Diffusion Modeling (MDM) to generate text in a bidirectional fashion, which (1) achieves near-autoregressive performance at scale, (2) exhibits strong instruction-following and in-context learning capabilities, and (3) circumvents the \u201creversal curse\u201d problem commonly observed in AR models.\n\nBelow is a deep dive into the key aspects of the paper:\n\n---\n\n## 1. Motivation and Background\n\n### 1.1 The Dominance of Autoregressive Models\nTraditionally, large language models are trained via next-token prediction:\n```\np(x) = p(x\u2081) * p(x\u2082 | x\u2081) * ... * p(x\u1d63 | x\u2081,..., x\u1d63\u208b\u2081).\n```\nThis \u201cautoregressive modeling\u201d (ARM) paradigm underpins GPT, LLaMA, and most other modern LLMs. It offers high text quality and straightforward generation but also has notable drawbacks:\n- **Token-by-token generation** is slow and difficult to parallelize.\n- AR models can struggle with tasks where reversing token order or reasoning about \u201cright-to-left\u201d context is essential (the \u201creversal curse\u201d problem).\n- Some of AR\u2019s limitations\u2014such as left-to-right generation\u2014might not be theoretically necessary to get LLM intelligence or in-context learning.\n\n### 1.2 Why Diffusion for Language?\nDiffusion-based approaches have become popular in image generation but remain less explored in large-scale language. The authors hypothesize that a diffusion-based text model, if scaled properly, can:\n- Still satisfy the \u201cgenerative modeling principles\u201d that drive strong LLM performance.\n- Potentially address issues like the reversal curse, since a diffusion-based method can reason over all tokens at once, in a bidirectional manner.\n\nWith these motivations, the paper questions whether next-token prediction is an absolute requirement for emergent LLM capabilities and proposes **LLaDA**\u2014a large language diffusion approach\u2014to test this.\n\n---\n\n## 2. The LLaDA Approach\n\n### 2.1 Masked Diffusion Model (MDM)\n\n#### Forward Process\nIn LLaDA, instead of adding Gaussian noise (as in image diffusion models), a simple \u201cmasking\u201d noise is added. Concretely, each token in the text is independently turned into a special \u201cmask\u201d token with probability \\( t \\). As \\( t \\) goes from \\( 0 \\) to \\( 1 \\):\n- \\( t = 0 \\) implies no tokens are masked.\n- \\( t = 1 \\) implies all tokens are masked.\n\nThis forward process gives a \u201ctrajectory\u201d of increasingly masked text. Formally:\n\\[\nq_{t|0}(x_t \\mid x_0)\n\\]\ndenotes the probability of going from a fully unmasked sequence \\( x_0 \\) to a partially masked sequence \\( x_t \\). At \\( t=1 \\), \\( x_1 \\) is a string of all mask tokens.  \n\n#### Reverse Process\nTo generate from the model, one needs the reverse transition \\( x_t \\to x_s \\) for \\( s < t \\). In principle, the model must \u201cde-mask\u201d tokens step by step until all tokens are recovered. LLaDA implements a neural network called a **mask predictor**, which takes the partially masked sequence \\( x_t \\) as input and predicts all masked tokens at once. This is sometimes referred to as \u201cdata prediction\u201d (the network tries to guess which tokens were under those masks).\n\n### 2.2 Training Objective\nLLaDA\u2019s training objective is:\n\\[\nL(\\theta) \\;=\\; -\\, \\mathbb{E}_{t,x_0,x_t}\\!\\Bigg[\n\\frac{1}{t} \\sum_{i=1}^{L}\\mathbf{1}[x_t^i = M]\n\\;\\log \\, p_\\theta\\bigl(x_0^i \\mid x_t\\bigr)\n\\Bigg].\n\\]\nHere:\n- \\( t \\) is drawn uniformly from \\([0,1]\\).\n- \\( x_t \\) is the forward-masked sequence of \\( x_0 \\).\n- The factor \\( \\frac{1}{t} \\) normalizes for how many tokens on average get masked at time \\( t \\).\n- This cost is proven to upper-bound the negative log-likelihood of the resulting diffusion model. Hence, LLaDA is a principled MLE approach.\n\n### 2.3 Architecture\n- **Transformer backbone** similar to typical LLMs (e.g., multi-head attention, feed-forward blocks, RMSNorm, etc.).\n- **No causal mask**: The \u201cattention mask\u201d is purely bidirectional, letting each token attend to all unmasked tokens.  \n- **Vocabulary & tokenizer**: A custom tokenizer is used.  \n- LLaDA is trained in two main phases:\n  1. **Pre-training** with random text data (2.3 trillion tokens total).\n  2. **Supervised fine-tuning** (SFT) on 4.5M instruction-like pairs to improve instruction-following.\n\n---\n\n## 3. Pre-training and Fine-tuning\n\n### 3.1 Pre-training Setup\n- Trained on a large curated dataset of size ~2.3 trillion tokens (mixing code, math, multilingual data, etc.).\n- The largest LLaDA variant (\u201cLLaDA 8B\u201d) has 8 billion parameters, with training cost of ~\\(10^{23}\\) FLOPs, similar to a large AR model of comparable scale.\n- During training, the model sees partially masked sequences (with a random mask ratio \\(t\\)) and learns to reconstruct the masked tokens.\n\n### 3.2 Supervised Fine-tuning (SFT)\n- After pre-training, LLaDA is further fine-tuned on 4.5M instruction-following pairs (similar to how many LLMs use SFT to better follow queries).\n- The only difference is that at SFT time, *only the response tokens* are masked (the prompt tokens are left untouched), so the model learns to specifically fill in masked tokens for the user\u2019s answer.\n\n### 3.3 Inference Strategies\n- Because a diffusion-based model can \u201cfill in all masks simultaneously,\u201d LLaDA uses a discrete-time schedule from \\( t=1 \\) (fully masked) down to \\( t=0 \\) (unmasked).  \n- Number of sampling steps \\(N\\) is a key hyperparameter: more steps often improves quality but increases decode time.  \n- The paper also discusses two remasking strategies:  \n  1. **Random remasking**: revert some fraction of tokens to masks at each step in a purely random manner.  \n  2. **Low-confidence remasking**: identify tokens with the lowest predicted probability and re-mask them for further refinement.  \n- For the SFT version, a \u201csemi-autoregressive remasking\u201d is sometimes employed (generating text in blocks from left to right), which helps produce more coherent longer responses (e.g., not overpopulating with EOS tokens too early).\n\n---\n\n## 4. Results and Key Findings\n\n### 4.1 Scalability\nOne of the core claims is that LLaDA can \u201cscale\u201d comparably to AR LLMs in terms of downstream performance:\n- On a variety of tasks (e.g., MMLU, ARC, GSM8K), LLaDA performance tracks with AR baselines as model size and training compute increase.  \n- For some tasks such as math problem-solving (GSM8K) or Chinese tasks (CMMLU, C-Eval), LLaDA 8B outperforms similarly sized AR baselines.\n\n### 4.2 In-Context Learning and Instruction Following\nLike GPT-style AR models, LLaDA shows emergent capabilities:\n- **Zero-/few-shot** results: LLaDA 8B is competitive with LLaMA 2 (7B) and LLaMA 3 (8B) on 15 standard tasks ranging from MMLU to HumanEval (code).\n- After supervised instruction tuning, LLaDA also handles multi-turn dialogues effectively\u2014even though it does not rely on next-token autoregression.\n\n### 4.3 Reversal Curse\n\u201cReversal curse\u201d refers to the phenomenon that AR models do poorly on tasks that require reversing text or reasoning in a right-to-left manner. Because AR models are \u201cbiased\u201d to read left-to-right, reversing the order can cause perplexity spikes.  \n- LLaDA addresses this directly: it generates all tokens in a more symmetric manner.  \n- In a poem-completion experiment (where the user must produce the prior line from a known poem), LLaDA 8B surpasses GPT-4 and Qwen on the \u201creversal\u201d test. AR models do very well at the forward direction but degrade heavily in the reversed direction, while LLaDA remains consistent in both directions.\n\n### 4.4 Case Studies\n- The authors show multi-step math solutions, code generation, multi-turn dialogues, and explicit \u201cvisualizations\u201d of the sampling process.  \n- Even though LLaDA\u2019s decoding is not token-by-token, it still yields coherent, correct text (often by refining partial guesses at each sampling step).\n\n---\n\n## 5. Discussion of Advantages and Limitations\n\n### 5.1 Advantages\n1. **Bidirectional context:** Because it sees all unmasked tokens simultaneously, LLaDA can reason with fewer constraints\u2014useful for reversal tasks or overall flexibility.  \n2. **Performance**: At scale, LLaDA matches or surpasses AR baselines on a range of benchmarks.  \n3. **Instruction-following**: The standard pipeline (pre-train + SFT) also works well for diffusion-based text models.  \n4. **Reversal tasks**: LLaDA avoids typical left-to-right modeling issues, performing robustly in reversed text queries.\n\n### 5.2 Limitations and Future Work\n1. **Computation:** Diffusion sampling typically takes multiple steps. LLaDA can be slower in inference if one uses many diffusion steps for refinement.  \n2. **Scaling beyond 8B**: The authors note that LLaDA is still smaller than leading LLMs (tens of billions to hundreds of billions), so further scaling is needed to see if it can match (or exceed) top-tier GPT-like models.  \n3. **Token-level noise only**: The approach focuses on random masking. It remains an open question whether other discrete noising schemes or specialized mask schedules might improve performance.  \n4. **Alignment**: LLaDA\u2019s SFT improves instruction-following, but it has not yet explored RLHF or other advanced alignment techniques that AR LLMs often employ.  \n\n---\n\n## 6. Conclusion\n\nThe central message is that large language models *do not strictly require* next-token autoregression to exhibit:\n- Strong \u201cin-context\u201d capabilities,\n- Reasoning with chain-of-thought,\n- Multi-turn, instruction-based dialogue.\n\nInstead, **Masked Diffusion Modeling** can be scaled successfully\u2014**LLaDA** is the first known 8B-parameter diffusion LLM trained (roughly) from scratch on trillions of tokens, then fine-tuned to achieve near-parity with strong AR LLM baselines. It also offers special advantages in tasks like text reversal. Looking forward, the authors see many open avenues, such as refining the sampling process (to reduce the number of steps) and exploring advanced alignment (RLHF), multi-modal expansions, or even bigger diffusion LLMs to test the ultimate limits of this approach.  \n\n---\n\n## References to the Paper\nAll quotations, figures, and formulation details in this summary come from \u201cLarge Language Diffusion Models\u201d by Shen Nie et al. \ue200cite\ue202turn0file0\ue201."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[{"start_ix":118,"end_ix":135,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"2502.09992v2.pdf","id":"file-EjEr99nXVn1FWaMNbWUohS","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nLarge Language Diffusion Models\n\n\nLarge Language Diffusion Models\n\nShen Nie 1 * \u2020 Fengqi Zhu 1 * \u2020 Zebin You 1 \u2020 Xiaolu Zhang 2 \u2021 Jingyang Ou 1 Jun Hu 2 \u2021 Jun Zhou 2\n\nYankai Lin 1 \u2021 Ji-Rong Wen 1 Chongxuan Li 1 \u2021 \u00b6\n\nAbstract\nAutoregressive models (ARMs) are widely re-\ngarded as the cornerstone of large language mod-\nels (LLMs). We challenge this notion by intro-\nducing LLaDA, a diffusion model trained from\nscratch under the pre-training and supervised fine-\ntuning (SFT) paradigm. LLaDA models distri-\nbutions through a forward data masking process\nand a reverse process, parameterized by a vanilla\nTransformer to predict masked tokens. By op-\ntimizing a likelihood bound, it provides a prin-\ncipled generative approach for probabilistic in-\nference. Across extensive benchmarks, LLaDA\ndemonstrates strong scalability, outperforming\nour self-constructed ARM baselines. Remark-\nably, LLaDA 8B is competitive with strong LLMs\nlike LLaMA3 8B in in-context learning and, af-\nter SFT, exhibits impressive instruction-following\nabilities in case studies such as multi-turn dia-\nlogue. Moreover, LLaDA addresses the rever-\nsal curse, surpassing GPT-4o in a reversal poem\ncompletion task. Our findings establish diffu-\nsion models as a viable and promising alternative\nto ARMs, challenging the assumption that key\nLLM capabilities discussed above are inherently\ntied to ARMs. Project page and codes: https:\n//ml-gsai.github.io/LLaDA-demo/.\n\n1. Introduction\nWhat is now proved was once only imagined.\nplaceholder,placeholder,placeho\u2014William Blake\n\nLarge language models (LLMs) (Zhao et al., 2023) fall en-\ntirely within the framework of generative modeling. Specifi-\ncally, LLMs aim to capture the true but unknown language\n\n*Equal contribution \u2020Work done during an internship at Ant\nGroup \u2021Project leaders 1Gaoling School of Artificial Intelli-\ngence, Renmin University of China; Beijing Key Laboratory\nof Big Data Management and Analysis Methods 2Ant Group.\n\u00b6Correspondence to: Chongxuan Li <chongxuanli@ruc.edu.cn>.\n\nPreprint.\n\n46\n\n33\n\n37\n50\n\n19\n\n24\n\n33\n49\n\n50\n\n56\n\n39\n\n45\n60\n\n23\n\n29\n\n40\n59\n\n60\n\n65\n\n46\n\n52\n70\n\n27\n\n34\n\n47\n69\n\n70\n\nMMLU\n\nTruthfulQA\n\nARC-C\nGSM8K\n\nMath\n\nHumanEval\n\nMBPP\nCMMLU\n\nC-Eval\n\nGeneral TasksM\nath\n\nem\nati\n\ncs\n\nCode\n\nChinese\n\nLLaDA 8B Base\nLLaMA 3 8B Base\nLLaMA 2 7B Base\n\nFigure 1. Zero/Few-Shot Benchmarks. We scale LLaDA to an\nunprecedented size of 8B parameters from scratch, achieving com-\npetitive performance with strong LLMs (Dubey et al., 2024).\n\ndistribution pdata(\u00b7) by optimizing a model distribution p\u03b8(\u00b7)\nthrough maximum likelihood estimation, or equivalently KL\ndivergence minimization between the two distributions:\n\nmax\n\u03b8\n\nEpdata(x) log p\u03b8(x)\u21d4 min\n\u03b8\n\nKL(pdata(x)||p\u03b8(x))\ufe38 \ufe37\ufe37 \ufe38\nGenerative modeling principles\n\n. (1)\n\nThe predominant approach relies on the autoregressive mod-\neling (ARM)\u2014commonly referred to as the next-token pre-\ndiction paradigm\u2014to define the model distribution:\n\np\u03b8(x) = p\u03b8(x\n1)\n\nL\u220f\ni=2\n\np\u03b8(x\ni | x1, . . . , xi\u22121)\ufe38 \ufe37\ufe37 \ufe38\n\nAutoregressive formulation\n\n, (2)\n\nwhere x is a sequence of length L, and xi is the i-th token.\n\nThis paradigm has proven remarkably effective (Radford,\n2018; Radford et al., 2019; Brown, 2020; OpenAI, 2022)\nand has become the foundation of current LLMs. Despite\nits widespread adoption, a fundamental question remains\nunanswered: Is the autoregressive paradigm the only viable\npath to achieving the intelligence exhibited by LLMs?\n\n1\n\nar\nX\n\niv\n:2\n\n50\n2.\n\n09\n99\n\n2v\n2 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n8 \nFe\n\nb \n20\n\n25\n\nhttps://ml-gsai.github.io/LLaDA-demo/\nhttps://ml-gsai.github.io/LLaDA-demo/\n\n\nLarge Language Diffusion Models\n\nWe argue that the answer is not a simple \u201cyes\u201d. The key\ninsight overlooked previously is: it is the generative mod-\neling principles (i.e., Eq. (1)), rather than the autoregres-\nsive formulation (i.e., Eq. (2)) itself, that fundamentally\nunderpin the essential properties of LLMs, as detailed be-\nlow. However, certain inherent limitations of LLMs can be\ndirectly traced to their autoregressive nature.\n\nIn particular, we argue that scalability is primarily a con-\nsequence of the interplay between Transformers (Vaswani,\n2017), model and data size, and Fisher consistency1 (Fisher,\n1922) induced by the generative principles in Eq. (1), rather\nthan a unique result of ARM. The success of diffusion trans-\nformers (Bao et al., 2023; Peebles & Xie, 2023) on visual\ndata (Brooks et al., 2024) supports this claim.\n\nFurthermore, the capabilities of instruction-following and\nin-context learning (Brown, 2020) appear to be intrinsic\nproperties of all proper conditional generative models on\nstructurally consistent linguistic tasks, rather than exclusive\nadvantages of ARMs. In addition, while ARMs can be\ninterpreted as a lossless data compressor (Deletang et al.;\nHuang et al., 2024b), any sufficiently expressive probabilis-\ntic model can achieve similar capabilities (Shannon, 1948).\n\nNevertheless, the autoregressive nature of LLMs presents\nnotable challenges. For example, sequential token-by-token\ngeneration incurs high computational costs, and the left-\nto-right modeling limits effectiveness in reversal reasoning\ntasks (Berglund et al., 2023). These inherent limitations\nconstrain LLMs in handling longer and more complex tasks.\n\nMotivated by these insights, we introduce LLaDA, a Large\nLanguage Diffusion with mAsking, to investigate whether\nthe capabilities exhibited by LLMs can emerge from gener-\native modeling principles beyond ARMs in Eq. (2), thereby\naddressing the fundamental question posed earlier. In con-\ntrast to traditional ARMs, LLaDA leverages a masked diffu-\nsion model (MDM) (Austin et al., 2021a; Lou et al., 2023;\nShi et al., 2024; Sahoo et al., 2024; Ou et al., 2024), which\nincorporates a discrete random masking process and trains\na mask predictor to approximate its reverse process. This\ndesign enables LLaDA to construct a model distribution\nwith bidirectional dependencies and optimize a lower bound\nof its log-likelihood, offering an unexplored and principled\nalternative to existing LLMs.\n\nWe adopt the standard pipeline of data preparation, pre-\ntraining, supervised fine-tuning (SFT), and evaluation, scal-\ning LLaDA to an unprecedented language diffusion of size\n8B. In particular, LLaDA 8B was pre-trained from scratch\non 2.3 trillion tokens using 0.13 million H800 GPU hours,\nfollowed by SFT on 4.5 million pairs. Across diverse tasks,\nincluding language understanding, math, code, and Chinese,\n\n1It suggests the ability to recover the true data distribution with\ninfinite data, a sufficiently large network and optimal training.\n\nLLaDA demonstrates the following contributions:\n\nScalability. LLaDA scales effectively up to a computational\nbudget of 1023 FLOPs, achieving comparable results to self-\nconstructed ARM baselines trained on the same data across\nsix tasks, e.g., MMLU and GSM8K.\n\nIn-Context Learning. Remarkably, LLaDA 8B surpasses\nLLaMA2 7B (Touvron et al., 2023) on nearly all 15 standard\nzero/few-shot learning tasks while performing on par with\nLLaMA3 8B (Dubey et al., 2024).\n\nInstruction-Following. LLaDA significantly enhances the\nability to follow instructions after SFT, as demonstrated in\ncase studies such as multi-turn dialogue.\n\nReversal Reasoning. LLaDA effectively breaks the re-\nversal curse (Berglund et al., 2023) with consistent per-\nformance across forward and reversal tasks. Notably, it\noutperforms GPT-4o in a reversal poem completion task.\n\n2. Approach\nIn this section, we introduce the probabilistic formulation2,\nalong with the pre-training, supervised fine-tuning, and in-\nference procedures for LLaDA, as illustrated in Fig. 2.\n\n2.1. Probabilistic Formulation\n\nUnlike ARMs in Eq. (2), LLaDA defines a model distribu-\ntion p\u03b8(x0) through a forward process and a reverse pro-\ncess (Austin et al., 2021a; Ou et al., 2024). The forward\nprocess gradually masks tokens independently in x0 until\nthe sequence is fully masked at t = 1. For t \u2208 (0, 1), the\nsequence xt is partially masked, with each being masked\nwith probability t or remaining unmasked with probability\n1\u2212 t. The reverse process recovers the data distribution by\niteratively predicting masked tokens as t moves from 1 to 0.\n\nThe core of LLaDA is a mask predictor, a parametric model\np\u03b8(\u00b7|xt) that takes xt as input and predicts all masked tokens\n(denoted M) simultaneously. It is trained using a cross-\nentropy loss computed only on the masked tokens:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (3)\n\nwhere x0 is sampled from the training data, t is sampled\nuniformly from [0, 1], and xt is sampled from the forward\nprocess. The indicator function 1[\u00b7] ensures that the loss is\ncomputed only for masked tokens.\n\nOnce trained, we can simulate a reverse process (see Sec. 2.4\nfor details) parameterized by the mask predictor and define\nthe model distribution p\u03b8(x0) as the marginal distribution\n\n2Here, we focus on the approach of LLaDA. A rigorous formu-\nlation of MDM is provided in Appendix A for interested readers.\n\n2\n\n\n\nLarge Language Diffusion Models\n\nMask predictor\n\nMask all tokens independently\n\nMask predictor\n\nPrompt Response\n\nMask predictor\n\nPrompt Response\n\n... \n\n... \n\nA\nn\n\n in\nte\n\nrm\ne\n\nd\nia\n\nte\n s\n\nte\np\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nNon-mask token Random mask\n\nRemaskMask token\n\n(c)(b)(a)\n\nRemask\n\nMask ratio \ud835\udc61 \u223c \ud835\udc48(0,1)\n\nFigure 2. A Conceptual Overview of LLaDA. (a) Pre-training. LLaDA is trained on text with random masks applied independently to all\ntokens at the same ratio t \u223c U [0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion\nprocess from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies.\n\ninduced at t = 0. Notably, the loss in Eq. (3) has been\nproven to be an upper bound on the negative log-likelihood\nof the model distribution (Shi et al., 2024; Ou et al., 2024):\n\n\u2212Epdata(x0) [log p\u03b8(x0)] \u2264 L(\u03b8), (4)\n\nmaking it a principled objective for generative modeling.\n\nNotably, LLaDA employs a masking ratio that varies ran-\ndomly between 0 and 1 while masked language models (De-\nvlin, 2018) use a fixed ratio. The subtly differences have\nsignificant implications, especially at scale: as shown in\nEq. (4), LLaDA is a principled generative model with the\npotential to perform in-context learning naturally, akin to\nLLMs. Moreover, its generative perspective ensures Fisher\nconsistency (Fisher, 1922) in extreme cases, suggesting\nstrong scalability with large data and models.\n\n2.2. Pre-training\n\nLLaDA employs a Transformer (Vaswani, 2017) as the mask\npredictor, whose architecture is similar to existing LLMs.\nHowever, LLaDA does not use a causal mask, as its formu-\nlation allows it to see the entire input for predictions.\n\nWe trained two variants of LLaDA with different sizes: 1\nbillion (B) and 8B. We summarize the model architecture\nof LLaDA 8B and LLaMA3 8B (Dubey et al., 2024) here\nand details are provided in Appendix B.2. We have ensured\nconsistency in most hyperparameters while making several\nnecessary modifications. We use vanilla multi-head atten-\ntion instead of grouped query attention (Ainslie et al., 2023)\nfor simplicity, as LLaDA is incompatible with KV caching,\nresulting in a different number of key and value heads. Con-\nsequently, the attention layer has more parameters, and we\nreduce the FFN dimension to maintain a comparable model\nsize. Additionally, the vocabulary size differs slightly due\nto a tokenizer (Brown, 2020) adapted on our data.\n\nThe LLaDA model is pre-trained on a dataset comprising\n2.3 trillion (T) tokens, adhering to a data protocol that aligns\n\nclosely with existing large language models (LLMs) (Tou-\nvron et al., 2023; Chu et al., 2024), without the incorporation\nof any special techniques. The data are derived from online\ncorpora, with low-quality content filtered through manually\ndesigned rules and LLM-based approaches. Beyond general\ntext, the dataset encompasses high-quality code, math, and\nmultilingual data. The mixing of data sources and domains\nis guided by scaled-down ARMs. The pre-training process\nutilizes a fixed sequence length of 4096 tokens, incurring a\ntotal computational cost of 0.13 million H800 GPU hours,\nsimilar to ARMs of the same scale and dataset size.\n\nFor a training sequence x0, we randomly sample t \u2208 [0, 1],\nmask each token independently with the same probability\nt to obtain xt (see Fig. 2 (a)) and estimate Eq. (3) via the\nMonte Carlo method for stochastic gradient descent training.\nIn addition, following Nie et al. (2024), to enhance the\nability of LLaDA to handle variable-length data, we set 1%\nof the pre-training data to a random length that is uniformly\nsampled from the range [1, 4096].\n\nWe adopted the Warmup-Stable-Decay (Hu et al., 2024)\nlearning rate scheduler to monitor the training progress\nwithout interrupting continuous training. Specifically, we\nlinearly increased the learning rate from 0 to 4\u00d7 10\u22124 over\nthe first 2000 iterations and maintained it at 4\u00d7 10\u22124. After\nprocessing 1.2T tokens, we decayed the learning rate to\n1\u00d7 10\u22124 and held it constant for the next 0.8T tokens to en-\nsure stable training. Finally, we linearly reduced the learning\nrate from 1\u00d710\u22124 to 1\u00d710\u22125 for the last 0.3T tokens. Fur-\nthermore, we utilized the AdamW optimizer (Loshchilov,\n2017) with a weight decay of 0.1, a batch size of 1280, and\na local batch size of 4 per GPU. The 8B experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.3. Supervised Fine-Tuning\n\nWe enhance the capability of LLaDA to follow instructions\nby supervised fine-tuning (SFT) with paired data (p0, r0),\n\n3\n\n\n\nLarge Language Diffusion Models\n\nwhere p0 is the prompt and r0 denotes the response. This\nis the simplest and most basic post-training method for\nLLMs. Technically, this requires to model the conditional\ndistribution p\u03b8(r0|p0) instead of p\u03b8(x0) in pre-training.\n\nThe implementation is similar to pre-training. As shown in\nFig. 2 (b), we leave the prompt unchanged and mask the\ntokens in the response independently, as done for x0. Then,\nwe feed both the prompt and the masked response rt to the\npre-trained mask predictor to compute the loss for SFT:\n\n\u2212Et,p0,r0,rt\n\n\uf8ee\uf8f01\n\nt\n\nL\u2032\u2211\ni=1\n\n1[rit = M] log p\u03b8(r\ni\n0|p0, rt)\n\n\uf8f9\uf8fb , (5)\n\nwhere L\u2032 denotes a dynamic length specified later, and all\nother notations remain the same as before.\n\nNote that this approach is fully compatible with pre-training.\nEssentially, the concatenation of p0 and r0 can be treated\nas clean pre-training data x0, while the concatenation of\np0 and rt serves as the masked version xt. The process is\nidentical to pre-training, with the only difference being that\nall masked tokens happen to appear in the r0 portion.\n\nThe LLaDA 8B model undergoes SFT on a dataset com-\nprising 4.5 million pairs. Consistent with the pre-training\nprocess, both data preparation and training follow the SFT\nprotocols utilized in existing LLMs (Chu et al., 2024; Yang\net al., 2024), without introducing any additional techniques\nto optimize LLaDA\u2019s performance. The dataset spans mul-\ntiple domains, including code, mathematics, instruction-\nfollowing, and structured data understanding. We append\n|EOS| tokens to the end of short pairs in each mini-batch\nto ensure equal lengths across all data. We treat |EOS| as a\nnormal token during training and remove it during sampling,\nenabling LLaDA to control the response length automati-\ncally. Please refer to Appendix B.1 for more details.\n\nWe train for 3 epochs on the SFT data using a similar sched-\nule to the pre-training phase. The learning rate is linearly\nincreased from 0 to 2.5\u00d7 10\u22125 over the first 50 iterations\nand then kept constant. During the final 10% of iterations,\nit is linearly reduced to 2.5 \u00d7 10\u22126. Additionally, we set\nthe weight decay to 0.1, the global batch size to 256, and\nthe local batch size to 2 per GPU. The SFT experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.4. Inference\n\nAs a generative model, LLaDA is capable of both sampling\nnew text and evaluating the likelihood of candidate text.\n\nWe begin with the sampling. As illustrated in Fig. 2 (c),\ngiven a prompt p0, we discretize the reverse process to sam-\nple from the model distribution p\u03b8(r0|p0), starting from a\nfully masked response. The total number of sampling steps\nis a hyperparameter, which naturally provides LLaDA with a\n\ntrade-off between efficiency and sample quality, as analyzed\nin Sec. 3.3. We employ uniformly distributed timesteps by\ndefault. In addition, the generation length is also treated as\na hyperparameter, specifying the length of the fully masked\nsentence at the beginning of the sampling process. As de-\ntailed in Appendix B.4, since both pre-training and SFT\nare conducted using datasets with variable lengths, the final\nresults are insensitive to this length hyperparameter.\n\nAt an intermediate step from time t \u2208 (0, 1] to s \u2208 [0, t), we\nfeed both p0 and rt into the mask predictor and predict all\nmasked tokens simultaneously. Subsequently, we remask s\n\nt\nof the predicted tokens in expectation to obtain rs, ensuring\nthat the transition of the reverse process aligns with the\nforward process for accurate sampling (Austin et al., 2021a).\n\nIn principle, the remasking strategy should be purely ran-\ndom. However, inspired by the annealing tricks of sampling\nin LLMs (Holtzman et al., 2019; Brown, 2020), we explore\ntwo deterministic yet effective remasking strategies. Specif-\nically, similarly to Chang et al. (2022), we remask the s\n\nt\nof predicted tokens with the lowest confidence based on\nthe predictions, called low-confidence remasking. Addition-\nally, for LLaDA after SFT, we can divide the sequence into\nseveral blocks and generate them from left to right, called\nsemi-autoregressive remasking. Within each block, we ap-\nply the reverse process to perform sampling. We provide\nmore details and ablation studies in Appendix. B.3.\n\nFor conditional likelihood evaluation, we can naturally uti-\nlize the upper bound in Eq. (5). However, we find that the\nfollowing equivalent form (Ou et al., 2024) exhibits lower\nvariance and is more stable for evaluation:\n\n\u2212El,r0,rl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[ril = M] log p\u03b8(r\ni\n0|p0, rl)\n\n]\n, (6)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and rl is\nobtained by uniformly sampling l tokens from r0 without\nreplacement for masking. In addition, we employ the unsu-\npervised classifier-free guidance (Nie et al., 2024). We refer\nthe readers to more details in Appendix A.2.\n\nWe present the training, sampling, and likelihood evaluation\nalgorithms, along with theoretical details, in Appendix A.\n\n3. Experiments\nWe evaluate the scalability, instruction-following, and in-\ncontext learning capabilities of LLaDA on standard bench-\nmarks, followed by analyses and case studies on more con-\ntrolled datasets to provide a comprehensive assessment.\n\n3.1. Scalability of LLaDA on Language Tasks\n\nWe first investigate the scalability of LLaDA on downstream\ntasks in comparison with the ARM baselines we constructed.\n\n4\n\n\n\nLarge Language Diffusion Models\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nA\nR\n\nC\n-C\n\n (0\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nC\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt) Autoregressive Baseline\n\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n50\n\n60\n\n70\n\n80\n\nPI\nQ\n\nA\n (0\n\n-s\nho\n\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n20\n\n40\n\n60\n\nG\nSM\n\n8K\n (4\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n8\n\n16\n\n24\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\nFigure 3. Scalability of LLaDA. We evaluate the performance of LLaDA and our ARM baselines trained on the same data across\nincreasing computational FLOPs. LLaDA exhibits strong scalability, matching the overall performance of ARMs on six tasks.\n\nSpecifically, at the 1B scale, we ensured that LLaDA and\nARM shared the same architecture, data, and all other config-\nurations. At larger scales, we also report results for LLaDA\nand ARM models of slightly different sizes trained on the\nsame data due to resource limit, as detailed in Appendix B.2.\nWe use the computational cost as a unified scaling metric.\nFor evaluation, we focused on six standard and diverse tasks.\n\nAs shown in Fig. 3, LLaDA demonstrates impressive scala-\nbility, with its overall trend being highly competitive with\nthat of ARM. Notably, in tasks such as MMLU and GSM8K,\nLLaDA exhibits even stronger scalability. Even on tasks like\nPIQA, where performance lags, LLaDA narrows the gap\nwith ARMs at larger scales. To account for the significant\ninfluence of outliers, we opted not to fit quantitative scaling\ncurves, avoiding potential misinterpretation. Nevertheless,\nthe results clearly demonstrate the scalability of LLaDA.\n\nNie et al. (2024) suggests that MDM requires 16 times\nmore computation than ARM to achieve the same likeli-\nhood. However, there are key differences that make the\nconclusions of this study more broadly applicable. In partic-\nular, likelihood is a relatively indirect metric for downstream\ntask performance, and diffusion optimizes a bound of the\nlikelihood, making it not directly comparable to ARM. Ad-\nditionally, we extended the scaling range from 1018 \u223c 1020\n\nin Nie et al. (2024) to 1020 \u223c 1023 in this work.\n\n3.2. Benchmark Results\n\nTo comprehensively evaluate the in-context learning and\ninstruction-following capabilities of LLaDA 8B, we con-\nducted detailed comparisons with existing LLMs (Touvron\n\net al., 2023; Dubey et al., 2024; Chu et al., 2024; Yang et al.,\n2024; Bi et al., 2024; Jiang et al., 2023) of similar scale.\nThe selection of tasks and evaluation protocols was aligned\nwith existing studies, encompassing 15 popular benchmarks\nin general tasks, mathematics, code, and Chinese. Further\ndetails are provided in Appendix B.5. For a more direct\ncomparison, we re-evaluated representative LLMs (Touvron\net al., 2023; Dubey et al., 2024) in our implementation.\n\nAs shown in Tab. 1, after pretraining on 2.3T tokens,\nLLaDA 8B demonstrates remarkable performance, surpass-\ning LLaMA2 7B on nearly all tasks, and is overall competi-\ntive with LLaMA3 8B. LLaDA shows advantages in math\nand Chinese tasks. We conjecture that the strengths stem\nfrom the same factors as its relatively weaker performance\nin some tasks\u2014differences in data quality and distribution,\nlargely due to the closed-source situation of LLM datasets.\n\nNotably, we have carefully ruled out the possibility of data\nleakage by taking GSM8K as an example. First, as shown\nin Fig. 3, LLaDA outperformed ARM baselines regarding\nGSM8K. Moreover, the conclusion remains on a fully un-\nseen GSM8K-like task (Ye et al., 2024) in Appendix B.7.\n\nFurther, Tab. 2 compares the performance of LLaDA 8B In-\nstruct with existing LLMs. We observed that SFT improved\nLLaDA\u2019s performance on most downstream tasks. A few\nmetrics, such as MMLU, showed declines, and we conjec-\nture may be due to the suboptimal quality of the SFT data.\nOverall, since we did not perform alignment with reinforce-\nment learning (RL), our results are slightly behind LLaMA3\n8B Instruct, though the gaps in many metrics remain small.\nNotably, even with only SFT, LLaDA demonstrates impres-\n\n5\n\n\n\nLarge Language Diffusion Models\n\nTable 1. Benchmark Results of Pre-trained LLMs. \u2217 indicates that LLaDA 8B Base, LLaMA2 7B Base, and LLaMA3 8B Base are\nevaluated under the same protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Chu et al. (2024); Yang et al.\n(2024) and Bi et al. (2024) respectively. The numbers in parentheses represent the number of shots used for evaluation. \u201c-\u201d indicates\nunknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Mistral 7B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T - 2T\n\nGeneral Tasks\n\nMMLU 65.9 (5) 65.4 (5) 45.9 (5) 70.3 (5) 74.2 (5) 64.2 (5) 48.2 (5)\nBBH 49.8 (3) 57.6 (3) 37.3 (3) 62.3 (3) 70.4 (3) 56.1 (3) 39.5 (3)\nARC-C 47.9 (0) 53.1 (0) 46.3 (0) 60.6 (25) 63.7 (25) 60.0 (25) 48.1 (0)\nHellaswag 72.5 (0) 79.1 (0) 76.0 (0) 80.7 (10) 80.2 (10) 83.3 (10) 75.4 (0)\nTruthfulQA 46.4 (0) 44.0 (0) 39.0 (0) 54.2 (0) 56.4 (0) 42.2 (0) -\nWinoGrande 74.8 (5) 77.3 (5) 72.5 (5) 77.0 (5) 75.9 (5) 78.4 (5) 70.5 (0)\nPIQA 74.4 (0) 80.6 (0) 79.1 (0) - - - 79.2 (0)\n\nMathematics & Science\n\nGSM8K 70.7 (4) 53.1 (4) 14.3 (4) 80.2 (4) 85.4 (4) 36.2 (4) 17.4 (8)\nMath 27.3 (4) 15.1 (4) 3.2 (4) 43.5 (4) 49.8 (4) 10.2 (4) 6.0 (4)\nGPQA 26.1 (5) 25.9 (5) 25.7 (5) 30.8 (5) 36.4 (5) 24.7 (5) -\n\nCode\n\nHumanEval 33.5 (0) 34.2 (0) 12.8 (0) 51.2 (0) 57.9 (0) 29.3 (0) 26.2 (0)\nHumanEval-FIM 73.8 (2) 73.3 (2) 26.9 (2) - - - -\nMBPP 38.2 (4) 47.4 (4) 18.4 (4) 64.2 (0) 74.9 (0) 51.1 (0) 39.0 (3)\n\nChinese\n\nCMMLU 69.9 (5) 50.7 (5) 32.5 (5) 83.9 (5) - - 47.2 (5)\nC-Eval 70.5 (5) 51.7 (5) 34.0 (5) 83.2 (5) - - 45.0 (5)\n\nTable 2. Benchmark Results of Post-trained LLMs. LLaDA only employs an SFT procedure while other models have extra reinforcement\nlearning (RL) alignment. \u2217 indicates that LLaDA 8B Instruct, LLaMA2 7B Instruct, and LLaMA3 8B Instruct are evaluated under the\nsame protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Yang et al. (2024) and Bi et al. (2024) respectively.\nThe numbers in parentheses represent the number of shots used for in-context learning. \u201c-\u201d indicates unknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Gemma2 9B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T 8T 2T\n\nPost-training SFT SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL\nAlignment pairs 4.5M - - 0.5M + - 1M + 0.15M - 1.5M + -\n\nGeneral Tasks\n\nMMLU 65.5 (5) 68.4 (5) 44.1 (5) - - - 49.4 (0)\nMMLU-pro 37.0 (0) 41.9 (0) 4.6 (0) 44.1 (5) 56.3 (5) 52.1 (5) -\nHellaswag 74.6 (0) 75.5 (0) 51.5 (0) - - - 68.5 (-)\nARC-C 88.5 (0) 82.4 (0) 57.3 (0) - - - 49.4 (-)\n\nMathematics & Science\n\nGSM8K 78.6 (4) 78.3 (4) 29.0 (4) 85.7 (0) 91.6 (0) 76.7 (0) 63.0 (0)\nMath 26.6 (0) 29.6 (0) 3.8 (0) 52.9 (0) 75.5 (0) 44.3 (0) 15.8 (0)\nGPQA 31.8 (5) 31.9 (5) 28.4 (5) 34.3 (0) 36.4 (0) 32.8 (0) -\n\nCode\n\nHumanEval 47.6 (0) 59.8 (0) 16.5 (0) 79.9 (0) 84.8 (0) 68.9 (0) 48.2 (-)\nMBPP 34.2 (4) 57.6 (4) 20.6 (4) 67.2 (0) 79.2 (0) 74.9 (0) 35.2 (-)\n\n6\n\n\n\nLarge Language Diffusion Models\n\nTable 3. Comparison in the Poem Completion Task.\n\nForward Reversal\n\nGPT-4o (2024-08-06) 82.7 34.3\nQwen2.5 7B Instruct 75.9 38.0\nLLaDA 8B Instruct 48.8 42.4\n\nsive instruction-following abilities, as detailed in Sec. 3.4.\nWe leave RL-based alignment for future work.\n\nOverall, despite the lack of data transparency, we have made\nevery effort to adopt standardized procedures and introduce\ndiverse tasks, we believe they sufficiently demonstrate the\nextraordinary capabilities of LLaDA, which is the only com-\npetitive non-autoregressive model to our knowledge.\n\n3.3. Reversal Reasoning and Analyses\n\nTo quantify the reversal reasoning ability of mod-\nels (Berglund et al., 2023), we follow the protocol estab-\nlished in Allen-Zhu & Li (2023). Specifically, we construct\na dataset of 496 famous Chinese poem sentence pairs. Given\na sentence from a poem, models are tasked with generating\nthe subsequent line (forward) or the preceding line (rever-\nsal) without additional fine-tuning. Examples can be found\nin Appendix B.8. This setting provides a straightforward\nand more realistic evaluation compared to previous stud-\nies (Nie et al., 2024; Kitouni et al., 2024).\n\nAs shown in Tab. 3, LLaDA effectively addresses the rever-\nsal curse (Berglund et al., 2023), demonstrating consistent\nzero-shot performance across both forward and reversal\ntasks. In contrast, both Qwen 2.5 and GPT-4o exhibit a\nsignificant gap between the two. The results on forward\ngeneration confirm that both ARMs are strong, benefiting\nfrom significantly larger datasets and greater computational\nresources than LLaDA. However, LLaDA outperforms both\nby a large margin in the reversal task.\n\nWe emphasize that we did not design anything special for\nreversal tasks. Intuitively, LLaDA treats tokens uniformly\nwithout inductive bias, leading to balanced performance.\nSee more details in Appendix A.2.\n\nWe also analyze the effect of remasking strategies and sam-\npling steps, as detailed in Appendix B.3 and Appendix B.6.\n\n3.4. Case Studies\n\nWe present samples generated by LLaDA 8B Instruct in\nTab. 4, showcasing its instruction-following capabilities.\nFirst, the table illustrates LLaDA\u2019s ability to generate co-\nherent, fluent, and extended text in a non-autoregressive\nmanner. Second, it highlights the model\u2019s multi-turn dia-\nlogue capability, effectively retaining conversation history\n\nand producing contextually appropriate responses across\nmultiple languages. Such chat capabilities of LLaDA are\nimpressive, as it departs from conventional ARMs for the\nfirst time, to the best of our knowledge. See more case\nstudies on remasking and reasoning tasks in Appendix B.9.\n\n4. Related Work\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020) have excelled in visual domains but\nremain unverified for LLMs despite extensive efforts.\n\nA simple approach is to continuousize text data and apply\ndiffusion models directly (Li et al., 2022; Gong et al., 2022;\nHan et al., 2022; Strudel et al., 2022; Chen et al., 2022;\nDieleman et al., 2022; Richemond et al., 2022; Wu et al.,\n2023; Mahabadi et al., 2024; Ye et al., 2023b). Alternatively,\nsome methods model continuous parameters of discrete dis-\ntributions instead (Lou & Ermon, 2023; Graves et al., 2023;\nLin et al., 2023; Xue et al., 2024). However, scalability\nremains a challenge, as a 1B parameter model requires 64\ntimes the compute of an ARM to achieve comparable per-\nformance (Gulrajani & Hashimoto, 2024).\n\nAnother approach replaces continuous diffusion with dis-\ncrete processes featuring new forward and reverse dy-\nnamics (Austin et al., 2021a), leading to numerous vari-\nants (Hoogeboom et al., 2021b;a; He et al., 2022; Campbell\net al., 2022; Meng et al., 2022; Reid et al., 2022; Sun et al.,\n2022; Kitouni et al., 2023; Zheng et al., 2023; Chen et al.,\n2023; Ye et al., 2023a; Gat et al., 2024; Zheng et al., 2024;\nSahoo et al., 2024; Shi et al., 2024). Notably, Lou et al.\n(2023) showed that masked diffusion, as a special case of\ndiscrete diffusion, achieves perplexity comparable to or sur-\npassing ARMs at GPT-2 scale. Ou et al. (2024) established\nfundamental theoretical results, which motivated our model\ndesign, training, and inference (see Appendix A). Nie et al.\n(2024) explored how MDM can be leveraged for language\ntasks such as question answering at GPT-2 scale. Gong et al.\n(2024) fine-tune ARMs in the MDM formulation. However,\nimprovements are confined to certain metrics, and it remains\nunclear whether this approach can yield a foundation model\ncomparable to strong LLMs under a comprehensive evalua-\ntion.\n\nIn comparison, this study scales MDM to an unprecedented\nsize of 8B parameters from scratch, achieving performance\ncomparable to leading LLMs such as LLaMA 3.\n\nAdditionally, a parallel line of work on image genera-\ntion (Chang et al., 2022; 2023) aligns well with the appli-\ncation of MDMs to text data. Moreover, MDMs have also\nshown promise in domains such as protein generation (Wang\net al., 2024b;c), where they have achieved promising results.\nNotably, Kou et al. (2024); Xu et al. (2025) demonstrate the\npotential of using distillation to accelerate MDMs sampling,\n\n7\n\n\n\nLarge Language Diffusion Models\n\nTable 4. Visualization of the Sampling Process and a Generated Multi-round Dialogue. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nSampling Process\n\nUser Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour.\nHow many kilometers can she run in 8 hours?\n\nLLaDA\n\nMulti-round Dialogue Case\n\nUser Could you please share with me what the initial two lines of the renowned poem \u2018The\nRoad Not Taken\u2019?\n\nLLaDA Certainly! The initial two lines of the renowned poem \u201cThe Road Not Taken\u201d by Robert\nFrost are: \u201cTwo roads diverged in a yellow wood, And sorry I could not travel both.\u201d These\nlines set the stage for the poem, introducing the idea of a speaker facing a choice between\ntwo paths.\n\nUser Please help me translate into Chinese.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nChinese as: \u201c\u4e24\u6761\u8def\u5206\u5c94\u5728\u9ec4\u8272\u7684\u6811\u6797\u4e2d\uff0c\u9057\u61be\u6211\u4e0d\u80fd\u540c\u65f6\u8d70\u201d\nUser Please translate into German.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nGerman as: \u201cZwei Wege trennten sich im gelben Wald, und leider konnte ich nicht beide\ntun.\u201d\n\nUser Please also write a poem about life choices. I require 5 sentences, and the first word of\neach sentence starts with C as the first letter.\n\nLLaDA Certainly! Here\u2019s a poem about life choices, with each sentence starting with the letter \u201cC\u201d:\nChoices cascade through life\u2019s journey, Creating connections with each decision, Careful\nconsideration leads to better paths, Courageous choices shape our destiny, Choosing wisely\nguides us through the years.\n\nfurther enhancing their efficiency.\n\n5. Conclusion and Discussion\nIn the middle of difficulty lies opportunity.\nplaceholder,placeholder,placeh\u2014Albert Einstein\n\nWe introduce LLaDA, a principled and previously unex-\nplored approach to large language modeling based on dif-\nfusion models. LLaDA demonstrates strong capabilities in\nscalability, in-context learning, and instruction-following,\nachieving performance comparable to strong LLMs. In addi-\ntion, LLaDA offers unique advantages such as bidirectional\nmodeling and enhanced robustness, effectively addressing\nseveral inherent limitations of existing LLMs. Our findings\nnot only establish diffusion models as a viable and promis-\ning alternative but also challenge the prevailing assumption\nthat these essential capabilities are inherently tied to ARMs.\n\nWhile promising, the full potential of diffusion models re-\n\nmains to be fully explored. Several limitations of this work\npresent significant opportunities for future research.\n\nDue to computational constraints, direct comparisons be-\ntween LLaDA and ARMs\u2014such as training on identical\ndatasets\u2014were restricted to a computational budget of less\nthan 1023 FLOPs. To allocate resources for training the\nlargest possible LLaDA model and showcasing its poten-\ntial, we were unable to scale the ARM baseline to the same\nextent. Moreover, no specialized attention mechanisms\nor position embeddings were designed for LLaDA, nor\nwere any system-level architectural optimizations applied.\nOn the inference side, our exploration of guidance mecha-\nnisms (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) re-\nmains preliminary, and LLaDA currently exhibits sensitivity\nto inference hyperparameters. Furthermore, LLaDA has yet\nto undergo alignment with reinforcement learning (Ouyang\net al., 2022; Rafailov et al., 2024), which is crucial for im-\nproving its performance and alignment with human intent.\n\n8\n\n\n\nLarge Language Diffusion Models\n\nLooking forward, the scale of LLaDA is still smaller than\nleading counterparts (Achiam et al., 2023; Dubey et al.,\n2024; Google, 2024; Anthropic, 2024; Yang et al., 2024;\nLiu et al., 2024), highlighting the need for further scaling\nto fully assess its capabilities. In addition, LLaDA\u2019s abil-\nity to handle multi-modal data remains unexplored. The\nimpact of LLaDA on prompt tuning techniques (Wei et al.,\n2022) and its integration into agent-based systems (Park\net al., 2023; Wang et al., 2024a) is yet to be fully under-\nstood. Finally, a systematic investigation into post-training\nfor LLaDA could facilitate the development of O1-like sys-\ntems (OpenAI, 2024; Guo et al., 2025).\n\nImpact Statement\nThis work challenges the prevailing dominance of autore-\ngressive modeling in LLMs by proposing a diffusion-based\nalternative. Our findings open new avenues for exploring\nalternative probabilistic paradigms in natural language pro-\ncessing, with potential applications in conversational AI,\ncode generation, and complex reasoning tasks.\n\nHowever, diffusion-based models, like traditional LLMs,\nraise similar societal concerns. These include the environ-\nmental impact of large-scale training, the potential misuse\nfor generating harmful or misleading content, and the ampli-\nfication of biases present in training data. Addressing these\nchallenges is critical to ensuring the responsible develop-\nment and deployment of diffusion-based language models.\n\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (No. 92470118); Beijing Natural Sci-\nence Foundation (No. L247030); Beijing Nova Program\n(No. 20220484044); Major Innovation & Planning Interdis-\nciplinary Platform for the \u201cDouble-First Class\u201d Initiative,\nRenmin University of China; the Fundamental Research\nFunds for the Central Universities, the Research Funds of\nRenmin University of China (22XNKJ13); Ant Group Re-\nsearch Fund. The work was partially done at the Engineering\nResearch Center of Next-Generation Intelligent Search and\nRecommendation, Ministry of Education.\n\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\n\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebron, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. In Proceedings of the 2023 Conference on Em-\n\npirical Methods in Natural Language Processing, pp.\n4895\u20134901, 2023.\n\nAllen-Zhu, Z. and Li, Y. Physics of Language Mod-\nels: Part 3.2, Knowledge Manipulation. ArXiv e-prints,\nabs/2309.14402, September 2023. Full version available\nat http://arxiv.org/abs/2309.14402.\n\nAnonymous. Interpolating autoregressive and discrete de-\nnoising diffusion language models. In The Thirteenth\nInternational Conference on Learning Representations,\n2025. URL https://openreview.net/forum?\nid=tyEyYT267x.\n\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.\nanthropic.com/news/claude-3-5-sonnet.\n\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van\nDen Berg, R. Structured denoising diffusion models in\ndiscrete state-spaces. Advances in Neural Information\nProcessing Systems, 34:17981\u201317993, 2021a.\n\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021b.\n\nBao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu,\nJ. All are worth words: A vit backbone for diffusion\nmodels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22669\u2013\n22679, 2023.\n\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M. Efficient training of\nlanguage models to fill in the middle. arXiv preprint\narXiv:2207.14255, 2022.\n\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O. The reversal curse:\nLlms trained on\u201d a is b\u201d fail to learn\u201d b is a\u201d. arXiv\npreprint arXiv:2309.12288, 2023.\n\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\nScaling open-source language models with longtermism.\narXiv preprint arXiv:2401.02954, 2024.\n\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\n2020.\n\nBrooks, T., Peebles, B., Holmes, C., DePue, W.,\nGuo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman,\nT., Luhman, E., Ng, C., Wang, R., and Ramesh,\nA. Video generation models as world simulators.\n2024. URL https://openai.com/research/\nvideo-generation-models-as-world-simulators.\n\n9\n\nhttp://arxiv.org/abs/2309.14402\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://openai.com/research/video-generation-models-as-world-simulators\nhttps://openai.com/research/video-generation-models-as-world-simulators\n\n\nLarge Language Diffusion Models\n\nBrown, T. B. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\n\nCampbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deli-\ngiannidis, G., and Doucet, A. A continuous time frame-\nwork for discrete denoising models. Advances in Neural\nInformation Processing Systems, 35:28266\u201328279, 2022.\n\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,\nW. T. Maskgit: Masked generative image transformer. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11315\u201311325, 2022.\n\nChang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J.,\nJiang, L., Yang, M.-H., Murphy, K., Freeman, W. T.,\nRubinstein, M., et al. Muse: Text-to-image genera-\ntion via masked generative transformers. arXiv preprint\narXiv:2301.00704, 2023.\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\n\nChen, T., Zhang, R., and Hinton, G. Analog bits: Gen-\nerating discrete data using diffusion models with self-\nconditioning. arXiv preprint arXiv:2208.04202, 2022.\n\nChen, Z., Yuan, H., Li, Y., Kou, Y., Zhang, J., and Gu, Q.\nFast sampling via de-randomization for discrete diffusion\nmodels. arXiv preprint arXiv:2312.09193, 2023.\n\nChu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng,\nY., Lv, Y., He, J., Lin, J., et al. Qwen2-audio technical\nreport. arXiv preprint arXiv:2407.10759, 2024.\n\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\n\nDeletang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Ge-\nnewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K.,\nAitchison, M., Orseau, L., et al. Language modeling is\ncompression. In The Twelfth International Conference on\nLearning Representations.\n\nDevlin, J. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021.\n\nDieleman, S., Sartran, L., Roshannai, A., Savinov, N.,\nGanin, Y., Richemond, P. H., Doucet, A., Strudel, R.,\nDyer, C., Durkan, C., et al. Continuous diffusion for\ncategorical data. arXiv preprint arXiv:2211.15089, 2022.\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nFisher, R. A. On the mathematical foundations of theoretical\nstatistics. Philosophical transactions of the Royal Society\nof London. Series A, containing papers of a mathematical\nor physical character, 222(594-604):309\u2013368, 1922.\n\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n07 2024. URL https://zenodo.org/records/\n12608602.\n\nGat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Syn-\nnaeve, G., Adi, Y., and Lipman, Y. Discrete flow match-\ning. arXiv preprint arXiv:2407.15595, 2024.\n\nGong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq:\nSequence to sequence text generation with diffusion mod-\nels. arXiv preprint arXiv:2210.08933, 2022.\n\nGong, S., Agarwal, S., Zhang, Y., Ye, J., Zheng, L., Li, M.,\nAn, C., Zhao, P., Bi, W., Han, J., et al. Scaling diffu-\nsion language models via adaptation from autoregressive\nmodels. arXiv preprint arXiv:2410.17891, 2024.\n\nGoogle. Our next-generation model: Gemini 1.5, 2024.\nURL https://blog.google/technology/ai/\ngoogle-gemini-next-generation-model-february-2024.\n\nGraves, A., Srivastava, R. K., Atkinson, T., and\nGomez, F. Bayesian flow networks. arXiv preprint\narXiv:2308.07037, 2023.\n\nGulrajani, I. and Hashimoto, T. B. Likelihood-based diffu-\nsion language models. Advances in Neural Information\nProcessing Systems, 36, 2024.\n\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\n\nHan, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semi-\nautoregressive simplex-based diffusion language model\nfor text generation and modular control. arXiv preprint\narXiv:2210.17432, 2022.\n\n10\n\nhttps://zenodo.org/records/12608602\nhttps://zenodo.org/records/12608602\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\n\n\nLarge Language Diffusion Models\n\nHe, Z., Sun, T., Wang, K., Huang, X., and Qiu, X. Diffu-\nsionbert: Improving generative masked language models\nwith diffusion models. arXiv preprint arXiv:2211.15029,\n2022.\n\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J. Measuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\n\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\n\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840\u20136851, 2020.\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\n\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The\ncurious case of neural text degeneration. arXiv preprint\narXiv:1904.09751, 2019.\n\nHoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B.,\nBerg, R. v. d., and Salimans, T. Autoregressive diffusion\nmodels. arXiv preprint arXiv:2110.02037, 2021a.\n\nHoogeboom, E., Nielsen, D., Jaini, P., Forre\u0301, P., and Welling,\nM. Argmax flows and multinomial diffusion: Learning\ncategorical distributions. Advances in Neural Information\nProcessing Systems, 34:12454\u201312465, 2021b.\n\nHu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z.,\nFang, Y., Huang, Y., Zhao, W., et al. Minicpm: Unveiling\nthe potential of small language models with scalable train-\ning strategies. arXiv preprint arXiv:2404.06395, 2024.\n\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu,\nJ., Lv, C., Zhang, Y., Fu, Y., et al. C-eval: A multi-level\nmulti-discipline chinese evaluation suite for foundation\nmodels. Advances in Neural Information Processing\nSystems, 36, 2024a.\n\nHuang, Y., Zhang, J., Shan, Z., and He, J. Compres-\nsion represents intelligence linearly. arXiv preprint\narXiv:2404.09937, 2024b.\n\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\n\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\n\nKitouni, O., Nolte, N., Hensman, J., and Mitra, B. Disk: A\ndiffusion model for structured knowledge. arXiv preprint\narXiv:2312.05253, 2023.\n\nKitouni, O., Nolte, N., Bouchacourt, D., Williams, A., Rab-\nbat, M., and Ibrahim, M. The factorization curse: Which\ntokens you predict underlie the reversal curse and more.\narXiv preprint arXiv:2406.05183, 2024.\n\nKou, S., Hu, L., He, Z., Deng, Z., and Zhang, H. Cllms:\nConsistency large language models. arXiv preprint\narXiv:2403.00835, 2024.\n\nLi, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y.,\nDuan, N., and Baldwin, T. Cmmlu: Measuring mas-\nsive multitask language understanding in chinese. arXiv\npreprint arXiv:2306.09212, 2023.\n\nLi, X., Thickstun, J., Gulrajani, I., Liang, P. S., and\nHashimoto, T. B. Diffusion-lm improves controllable\ntext generation. Advances in Neural Information Process-\ning Systems, 35:4328\u20134343, 2022.\n\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\n\nLin, Z., Gong, Y., Shen, Y., Wu, T., Fan, Z., Lin, C., Duan,\nN., and Chen, W. Text generation with diffusion language\nmodels: A pre-training approach with continuous para-\ngraph denoise. In International Conference on Machine\nLearning, pp. 21051\u201321064. PMLR, 2023.\n\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,\nC., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437, 2024.\n\nLoshchilov, I. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017.\n\nLou, A. and Ermon, S. Reflected diffusion models, 2023.\n\nLou, A., Meng, C., and Ermon, S. Discrete diffusion lan-\nguage modeling by estimating the ratios of the data distri-\nbution. arXiv preprint arXiv:2310.16834, 2023.\n\nMahabadi, R. K., Ivison, H., Tae, J., Henderson, J., Beltagy,\nI., Peters, M. E., and Cohan, A. Tess: Text-to-text self-\nconditioned simplex diffusion, 2024.\n\n11\n\n\n\nLarge Language Diffusion Models\n\nMeng, C., Choi, K., Song, J., and Ermon, S. Concrete score\nmatching: Generalized score matching for discrete data.\nAdvances in Neural Information Processing Systems, 35:\n34532\u201334545, 2022.\n\nNie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M.,\nand Li, C. Scaling up masked diffusion models on text.\narXiv preprint arXiv:2410.18514, 2024.\n\nOpenAI. ChatGPT: Optimizing Language Models for Di-\nalogue. OpenAI blog, November 2022. URL https:\n//openai.com/blog/chatgpt/.\n\nOpenAI. Learning to reason with llms, 2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n\nOu, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li,\nC. Your absorbing discrete diffusion secretly models the\nconditional distributions of clean data. arXiv preprint\narXiv:2406.03736, 2024.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems, 35:27730\u201327744, 2022.\n\nPark, J. S., O\u2019Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and\ntechnology, pp. 1\u201322, 2023.\n\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pp. 4195\u20134205,\n2023.\n\nRadford, A. Improving language understanding by genera-\ntive pre-training, 2018.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\n\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\n\nReid, M., Hellendoorn, V. J., and Neubig, G. Diffuser:\nDiscrete diffusion via edit-based reconstruction, 2022.\n\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang,\nR. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa:\nA graduate-level google-proof q&a benchmark. arXiv\npreprint arXiv:2311.12022, 2023.\n\nRichemond, P. H., Dieleman, S., and Doucet, A. Categorical\nsdes with simplex diffusion, 2022.\n\nSahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marro-\nquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. Simple\nand effective masked diffusion language models. arXiv\npreprint arXiv:2406.07524, 2024.\n\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\n\nShannon, C. E. A mathematical theory of communication.\nThe Bell system technical journal, 27(3):379\u2013423, 1948.\n\nShazeer, N. Glu variants improve transformer. arXiv\npreprint arXiv:2002.05202, 2020.\n\nShi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. K.\nSimplified and generalized masked diffusion for discrete\ndata. arXiv preprint arXiv:2406.04329, 2024.\n\nShih, A., Sadigh, D., and Ermon, S. Training and infer-\nence on any-order autoregressive models the right way.\nIn Proceedings of the 31th International Conference on\nMachine Learning, 2022.\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256\u20132265. PMLR, 2015.\n\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\n\nStrudel, R., Tallec, C., Altche\u0301, F., Du, Y., Ganin, Y., Men-\nsch, A., Grathwohl, W., Savinov, N., Dieleman, S., Sifre,\nL., et al. Self-conditioned embedding diffusion for text\ngeneration. arXiv preprint arXiv:2211.04236, 2022.\n\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, 568:127063, 2024.\n\nSun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Score-\nbased continuous-time discrete diffusion models. arXiv\npreprint arXiv:2211.16750, 2022.\n\nSuzgun, M., Scales, N., Scha\u0308rli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\n\n12\n\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/index/learning-to-reason-with-llms/\nhttps://openai.com/index/learning-to-reason-with-llms/\n\n\nLarge Language Diffusion Models\n\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n\nUria, B., Murray, I., and Larochelle, H. A deep and tractable\ndensity estimator. In Proceedings of the 31th Interna-\ntional Conference on Machine Learning, 2014.\n\nVaswani, A. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J.,\nChen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on\nlarge language model based autonomous agents. Frontiers\nof Computer Science, 18(6):186345, 2024a.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDiffusion language models are versatile protein learners.\narXiv preprint arXiv:2402.18567, 2024b.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDplm-2: A multimodal diffusion protein language model.\narXiv preprint arXiv:2410.13782, 2024c.\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\nelicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824\u201324837,\n2022.\n\nWu, T., Fan, Z., Liu, X., Gong, Y., Shen, Y., Jiao, J., Zheng,\nH.-T., Li, J., Wei, Z., Guo, J., Duan, N., and Chen, W.\nAr-diffusion: Auto-regressive diffusion model for text\ngeneration, 2023.\n\nXu, C., Wang, X., Liao, Z., Li, Y., Hou, T., and Deng,\nZ. Show-o turbo: Towards accelerated unified multi-\nmodal understanding and generation. arXiv preprint\narXiv:2502.05415, 2025.\n\nXue, K., Zhou, Y., Nie, S., Min, X., Zhang, X., Zhou, J.,\nand Li, C. Unifying bayesian flow networks and diffusion\nmodels through stochastic differential equations. arXiv\npreprint arXiv:2404.15766, 2024.\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B.,\nLi, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu,\nJ., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang,\nK., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M.,\nZhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren,\nX., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y.,\nCui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\narXiv preprint arXiv:2412.15115, 2024.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Gu, Q. Diffusion\nlanguage models can perform many tasks with scaling and\n\ninstruction-finetuning. arXiv preprint arXiv:2308.12219,\n2023a.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Wang, M. Dinoiser:\nDiffused conditional sequence learning by manipulating\nnoises. arXiv preprint arXiv:2302.10025, 2023b.\n\nYe, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics of Lan-\nguage Models: Part 2.1, Grade-School Math and the Hid-\nden Reasoning Process. ArXiv e-prints, abs/2407.20311,\nJuly 2024. Full version available at http://arxiv.\norg/abs/2407.20311.\n\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\n\nZhang, B. and Sennrich, R. Root mean square layer nor-\nmalization. Advances in Neural Information Processing\nSystems, 32, 2019.\n\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,\nMin, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of\nlarge language models. arXiv preprint arXiv:2303.18223,\n2023.\n\nZheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and\nZhang, Q. Masked diffusion models are secretly time-\nagnostic masked models and exploit inaccurate categor-\nical sampling, 2024. URL https://arxiv.org/\nabs/2409.02908.\n\nZheng, L., Yuan, J., Yu, L., and Kong, L. A reparameter-\nized discrete diffusion model for text generation. ArXiv,\nabs/2302.05737, 2023.\n\n13\n\nhttp://arxiv.org/abs/2407.20311\nhttp://arxiv.org/abs/2407.20311\nhttps://arxiv.org/abs/2409.02908\nhttps://arxiv.org/abs/2409.02908\n\n\nLarge Language Diffusion Models\n\nAlgorithm 1 Pre-training of LLaDA\n\nRequire: mask predictor p\u03b8, data distribution pdata\n1: repeat\n2: x0 \u223c pdata, t \u223c U(0, 1] # with a probability of 1%, the sequence length of x0 follows U[1, 4096]\n3: xt \u223c qt|0(xt|x0) # qt|0 is defined in Eq. (7)\n4: Calculate L = \u2212 1\n\nt\u2217L\n\u2211L\n\ni=1 1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt) # L is the sequence length of x0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 2 Supervised Fine-Tuning of LLaDA\n\nRequire: mask predictor p\u03b8, pair data distribution pdata\n1: repeat\n2: p0, r0 \u223c pdata, t \u223c U(0, 1] # please refer to Appendix B.1 for details on the SFT data processing.\n3: rt \u223c qt|0(rt|r0) # qt|0 is defined in Eq. (7)\n\n4: Calculate L = \u2212 1\nt\u2217L\u2032\n\n\u2211L\u2032\n\ni=1 1[rit = M] log p\u03b8(r\ni\n0|p0, rt) # L\u2032 is the sequence length of r0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 3 Conditional Log-likelihood Evaluation of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, response r0, the number of Monte Carlo estimations nmc\n\n1: log likelihood = 0\n2: for i\u2190 1 to nmc do\n3: l \u223c {1, 2, . . . , L} # L is the sequence length of r0\n4: Obtain rl by uniformly sampling l tokens from r0 without replacement for masking\n5: log likelihood = log likelihood + L\n\nl\n\n\u2211L\ni=1 1[ril = M] log p\u03b8(r\n\ni\n0|p0, rl)\n\n6: end for\n7: log likelihood = log likelihood/nmc\n\n8: Return log likelihood\n\nA. Formulation of Masked Diffusion Models\n\nA.1. Training\n\nMasked diffusion models (MDMs) (Austin et al., 2021a; Lou et al., 2023; Ou et al., 2024) define the model distribution\np\u03b8(x0) in a manner distinct from autoregressive models.\n\nThese models introduce a forward process {xt} indexed by a time t \u2208 [0, 1]. This process gradually and independently\nmasks all tokens in the sequence x0. At time t = 0, the data point x0 is fully observed with no masks, while for t \u2208 (0, 1],\nxt represents latent variables with varying mask ratios in expectation.\n\nFormally, the conditional distribution of xt given x0 is defined by a fully factorized form:\n\nqt|0(xt|x0) =\n\nL\u220f\ni=1\n\nqt|0(x\ni\nt|xi\n\n0), (7)\n\nwhere the conditional distribution for each token is given by:\n\nqt|0(x\ni\nt|xi\n\n0) =\n\n{\n1\u2212 t, xi\n\nt = xi\n0,\n\nt, xi\nt = M.\n\n(8)\n\nHere, M denotes the mask token. Intuitively, each token either remains unchanged or is masked, with the probability of\n\n14\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 4 Reverse Process of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: r0 = argmaxr0 p\u03b8(r0|p0, rt) # we employ greedy sampling when predicting masked tokens\n5: for i\u2190 1 to L do\n6: if rt \u0338= M then\n7: ri0 = rit\n8: else\n9: With probability s\n\nt , ri0 is set to M\n10: end if\n11: end for\n12: rs = r0\n13: end for\n14: Return r0\n\nbeing masked increasing linearly as t progresses from 0 to 1. At t = 1, all tokens are guaranteed to be masked, meaning that\nx1 follows a Dirac distribution concentrated on a sequence of fully masked tokens. Notably, the linear masking probability\nis analogous to but distinct from, the noise schedule in continuous diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020). This linearity is motivated by the assumption that the information in the text is proportional to the\nnumber of tokens on average, making it reasonable to lose information linearly during the forward process.\n\nThe forward process is not only reversible but also corresponds to a reverse process that is fully factorized across all\ntokens (Austin et al., 2021a). The reverse process, from time t = 1 to 0, generates new data from sequences of fully masked\ntokens. The conditional distribution for the reverse process, for 0 \u2264 s < t \u2264 1, is factorized as:\n\nqs|t(xs|xt) =\n\nL\u220f\ni=1\n\nqs|t(x\ni\ns|xt), (9)\n\nwhere the conditional distribution for each token is:\n\nqs|t(x\ni\ns|xt) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n1, xi\n\nt \u0338= M, xi\ns = xi\n\nt,\ns\nt , xi\n\nt = M, xi\ns = M,\n\nt\u2212s\nt q0|t(x\n\ni\ns|xt), xi\n\nt = M, xi\ns \u0338= M,\n\n0, otherwise.\n\n(10)\n\nThus, the key function to estimate is the conditional distribution q0|t(x\ni\ns|xt), which predicts the original token if it is masked\n\nin the input xt. This is analogous to the data prediction form in continuous diffusion models.\n\nAs proven in (Ou et al., 2024), an equivalent yet time-free parameterization can be derived as:\n\nq0|t(x\ni\ns|xt) = pdata(x\n\ni\n0|xUM\n\nt ), \u2200i such that xi\nt = M, (11)\n\nwhere xUM\nt denotes the collection of unmasked tokens in xt, which is identical to the corresponding tokens in the original\n\ndata x0 since unmasked tokens are solely determined by x0 and are independent of time t. Intuitively, this implies that\nestimating the data prediction function is equivalent to estimating the conditional distributions on clean data, which is\ntime-invariant. Consequently, the time t need not be provided as input to the parametric model.\n\nAlthough the development of masked diffusion is nontrivial, the implementation is straightforward. We first introduce the\nmask predictor, a parametric model p\u03b8(\u00b7|xt) (e.g., a Transformer without a causal mask), which takes xt for any t as input\nand predict all masked tokens simultaneously. Then, we define the model distribution p\u03b8(x0) as follows: starting with x1 as\na sequence of fully masked tokens, we simulate an approximate reverse process parameterized by p\u03b8(\u00b7|xt) from t = 1 to 0.\nThe marginal distribution induced at t = 0 then represents the model distribution p\u03b8(x0).\n\n15\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 5 Low-confidence Remasking Strategy of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: for i\u2190 1 to L do\n5: if rit \u0338= M then\n6: ri0 = rit, c\n\ni = 1\n7: else\n8: ri0 = argmaxri0 p\u03b8(r\n\ni\n0|p0, rt)\n\n9: ci = p\u03b8(r\ni\n0|p0, rt)ri0\n\n10: end if\n11: end for\n12: nun = \u230aL(1\u2212 s)\u230b # the number of unmasked tokens is nun in timestep s\n13: for i\u2190 1 to L do\n14: if ci \u2208 Lowest\u2212 nun\n\n(\n{ci}L1\n\n)\nthen\n\n15: ri0 = M # the nun positions with the least confidence are selected for remasking.\n16: end if\n17: end for\n18: rs = r0\n19: end for\n20: Return r0\n\nFormally, the mask predictor is trained using a cross-entropy loss with masking:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (12)\n\nwhere x0 is sampled from the training data, t is sampled uniformly from [0, 1], and xt is sampled from qt|0(xt|x0). The\nindicator function 1[\u00b7] ensures that the cross-entropy loss is computed only for masked tokens. In Ou et al. (2024), it has\nbeen proven that the loss function L(\u03b8) is an upper bound on the negative log-likelihood of the model distribution:\n\n\u2212Ex0\u223cpdata(x0)\n[log p\u03b8(x0)] \u2264 L(\u03b8). (13)\n\nIn summary, this principled approach trains a generative model by progressively masking tokens during a forward process\nand learning to recover the data distribution during a reverse process, all under the (approximate) maximum likelihood\nestimation framework.\n\nA.2. Inference\n\nThe cross-entropy loss in Eq. (12) has several equivalent forms (Ou et al., 2024). The first one is given by\n\n\u2212El\u223c{1,2,...,L},x0,xl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[xi\nl = M] log p\u03b8(x\n\ni\n0|xl)\n\n]\n, (14)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and xl is obtained by uniformly sampling l tokens from x0 without\nreplacement for masking. Despite masking exactly l tokens is different from masking each token independently with\nprobability t, these two masking methods lead to equivalent results in expectation (Ou et al., 2024).\n\nWhile Eq. (12) and Eq. (14) share the same expectation, their variances differ. Intuitively, in Eq. (12), we expect xt to\nhave a fraction of t tokens masked. However, the randomness of the forward process (i.e., Eq. (7)) often causes deviations,\nespecially when xt contains few tokens. In contrast, in Eq. (14), the fraction of masked tokens in xl is deterministically l\n\nL .\nWhile a theoretical analysis depends on the data distribution, empirical results show that Eq. (12) requires over 1000 Monte\nCarlo estimates for stable results, whereas Eq. (14) achieves stability with only 128 estimates. In addition, we can simply\nmodify Eq. (14) to its conditional version (i.e., Eq. (6)) based on Eq. (5).\n\n16\n\n\n\nLarge Language Diffusion Models\n\nPrompt Block 0\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nBlock 1 Block 2\n\nNon-mask token\n\nMask token\n\nFigure 4. A Conceptual Overview of the Semi-autoregressive Sampling.\n\nAny-order autoregressive models (AO-ARM) (Uria et al., 2014; Hoogeboom et al., 2021a; Shih et al., 2022) characterize the\njoint distribution autoregressively for all possible orders \u03c0 of the L variables. To learn such a distribution, an AO-ARM\nutilizes a weight-sharing neural network to model all univariate conditionals and employs mask tokens to represent absent\nvariables. During training, the expected negative log-likelihood over the uniform distribution of all orders U\u03c0 is minimized:\n\n\u2212Ex0,\u03c0\u223cU\u03c0\n\n[\nL\u2211\n\ni=1\n\nlog p\u03b8(x\n\u03c0(i)\n0 |x\u03c0(<i)\n\n0 ;\u03c0)\n\n]\n. (15)\n\nIntuitively, x\u03c0(<i)\n0 can be understood as a masked token xt with index in \u03c0(\u2265 i) being masked. It can be further proved that\n\nEq. (15) is equivalent to Eq. (12). This connection explains the bidirectional reasoning capabilities of LLaDA, even though\nit was never used explicitly in the inference procedure.\n\nNie et al. (2024) introduce unsupervised classifier-free guidance, a plug-and-play technique that balances alignment with\nprompts and text diversity. Specifically, unsupervised classifier-free guidance employs the following modified mask predictor\nfor inference:\n\np\u0303\u03b8(r0|p0, rt) \u221d\np\u03b8(r0|p0, rt)1+w\n\np\u03b8(r0|m, rt)w\n, (16)\n\nwhere m is a mask sequence of the same length as p0 and w is a hyperparameter that controls the strength of p0. We employ\nunsupervised classifier-free guidance in the downstream tasks, as detailed in Appendix B.5.\n\nA.3. Algorithms\n\nIn this section, we present the training and inference algorithms. Specifically, we introduce the pre-training and supervised\nfine-tuning algorithms in Algorithm 1 and Algorithm 2, respectively. In addition, the likelihood evaluation algorithm is\nprovided in Algorithm 3. Finally, we present the sampling algorithms in Algorithm 4 and Algorithm 5, which correspond to\nthe original reverse process (i.e., random remasking) and the low-confidence remasking strategy (Chang et al., 2022) of\nLLaDA, respectively.\n\nFurthermore, we apply a semi-autoregressive remasking strategy for the instruct model. Specifically, the sequence is\ndivided into several blocks and generated from left to right. Within each block, we apply the random remasking or the\nlow-confidence remasking strategy for sampling. For clarity and better visualization, we illustrate the semi-autoregressive\nsampling process in Fig. 4.\n\nOur semi-autoregressive remasking strategy is inspired by the prior work of Anonymous (2025). However, it is important to\nnote that Anonymous (2025) and LLaDA have entirely different training objectives and network architectures. Specifically,\nthe pre-training objective of Anonymous (2025) differs from Eq. (3). Moreover, their approach incorporates a causal mask\nin the Transformer attention block, whereas LLaDA employs a bidirectional Transformer.\n\n17\n\n\n\nLarge Language Diffusion Models\n\nTable 5. Model Architecture. We provide the configurations of our 1B and 7B ARM baselines, as well as the 1B and 8B LLaDA and the\n8B LLaMA3 models.\n\nOur ARM Baseline 1B LLaDA 1B Our ARM Baseline 7B LLaDA 8B LLaMA3 8B\n\nLayers 22 22 28 32 32\n\nModel dimension 2048 2048 4096 4096 4096\n\nAttention heads 32 32 32 32 32\n\nVocabulary size 126,464 126,464 126,464 126,464 128,000\n\nFFN dimension 5634 5634 13,440 12,288 14,336\n\nKey/Value heads 4 4 8 32 8\n\nTotal parameters 1.49 B 1.49 B 6.83 B 8.02 B 8.03 B\n\nNon-embedding parameters 0.97 B 0.97 B 5.80 B 6.98 B 6.98 B\n\nB. Experiments\nB.1. Details of SFT Data\n\nA dynamic sequence length is employed during supervised SFT, where |EOS| tokens are appended to the end of short pairs\nto ensure equal sequence lengths across all data in each mini-batch. Notably, the padding |EOS| tokens are treated as part of\nthe response, i.e., masked and included in the training objective. The |EOS| tokens are removed from the generated outputs\nduring sampling. This strategy is crucial for LLaDA and ensures that the model learns to control the length of its responses\nby generating |EOS|, enabling the response length to align effectively with the given prompt.\n\nIn addition, for n-turn dialogues (p00, r\n0\n0, p\n\n1\n0, r\n\n1\n0, . . . , p\n\nn\u22121\n0 , rn\u22121\n\n0 ), we treat it as n single-turn dialogue pairs, i.e.,\n(p00, r\n\n0\n0), (p\n\n0\n0r\n\n0\n0p\n\n1\n0, r\n\n1\n0), . . . , (p\n\n0\n0r\n\n0\n0p\n\n1\n0r\n\n1\n0 . . . p\n\nn\u22121\n0 , rn\u22121\n\n0 ) and randomly sample one. This data partitioning strategy not only\nequips LLaDA with multi-turn dialogue capabilities but also aligns with the padding strategy.\n\nB.2. Architectures and Scaling Experiments\n\nThis section provides detailed information about the models we trained and the scalability experiments described in Sec. 3.1.\n\nFirstly, for efficiency, we trained an ARM and a MDM, both with 1.5B parameters and identical architectures. Additionally,\nwe scaled the MDM to 8B parameters. Due to computational resource constraints, we did not train an 8B autoregressive\nmodel with the same architecture. Instead, we utilized our previously trained 7B autoregressive model for comparison.\nThese four models are utilized in the scalability analysis in Sec. 3.1.\n\nWe adopted a Transformer architecture similar to LLaMA (Touvron et al., 2023; Dubey et al., 2024) for the ARMs and MDMs\nwe trained. Specifically, we employ RMSNorm (Zhang & Sennrich, 2019) to stabilize training, used SwiGLU (Shazeer,\n2020) as the activation function to enhance non-linearity, and integrate RoPE (Su et al., 2024) for more expressive positional\nencoding. Tab. 5 provides an overview of the model architectures.\n\nAdditionally, we employ the widely used 6ND formulation (Kaplan et al., 2020; Hoffmann et al., 2022) to calculate the\ntraining FLOPs in Fig. 3, where N represents the number of non-embedding parameters, and D denotes the total number of\ntraining tokens.\n\nB.3. Details and Ablation on Remasking\n\nThis section first explains the inference methods in detail, followed by ablation studies on various remasking strategies.\n\nFor instruct model, since our training includes a large number of |EOS| tokens, directly applying the lowest confidence\nremasking strategy would result in an excessively high and unnatural occurrence of |EOS| tokens in the generated sentences.\nTherefore, we divide the entire generation length into several blocks. Each block is generated from left to right, and within\neach block, we apply the reverse process to perform sampling. However, for the base model, since |EOS| tokens are not\npadded during training, using the semi-autoregressive remasking strategy makes little difference in the results.\n\n18\n\n\n\nLarge Language Diffusion Models\n\nTable 6. Ablation on Remasking. We use GSM8K accuracy as the evaluation metric. The lowest-confidence & semi-autoregressive\nremasking strategy refers to generating each block from left to right, while applying the lowest-confidence remasking strategy within each\nblock.\n\nLLaDA 8B Base LLaDA 8B Instruct\n\nRandomly remasking 52.3 72.0\nLowest confidence remasking 64.7 12.9\nLowest confidence & semi-autoregressive remasking 64.4 73.8\n\nTable 7. Ablation on Generated Length. We use the GSM8K accuracy as the evaluation metric and set the number of sampling steps to\nhalf of the generated length.\n\nLength LLaDA 8B Base LLaDA 8B Instruct\n\n256 62.5 75.3\n512 64.7 73.8\n1024 65.9 75.3\n\nFor efficiency, we set the answer length to 512 and the number of sampling steps to 256 in our ablation studies. For the\nsemi-autoregressive remasking strategy, we set the block length to 32. To simplify the evaluation, we use GSM8K accuracy\nas the performance metric. As shown in Table 6, for the base model, applying the lowest confidence remasking strategy\nsignificantly improves performance, whereas the semi-autoregressive remasking strategy does not yield any noticeable\nbenefits. In contrast, for the instruct model, directly applying the lowest confidence remasking strategy leads to a substantial\nperformance drop. However, combining the lowest confidence remasking strategy with the semi-autoregressive remasking\nstrategy results in the best performance for the instruct model.\n\nWe provide the specific remasking strategies and hyperparameters used for each benchmark in Appendix B.5.\n\nB.4. Ablation on Generated Length\n\nIn this section, we conduct ablation studies on the generated length. For simplicity, we use GSM8K as the evaluation metric.\nTo ensure efficiency and fairness, for each setting, we set the number of sampling steps to half of the generated length,\nensuring that in each sampling step, two tokens are transferred from the mask to the text.\n\nFor the LLaDA 8B Base model, we employ the lowest-confidence remasking strategy (i.e., Algorithm 5). For the LLaDA\n8B Instruct model, we adopt semi-autoregressive sampling with a block length of 32.\n\nAs reported in Tab. 7, both the Base model and the Instruct model exhibit minimal sensitivity to the length hyperparameter.\n\nB.5. Standard Benchmarks and Evaluation\n\nIn this section, we briefly introduce the benchmarks used in our study and describe the details of our evaluation process.\n\nFollowing standard LLM (Chu et al., 2024; Yang et al., 2024) evaluation practices, we assess LLaDA across four dimensions:\n\nGeneral ability: MMLU (Hendrycks et al., 2020), BBH (Suzgun et al., 2022), ARC-C (Clark et al., 2018), Hel-\nlaswag (Zellers et al., 2019), TruthfulQA (Lin et al., 2021), WinoGrande (Sakaguchi et al., 2021) and PIQA (Bisk\net al., 2020).\n\nMath and science ability: GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021) and GPQA (Rein et al., 2023).\n\nCode generation: HumanEval (Chen et al., 2021), HumanEval-FIM (Bavarian et al., 2022) and MBPP (Austin et al.,\n2021b).\n\nChinese understanding: CMMLU (Li et al., 2023) and C-Eval (Huang et al., 2024a).\n\nFor all the aforementioned benchmarks, we follow the widely adopted evaluation process (Gao et al., 2024) used in LLM\nassessments, primarily employing conditional likelihood estimation and conditional generation. Specifically, for certain\nbenchmarks, a prompt and multiple candidate answers are provided, and the model is required to compute each candidate\u2019s\n\n19\n\n\n\nLarge Language Diffusion Models\n\nTable 8. Inference Configuration for LLaDA 8B Instruct. We apply the low-confidence remasking strategy across all datasets. A block\nlength smaller than the answer length indicates the use of the semi-autoregressive remasking strategy; otherwise, it is not applied. The\nshorter answer lengths for MMLU and HellaSwag result from their requirement to generate only a single token (i.e., A, B, C, or D) and\ntwo special end tokens (i.e., end of the model response and end of the sentence).\n\nAnswer length Block length\n\nMMLU 3 3\nHellaswag 3 3\nGPQA 128 64\nGSM8K 256 8\nHumanEval 512 32\nMBPP 512 32\nMath 256 256\nMMLU-pro 256 256\nARC-C 512 512\n\nTable 9. Comparison on iGSM Dataset.\n\n4 steps 5 steps 6 steps\n\nLLaMA3 8B Base 38.0 35.0 34.0\nLLaDA 8B Base 64.0 41.0 44.0\n\nconditional likelihood. The candidate with the highest likelihood is then selected as the model\u2019s final answer, and accuracy\nis used as the evaluation metric. For the remaining benchmarks, the model generates responses based on the given prompt,\nand performance is evaluated using metrics such as exact match and other relevant criteria.\n\nFor the base model, we use conditional likelihood estimation for MMLU, CMMLU, C-Eval, ARC-C, Hellaswag, TruthfulQA,\nWinoGrande, PIQA, and GPQA, while the remaining benchmarks are evaluated using conditional generation. For the\ninstruct model, we evaluate all benchmarks using conditional generation. For datasets evaluated with conditional likelihood\nestimation, we adopt the widely used lm-evaluation-harness (Gao et al., 2024) framework. However, for datasets evaluated\nwith conditional generation, we rely on an internal library for evaluation, as lm-evaluation-harness lacks implementations\nfor certain key metrics (e.g., HumanEval-FIM).\n\nFor benchmarks evaluated using conditional likelihood estimation, we use Monte Carlo estimation to approximate Eq. (6)\nfor conditional likelihood estimation. Since MMLU, CMMLU, and C-EVAL only require the likelihood of a single token, a\nsingle Monte Carlo estimate is sufficient for these benchmarks. For all other benchmarks, we find that 128 Monte Carlo\nsamples are adequate to produce stable results. For ARC-C, Hellaswag, TruthfulQA, WinoGrande, PIQA, and GPQA, we\nreport the results with the optimal unsupervised classifier-free guidance scale searched in {0, 0.5, 1, 1.5, 2}.\n\nFor benchmarks evaluated using conditional generation, we apply the lowest confidence remasking strategy to the base model,\nsetting both the response length and the number of sampling steps to 1024. For the instruct model, we set the sampling steps\nequal to the answer length and provide the inference hyperparameters in Table 8. Due to computational resource constraints,\nwe do not use the unsupervised classifier-free guidance for these benchmarks evaluated using conditional generation.\n\nB.6. Analysis of Sampling Steps\n\nIn this section, we analyze the impact of sampling steps. For simplicity, we use the LLaDA 8B Base model and evaluate it\non the GSM8K and HumanEval datasets. Additionally, we fix the answer length at 1024. As shown in Fig. 5, as the number\nof sampling steps increases to match the answer length, the model\u2019s accuracy improves accordingly.\n\nB.7. Evaluation on iGSM Dataset\n\nTo further assess the mathematical capabilities of LLaDA, we test its performance on iGSM (Ye et al., 2024), an infinite,\nsynthetic GSM8K-like dataset. iGSM is generated via specific rules, with parameters that control the difficulty of problems\n(i.e., the number of solution steps). For evaluation consistency, we append \u201d#### $answer\u201d to the final solution, adhering to\n\n20\n\n\n\nLarge Language Diffusion Models\n\n64 128 256 512 1,024\nNFEs\n\n20\n\n40\n\n60\nG\n\nSM\n8K\n\n (4\n-s\n\nho\nt)\n\n64 128 256 512 1,024\nNFEs\n\n10\n\n20\n\n30\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt)\n\nFigure 5. Analysis of Sampling Steps.\n\nthe GSM8K format. Below is an example with solution steps set to 4:\n\n(Question) The number of each North Star Elementary\u2019s Cultural Studies Classroom equals 1. The number of each\nWestridge Elementary\u2019s Dance Studio equals 3 times as much as the sum of each North Star Elementary\u2019s Classroom\nand each North Star Elementary\u2019s Cultural Studies Classroom. How many Dance Studio does Westridge Elementary\nhave?\n(Solution) Define North Star Elementary\u2019s Cultural Studies Classroom as x; so x = 1.\nDefine North Star Elementary\u2019s Classroom as m; so m = x = 1.\nDefine Westridge Elementary\u2019s Dance Studio as n; w = m + x = 1 + 1 = 2;\nso n = 3 * w = 3 * 2 = 1 #### 1\n\nSince there are slight differences between GSM8K and iGSM (e.g., the use of a mod 5 algorithmic system), we follow (Ye\net al., 2024) and provide a system prompt along with four-shot question-answer pairs for each problem.\n\n(Prompt) You\u2019re an expert at solving elementary math problems involving addition, subtraction, and multiplication.\nYou solve all the problems in a uniform format. All calculations are done modulo 5. For example, 4 + 4 equals 3, 2 + 4\nequals 1, 3 + 3 + 3 equals 4, 3 * 3 equals 4, and 2 * 2 equals 4. When providing your solution, please end with \u2019#### x.\u2019\nwhere x is your final answer, an integer between 0 and 4. You must solve all the problems using the same solution\nformat. Our scenarios involve up to four categories of objects: schools, classrooms, backpacks and stationeries. Each\nschool may contain classrooms, each classroom may contain backpacks, and each backpack may contain stationeries.\nWe can specify quantities, such as t\u0308he number of dance studios at each Lakeshore High.A\u0308ssume that every entity\nwith the same name has an identical configuration; for example, each Lakeshore High contains the same number of\ndance studios. Another guiding principle is that what is not mentioned does not exist: when we refer to classrooms at\nLakeshore High, we are only discussing the classrooms explicitly mentioned in our scenario. Furthermore, if Lakeshore\nHigh is not even mentioned, any classroom within it is automatically considered to be non-existent (i.e. 0).\n\nFor solution steps ranging from 4 to 6, we generate 100 questions for each case and report the corresponding accuracy in\nTable 9. As shown in the table, LLaDA Base demonstrates significant and consistent advantages over LLaMA3 on unseen\nmathematical problems, aligning with the results in Table 1.\n\nB.8. Poem Completion Tasks\n\nIn this section, we present examples from our poem completion dataset as follows.\n\nExample 1:\nPrompt: \u7a88\u7a95\u6dd1\u5973\u7684\u4e0b\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u541b\u5b50\u597d\u9011\u3002\n\n21\n\n\n\nLarge Language Diffusion Models\n\nTable 10. Visualization of the Semi-Autoregressive Sampling Process. In the response of LLaDA, darker colors indicate tokens\npredicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser What are the benefits of regular exercise for physical and mental health?\nLLaDA\n\nTable 11. Visualization of the Multi-turn Dialogue. We employ random remasking strategy. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser I want to have a pet, but I don\u2019t know whether to get a cat or a dog. Do you have any\nsuggestions?\n\nLLaDA\n\nUser I\u2019m very busy at work, so it seems like a cat would be a more suitable option. Do you have\nany suggestions for getting a cat?\n\nLLaDA\n\nExample 2:\nPrompt: \u4e0d\u62d8\u4e00\u683c\u964d\u4eba\u624d\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u6211\u529d\u5929\u516c\u91cd\u6296\u64de\u3002\n\nB.9. More Case Studies\n\nIn this section, we present additional case studies of LLaDA 8B Instruct. First, Tab. 10 illustrates the sampling process of\nsemi-autoregressive remasking, while Tab. 11 depicts the sampling process for multi-turn dialogues with random remasking.\nAdditionally, Tab. 12 and Tab. 13 provide further examples of single-turn and multi-turn dialogues. Finally, Tab. 14 presents\nexamples of poem reversal completions where the LLaDA 8B Instruct model succeeds, in contrast to the failure of GPT-4o.\n\n22\n\n\n\nLarge Language Diffusion Models\n\nTable 12. Single-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser What\u2019s a piece of advice you\u2019ve received that you\u2019ve found helpful?\nLLaDA One helpful piece of advice I\u2019ve received is to always be open to learning and growth.\n\nI\u2019ve found that being willing to explore new ideas and experiences has helped me become\nmore resilient and adaptable. By embracing a growth mindset, I\u2019ve been able to identify\nopportunities for improvement and development, both personally and professionally.\n\nDaily life\nUser Please recommend me three famous movies\nLLaDA Certainly! Here are three famous movies that have achieved widespread popularity and\n\ncritical acclaim:\n1. **The Shawshank Redemption** (1994) - Inspired by a novella and directed by Frank\nDarabont, this film tells the story of Andy Dufresne, a man convicted of murdering his\nwife, dealing with his harsh life in Shawshank Prison and forming an unlikely friendship\nwith a fellow inmate.\n2. **The Godfather** (1972) - Directed by Francis Ford Coppola, this film is based\non Mario Puzo\u2019s novel of the same name. It follows the story of the Corleone family,\nparticularly Michael Corleone, who rises to power and takes over the family\u2019s criminal\nempire.\n3. **The Empire Strikes Back** (1980) - Directed by George Lucas, this film continues\nthe story of a young Skywalker in the Star Wars saga. It follows Luke Skywalker as\nhe helps the Rebel Alliance fight against the Galactic Empire and destroy the powerful\nweapon, the Death Star.\nAll of these films are celebrated for their cinematic achievements, powerful storytelling,\nand memorable characters.\n\nTranslation\nUser Please help me translate into Chinese: \u2018What is now proved was once only imagined\u2019\nLLaDA \u73b0\u5728\u88ab\u8bc1\u660e\u7684\u66fe\u7ecf\u53ea\u662f\u60f3\u8c61\n\nTranslation\nUser Please help me translate into German: \u2018What is now proved was once only imagined\u2019\nLLaDA Wer heute besta\u0308tigt ist, war einst nur immaginiert.\n\nMath\nUser Emma decides to run 4 sprints 5 times a week. She runs 75 meters each sprint. How many\n\ntotal meters does she run a week?\nLLaDA Emma runs 4 sprints each time, so she runs a total of 4 x 5 = 20 sprints a week. Each sprint\n\nis 75 meters long, so Emma runs a total of 20 x 75 = 1500 meters a week. The final result\nis 1500\n\nCode\nUser I am writing python code, I have an array, please help me sort it from largest to smallest\nLLaDA Certainly! You can sort an array from the largest to smallest in Python using the \u2018sort()\u2019\n\nmethod or the \u2018sorted()\u2018 function. Here\u2019s an example:\n### Using \u2018sort()\u2019 method:\nmy array = [3, 1, 4, 1, 5, 2]\nmy array.sort(reverse=True)\nprint(my array)\n\n23\n\n\n\nLarge Language Diffusion Models\n\nTable 13. Multi-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser How do I prepare for a job interview?\nLLaDA To prepare for a job interview, you should research the company and the job, practice\n\ncommon interview questions, and dress appropriately.\nUser What are some common interview questions?\nLLaDA Some common interview questions include \u201cTell me about yourself\u201d and \u201cWhy do you\n\nwant to work for us\u201d?\n\nDaily life\nUser How do I start learning programming?\nLLaDA To learn programming, you can start by choosing a programming language that you are\n\ninterested in. Some popular languages include Python, Java, and C++.\nUser Which programming language should I learn first?\nLLaDA It depends on your interests and goals. Python is a great language for beginners for its\n\nsimplicity and versatility.\nUser How long does it take to learn Python?\nLLaDA It can take anywhere from a few weeks to several months to learn Python.\nUser What are some good Python projects for beginners?\nLLaDA Some good Python projects for beginners include building a simple calculator, a game, and\n\na web application.\n\nMath\nUser Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.\n\nHow many clips did Natalia sell altogether in April and May?\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n\n48/2 = 24 clips in May. Altogether, Natalia sold 48 + 24 = 72 clips in April and May. The\nfinal result is 72.\n\nUser Natalia sold three times as many items in June as she did in May. How many clips did\nNatalia sell altogether in these three months?\n\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May,\nso she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in\nthese three months. The final result is 144.\n\nTable 14. Poem Reversal Completion Cases where LLaDA 8B Instruct Succeeds but GPT-4o Fails.\n\nQuestion1\nUser \u2018\u4f46\u95fb\u4eba\u8bed\u54cd\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\u201d,\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bcd\u4eba\u674e\u6e05\u7167\u7684\uff08\u6b66\u9675\n\n\u6625\uff0c\u6625\u665a:\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\uff0c\u4f46\u95fb\u4eba\u8bed\u54cd\u3002\u8fd9\u9996\u8bcd\u8868\u8fbe\u4e86\u4f5c\u8005\u5728\u6625\u5929\u65f6\u8282\u7684\u5b64\u72ec\u4e0e\n\u611f\u4f24\uff0c\u501f\u82b1\u843d\u548c\u4eba\u8bed\u58f0\u7684\u63cf\u5199\uff0c\u53cd\u6620\u51fa\u8bcd\u4eba\u5185\u5fc3\u7684\u65e0\u4eb2\u4e0e\u5931\u843d\u3002\n\nLLaDA \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u4e0d\u89c1\u4eba\u201d\n\nQuestion2\nUser \u2018\u591c\u9759\u6625\u5c71\u7a7a\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u65b0\u96e8\u540e\u201d.\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bd7\u4eba\u738b\u7ef4\u7684\uff08\u5c71\u5c45\u79cb\n\n\u819c\uff09:\u7a7a\u5c71\u65b0\u96e8\u540e\uff0c\u591c\u9759\u6625\u5c71\u7a7a\u3002\u8fd9\u9996\u8bd7\u63cf\u7ed8\u4e86\u6625\u5929\u5c71\u4e2d\u96e8\u540e\u7684\u5bc2\u9759\u4e0e\u7a7a\u7075\uff0c\n\u8868\u73b0\u51fa\u81ea\u7136\u7684\u5b81\u9759\u4e0e\u548c\u8c10\u7f8e\u3002\n\nLLaDA \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u4eba\u95f2\u6842\u82b1\u843d\u201d\n\n24","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":10402,"end_ix":10419,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"2502.09992v2.pdf","id":"file-EjEr99nXVn1FWaMNbWUohS","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nLarge Language Diffusion Models\n\n\nLarge Language Diffusion Models\n\nShen Nie 1 * \u2020 Fengqi Zhu 1 * \u2020 Zebin You 1 \u2020 Xiaolu Zhang 2 \u2021 Jingyang Ou 1 Jun Hu 2 \u2021 Jun Zhou 2\n\nYankai Lin 1 \u2021 Ji-Rong Wen 1 Chongxuan Li 1 \u2021 \u00b6\n\nAbstract\nAutoregressive models (ARMs) are widely re-\ngarded as the cornerstone of large language mod-\nels (LLMs). We challenge this notion by intro-\nducing LLaDA, a diffusion model trained from\nscratch under the pre-training and supervised fine-\ntuning (SFT) paradigm. LLaDA models distri-\nbutions through a forward data masking process\nand a reverse process, parameterized by a vanilla\nTransformer to predict masked tokens. By op-\ntimizing a likelihood bound, it provides a prin-\ncipled generative approach for probabilistic in-\nference. Across extensive benchmarks, LLaDA\ndemonstrates strong scalability, outperforming\nour self-constructed ARM baselines. Remark-\nably, LLaDA 8B is competitive with strong LLMs\nlike LLaMA3 8B in in-context learning and, af-\nter SFT, exhibits impressive instruction-following\nabilities in case studies such as multi-turn dia-\nlogue. Moreover, LLaDA addresses the rever-\nsal curse, surpassing GPT-4o in a reversal poem\ncompletion task. Our findings establish diffu-\nsion models as a viable and promising alternative\nto ARMs, challenging the assumption that key\nLLM capabilities discussed above are inherently\ntied to ARMs. Project page and codes: https:\n//ml-gsai.github.io/LLaDA-demo/.\n\n1. Introduction\nWhat is now proved was once only imagined.\nplaceholder,placeholder,placeho\u2014William Blake\n\nLarge language models (LLMs) (Zhao et al., 2023) fall en-\ntirely within the framework of generative modeling. Specifi-\ncally, LLMs aim to capture the true but unknown language\n\n*Equal contribution \u2020Work done during an internship at Ant\nGroup \u2021Project leaders 1Gaoling School of Artificial Intelli-\ngence, Renmin University of China; Beijing Key Laboratory\nof Big Data Management and Analysis Methods 2Ant Group.\n\u00b6Correspondence to: Chongxuan Li <chongxuanli@ruc.edu.cn>.\n\nPreprint.\n\n46\n\n33\n\n37\n50\n\n19\n\n24\n\n33\n49\n\n50\n\n56\n\n39\n\n45\n60\n\n23\n\n29\n\n40\n59\n\n60\n\n65\n\n46\n\n52\n70\n\n27\n\n34\n\n47\n69\n\n70\n\nMMLU\n\nTruthfulQA\n\nARC-C\nGSM8K\n\nMath\n\nHumanEval\n\nMBPP\nCMMLU\n\nC-Eval\n\nGeneral TasksM\nath\n\nem\nati\n\ncs\n\nCode\n\nChinese\n\nLLaDA 8B Base\nLLaMA 3 8B Base\nLLaMA 2 7B Base\n\nFigure 1. Zero/Few-Shot Benchmarks. We scale LLaDA to an\nunprecedented size of 8B parameters from scratch, achieving com-\npetitive performance with strong LLMs (Dubey et al., 2024).\n\ndistribution pdata(\u00b7) by optimizing a model distribution p\u03b8(\u00b7)\nthrough maximum likelihood estimation, or equivalently KL\ndivergence minimization between the two distributions:\n\nmax\n\u03b8\n\nEpdata(x) log p\u03b8(x)\u21d4 min\n\u03b8\n\nKL(pdata(x)||p\u03b8(x))\ufe38 \ufe37\ufe37 \ufe38\nGenerative modeling principles\n\n. (1)\n\nThe predominant approach relies on the autoregressive mod-\neling (ARM)\u2014commonly referred to as the next-token pre-\ndiction paradigm\u2014to define the model distribution:\n\np\u03b8(x) = p\u03b8(x\n1)\n\nL\u220f\ni=2\n\np\u03b8(x\ni | x1, . . . , xi\u22121)\ufe38 \ufe37\ufe37 \ufe38\n\nAutoregressive formulation\n\n, (2)\n\nwhere x is a sequence of length L, and xi is the i-th token.\n\nThis paradigm has proven remarkably effective (Radford,\n2018; Radford et al., 2019; Brown, 2020; OpenAI, 2022)\nand has become the foundation of current LLMs. Despite\nits widespread adoption, a fundamental question remains\nunanswered: Is the autoregressive paradigm the only viable\npath to achieving the intelligence exhibited by LLMs?\n\n1\n\nar\nX\n\niv\n:2\n\n50\n2.\n\n09\n99\n\n2v\n2 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n8 \nFe\n\nb \n20\n\n25\n\nhttps://ml-gsai.github.io/LLaDA-demo/\nhttps://ml-gsai.github.io/LLaDA-demo/\n\n\nLarge Language Diffusion Models\n\nWe argue that the answer is not a simple \u201cyes\u201d. The key\ninsight overlooked previously is: it is the generative mod-\neling principles (i.e., Eq. (1)), rather than the autoregres-\nsive formulation (i.e., Eq. (2)) itself, that fundamentally\nunderpin the essential properties of LLMs, as detailed be-\nlow. However, certain inherent limitations of LLMs can be\ndirectly traced to their autoregressive nature.\n\nIn particular, we argue that scalability is primarily a con-\nsequence of the interplay between Transformers (Vaswani,\n2017), model and data size, and Fisher consistency1 (Fisher,\n1922) induced by the generative principles in Eq. (1), rather\nthan a unique result of ARM. The success of diffusion trans-\nformers (Bao et al., 2023; Peebles & Xie, 2023) on visual\ndata (Brooks et al., 2024) supports this claim.\n\nFurthermore, the capabilities of instruction-following and\nin-context learning (Brown, 2020) appear to be intrinsic\nproperties of all proper conditional generative models on\nstructurally consistent linguistic tasks, rather than exclusive\nadvantages of ARMs. In addition, while ARMs can be\ninterpreted as a lossless data compressor (Deletang et al.;\nHuang et al., 2024b), any sufficiently expressive probabilis-\ntic model can achieve similar capabilities (Shannon, 1948).\n\nNevertheless, the autoregressive nature of LLMs presents\nnotable challenges. For example, sequential token-by-token\ngeneration incurs high computational costs, and the left-\nto-right modeling limits effectiveness in reversal reasoning\ntasks (Berglund et al., 2023). These inherent limitations\nconstrain LLMs in handling longer and more complex tasks.\n\nMotivated by these insights, we introduce LLaDA, a Large\nLanguage Diffusion with mAsking, to investigate whether\nthe capabilities exhibited by LLMs can emerge from gener-\native modeling principles beyond ARMs in Eq. (2), thereby\naddressing the fundamental question posed earlier. In con-\ntrast to traditional ARMs, LLaDA leverages a masked diffu-\nsion model (MDM) (Austin et al., 2021a; Lou et al., 2023;\nShi et al., 2024; Sahoo et al., 2024; Ou et al., 2024), which\nincorporates a discrete random masking process and trains\na mask predictor to approximate its reverse process. This\ndesign enables LLaDA to construct a model distribution\nwith bidirectional dependencies and optimize a lower bound\nof its log-likelihood, offering an unexplored and principled\nalternative to existing LLMs.\n\nWe adopt the standard pipeline of data preparation, pre-\ntraining, supervised fine-tuning (SFT), and evaluation, scal-\ning LLaDA to an unprecedented language diffusion of size\n8B. In particular, LLaDA 8B was pre-trained from scratch\non 2.3 trillion tokens using 0.13 million H800 GPU hours,\nfollowed by SFT on 4.5 million pairs. Across diverse tasks,\nincluding language understanding, math, code, and Chinese,\n\n1It suggests the ability to recover the true data distribution with\ninfinite data, a sufficiently large network and optimal training.\n\nLLaDA demonstrates the following contributions:\n\nScalability. LLaDA scales effectively up to a computational\nbudget of 1023 FLOPs, achieving comparable results to self-\nconstructed ARM baselines trained on the same data across\nsix tasks, e.g., MMLU and GSM8K.\n\nIn-Context Learning. Remarkably, LLaDA 8B surpasses\nLLaMA2 7B (Touvron et al., 2023) on nearly all 15 standard\nzero/few-shot learning tasks while performing on par with\nLLaMA3 8B (Dubey et al., 2024).\n\nInstruction-Following. LLaDA significantly enhances the\nability to follow instructions after SFT, as demonstrated in\ncase studies such as multi-turn dialogue.\n\nReversal Reasoning. LLaDA effectively breaks the re-\nversal curse (Berglund et al., 2023) with consistent per-\nformance across forward and reversal tasks. Notably, it\noutperforms GPT-4o in a reversal poem completion task.\n\n2. Approach\nIn this section, we introduce the probabilistic formulation2,\nalong with the pre-training, supervised fine-tuning, and in-\nference procedures for LLaDA, as illustrated in Fig. 2.\n\n2.1. Probabilistic Formulation\n\nUnlike ARMs in Eq. (2), LLaDA defines a model distribu-\ntion p\u03b8(x0) through a forward process and a reverse pro-\ncess (Austin et al., 2021a; Ou et al., 2024). The forward\nprocess gradually masks tokens independently in x0 until\nthe sequence is fully masked at t = 1. For t \u2208 (0, 1), the\nsequence xt is partially masked, with each being masked\nwith probability t or remaining unmasked with probability\n1\u2212 t. The reverse process recovers the data distribution by\niteratively predicting masked tokens as t moves from 1 to 0.\n\nThe core of LLaDA is a mask predictor, a parametric model\np\u03b8(\u00b7|xt) that takes xt as input and predicts all masked tokens\n(denoted M) simultaneously. It is trained using a cross-\nentropy loss computed only on the masked tokens:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (3)\n\nwhere x0 is sampled from the training data, t is sampled\nuniformly from [0, 1], and xt is sampled from the forward\nprocess. The indicator function 1[\u00b7] ensures that the loss is\ncomputed only for masked tokens.\n\nOnce trained, we can simulate a reverse process (see Sec. 2.4\nfor details) parameterized by the mask predictor and define\nthe model distribution p\u03b8(x0) as the marginal distribution\n\n2Here, we focus on the approach of LLaDA. A rigorous formu-\nlation of MDM is provided in Appendix A for interested readers.\n\n2\n\n\n\nLarge Language Diffusion Models\n\nMask predictor\n\nMask all tokens independently\n\nMask predictor\n\nPrompt Response\n\nMask predictor\n\nPrompt Response\n\n... \n\n... \n\nA\nn\n\n in\nte\n\nrm\ne\n\nd\nia\n\nte\n s\n\nte\np\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nNon-mask token Random mask\n\nRemaskMask token\n\n(c)(b)(a)\n\nRemask\n\nMask ratio \ud835\udc61 \u223c \ud835\udc48(0,1)\n\nFigure 2. A Conceptual Overview of LLaDA. (a) Pre-training. LLaDA is trained on text with random masks applied independently to all\ntokens at the same ratio t \u223c U [0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion\nprocess from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies.\n\ninduced at t = 0. Notably, the loss in Eq. (3) has been\nproven to be an upper bound on the negative log-likelihood\nof the model distribution (Shi et al., 2024; Ou et al., 2024):\n\n\u2212Epdata(x0) [log p\u03b8(x0)] \u2264 L(\u03b8), (4)\n\nmaking it a principled objective for generative modeling.\n\nNotably, LLaDA employs a masking ratio that varies ran-\ndomly between 0 and 1 while masked language models (De-\nvlin, 2018) use a fixed ratio. The subtly differences have\nsignificant implications, especially at scale: as shown in\nEq. (4), LLaDA is a principled generative model with the\npotential to perform in-context learning naturally, akin to\nLLMs. Moreover, its generative perspective ensures Fisher\nconsistency (Fisher, 1922) in extreme cases, suggesting\nstrong scalability with large data and models.\n\n2.2. Pre-training\n\nLLaDA employs a Transformer (Vaswani, 2017) as the mask\npredictor, whose architecture is similar to existing LLMs.\nHowever, LLaDA does not use a causal mask, as its formu-\nlation allows it to see the entire input for predictions.\n\nWe trained two variants of LLaDA with different sizes: 1\nbillion (B) and 8B. We summarize the model architecture\nof LLaDA 8B and LLaMA3 8B (Dubey et al., 2024) here\nand details are provided in Appendix B.2. We have ensured\nconsistency in most hyperparameters while making several\nnecessary modifications. We use vanilla multi-head atten-\ntion instead of grouped query attention (Ainslie et al., 2023)\nfor simplicity, as LLaDA is incompatible with KV caching,\nresulting in a different number of key and value heads. Con-\nsequently, the attention layer has more parameters, and we\nreduce the FFN dimension to maintain a comparable model\nsize. Additionally, the vocabulary size differs slightly due\nto a tokenizer (Brown, 2020) adapted on our data.\n\nThe LLaDA model is pre-trained on a dataset comprising\n2.3 trillion (T) tokens, adhering to a data protocol that aligns\n\nclosely with existing large language models (LLMs) (Tou-\nvron et al., 2023; Chu et al., 2024), without the incorporation\nof any special techniques. The data are derived from online\ncorpora, with low-quality content filtered through manually\ndesigned rules and LLM-based approaches. Beyond general\ntext, the dataset encompasses high-quality code, math, and\nmultilingual data. The mixing of data sources and domains\nis guided by scaled-down ARMs. The pre-training process\nutilizes a fixed sequence length of 4096 tokens, incurring a\ntotal computational cost of 0.13 million H800 GPU hours,\nsimilar to ARMs of the same scale and dataset size.\n\nFor a training sequence x0, we randomly sample t \u2208 [0, 1],\nmask each token independently with the same probability\nt to obtain xt (see Fig. 2 (a)) and estimate Eq. (3) via the\nMonte Carlo method for stochastic gradient descent training.\nIn addition, following Nie et al. (2024), to enhance the\nability of LLaDA to handle variable-length data, we set 1%\nof the pre-training data to a random length that is uniformly\nsampled from the range [1, 4096].\n\nWe adopted the Warmup-Stable-Decay (Hu et al., 2024)\nlearning rate scheduler to monitor the training progress\nwithout interrupting continuous training. Specifically, we\nlinearly increased the learning rate from 0 to 4\u00d7 10\u22124 over\nthe first 2000 iterations and maintained it at 4\u00d7 10\u22124. After\nprocessing 1.2T tokens, we decayed the learning rate to\n1\u00d7 10\u22124 and held it constant for the next 0.8T tokens to en-\nsure stable training. Finally, we linearly reduced the learning\nrate from 1\u00d710\u22124 to 1\u00d710\u22125 for the last 0.3T tokens. Fur-\nthermore, we utilized the AdamW optimizer (Loshchilov,\n2017) with a weight decay of 0.1, a batch size of 1280, and\na local batch size of 4 per GPU. The 8B experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.3. Supervised Fine-Tuning\n\nWe enhance the capability of LLaDA to follow instructions\nby supervised fine-tuning (SFT) with paired data (p0, r0),\n\n3\n\n\n\nLarge Language Diffusion Models\n\nwhere p0 is the prompt and r0 denotes the response. This\nis the simplest and most basic post-training method for\nLLMs. Technically, this requires to model the conditional\ndistribution p\u03b8(r0|p0) instead of p\u03b8(x0) in pre-training.\n\nThe implementation is similar to pre-training. As shown in\nFig. 2 (b), we leave the prompt unchanged and mask the\ntokens in the response independently, as done for x0. Then,\nwe feed both the prompt and the masked response rt to the\npre-trained mask predictor to compute the loss for SFT:\n\n\u2212Et,p0,r0,rt\n\n\uf8ee\uf8f01\n\nt\n\nL\u2032\u2211\ni=1\n\n1[rit = M] log p\u03b8(r\ni\n0|p0, rt)\n\n\uf8f9\uf8fb , (5)\n\nwhere L\u2032 denotes a dynamic length specified later, and all\nother notations remain the same as before.\n\nNote that this approach is fully compatible with pre-training.\nEssentially, the concatenation of p0 and r0 can be treated\nas clean pre-training data x0, while the concatenation of\np0 and rt serves as the masked version xt. The process is\nidentical to pre-training, with the only difference being that\nall masked tokens happen to appear in the r0 portion.\n\nThe LLaDA 8B model undergoes SFT on a dataset com-\nprising 4.5 million pairs. Consistent with the pre-training\nprocess, both data preparation and training follow the SFT\nprotocols utilized in existing LLMs (Chu et al., 2024; Yang\net al., 2024), without introducing any additional techniques\nto optimize LLaDA\u2019s performance. The dataset spans mul-\ntiple domains, including code, mathematics, instruction-\nfollowing, and structured data understanding. We append\n|EOS| tokens to the end of short pairs in each mini-batch\nto ensure equal lengths across all data. We treat |EOS| as a\nnormal token during training and remove it during sampling,\nenabling LLaDA to control the response length automati-\ncally. Please refer to Appendix B.1 for more details.\n\nWe train for 3 epochs on the SFT data using a similar sched-\nule to the pre-training phase. The learning rate is linearly\nincreased from 0 to 2.5\u00d7 10\u22125 over the first 50 iterations\nand then kept constant. During the final 10% of iterations,\nit is linearly reduced to 2.5 \u00d7 10\u22126. Additionally, we set\nthe weight decay to 0.1, the global batch size to 256, and\nthe local batch size to 2 per GPU. The SFT experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.4. Inference\n\nAs a generative model, LLaDA is capable of both sampling\nnew text and evaluating the likelihood of candidate text.\n\nWe begin with the sampling. As illustrated in Fig. 2 (c),\ngiven a prompt p0, we discretize the reverse process to sam-\nple from the model distribution p\u03b8(r0|p0), starting from a\nfully masked response. The total number of sampling steps\nis a hyperparameter, which naturally provides LLaDA with a\n\ntrade-off between efficiency and sample quality, as analyzed\nin Sec. 3.3. We employ uniformly distributed timesteps by\ndefault. In addition, the generation length is also treated as\na hyperparameter, specifying the length of the fully masked\nsentence at the beginning of the sampling process. As de-\ntailed in Appendix B.4, since both pre-training and SFT\nare conducted using datasets with variable lengths, the final\nresults are insensitive to this length hyperparameter.\n\nAt an intermediate step from time t \u2208 (0, 1] to s \u2208 [0, t), we\nfeed both p0 and rt into the mask predictor and predict all\nmasked tokens simultaneously. Subsequently, we remask s\n\nt\nof the predicted tokens in expectation to obtain rs, ensuring\nthat the transition of the reverse process aligns with the\nforward process for accurate sampling (Austin et al., 2021a).\n\nIn principle, the remasking strategy should be purely ran-\ndom. However, inspired by the annealing tricks of sampling\nin LLMs (Holtzman et al., 2019; Brown, 2020), we explore\ntwo deterministic yet effective remasking strategies. Specif-\nically, similarly to Chang et al. (2022), we remask the s\n\nt\nof predicted tokens with the lowest confidence based on\nthe predictions, called low-confidence remasking. Addition-\nally, for LLaDA after SFT, we can divide the sequence into\nseveral blocks and generate them from left to right, called\nsemi-autoregressive remasking. Within each block, we ap-\nply the reverse process to perform sampling. We provide\nmore details and ablation studies in Appendix. B.3.\n\nFor conditional likelihood evaluation, we can naturally uti-\nlize the upper bound in Eq. (5). However, we find that the\nfollowing equivalent form (Ou et al., 2024) exhibits lower\nvariance and is more stable for evaluation:\n\n\u2212El,r0,rl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[ril = M] log p\u03b8(r\ni\n0|p0, rl)\n\n]\n, (6)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and rl is\nobtained by uniformly sampling l tokens from r0 without\nreplacement for masking. In addition, we employ the unsu-\npervised classifier-free guidance (Nie et al., 2024). We refer\nthe readers to more details in Appendix A.2.\n\nWe present the training, sampling, and likelihood evaluation\nalgorithms, along with theoretical details, in Appendix A.\n\n3. Experiments\nWe evaluate the scalability, instruction-following, and in-\ncontext learning capabilities of LLaDA on standard bench-\nmarks, followed by analyses and case studies on more con-\ntrolled datasets to provide a comprehensive assessment.\n\n3.1. Scalability of LLaDA on Language Tasks\n\nWe first investigate the scalability of LLaDA on downstream\ntasks in comparison with the ARM baselines we constructed.\n\n4\n\n\n\nLarge Language Diffusion Models\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nA\nR\n\nC\n-C\n\n (0\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nC\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt) Autoregressive Baseline\n\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n50\n\n60\n\n70\n\n80\n\nPI\nQ\n\nA\n (0\n\n-s\nho\n\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n20\n\n40\n\n60\n\nG\nSM\n\n8K\n (4\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n8\n\n16\n\n24\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\nFigure 3. Scalability of LLaDA. We evaluate the performance of LLaDA and our ARM baselines trained on the same data across\nincreasing computational FLOPs. LLaDA exhibits strong scalability, matching the overall performance of ARMs on six tasks.\n\nSpecifically, at the 1B scale, we ensured that LLaDA and\nARM shared the same architecture, data, and all other config-\nurations. At larger scales, we also report results for LLaDA\nand ARM models of slightly different sizes trained on the\nsame data due to resource limit, as detailed in Appendix B.2.\nWe use the computational cost as a unified scaling metric.\nFor evaluation, we focused on six standard and diverse tasks.\n\nAs shown in Fig. 3, LLaDA demonstrates impressive scala-\nbility, with its overall trend being highly competitive with\nthat of ARM. Notably, in tasks such as MMLU and GSM8K,\nLLaDA exhibits even stronger scalability. Even on tasks like\nPIQA, where performance lags, LLaDA narrows the gap\nwith ARMs at larger scales. To account for the significant\ninfluence of outliers, we opted not to fit quantitative scaling\ncurves, avoiding potential misinterpretation. Nevertheless,\nthe results clearly demonstrate the scalability of LLaDA.\n\nNie et al. (2024) suggests that MDM requires 16 times\nmore computation than ARM to achieve the same likeli-\nhood. However, there are key differences that make the\nconclusions of this study more broadly applicable. In partic-\nular, likelihood is a relatively indirect metric for downstream\ntask performance, and diffusion optimizes a bound of the\nlikelihood, making it not directly comparable to ARM. Ad-\nditionally, we extended the scaling range from 1018 \u223c 1020\n\nin Nie et al. (2024) to 1020 \u223c 1023 in this work.\n\n3.2. Benchmark Results\n\nTo comprehensively evaluate the in-context learning and\ninstruction-following capabilities of LLaDA 8B, we con-\nducted detailed comparisons with existing LLMs (Touvron\n\net al., 2023; Dubey et al., 2024; Chu et al., 2024; Yang et al.,\n2024; Bi et al., 2024; Jiang et al., 2023) of similar scale.\nThe selection of tasks and evaluation protocols was aligned\nwith existing studies, encompassing 15 popular benchmarks\nin general tasks, mathematics, code, and Chinese. Further\ndetails are provided in Appendix B.5. For a more direct\ncomparison, we re-evaluated representative LLMs (Touvron\net al., 2023; Dubey et al., 2024) in our implementation.\n\nAs shown in Tab. 1, after pretraining on 2.3T tokens,\nLLaDA 8B demonstrates remarkable performance, surpass-\ning LLaMA2 7B on nearly all tasks, and is overall competi-\ntive with LLaMA3 8B. LLaDA shows advantages in math\nand Chinese tasks. We conjecture that the strengths stem\nfrom the same factors as its relatively weaker performance\nin some tasks\u2014differences in data quality and distribution,\nlargely due to the closed-source situation of LLM datasets.\n\nNotably, we have carefully ruled out the possibility of data\nleakage by taking GSM8K as an example. First, as shown\nin Fig. 3, LLaDA outperformed ARM baselines regarding\nGSM8K. Moreover, the conclusion remains on a fully un-\nseen GSM8K-like task (Ye et al., 2024) in Appendix B.7.\n\nFurther, Tab. 2 compares the performance of LLaDA 8B In-\nstruct with existing LLMs. We observed that SFT improved\nLLaDA\u2019s performance on most downstream tasks. A few\nmetrics, such as MMLU, showed declines, and we conjec-\nture may be due to the suboptimal quality of the SFT data.\nOverall, since we did not perform alignment with reinforce-\nment learning (RL), our results are slightly behind LLaMA3\n8B Instruct, though the gaps in many metrics remain small.\nNotably, even with only SFT, LLaDA demonstrates impres-\n\n5\n\n\n\nLarge Language Diffusion Models\n\nTable 1. Benchmark Results of Pre-trained LLMs. \u2217 indicates that LLaDA 8B Base, LLaMA2 7B Base, and LLaMA3 8B Base are\nevaluated under the same protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Chu et al. (2024); Yang et al.\n(2024) and Bi et al. (2024) respectively. The numbers in parentheses represent the number of shots used for evaluation. \u201c-\u201d indicates\nunknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Mistral 7B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T - 2T\n\nGeneral Tasks\n\nMMLU 65.9 (5) 65.4 (5) 45.9 (5) 70.3 (5) 74.2 (5) 64.2 (5) 48.2 (5)\nBBH 49.8 (3) 57.6 (3) 37.3 (3) 62.3 (3) 70.4 (3) 56.1 (3) 39.5 (3)\nARC-C 47.9 (0) 53.1 (0) 46.3 (0) 60.6 (25) 63.7 (25) 60.0 (25) 48.1 (0)\nHellaswag 72.5 (0) 79.1 (0) 76.0 (0) 80.7 (10) 80.2 (10) 83.3 (10) 75.4 (0)\nTruthfulQA 46.4 (0) 44.0 (0) 39.0 (0) 54.2 (0) 56.4 (0) 42.2 (0) -\nWinoGrande 74.8 (5) 77.3 (5) 72.5 (5) 77.0 (5) 75.9 (5) 78.4 (5) 70.5 (0)\nPIQA 74.4 (0) 80.6 (0) 79.1 (0) - - - 79.2 (0)\n\nMathematics & Science\n\nGSM8K 70.7 (4) 53.1 (4) 14.3 (4) 80.2 (4) 85.4 (4) 36.2 (4) 17.4 (8)\nMath 27.3 (4) 15.1 (4) 3.2 (4) 43.5 (4) 49.8 (4) 10.2 (4) 6.0 (4)\nGPQA 26.1 (5) 25.9 (5) 25.7 (5) 30.8 (5) 36.4 (5) 24.7 (5) -\n\nCode\n\nHumanEval 33.5 (0) 34.2 (0) 12.8 (0) 51.2 (0) 57.9 (0) 29.3 (0) 26.2 (0)\nHumanEval-FIM 73.8 (2) 73.3 (2) 26.9 (2) - - - -\nMBPP 38.2 (4) 47.4 (4) 18.4 (4) 64.2 (0) 74.9 (0) 51.1 (0) 39.0 (3)\n\nChinese\n\nCMMLU 69.9 (5) 50.7 (5) 32.5 (5) 83.9 (5) - - 47.2 (5)\nC-Eval 70.5 (5) 51.7 (5) 34.0 (5) 83.2 (5) - - 45.0 (5)\n\nTable 2. Benchmark Results of Post-trained LLMs. LLaDA only employs an SFT procedure while other models have extra reinforcement\nlearning (RL) alignment. \u2217 indicates that LLaDA 8B Instruct, LLaMA2 7B Instruct, and LLaMA3 8B Instruct are evaluated under the\nsame protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Yang et al. (2024) and Bi et al. (2024) respectively.\nThe numbers in parentheses represent the number of shots used for in-context learning. \u201c-\u201d indicates unknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Gemma2 9B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T 8T 2T\n\nPost-training SFT SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL\nAlignment pairs 4.5M - - 0.5M + - 1M + 0.15M - 1.5M + -\n\nGeneral Tasks\n\nMMLU 65.5 (5) 68.4 (5) 44.1 (5) - - - 49.4 (0)\nMMLU-pro 37.0 (0) 41.9 (0) 4.6 (0) 44.1 (5) 56.3 (5) 52.1 (5) -\nHellaswag 74.6 (0) 75.5 (0) 51.5 (0) - - - 68.5 (-)\nARC-C 88.5 (0) 82.4 (0) 57.3 (0) - - - 49.4 (-)\n\nMathematics & Science\n\nGSM8K 78.6 (4) 78.3 (4) 29.0 (4) 85.7 (0) 91.6 (0) 76.7 (0) 63.0 (0)\nMath 26.6 (0) 29.6 (0) 3.8 (0) 52.9 (0) 75.5 (0) 44.3 (0) 15.8 (0)\nGPQA 31.8 (5) 31.9 (5) 28.4 (5) 34.3 (0) 36.4 (0) 32.8 (0) -\n\nCode\n\nHumanEval 47.6 (0) 59.8 (0) 16.5 (0) 79.9 (0) 84.8 (0) 68.9 (0) 48.2 (-)\nMBPP 34.2 (4) 57.6 (4) 20.6 (4) 67.2 (0) 79.2 (0) 74.9 (0) 35.2 (-)\n\n6\n\n\n\nLarge Language Diffusion Models\n\nTable 3. Comparison in the Poem Completion Task.\n\nForward Reversal\n\nGPT-4o (2024-08-06) 82.7 34.3\nQwen2.5 7B Instruct 75.9 38.0\nLLaDA 8B Instruct 48.8 42.4\n\nsive instruction-following abilities, as detailed in Sec. 3.4.\nWe leave RL-based alignment for future work.\n\nOverall, despite the lack of data transparency, we have made\nevery effort to adopt standardized procedures and introduce\ndiverse tasks, we believe they sufficiently demonstrate the\nextraordinary capabilities of LLaDA, which is the only com-\npetitive non-autoregressive model to our knowledge.\n\n3.3. Reversal Reasoning and Analyses\n\nTo quantify the reversal reasoning ability of mod-\nels (Berglund et al., 2023), we follow the protocol estab-\nlished in Allen-Zhu & Li (2023). Specifically, we construct\na dataset of 496 famous Chinese poem sentence pairs. Given\na sentence from a poem, models are tasked with generating\nthe subsequent line (forward) or the preceding line (rever-\nsal) without additional fine-tuning. Examples can be found\nin Appendix B.8. This setting provides a straightforward\nand more realistic evaluation compared to previous stud-\nies (Nie et al., 2024; Kitouni et al., 2024).\n\nAs shown in Tab. 3, LLaDA effectively addresses the rever-\nsal curse (Berglund et al., 2023), demonstrating consistent\nzero-shot performance across both forward and reversal\ntasks. In contrast, both Qwen 2.5 and GPT-4o exhibit a\nsignificant gap between the two. The results on forward\ngeneration confirm that both ARMs are strong, benefiting\nfrom significantly larger datasets and greater computational\nresources than LLaDA. However, LLaDA outperforms both\nby a large margin in the reversal task.\n\nWe emphasize that we did not design anything special for\nreversal tasks. Intuitively, LLaDA treats tokens uniformly\nwithout inductive bias, leading to balanced performance.\nSee more details in Appendix A.2.\n\nWe also analyze the effect of remasking strategies and sam-\npling steps, as detailed in Appendix B.3 and Appendix B.6.\n\n3.4. Case Studies\n\nWe present samples generated by LLaDA 8B Instruct in\nTab. 4, showcasing its instruction-following capabilities.\nFirst, the table illustrates LLaDA\u2019s ability to generate co-\nherent, fluent, and extended text in a non-autoregressive\nmanner. Second, it highlights the model\u2019s multi-turn dia-\nlogue capability, effectively retaining conversation history\n\nand producing contextually appropriate responses across\nmultiple languages. Such chat capabilities of LLaDA are\nimpressive, as it departs from conventional ARMs for the\nfirst time, to the best of our knowledge. See more case\nstudies on remasking and reasoning tasks in Appendix B.9.\n\n4. Related Work\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020) have excelled in visual domains but\nremain unverified for LLMs despite extensive efforts.\n\nA simple approach is to continuousize text data and apply\ndiffusion models directly (Li et al., 2022; Gong et al., 2022;\nHan et al., 2022; Strudel et al., 2022; Chen et al., 2022;\nDieleman et al., 2022; Richemond et al., 2022; Wu et al.,\n2023; Mahabadi et al., 2024; Ye et al., 2023b). Alternatively,\nsome methods model continuous parameters of discrete dis-\ntributions instead (Lou & Ermon, 2023; Graves et al., 2023;\nLin et al., 2023; Xue et al., 2024). However, scalability\nremains a challenge, as a 1B parameter model requires 64\ntimes the compute of an ARM to achieve comparable per-\nformance (Gulrajani & Hashimoto, 2024).\n\nAnother approach replaces continuous diffusion with dis-\ncrete processes featuring new forward and reverse dy-\nnamics (Austin et al., 2021a), leading to numerous vari-\nants (Hoogeboom et al., 2021b;a; He et al., 2022; Campbell\net al., 2022; Meng et al., 2022; Reid et al., 2022; Sun et al.,\n2022; Kitouni et al., 2023; Zheng et al., 2023; Chen et al.,\n2023; Ye et al., 2023a; Gat et al., 2024; Zheng et al., 2024;\nSahoo et al., 2024; Shi et al., 2024). Notably, Lou et al.\n(2023) showed that masked diffusion, as a special case of\ndiscrete diffusion, achieves perplexity comparable to or sur-\npassing ARMs at GPT-2 scale. Ou et al. (2024) established\nfundamental theoretical results, which motivated our model\ndesign, training, and inference (see Appendix A). Nie et al.\n(2024) explored how MDM can be leveraged for language\ntasks such as question answering at GPT-2 scale. Gong et al.\n(2024) fine-tune ARMs in the MDM formulation. However,\nimprovements are confined to certain metrics, and it remains\nunclear whether this approach can yield a foundation model\ncomparable to strong LLMs under a comprehensive evalua-\ntion.\n\nIn comparison, this study scales MDM to an unprecedented\nsize of 8B parameters from scratch, achieving performance\ncomparable to leading LLMs such as LLaMA 3.\n\nAdditionally, a parallel line of work on image genera-\ntion (Chang et al., 2022; 2023) aligns well with the appli-\ncation of MDMs to text data. Moreover, MDMs have also\nshown promise in domains such as protein generation (Wang\net al., 2024b;c), where they have achieved promising results.\nNotably, Kou et al. (2024); Xu et al. (2025) demonstrate the\npotential of using distillation to accelerate MDMs sampling,\n\n7\n\n\n\nLarge Language Diffusion Models\n\nTable 4. Visualization of the Sampling Process and a Generated Multi-round Dialogue. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nSampling Process\n\nUser Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour.\nHow many kilometers can she run in 8 hours?\n\nLLaDA\n\nMulti-round Dialogue Case\n\nUser Could you please share with me what the initial two lines of the renowned poem \u2018The\nRoad Not Taken\u2019?\n\nLLaDA Certainly! The initial two lines of the renowned poem \u201cThe Road Not Taken\u201d by Robert\nFrost are: \u201cTwo roads diverged in a yellow wood, And sorry I could not travel both.\u201d These\nlines set the stage for the poem, introducing the idea of a speaker facing a choice between\ntwo paths.\n\nUser Please help me translate into Chinese.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nChinese as: \u201c\u4e24\u6761\u8def\u5206\u5c94\u5728\u9ec4\u8272\u7684\u6811\u6797\u4e2d\uff0c\u9057\u61be\u6211\u4e0d\u80fd\u540c\u65f6\u8d70\u201d\nUser Please translate into German.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nGerman as: \u201cZwei Wege trennten sich im gelben Wald, und leider konnte ich nicht beide\ntun.\u201d\n\nUser Please also write a poem about life choices. I require 5 sentences, and the first word of\neach sentence starts with C as the first letter.\n\nLLaDA Certainly! Here\u2019s a poem about life choices, with each sentence starting with the letter \u201cC\u201d:\nChoices cascade through life\u2019s journey, Creating connections with each decision, Careful\nconsideration leads to better paths, Courageous choices shape our destiny, Choosing wisely\nguides us through the years.\n\nfurther enhancing their efficiency.\n\n5. Conclusion and Discussion\nIn the middle of difficulty lies opportunity.\nplaceholder,placeholder,placeh\u2014Albert Einstein\n\nWe introduce LLaDA, a principled and previously unex-\nplored approach to large language modeling based on dif-\nfusion models. LLaDA demonstrates strong capabilities in\nscalability, in-context learning, and instruction-following,\nachieving performance comparable to strong LLMs. In addi-\ntion, LLaDA offers unique advantages such as bidirectional\nmodeling and enhanced robustness, effectively addressing\nseveral inherent limitations of existing LLMs. Our findings\nnot only establish diffusion models as a viable and promis-\ning alternative but also challenge the prevailing assumption\nthat these essential capabilities are inherently tied to ARMs.\n\nWhile promising, the full potential of diffusion models re-\n\nmains to be fully explored. Several limitations of this work\npresent significant opportunities for future research.\n\nDue to computational constraints, direct comparisons be-\ntween LLaDA and ARMs\u2014such as training on identical\ndatasets\u2014were restricted to a computational budget of less\nthan 1023 FLOPs. To allocate resources for training the\nlargest possible LLaDA model and showcasing its poten-\ntial, we were unable to scale the ARM baseline to the same\nextent. Moreover, no specialized attention mechanisms\nor position embeddings were designed for LLaDA, nor\nwere any system-level architectural optimizations applied.\nOn the inference side, our exploration of guidance mecha-\nnisms (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) re-\nmains preliminary, and LLaDA currently exhibits sensitivity\nto inference hyperparameters. Furthermore, LLaDA has yet\nto undergo alignment with reinforcement learning (Ouyang\net al., 2022; Rafailov et al., 2024), which is crucial for im-\nproving its performance and alignment with human intent.\n\n8\n\n\n\nLarge Language Diffusion Models\n\nLooking forward, the scale of LLaDA is still smaller than\nleading counterparts (Achiam et al., 2023; Dubey et al.,\n2024; Google, 2024; Anthropic, 2024; Yang et al., 2024;\nLiu et al., 2024), highlighting the need for further scaling\nto fully assess its capabilities. In addition, LLaDA\u2019s abil-\nity to handle multi-modal data remains unexplored. The\nimpact of LLaDA on prompt tuning techniques (Wei et al.,\n2022) and its integration into agent-based systems (Park\net al., 2023; Wang et al., 2024a) is yet to be fully under-\nstood. Finally, a systematic investigation into post-training\nfor LLaDA could facilitate the development of O1-like sys-\ntems (OpenAI, 2024; Guo et al., 2025).\n\nImpact Statement\nThis work challenges the prevailing dominance of autore-\ngressive modeling in LLMs by proposing a diffusion-based\nalternative. Our findings open new avenues for exploring\nalternative probabilistic paradigms in natural language pro-\ncessing, with potential applications in conversational AI,\ncode generation, and complex reasoning tasks.\n\nHowever, diffusion-based models, like traditional LLMs,\nraise similar societal concerns. These include the environ-\nmental impact of large-scale training, the potential misuse\nfor generating harmful or misleading content, and the ampli-\nfication of biases present in training data. Addressing these\nchallenges is critical to ensuring the responsible develop-\nment and deployment of diffusion-based language models.\n\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (No. 92470118); Beijing Natural Sci-\nence Foundation (No. L247030); Beijing Nova Program\n(No. 20220484044); Major Innovation & Planning Interdis-\nciplinary Platform for the \u201cDouble-First Class\u201d Initiative,\nRenmin University of China; the Fundamental Research\nFunds for the Central Universities, the Research Funds of\nRenmin University of China (22XNKJ13); Ant Group Re-\nsearch Fund. The work was partially done at the Engineering\nResearch Center of Next-Generation Intelligent Search and\nRecommendation, Ministry of Education.\n\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\n\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebron, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. In Proceedings of the 2023 Conference on Em-\n\npirical Methods in Natural Language Processing, pp.\n4895\u20134901, 2023.\n\nAllen-Zhu, Z. and Li, Y. Physics of Language Mod-\nels: Part 3.2, Knowledge Manipulation. ArXiv e-prints,\nabs/2309.14402, September 2023. Full version available\nat http://arxiv.org/abs/2309.14402.\n\nAnonymous. Interpolating autoregressive and discrete de-\nnoising diffusion language models. In The Thirteenth\nInternational Conference on Learning Representations,\n2025. URL https://openreview.net/forum?\nid=tyEyYT267x.\n\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.\nanthropic.com/news/claude-3-5-sonnet.\n\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van\nDen Berg, R. Structured denoising diffusion models in\ndiscrete state-spaces. Advances in Neural Information\nProcessing Systems, 34:17981\u201317993, 2021a.\n\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021b.\n\nBao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu,\nJ. All are worth words: A vit backbone for diffusion\nmodels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22669\u2013\n22679, 2023.\n\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M. Efficient training of\nlanguage models to fill in the middle. arXiv preprint\narXiv:2207.14255, 2022.\n\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O. The reversal curse:\nLlms trained on\u201d a is b\u201d fail to learn\u201d b is a\u201d. arXiv\npreprint arXiv:2309.12288, 2023.\n\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\nScaling open-source language models with longtermism.\narXiv preprint arXiv:2401.02954, 2024.\n\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\n2020.\n\nBrooks, T., Peebles, B., Holmes, C., DePue, W.,\nGuo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman,\nT., Luhman, E., Ng, C., Wang, R., and Ramesh,\nA. Video generation models as world simulators.\n2024. URL https://openai.com/research/\nvideo-generation-models-as-world-simulators.\n\n9\n\nhttp://arxiv.org/abs/2309.14402\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://openai.com/research/video-generation-models-as-world-simulators\nhttps://openai.com/research/video-generation-models-as-world-simulators\n\n\nLarge Language Diffusion Models\n\nBrown, T. B. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\n\nCampbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deli-\ngiannidis, G., and Doucet, A. A continuous time frame-\nwork for discrete denoising models. Advances in Neural\nInformation Processing Systems, 35:28266\u201328279, 2022.\n\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,\nW. T. Maskgit: Masked generative image transformer. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11315\u201311325, 2022.\n\nChang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J.,\nJiang, L., Yang, M.-H., Murphy, K., Freeman, W. T.,\nRubinstein, M., et al. Muse: Text-to-image genera-\ntion via masked generative transformers. arXiv preprint\narXiv:2301.00704, 2023.\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\n\nChen, T., Zhang, R., and Hinton, G. Analog bits: Gen-\nerating discrete data using diffusion models with self-\nconditioning. arXiv preprint arXiv:2208.04202, 2022.\n\nChen, Z., Yuan, H., Li, Y., Kou, Y., Zhang, J., and Gu, Q.\nFast sampling via de-randomization for discrete diffusion\nmodels. arXiv preprint arXiv:2312.09193, 2023.\n\nChu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng,\nY., Lv, Y., He, J., Lin, J., et al. Qwen2-audio technical\nreport. arXiv preprint arXiv:2407.10759, 2024.\n\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\n\nDeletang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Ge-\nnewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K.,\nAitchison, M., Orseau, L., et al. Language modeling is\ncompression. In The Twelfth International Conference on\nLearning Representations.\n\nDevlin, J. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021.\n\nDieleman, S., Sartran, L., Roshannai, A., Savinov, N.,\nGanin, Y., Richemond, P. H., Doucet, A., Strudel, R.,\nDyer, C., Durkan, C., et al. Continuous diffusion for\ncategorical data. arXiv preprint arXiv:2211.15089, 2022.\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nFisher, R. A. On the mathematical foundations of theoretical\nstatistics. Philosophical transactions of the Royal Society\nof London. Series A, containing papers of a mathematical\nor physical character, 222(594-604):309\u2013368, 1922.\n\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n07 2024. URL https://zenodo.org/records/\n12608602.\n\nGat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Syn-\nnaeve, G., Adi, Y., and Lipman, Y. Discrete flow match-\ning. arXiv preprint arXiv:2407.15595, 2024.\n\nGong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq:\nSequence to sequence text generation with diffusion mod-\nels. arXiv preprint arXiv:2210.08933, 2022.\n\nGong, S., Agarwal, S., Zhang, Y., Ye, J., Zheng, L., Li, M.,\nAn, C., Zhao, P., Bi, W., Han, J., et al. Scaling diffu-\nsion language models via adaptation from autoregressive\nmodels. arXiv preprint arXiv:2410.17891, 2024.\n\nGoogle. Our next-generation model: Gemini 1.5, 2024.\nURL https://blog.google/technology/ai/\ngoogle-gemini-next-generation-model-february-2024.\n\nGraves, A., Srivastava, R. K., Atkinson, T., and\nGomez, F. Bayesian flow networks. arXiv preprint\narXiv:2308.07037, 2023.\n\nGulrajani, I. and Hashimoto, T. B. Likelihood-based diffu-\nsion language models. Advances in Neural Information\nProcessing Systems, 36, 2024.\n\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\n\nHan, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semi-\nautoregressive simplex-based diffusion language model\nfor text generation and modular control. arXiv preprint\narXiv:2210.17432, 2022.\n\n10\n\nhttps://zenodo.org/records/12608602\nhttps://zenodo.org/records/12608602\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\n\n\nLarge Language Diffusion Models\n\nHe, Z., Sun, T., Wang, K., Huang, X., and Qiu, X. Diffu-\nsionbert: Improving generative masked language models\nwith diffusion models. arXiv preprint arXiv:2211.15029,\n2022.\n\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J. Measuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\n\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\n\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840\u20136851, 2020.\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\n\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The\ncurious case of neural text degeneration. arXiv preprint\narXiv:1904.09751, 2019.\n\nHoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B.,\nBerg, R. v. d., and Salimans, T. Autoregressive diffusion\nmodels. arXiv preprint arXiv:2110.02037, 2021a.\n\nHoogeboom, E., Nielsen, D., Jaini, P., Forre\u0301, P., and Welling,\nM. Argmax flows and multinomial diffusion: Learning\ncategorical distributions. Advances in Neural Information\nProcessing Systems, 34:12454\u201312465, 2021b.\n\nHu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z.,\nFang, Y., Huang, Y., Zhao, W., et al. Minicpm: Unveiling\nthe potential of small language models with scalable train-\ning strategies. arXiv preprint arXiv:2404.06395, 2024.\n\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu,\nJ., Lv, C., Zhang, Y., Fu, Y., et al. C-eval: A multi-level\nmulti-discipline chinese evaluation suite for foundation\nmodels. Advances in Neural Information Processing\nSystems, 36, 2024a.\n\nHuang, Y., Zhang, J., Shan, Z., and He, J. Compres-\nsion represents intelligence linearly. arXiv preprint\narXiv:2404.09937, 2024b.\n\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\n\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\n\nKitouni, O., Nolte, N., Hensman, J., and Mitra, B. Disk: A\ndiffusion model for structured knowledge. arXiv preprint\narXiv:2312.05253, 2023.\n\nKitouni, O., Nolte, N., Bouchacourt, D., Williams, A., Rab-\nbat, M., and Ibrahim, M. The factorization curse: Which\ntokens you predict underlie the reversal curse and more.\narXiv preprint arXiv:2406.05183, 2024.\n\nKou, S., Hu, L., He, Z., Deng, Z., and Zhang, H. Cllms:\nConsistency large language models. arXiv preprint\narXiv:2403.00835, 2024.\n\nLi, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y.,\nDuan, N., and Baldwin, T. Cmmlu: Measuring mas-\nsive multitask language understanding in chinese. arXiv\npreprint arXiv:2306.09212, 2023.\n\nLi, X., Thickstun, J., Gulrajani, I., Liang, P. S., and\nHashimoto, T. B. Diffusion-lm improves controllable\ntext generation. Advances in Neural Information Process-\ning Systems, 35:4328\u20134343, 2022.\n\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\n\nLin, Z., Gong, Y., Shen, Y., Wu, T., Fan, Z., Lin, C., Duan,\nN., and Chen, W. Text generation with diffusion language\nmodels: A pre-training approach with continuous para-\ngraph denoise. In International Conference on Machine\nLearning, pp. 21051\u201321064. PMLR, 2023.\n\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,\nC., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437, 2024.\n\nLoshchilov, I. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017.\n\nLou, A. and Ermon, S. Reflected diffusion models, 2023.\n\nLou, A., Meng, C., and Ermon, S. Discrete diffusion lan-\nguage modeling by estimating the ratios of the data distri-\nbution. arXiv preprint arXiv:2310.16834, 2023.\n\nMahabadi, R. K., Ivison, H., Tae, J., Henderson, J., Beltagy,\nI., Peters, M. E., and Cohan, A. Tess: Text-to-text self-\nconditioned simplex diffusion, 2024.\n\n11\n\n\n\nLarge Language Diffusion Models\n\nMeng, C., Choi, K., Song, J., and Ermon, S. Concrete score\nmatching: Generalized score matching for discrete data.\nAdvances in Neural Information Processing Systems, 35:\n34532\u201334545, 2022.\n\nNie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M.,\nand Li, C. Scaling up masked diffusion models on text.\narXiv preprint arXiv:2410.18514, 2024.\n\nOpenAI. ChatGPT: Optimizing Language Models for Di-\nalogue. OpenAI blog, November 2022. URL https:\n//openai.com/blog/chatgpt/.\n\nOpenAI. Learning to reason with llms, 2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n\nOu, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li,\nC. Your absorbing discrete diffusion secretly models the\nconditional distributions of clean data. arXiv preprint\narXiv:2406.03736, 2024.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems, 35:27730\u201327744, 2022.\n\nPark, J. S., O\u2019Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and\ntechnology, pp. 1\u201322, 2023.\n\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pp. 4195\u20134205,\n2023.\n\nRadford, A. Improving language understanding by genera-\ntive pre-training, 2018.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\n\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\n\nReid, M., Hellendoorn, V. J., and Neubig, G. Diffuser:\nDiscrete diffusion via edit-based reconstruction, 2022.\n\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang,\nR. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa:\nA graduate-level google-proof q&a benchmark. arXiv\npreprint arXiv:2311.12022, 2023.\n\nRichemond, P. H., Dieleman, S., and Doucet, A. Categorical\nsdes with simplex diffusion, 2022.\n\nSahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marro-\nquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. Simple\nand effective masked diffusion language models. arXiv\npreprint arXiv:2406.07524, 2024.\n\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\n\nShannon, C. E. A mathematical theory of communication.\nThe Bell system technical journal, 27(3):379\u2013423, 1948.\n\nShazeer, N. Glu variants improve transformer. arXiv\npreprint arXiv:2002.05202, 2020.\n\nShi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. K.\nSimplified and generalized masked diffusion for discrete\ndata. arXiv preprint arXiv:2406.04329, 2024.\n\nShih, A., Sadigh, D., and Ermon, S. Training and infer-\nence on any-order autoregressive models the right way.\nIn Proceedings of the 31th International Conference on\nMachine Learning, 2022.\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256\u20132265. PMLR, 2015.\n\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\n\nStrudel, R., Tallec, C., Altche\u0301, F., Du, Y., Ganin, Y., Men-\nsch, A., Grathwohl, W., Savinov, N., Dieleman, S., Sifre,\nL., et al. Self-conditioned embedding diffusion for text\ngeneration. arXiv preprint arXiv:2211.04236, 2022.\n\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, 568:127063, 2024.\n\nSun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Score-\nbased continuous-time discrete diffusion models. arXiv\npreprint arXiv:2211.16750, 2022.\n\nSuzgun, M., Scales, N., Scha\u0308rli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\n\n12\n\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/index/learning-to-reason-with-llms/\nhttps://openai.com/index/learning-to-reason-with-llms/\n\n\nLarge Language Diffusion Models\n\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n\nUria, B., Murray, I., and Larochelle, H. A deep and tractable\ndensity estimator. In Proceedings of the 31th Interna-\ntional Conference on Machine Learning, 2014.\n\nVaswani, A. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J.,\nChen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on\nlarge language model based autonomous agents. Frontiers\nof Computer Science, 18(6):186345, 2024a.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDiffusion language models are versatile protein learners.\narXiv preprint arXiv:2402.18567, 2024b.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDplm-2: A multimodal diffusion protein language model.\narXiv preprint arXiv:2410.13782, 2024c.\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\nelicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824\u201324837,\n2022.\n\nWu, T., Fan, Z., Liu, X., Gong, Y., Shen, Y., Jiao, J., Zheng,\nH.-T., Li, J., Wei, Z., Guo, J., Duan, N., and Chen, W.\nAr-diffusion: Auto-regressive diffusion model for text\ngeneration, 2023.\n\nXu, C., Wang, X., Liao, Z., Li, Y., Hou, T., and Deng,\nZ. Show-o turbo: Towards accelerated unified multi-\nmodal understanding and generation. arXiv preprint\narXiv:2502.05415, 2025.\n\nXue, K., Zhou, Y., Nie, S., Min, X., Zhang, X., Zhou, J.,\nand Li, C. Unifying bayesian flow networks and diffusion\nmodels through stochastic differential equations. arXiv\npreprint arXiv:2404.15766, 2024.\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B.,\nLi, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu,\nJ., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang,\nK., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M.,\nZhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren,\nX., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y.,\nCui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\narXiv preprint arXiv:2412.15115, 2024.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Gu, Q. Diffusion\nlanguage models can perform many tasks with scaling and\n\ninstruction-finetuning. arXiv preprint arXiv:2308.12219,\n2023a.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Wang, M. Dinoiser:\nDiffused conditional sequence learning by manipulating\nnoises. arXiv preprint arXiv:2302.10025, 2023b.\n\nYe, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics of Lan-\nguage Models: Part 2.1, Grade-School Math and the Hid-\nden Reasoning Process. ArXiv e-prints, abs/2407.20311,\nJuly 2024. Full version available at http://arxiv.\norg/abs/2407.20311.\n\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\n\nZhang, B. and Sennrich, R. Root mean square layer nor-\nmalization. Advances in Neural Information Processing\nSystems, 32, 2019.\n\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,\nMin, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of\nlarge language models. arXiv preprint arXiv:2303.18223,\n2023.\n\nZheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and\nZhang, Q. Masked diffusion models are secretly time-\nagnostic masked models and exploit inaccurate categor-\nical sampling, 2024. URL https://arxiv.org/\nabs/2409.02908.\n\nZheng, L., Yuan, J., Yu, L., and Kong, L. A reparameter-\nized discrete diffusion model for text generation. ArXiv,\nabs/2302.05737, 2023.\n\n13\n\nhttp://arxiv.org/abs/2407.20311\nhttp://arxiv.org/abs/2407.20311\nhttps://arxiv.org/abs/2409.02908\nhttps://arxiv.org/abs/2409.02908\n\n\nLarge Language Diffusion Models\n\nAlgorithm 1 Pre-training of LLaDA\n\nRequire: mask predictor p\u03b8, data distribution pdata\n1: repeat\n2: x0 \u223c pdata, t \u223c U(0, 1] # with a probability of 1%, the sequence length of x0 follows U[1, 4096]\n3: xt \u223c qt|0(xt|x0) # qt|0 is defined in Eq. (7)\n4: Calculate L = \u2212 1\n\nt\u2217L\n\u2211L\n\ni=1 1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt) # L is the sequence length of x0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 2 Supervised Fine-Tuning of LLaDA\n\nRequire: mask predictor p\u03b8, pair data distribution pdata\n1: repeat\n2: p0, r0 \u223c pdata, t \u223c U(0, 1] # please refer to Appendix B.1 for details on the SFT data processing.\n3: rt \u223c qt|0(rt|r0) # qt|0 is defined in Eq. (7)\n\n4: Calculate L = \u2212 1\nt\u2217L\u2032\n\n\u2211L\u2032\n\ni=1 1[rit = M] log p\u03b8(r\ni\n0|p0, rt) # L\u2032 is the sequence length of r0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 3 Conditional Log-likelihood Evaluation of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, response r0, the number of Monte Carlo estimations nmc\n\n1: log likelihood = 0\n2: for i\u2190 1 to nmc do\n3: l \u223c {1, 2, . . . , L} # L is the sequence length of r0\n4: Obtain rl by uniformly sampling l tokens from r0 without replacement for masking\n5: log likelihood = log likelihood + L\n\nl\n\n\u2211L\ni=1 1[ril = M] log p\u03b8(r\n\ni\n0|p0, rl)\n\n6: end for\n7: log likelihood = log likelihood/nmc\n\n8: Return log likelihood\n\nA. Formulation of Masked Diffusion Models\n\nA.1. Training\n\nMasked diffusion models (MDMs) (Austin et al., 2021a; Lou et al., 2023; Ou et al., 2024) define the model distribution\np\u03b8(x0) in a manner distinct from autoregressive models.\n\nThese models introduce a forward process {xt} indexed by a time t \u2208 [0, 1]. This process gradually and independently\nmasks all tokens in the sequence x0. At time t = 0, the data point x0 is fully observed with no masks, while for t \u2208 (0, 1],\nxt represents latent variables with varying mask ratios in expectation.\n\nFormally, the conditional distribution of xt given x0 is defined by a fully factorized form:\n\nqt|0(xt|x0) =\n\nL\u220f\ni=1\n\nqt|0(x\ni\nt|xi\n\n0), (7)\n\nwhere the conditional distribution for each token is given by:\n\nqt|0(x\ni\nt|xi\n\n0) =\n\n{\n1\u2212 t, xi\n\nt = xi\n0,\n\nt, xi\nt = M.\n\n(8)\n\nHere, M denotes the mask token. Intuitively, each token either remains unchanged or is masked, with the probability of\n\n14\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 4 Reverse Process of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: r0 = argmaxr0 p\u03b8(r0|p0, rt) # we employ greedy sampling when predicting masked tokens\n5: for i\u2190 1 to L do\n6: if rt \u0338= M then\n7: ri0 = rit\n8: else\n9: With probability s\n\nt , ri0 is set to M\n10: end if\n11: end for\n12: rs = r0\n13: end for\n14: Return r0\n\nbeing masked increasing linearly as t progresses from 0 to 1. At t = 1, all tokens are guaranteed to be masked, meaning that\nx1 follows a Dirac distribution concentrated on a sequence of fully masked tokens. Notably, the linear masking probability\nis analogous to but distinct from, the noise schedule in continuous diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020). This linearity is motivated by the assumption that the information in the text is proportional to the\nnumber of tokens on average, making it reasonable to lose information linearly during the forward process.\n\nThe forward process is not only reversible but also corresponds to a reverse process that is fully factorized across all\ntokens (Austin et al., 2021a). The reverse process, from time t = 1 to 0, generates new data from sequences of fully masked\ntokens. The conditional distribution for the reverse process, for 0 \u2264 s < t \u2264 1, is factorized as:\n\nqs|t(xs|xt) =\n\nL\u220f\ni=1\n\nqs|t(x\ni\ns|xt), (9)\n\nwhere the conditional distribution for each token is:\n\nqs|t(x\ni\ns|xt) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n1, xi\n\nt \u0338= M, xi\ns = xi\n\nt,\ns\nt , xi\n\nt = M, xi\ns = M,\n\nt\u2212s\nt q0|t(x\n\ni\ns|xt), xi\n\nt = M, xi\ns \u0338= M,\n\n0, otherwise.\n\n(10)\n\nThus, the key function to estimate is the conditional distribution q0|t(x\ni\ns|xt), which predicts the original token if it is masked\n\nin the input xt. This is analogous to the data prediction form in continuous diffusion models.\n\nAs proven in (Ou et al., 2024), an equivalent yet time-free parameterization can be derived as:\n\nq0|t(x\ni\ns|xt) = pdata(x\n\ni\n0|xUM\n\nt ), \u2200i such that xi\nt = M, (11)\n\nwhere xUM\nt denotes the collection of unmasked tokens in xt, which is identical to the corresponding tokens in the original\n\ndata x0 since unmasked tokens are solely determined by x0 and are independent of time t. Intuitively, this implies that\nestimating the data prediction function is equivalent to estimating the conditional distributions on clean data, which is\ntime-invariant. Consequently, the time t need not be provided as input to the parametric model.\n\nAlthough the development of masked diffusion is nontrivial, the implementation is straightforward. We first introduce the\nmask predictor, a parametric model p\u03b8(\u00b7|xt) (e.g., a Transformer without a causal mask), which takes xt for any t as input\nand predict all masked tokens simultaneously. Then, we define the model distribution p\u03b8(x0) as follows: starting with x1 as\na sequence of fully masked tokens, we simulate an approximate reverse process parameterized by p\u03b8(\u00b7|xt) from t = 1 to 0.\nThe marginal distribution induced at t = 0 then represents the model distribution p\u03b8(x0).\n\n15\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 5 Low-confidence Remasking Strategy of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: for i\u2190 1 to L do\n5: if rit \u0338= M then\n6: ri0 = rit, c\n\ni = 1\n7: else\n8: ri0 = argmaxri0 p\u03b8(r\n\ni\n0|p0, rt)\n\n9: ci = p\u03b8(r\ni\n0|p0, rt)ri0\n\n10: end if\n11: end for\n12: nun = \u230aL(1\u2212 s)\u230b # the number of unmasked tokens is nun in timestep s\n13: for i\u2190 1 to L do\n14: if ci \u2208 Lowest\u2212 nun\n\n(\n{ci}L1\n\n)\nthen\n\n15: ri0 = M # the nun positions with the least confidence are selected for remasking.\n16: end if\n17: end for\n18: rs = r0\n19: end for\n20: Return r0\n\nFormally, the mask predictor is trained using a cross-entropy loss with masking:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (12)\n\nwhere x0 is sampled from the training data, t is sampled uniformly from [0, 1], and xt is sampled from qt|0(xt|x0). The\nindicator function 1[\u00b7] ensures that the cross-entropy loss is computed only for masked tokens. In Ou et al. (2024), it has\nbeen proven that the loss function L(\u03b8) is an upper bound on the negative log-likelihood of the model distribution:\n\n\u2212Ex0\u223cpdata(x0)\n[log p\u03b8(x0)] \u2264 L(\u03b8). (13)\n\nIn summary, this principled approach trains a generative model by progressively masking tokens during a forward process\nand learning to recover the data distribution during a reverse process, all under the (approximate) maximum likelihood\nestimation framework.\n\nA.2. Inference\n\nThe cross-entropy loss in Eq. (12) has several equivalent forms (Ou et al., 2024). The first one is given by\n\n\u2212El\u223c{1,2,...,L},x0,xl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[xi\nl = M] log p\u03b8(x\n\ni\n0|xl)\n\n]\n, (14)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and xl is obtained by uniformly sampling l tokens from x0 without\nreplacement for masking. Despite masking exactly l tokens is different from masking each token independently with\nprobability t, these two masking methods lead to equivalent results in expectation (Ou et al., 2024).\n\nWhile Eq. (12) and Eq. (14) share the same expectation, their variances differ. Intuitively, in Eq. (12), we expect xt to\nhave a fraction of t tokens masked. However, the randomness of the forward process (i.e., Eq. (7)) often causes deviations,\nespecially when xt contains few tokens. In contrast, in Eq. (14), the fraction of masked tokens in xl is deterministically l\n\nL .\nWhile a theoretical analysis depends on the data distribution, empirical results show that Eq. (12) requires over 1000 Monte\nCarlo estimates for stable results, whereas Eq. (14) achieves stability with only 128 estimates. In addition, we can simply\nmodify Eq. (14) to its conditional version (i.e., Eq. (6)) based on Eq. (5).\n\n16\n\n\n\nLarge Language Diffusion Models\n\nPrompt Block 0\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nBlock 1 Block 2\n\nNon-mask token\n\nMask token\n\nFigure 4. A Conceptual Overview of the Semi-autoregressive Sampling.\n\nAny-order autoregressive models (AO-ARM) (Uria et al., 2014; Hoogeboom et al., 2021a; Shih et al., 2022) characterize the\njoint distribution autoregressively for all possible orders \u03c0 of the L variables. To learn such a distribution, an AO-ARM\nutilizes a weight-sharing neural network to model all univariate conditionals and employs mask tokens to represent absent\nvariables. During training, the expected negative log-likelihood over the uniform distribution of all orders U\u03c0 is minimized:\n\n\u2212Ex0,\u03c0\u223cU\u03c0\n\n[\nL\u2211\n\ni=1\n\nlog p\u03b8(x\n\u03c0(i)\n0 |x\u03c0(<i)\n\n0 ;\u03c0)\n\n]\n. (15)\n\nIntuitively, x\u03c0(<i)\n0 can be understood as a masked token xt with index in \u03c0(\u2265 i) being masked. It can be further proved that\n\nEq. (15) is equivalent to Eq. (12). This connection explains the bidirectional reasoning capabilities of LLaDA, even though\nit was never used explicitly in the inference procedure.\n\nNie et al. (2024) introduce unsupervised classifier-free guidance, a plug-and-play technique that balances alignment with\nprompts and text diversity. Specifically, unsupervised classifier-free guidance employs the following modified mask predictor\nfor inference:\n\np\u0303\u03b8(r0|p0, rt) \u221d\np\u03b8(r0|p0, rt)1+w\n\np\u03b8(r0|m, rt)w\n, (16)\n\nwhere m is a mask sequence of the same length as p0 and w is a hyperparameter that controls the strength of p0. We employ\nunsupervised classifier-free guidance in the downstream tasks, as detailed in Appendix B.5.\n\nA.3. Algorithms\n\nIn this section, we present the training and inference algorithms. Specifically, we introduce the pre-training and supervised\nfine-tuning algorithms in Algorithm 1 and Algorithm 2, respectively. In addition, the likelihood evaluation algorithm is\nprovided in Algorithm 3. Finally, we present the sampling algorithms in Algorithm 4 and Algorithm 5, which correspond to\nthe original reverse process (i.e., random remasking) and the low-confidence remasking strategy (Chang et al., 2022) of\nLLaDA, respectively.\n\nFurthermore, we apply a semi-autoregressive remasking strategy for the instruct model. Specifically, the sequence is\ndivided into several blocks and generated from left to right. Within each block, we apply the random remasking or the\nlow-confidence remasking strategy for sampling. For clarity and better visualization, we illustrate the semi-autoregressive\nsampling process in Fig. 4.\n\nOur semi-autoregressive remasking strategy is inspired by the prior work of Anonymous (2025). However, it is important to\nnote that Anonymous (2025) and LLaDA have entirely different training objectives and network architectures. Specifically,\nthe pre-training objective of Anonymous (2025) differs from Eq. (3). Moreover, their approach incorporates a causal mask\nin the Transformer attention block, whereas LLaDA employs a bidirectional Transformer.\n\n17\n\n\n\nLarge Language Diffusion Models\n\nTable 5. Model Architecture. We provide the configurations of our 1B and 7B ARM baselines, as well as the 1B and 8B LLaDA and the\n8B LLaMA3 models.\n\nOur ARM Baseline 1B LLaDA 1B Our ARM Baseline 7B LLaDA 8B LLaMA3 8B\n\nLayers 22 22 28 32 32\n\nModel dimension 2048 2048 4096 4096 4096\n\nAttention heads 32 32 32 32 32\n\nVocabulary size 126,464 126,464 126,464 126,464 128,000\n\nFFN dimension 5634 5634 13,440 12,288 14,336\n\nKey/Value heads 4 4 8 32 8\n\nTotal parameters 1.49 B 1.49 B 6.83 B 8.02 B 8.03 B\n\nNon-embedding parameters 0.97 B 0.97 B 5.80 B 6.98 B 6.98 B\n\nB. Experiments\nB.1. Details of SFT Data\n\nA dynamic sequence length is employed during supervised SFT, where |EOS| tokens are appended to the end of short pairs\nto ensure equal sequence lengths across all data in each mini-batch. Notably, the padding |EOS| tokens are treated as part of\nthe response, i.e., masked and included in the training objective. The |EOS| tokens are removed from the generated outputs\nduring sampling. This strategy is crucial for LLaDA and ensures that the model learns to control the length of its responses\nby generating |EOS|, enabling the response length to align effectively with the given prompt.\n\nIn addition, for n-turn dialogues (p00, r\n0\n0, p\n\n1\n0, r\n\n1\n0, . . . , p\n\nn\u22121\n0 , rn\u22121\n\n0 ), we treat it as n single-turn dialogue pairs, i.e.,\n(p00, r\n\n0\n0), (p\n\n0\n0r\n\n0\n0p\n\n1\n0, r\n\n1\n0), . . . , (p\n\n0\n0r\n\n0\n0p\n\n1\n0r\n\n1\n0 . . . p\n\nn\u22121\n0 , rn\u22121\n\n0 ) and randomly sample one. This data partitioning strategy not only\nequips LLaDA with multi-turn dialogue capabilities but also aligns with the padding strategy.\n\nB.2. Architectures and Scaling Experiments\n\nThis section provides detailed information about the models we trained and the scalability experiments described in Sec. 3.1.\n\nFirstly, for efficiency, we trained an ARM and a MDM, both with 1.5B parameters and identical architectures. Additionally,\nwe scaled the MDM to 8B parameters. Due to computational resource constraints, we did not train an 8B autoregressive\nmodel with the same architecture. Instead, we utilized our previously trained 7B autoregressive model for comparison.\nThese four models are utilized in the scalability analysis in Sec. 3.1.\n\nWe adopted a Transformer architecture similar to LLaMA (Touvron et al., 2023; Dubey et al., 2024) for the ARMs and MDMs\nwe trained. Specifically, we employ RMSNorm (Zhang & Sennrich, 2019) to stabilize training, used SwiGLU (Shazeer,\n2020) as the activation function to enhance non-linearity, and integrate RoPE (Su et al., 2024) for more expressive positional\nencoding. Tab. 5 provides an overview of the model architectures.\n\nAdditionally, we employ the widely used 6ND formulation (Kaplan et al., 2020; Hoffmann et al., 2022) to calculate the\ntraining FLOPs in Fig. 3, where N represents the number of non-embedding parameters, and D denotes the total number of\ntraining tokens.\n\nB.3. Details and Ablation on Remasking\n\nThis section first explains the inference methods in detail, followed by ablation studies on various remasking strategies.\n\nFor instruct model, since our training includes a large number of |EOS| tokens, directly applying the lowest confidence\nremasking strategy would result in an excessively high and unnatural occurrence of |EOS| tokens in the generated sentences.\nTherefore, we divide the entire generation length into several blocks. Each block is generated from left to right, and within\neach block, we apply the reverse process to perform sampling. However, for the base model, since |EOS| tokens are not\npadded during training, using the semi-autoregressive remasking strategy makes little difference in the results.\n\n18\n\n\n\nLarge Language Diffusion Models\n\nTable 6. Ablation on Remasking. We use GSM8K accuracy as the evaluation metric. The lowest-confidence & semi-autoregressive\nremasking strategy refers to generating each block from left to right, while applying the lowest-confidence remasking strategy within each\nblock.\n\nLLaDA 8B Base LLaDA 8B Instruct\n\nRandomly remasking 52.3 72.0\nLowest confidence remasking 64.7 12.9\nLowest confidence & semi-autoregressive remasking 64.4 73.8\n\nTable 7. Ablation on Generated Length. We use the GSM8K accuracy as the evaluation metric and set the number of sampling steps to\nhalf of the generated length.\n\nLength LLaDA 8B Base LLaDA 8B Instruct\n\n256 62.5 75.3\n512 64.7 73.8\n1024 65.9 75.3\n\nFor efficiency, we set the answer length to 512 and the number of sampling steps to 256 in our ablation studies. For the\nsemi-autoregressive remasking strategy, we set the block length to 32. To simplify the evaluation, we use GSM8K accuracy\nas the performance metric. As shown in Table 6, for the base model, applying the lowest confidence remasking strategy\nsignificantly improves performance, whereas the semi-autoregressive remasking strategy does not yield any noticeable\nbenefits. In contrast, for the instruct model, directly applying the lowest confidence remasking strategy leads to a substantial\nperformance drop. However, combining the lowest confidence remasking strategy with the semi-autoregressive remasking\nstrategy results in the best performance for the instruct model.\n\nWe provide the specific remasking strategies and hyperparameters used for each benchmark in Appendix B.5.\n\nB.4. Ablation on Generated Length\n\nIn this section, we conduct ablation studies on the generated length. For simplicity, we use GSM8K as the evaluation metric.\nTo ensure efficiency and fairness, for each setting, we set the number of sampling steps to half of the generated length,\nensuring that in each sampling step, two tokens are transferred from the mask to the text.\n\nFor the LLaDA 8B Base model, we employ the lowest-confidence remasking strategy (i.e., Algorithm 5). For the LLaDA\n8B Instruct model, we adopt semi-autoregressive sampling with a block length of 32.\n\nAs reported in Tab. 7, both the Base model and the Instruct model exhibit minimal sensitivity to the length hyperparameter.\n\nB.5. Standard Benchmarks and Evaluation\n\nIn this section, we briefly introduce the benchmarks used in our study and describe the details of our evaluation process.\n\nFollowing standard LLM (Chu et al., 2024; Yang et al., 2024) evaluation practices, we assess LLaDA across four dimensions:\n\nGeneral ability: MMLU (Hendrycks et al., 2020), BBH (Suzgun et al., 2022), ARC-C (Clark et al., 2018), Hel-\nlaswag (Zellers et al., 2019), TruthfulQA (Lin et al., 2021), WinoGrande (Sakaguchi et al., 2021) and PIQA (Bisk\net al., 2020).\n\nMath and science ability: GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021) and GPQA (Rein et al., 2023).\n\nCode generation: HumanEval (Chen et al., 2021), HumanEval-FIM (Bavarian et al., 2022) and MBPP (Austin et al.,\n2021b).\n\nChinese understanding: CMMLU (Li et al., 2023) and C-Eval (Huang et al., 2024a).\n\nFor all the aforementioned benchmarks, we follow the widely adopted evaluation process (Gao et al., 2024) used in LLM\nassessments, primarily employing conditional likelihood estimation and conditional generation. Specifically, for certain\nbenchmarks, a prompt and multiple candidate answers are provided, and the model is required to compute each candidate\u2019s\n\n19\n\n\n\nLarge Language Diffusion Models\n\nTable 8. Inference Configuration for LLaDA 8B Instruct. We apply the low-confidence remasking strategy across all datasets. A block\nlength smaller than the answer length indicates the use of the semi-autoregressive remasking strategy; otherwise, it is not applied. The\nshorter answer lengths for MMLU and HellaSwag result from their requirement to generate only a single token (i.e., A, B, C, or D) and\ntwo special end tokens (i.e., end of the model response and end of the sentence).\n\nAnswer length Block length\n\nMMLU 3 3\nHellaswag 3 3\nGPQA 128 64\nGSM8K 256 8\nHumanEval 512 32\nMBPP 512 32\nMath 256 256\nMMLU-pro 256 256\nARC-C 512 512\n\nTable 9. Comparison on iGSM Dataset.\n\n4 steps 5 steps 6 steps\n\nLLaMA3 8B Base 38.0 35.0 34.0\nLLaDA 8B Base 64.0 41.0 44.0\n\nconditional likelihood. The candidate with the highest likelihood is then selected as the model\u2019s final answer, and accuracy\nis used as the evaluation metric. For the remaining benchmarks, the model generates responses based on the given prompt,\nand performance is evaluated using metrics such as exact match and other relevant criteria.\n\nFor the base model, we use conditional likelihood estimation for MMLU, CMMLU, C-Eval, ARC-C, Hellaswag, TruthfulQA,\nWinoGrande, PIQA, and GPQA, while the remaining benchmarks are evaluated using conditional generation. For the\ninstruct model, we evaluate all benchmarks using conditional generation. For datasets evaluated with conditional likelihood\nestimation, we adopt the widely used lm-evaluation-harness (Gao et al., 2024) framework. However, for datasets evaluated\nwith conditional generation, we rely on an internal library for evaluation, as lm-evaluation-harness lacks implementations\nfor certain key metrics (e.g., HumanEval-FIM).\n\nFor benchmarks evaluated using conditional likelihood estimation, we use Monte Carlo estimation to approximate Eq. (6)\nfor conditional likelihood estimation. Since MMLU, CMMLU, and C-EVAL only require the likelihood of a single token, a\nsingle Monte Carlo estimate is sufficient for these benchmarks. For all other benchmarks, we find that 128 Monte Carlo\nsamples are adequate to produce stable results. For ARC-C, Hellaswag, TruthfulQA, WinoGrande, PIQA, and GPQA, we\nreport the results with the optimal unsupervised classifier-free guidance scale searched in {0, 0.5, 1, 1.5, 2}.\n\nFor benchmarks evaluated using conditional generation, we apply the lowest confidence remasking strategy to the base model,\nsetting both the response length and the number of sampling steps to 1024. For the instruct model, we set the sampling steps\nequal to the answer length and provide the inference hyperparameters in Table 8. Due to computational resource constraints,\nwe do not use the unsupervised classifier-free guidance for these benchmarks evaluated using conditional generation.\n\nB.6. Analysis of Sampling Steps\n\nIn this section, we analyze the impact of sampling steps. For simplicity, we use the LLaDA 8B Base model and evaluate it\non the GSM8K and HumanEval datasets. Additionally, we fix the answer length at 1024. As shown in Fig. 5, as the number\nof sampling steps increases to match the answer length, the model\u2019s accuracy improves accordingly.\n\nB.7. Evaluation on iGSM Dataset\n\nTo further assess the mathematical capabilities of LLaDA, we test its performance on iGSM (Ye et al., 2024), an infinite,\nsynthetic GSM8K-like dataset. iGSM is generated via specific rules, with parameters that control the difficulty of problems\n(i.e., the number of solution steps). For evaluation consistency, we append \u201d#### $answer\u201d to the final solution, adhering to\n\n20\n\n\n\nLarge Language Diffusion Models\n\n64 128 256 512 1,024\nNFEs\n\n20\n\n40\n\n60\nG\n\nSM\n8K\n\n (4\n-s\n\nho\nt)\n\n64 128 256 512 1,024\nNFEs\n\n10\n\n20\n\n30\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt)\n\nFigure 5. Analysis of Sampling Steps.\n\nthe GSM8K format. Below is an example with solution steps set to 4:\n\n(Question) The number of each North Star Elementary\u2019s Cultural Studies Classroom equals 1. The number of each\nWestridge Elementary\u2019s Dance Studio equals 3 times as much as the sum of each North Star Elementary\u2019s Classroom\nand each North Star Elementary\u2019s Cultural Studies Classroom. How many Dance Studio does Westridge Elementary\nhave?\n(Solution) Define North Star Elementary\u2019s Cultural Studies Classroom as x; so x = 1.\nDefine North Star Elementary\u2019s Classroom as m; so m = x = 1.\nDefine Westridge Elementary\u2019s Dance Studio as n; w = m + x = 1 + 1 = 2;\nso n = 3 * w = 3 * 2 = 1 #### 1\n\nSince there are slight differences between GSM8K and iGSM (e.g., the use of a mod 5 algorithmic system), we follow (Ye\net al., 2024) and provide a system prompt along with four-shot question-answer pairs for each problem.\n\n(Prompt) You\u2019re an expert at solving elementary math problems involving addition, subtraction, and multiplication.\nYou solve all the problems in a uniform format. All calculations are done modulo 5. For example, 4 + 4 equals 3, 2 + 4\nequals 1, 3 + 3 + 3 equals 4, 3 * 3 equals 4, and 2 * 2 equals 4. When providing your solution, please end with \u2019#### x.\u2019\nwhere x is your final answer, an integer between 0 and 4. You must solve all the problems using the same solution\nformat. Our scenarios involve up to four categories of objects: schools, classrooms, backpacks and stationeries. Each\nschool may contain classrooms, each classroom may contain backpacks, and each backpack may contain stationeries.\nWe can specify quantities, such as t\u0308he number of dance studios at each Lakeshore High.A\u0308ssume that every entity\nwith the same name has an identical configuration; for example, each Lakeshore High contains the same number of\ndance studios. Another guiding principle is that what is not mentioned does not exist: when we refer to classrooms at\nLakeshore High, we are only discussing the classrooms explicitly mentioned in our scenario. Furthermore, if Lakeshore\nHigh is not even mentioned, any classroom within it is automatically considered to be non-existent (i.e. 0).\n\nFor solution steps ranging from 4 to 6, we generate 100 questions for each case and report the corresponding accuracy in\nTable 9. As shown in the table, LLaDA Base demonstrates significant and consistent advantages over LLaMA3 on unseen\nmathematical problems, aligning with the results in Table 1.\n\nB.8. Poem Completion Tasks\n\nIn this section, we present examples from our poem completion dataset as follows.\n\nExample 1:\nPrompt: \u7a88\u7a95\u6dd1\u5973\u7684\u4e0b\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u541b\u5b50\u597d\u9011\u3002\n\n21\n\n\n\nLarge Language Diffusion Models\n\nTable 10. Visualization of the Semi-Autoregressive Sampling Process. In the response of LLaDA, darker colors indicate tokens\npredicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser What are the benefits of regular exercise for physical and mental health?\nLLaDA\n\nTable 11. Visualization of the Multi-turn Dialogue. We employ random remasking strategy. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser I want to have a pet, but I don\u2019t know whether to get a cat or a dog. Do you have any\nsuggestions?\n\nLLaDA\n\nUser I\u2019m very busy at work, so it seems like a cat would be a more suitable option. Do you have\nany suggestions for getting a cat?\n\nLLaDA\n\nExample 2:\nPrompt: \u4e0d\u62d8\u4e00\u683c\u964d\u4eba\u624d\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u6211\u529d\u5929\u516c\u91cd\u6296\u64de\u3002\n\nB.9. More Case Studies\n\nIn this section, we present additional case studies of LLaDA 8B Instruct. First, Tab. 10 illustrates the sampling process of\nsemi-autoregressive remasking, while Tab. 11 depicts the sampling process for multi-turn dialogues with random remasking.\nAdditionally, Tab. 12 and Tab. 13 provide further examples of single-turn and multi-turn dialogues. Finally, Tab. 14 presents\nexamples of poem reversal completions where the LLaDA 8B Instruct model succeeds, in contrast to the failure of GPT-4o.\n\n22\n\n\n\nLarge Language Diffusion Models\n\nTable 12. Single-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser What\u2019s a piece of advice you\u2019ve received that you\u2019ve found helpful?\nLLaDA One helpful piece of advice I\u2019ve received is to always be open to learning and growth.\n\nI\u2019ve found that being willing to explore new ideas and experiences has helped me become\nmore resilient and adaptable. By embracing a growth mindset, I\u2019ve been able to identify\nopportunities for improvement and development, both personally and professionally.\n\nDaily life\nUser Please recommend me three famous movies\nLLaDA Certainly! Here are three famous movies that have achieved widespread popularity and\n\ncritical acclaim:\n1. **The Shawshank Redemption** (1994) - Inspired by a novella and directed by Frank\nDarabont, this film tells the story of Andy Dufresne, a man convicted of murdering his\nwife, dealing with his harsh life in Shawshank Prison and forming an unlikely friendship\nwith a fellow inmate.\n2. **The Godfather** (1972) - Directed by Francis Ford Coppola, this film is based\non Mario Puzo\u2019s novel of the same name. It follows the story of the Corleone family,\nparticularly Michael Corleone, who rises to power and takes over the family\u2019s criminal\nempire.\n3. **The Empire Strikes Back** (1980) - Directed by George Lucas, this film continues\nthe story of a young Skywalker in the Star Wars saga. It follows Luke Skywalker as\nhe helps the Rebel Alliance fight against the Galactic Empire and destroy the powerful\nweapon, the Death Star.\nAll of these films are celebrated for their cinematic achievements, powerful storytelling,\nand memorable characters.\n\nTranslation\nUser Please help me translate into Chinese: \u2018What is now proved was once only imagined\u2019\nLLaDA \u73b0\u5728\u88ab\u8bc1\u660e\u7684\u66fe\u7ecf\u53ea\u662f\u60f3\u8c61\n\nTranslation\nUser Please help me translate into German: \u2018What is now proved was once only imagined\u2019\nLLaDA Wer heute besta\u0308tigt ist, war einst nur immaginiert.\n\nMath\nUser Emma decides to run 4 sprints 5 times a week. She runs 75 meters each sprint. How many\n\ntotal meters does she run a week?\nLLaDA Emma runs 4 sprints each time, so she runs a total of 4 x 5 = 20 sprints a week. Each sprint\n\nis 75 meters long, so Emma runs a total of 20 x 75 = 1500 meters a week. The final result\nis 1500\n\nCode\nUser I am writing python code, I have an array, please help me sort it from largest to smallest\nLLaDA Certainly! You can sort an array from the largest to smallest in Python using the \u2018sort()\u2019\n\nmethod or the \u2018sorted()\u2018 function. Here\u2019s an example:\n### Using \u2018sort()\u2019 method:\nmy array = [3, 1, 4, 1, 5, 2]\nmy array.sort(reverse=True)\nprint(my array)\n\n23\n\n\n\nLarge Language Diffusion Models\n\nTable 13. Multi-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser How do I prepare for a job interview?\nLLaDA To prepare for a job interview, you should research the company and the job, practice\n\ncommon interview questions, and dress appropriately.\nUser What are some common interview questions?\nLLaDA Some common interview questions include \u201cTell me about yourself\u201d and \u201cWhy do you\n\nwant to work for us\u201d?\n\nDaily life\nUser How do I start learning programming?\nLLaDA To learn programming, you can start by choosing a programming language that you are\n\ninterested in. Some popular languages include Python, Java, and C++.\nUser Which programming language should I learn first?\nLLaDA It depends on your interests and goals. Python is a great language for beginners for its\n\nsimplicity and versatility.\nUser How long does it take to learn Python?\nLLaDA It can take anywhere from a few weeks to several months to learn Python.\nUser What are some good Python projects for beginners?\nLLaDA Some good Python projects for beginners include building a simple calculator, a game, and\n\na web application.\n\nMath\nUser Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.\n\nHow many clips did Natalia sell altogether in April and May?\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n\n48/2 = 24 clips in May. Altogether, Natalia sold 48 + 24 = 72 clips in April and May. The\nfinal result is 72.\n\nUser Natalia sold three times as many items in June as she did in May. How many clips did\nNatalia sell altogether in these three months?\n\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May,\nso she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in\nthese three months. The final result is 144.\n\nTable 14. Poem Reversal Completion Cases where LLaDA 8B Instruct Succeeds but GPT-4o Fails.\n\nQuestion1\nUser \u2018\u4f46\u95fb\u4eba\u8bed\u54cd\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\u201d,\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bcd\u4eba\u674e\u6e05\u7167\u7684\uff08\u6b66\u9675\n\n\u6625\uff0c\u6625\u665a:\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\uff0c\u4f46\u95fb\u4eba\u8bed\u54cd\u3002\u8fd9\u9996\u8bcd\u8868\u8fbe\u4e86\u4f5c\u8005\u5728\u6625\u5929\u65f6\u8282\u7684\u5b64\u72ec\u4e0e\n\u611f\u4f24\uff0c\u501f\u82b1\u843d\u548c\u4eba\u8bed\u58f0\u7684\u63cf\u5199\uff0c\u53cd\u6620\u51fa\u8bcd\u4eba\u5185\u5fc3\u7684\u65e0\u4eb2\u4e0e\u5931\u843d\u3002\n\nLLaDA \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u4e0d\u89c1\u4eba\u201d\n\nQuestion2\nUser \u2018\u591c\u9759\u6625\u5c71\u7a7a\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u65b0\u96e8\u540e\u201d.\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bd7\u4eba\u738b\u7ef4\u7684\uff08\u5c71\u5c45\u79cb\n\n\u819c\uff09:\u7a7a\u5c71\u65b0\u96e8\u540e\uff0c\u591c\u9759\u6625\u5c71\u7a7a\u3002\u8fd9\u9996\u8bd7\u63cf\u7ed8\u4e86\u6625\u5929\u5c71\u4e2d\u96e8\u540e\u7684\u5bc2\u9759\u4e0e\u7a7a\u7075\uff0c\n\u8868\u73b0\u51fa\u81ea\u7136\u7684\u5b81\u9759\u4e0e\u548c\u8c10\u7f8e\u3002\n\nLLaDA \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u4eba\u95f2\u6842\u82b1\u843d\u201d\n\n24","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}}],"content_references":[{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":118,"end_idx":135,"alt":null,"type":"file","name":"2502.09992v2.pdf","id":"file-EjEr99nXVn1FWaMNbWUohS","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nLarge Language Diffusion Models\n\n\nLarge Language Diffusion Models\n\nShen Nie 1 * \u2020 Fengqi Zhu 1 * \u2020 Zebin You 1 \u2020 Xiaolu Zhang 2 \u2021 Jingyang Ou 1 Jun Hu 2 \u2021 Jun Zhou 2\n\nYankai Lin 1 \u2021 Ji-Rong Wen 1 Chongxuan Li 1 \u2021 \u00b6\n\nAbstract\nAutoregressive models (ARMs) are widely re-\ngarded as the cornerstone of large language mod-\nels (LLMs). We challenge this notion by intro-\nducing LLaDA, a diffusion model trained from\nscratch under the pre-training and supervised fine-\ntuning (SFT) paradigm. LLaDA models distri-\nbutions through a forward data masking process\nand a reverse process, parameterized by a vanilla\nTransformer to predict masked tokens. By op-\ntimizing a likelihood bound, it provides a prin-\ncipled generative approach for probabilistic in-\nference. Across extensive benchmarks, LLaDA\ndemonstrates strong scalability, outperforming\nour self-constructed ARM baselines. Remark-\nably, LLaDA 8B is competitive with strong LLMs\nlike LLaMA3 8B in in-context learning and, af-\nter SFT, exhibits impressive instruction-following\nabilities in case studies such as multi-turn dia-\nlogue. Moreover, LLaDA addresses the rever-\nsal curse, surpassing GPT-4o in a reversal poem\ncompletion task. Our findings establish diffu-\nsion models as a viable and promising alternative\nto ARMs, challenging the assumption that key\nLLM capabilities discussed above are inherently\ntied to ARMs. Project page and codes: https:\n//ml-gsai.github.io/LLaDA-demo/.\n\n1. Introduction\nWhat is now proved was once only imagined.\nplaceholder,placeholder,placeho\u2014William Blake\n\nLarge language models (LLMs) (Zhao et al., 2023) fall en-\ntirely within the framework of generative modeling. Specifi-\ncally, LLMs aim to capture the true but unknown language\n\n*Equal contribution \u2020Work done during an internship at Ant\nGroup \u2021Project leaders 1Gaoling School of Artificial Intelli-\ngence, Renmin University of China; Beijing Key Laboratory\nof Big Data Management and Analysis Methods 2Ant Group.\n\u00b6Correspondence to: Chongxuan Li <chongxuanli@ruc.edu.cn>.\n\nPreprint.\n\n46\n\n33\n\n37\n50\n\n19\n\n24\n\n33\n49\n\n50\n\n56\n\n39\n\n45\n60\n\n23\n\n29\n\n40\n59\n\n60\n\n65\n\n46\n\n52\n70\n\n27\n\n34\n\n47\n69\n\n70\n\nMMLU\n\nTruthfulQA\n\nARC-C\nGSM8K\n\nMath\n\nHumanEval\n\nMBPP\nCMMLU\n\nC-Eval\n\nGeneral TasksM\nath\n\nem\nati\n\ncs\n\nCode\n\nChinese\n\nLLaDA 8B Base\nLLaMA 3 8B Base\nLLaMA 2 7B Base\n\nFigure 1. Zero/Few-Shot Benchmarks. We scale LLaDA to an\nunprecedented size of 8B parameters from scratch, achieving com-\npetitive performance with strong LLMs (Dubey et al., 2024).\n\ndistribution pdata(\u00b7) by optimizing a model distribution p\u03b8(\u00b7)\nthrough maximum likelihood estimation, or equivalently KL\ndivergence minimization between the two distributions:\n\nmax\n\u03b8\n\nEpdata(x) log p\u03b8(x)\u21d4 min\n\u03b8\n\nKL(pdata(x)||p\u03b8(x))\ufe38 \ufe37\ufe37 \ufe38\nGenerative modeling principles\n\n. (1)\n\nThe predominant approach relies on the autoregressive mod-\neling (ARM)\u2014commonly referred to as the next-token pre-\ndiction paradigm\u2014to define the model distribution:\n\np\u03b8(x) = p\u03b8(x\n1)\n\nL\u220f\ni=2\n\np\u03b8(x\ni | x1, . . . , xi\u22121)\ufe38 \ufe37\ufe37 \ufe38\n\nAutoregressive formulation\n\n, (2)\n\nwhere x is a sequence of length L, and xi is the i-th token.\n\nThis paradigm has proven remarkably effective (Radford,\n2018; Radford et al., 2019; Brown, 2020; OpenAI, 2022)\nand has become the foundation of current LLMs. Despite\nits widespread adoption, a fundamental question remains\nunanswered: Is the autoregressive paradigm the only viable\npath to achieving the intelligence exhibited by LLMs?\n\n1\n\nar\nX\n\niv\n:2\n\n50\n2.\n\n09\n99\n\n2v\n2 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n8 \nFe\n\nb \n20\n\n25\n\nhttps://ml-gsai.github.io/LLaDA-demo/\nhttps://ml-gsai.github.io/LLaDA-demo/\n\n\nLarge Language Diffusion Models\n\nWe argue that the answer is not a simple \u201cyes\u201d. The key\ninsight overlooked previously is: it is the generative mod-\neling principles (i.e., Eq. (1)), rather than the autoregres-\nsive formulation (i.e., Eq. (2)) itself, that fundamentally\nunderpin the essential properties of LLMs, as detailed be-\nlow. However, certain inherent limitations of LLMs can be\ndirectly traced to their autoregressive nature.\n\nIn particular, we argue that scalability is primarily a con-\nsequence of the interplay between Transformers (Vaswani,\n2017), model and data size, and Fisher consistency1 (Fisher,\n1922) induced by the generative principles in Eq. (1), rather\nthan a unique result of ARM. The success of diffusion trans-\nformers (Bao et al., 2023; Peebles & Xie, 2023) on visual\ndata (Brooks et al., 2024) supports this claim.\n\nFurthermore, the capabilities of instruction-following and\nin-context learning (Brown, 2020) appear to be intrinsic\nproperties of all proper conditional generative models on\nstructurally consistent linguistic tasks, rather than exclusive\nadvantages of ARMs. In addition, while ARMs can be\ninterpreted as a lossless data compressor (Deletang et al.;\nHuang et al., 2024b), any sufficiently expressive probabilis-\ntic model can achieve similar capabilities (Shannon, 1948).\n\nNevertheless, the autoregressive nature of LLMs presents\nnotable challenges. For example, sequential token-by-token\ngeneration incurs high computational costs, and the left-\nto-right modeling limits effectiveness in reversal reasoning\ntasks (Berglund et al., 2023). These inherent limitations\nconstrain LLMs in handling longer and more complex tasks.\n\nMotivated by these insights, we introduce LLaDA, a Large\nLanguage Diffusion with mAsking, to investigate whether\nthe capabilities exhibited by LLMs can emerge from gener-\native modeling principles beyond ARMs in Eq. (2), thereby\naddressing the fundamental question posed earlier. In con-\ntrast to traditional ARMs, LLaDA leverages a masked diffu-\nsion model (MDM) (Austin et al., 2021a; Lou et al., 2023;\nShi et al., 2024; Sahoo et al., 2024; Ou et al., 2024), which\nincorporates a discrete random masking process and trains\na mask predictor to approximate its reverse process. This\ndesign enables LLaDA to construct a model distribution\nwith bidirectional dependencies and optimize a lower bound\nof its log-likelihood, offering an unexplored and principled\nalternative to existing LLMs.\n\nWe adopt the standard pipeline of data preparation, pre-\ntraining, supervised fine-tuning (SFT), and evaluation, scal-\ning LLaDA to an unprecedented language diffusion of size\n8B. In particular, LLaDA 8B was pre-trained from scratch\non 2.3 trillion tokens using 0.13 million H800 GPU hours,\nfollowed by SFT on 4.5 million pairs. Across diverse tasks,\nincluding language understanding, math, code, and Chinese,\n\n1It suggests the ability to recover the true data distribution with\ninfinite data, a sufficiently large network and optimal training.\n\nLLaDA demonstrates the following contributions:\n\nScalability. LLaDA scales effectively up to a computational\nbudget of 1023 FLOPs, achieving comparable results to self-\nconstructed ARM baselines trained on the same data across\nsix tasks, e.g., MMLU and GSM8K.\n\nIn-Context Learning. Remarkably, LLaDA 8B surpasses\nLLaMA2 7B (Touvron et al., 2023) on nearly all 15 standard\nzero/few-shot learning tasks while performing on par with\nLLaMA3 8B (Dubey et al., 2024).\n\nInstruction-Following. LLaDA significantly enhances the\nability to follow instructions after SFT, as demonstrated in\ncase studies such as multi-turn dialogue.\n\nReversal Reasoning. LLaDA effectively breaks the re-\nversal curse (Berglund et al., 2023) with consistent per-\nformance across forward and reversal tasks. Notably, it\noutperforms GPT-4o in a reversal poem completion task.\n\n2. Approach\nIn this section, we introduce the probabilistic formulation2,\nalong with the pre-training, supervised fine-tuning, and in-\nference procedures for LLaDA, as illustrated in Fig. 2.\n\n2.1. Probabilistic Formulation\n\nUnlike ARMs in Eq. (2), LLaDA defines a model distribu-\ntion p\u03b8(x0) through a forward process and a reverse pro-\ncess (Austin et al., 2021a; Ou et al., 2024). The forward\nprocess gradually masks tokens independently in x0 until\nthe sequence is fully masked at t = 1. For t \u2208 (0, 1), the\nsequence xt is partially masked, with each being masked\nwith probability t or remaining unmasked with probability\n1\u2212 t. The reverse process recovers the data distribution by\niteratively predicting masked tokens as t moves from 1 to 0.\n\nThe core of LLaDA is a mask predictor, a parametric model\np\u03b8(\u00b7|xt) that takes xt as input and predicts all masked tokens\n(denoted M) simultaneously. It is trained using a cross-\nentropy loss computed only on the masked tokens:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (3)\n\nwhere x0 is sampled from the training data, t is sampled\nuniformly from [0, 1], and xt is sampled from the forward\nprocess. The indicator function 1[\u00b7] ensures that the loss is\ncomputed only for masked tokens.\n\nOnce trained, we can simulate a reverse process (see Sec. 2.4\nfor details) parameterized by the mask predictor and define\nthe model distribution p\u03b8(x0) as the marginal distribution\n\n2Here, we focus on the approach of LLaDA. A rigorous formu-\nlation of MDM is provided in Appendix A for interested readers.\n\n2\n\n\n\nLarge Language Diffusion Models\n\nMask predictor\n\nMask all tokens independently\n\nMask predictor\n\nPrompt Response\n\nMask predictor\n\nPrompt Response\n\n... \n\n... \n\nA\nn\n\n in\nte\n\nrm\ne\n\nd\nia\n\nte\n s\n\nte\np\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nNon-mask token Random mask\n\nRemaskMask token\n\n(c)(b)(a)\n\nRemask\n\nMask ratio \ud835\udc61 \u223c \ud835\udc48(0,1)\n\nFigure 2. A Conceptual Overview of LLaDA. (a) Pre-training. LLaDA is trained on text with random masks applied independently to all\ntokens at the same ratio t \u223c U [0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion\nprocess from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies.\n\ninduced at t = 0. Notably, the loss in Eq. (3) has been\nproven to be an upper bound on the negative log-likelihood\nof the model distribution (Shi et al., 2024; Ou et al., 2024):\n\n\u2212Epdata(x0) [log p\u03b8(x0)] \u2264 L(\u03b8), (4)\n\nmaking it a principled objective for generative modeling.\n\nNotably, LLaDA employs a masking ratio that varies ran-\ndomly between 0 and 1 while masked language models (De-\nvlin, 2018) use a fixed ratio. The subtly differences have\nsignificant implications, especially at scale: as shown in\nEq. (4), LLaDA is a principled generative model with the\npotential to perform in-context learning naturally, akin to\nLLMs. Moreover, its generative perspective ensures Fisher\nconsistency (Fisher, 1922) in extreme cases, suggesting\nstrong scalability with large data and models.\n\n2.2. Pre-training\n\nLLaDA employs a Transformer (Vaswani, 2017) as the mask\npredictor, whose architecture is similar to existing LLMs.\nHowever, LLaDA does not use a causal mask, as its formu-\nlation allows it to see the entire input for predictions.\n\nWe trained two variants of LLaDA with different sizes: 1\nbillion (B) and 8B. We summarize the model architecture\nof LLaDA 8B and LLaMA3 8B (Dubey et al., 2024) here\nand details are provided in Appendix B.2. We have ensured\nconsistency in most hyperparameters while making several\nnecessary modifications. We use vanilla multi-head atten-\ntion instead of grouped query attention (Ainslie et al., 2023)\nfor simplicity, as LLaDA is incompatible with KV caching,\nresulting in a different number of key and value heads. Con-\nsequently, the attention layer has more parameters, and we\nreduce the FFN dimension to maintain a comparable model\nsize. Additionally, the vocabulary size differs slightly due\nto a tokenizer (Brown, 2020) adapted on our data.\n\nThe LLaDA model is pre-trained on a dataset comprising\n2.3 trillion (T) tokens, adhering to a data protocol that aligns\n\nclosely with existing large language models (LLMs) (Tou-\nvron et al., 2023; Chu et al., 2024), without the incorporation\nof any special techniques. The data are derived from online\ncorpora, with low-quality content filtered through manually\ndesigned rules and LLM-based approaches. Beyond general\ntext, the dataset encompasses high-quality code, math, and\nmultilingual data. The mixing of data sources and domains\nis guided by scaled-down ARMs. The pre-training process\nutilizes a fixed sequence length of 4096 tokens, incurring a\ntotal computational cost of 0.13 million H800 GPU hours,\nsimilar to ARMs of the same scale and dataset size.\n\nFor a training sequence x0, we randomly sample t \u2208 [0, 1],\nmask each token independently with the same probability\nt to obtain xt (see Fig. 2 (a)) and estimate Eq. (3) via the\nMonte Carlo method for stochastic gradient descent training.\nIn addition, following Nie et al. (2024), to enhance the\nability of LLaDA to handle variable-length data, we set 1%\nof the pre-training data to a random length that is uniformly\nsampled from the range [1, 4096].\n\nWe adopted the Warmup-Stable-Decay (Hu et al., 2024)\nlearning rate scheduler to monitor the training progress\nwithout interrupting continuous training. Specifically, we\nlinearly increased the learning rate from 0 to 4\u00d7 10\u22124 over\nthe first 2000 iterations and maintained it at 4\u00d7 10\u22124. After\nprocessing 1.2T tokens, we decayed the learning rate to\n1\u00d7 10\u22124 and held it constant for the next 0.8T tokens to en-\nsure stable training. Finally, we linearly reduced the learning\nrate from 1\u00d710\u22124 to 1\u00d710\u22125 for the last 0.3T tokens. Fur-\nthermore, we utilized the AdamW optimizer (Loshchilov,\n2017) with a weight decay of 0.1, a batch size of 1280, and\na local batch size of 4 per GPU. The 8B experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.3. Supervised Fine-Tuning\n\nWe enhance the capability of LLaDA to follow instructions\nby supervised fine-tuning (SFT) with paired data (p0, r0),\n\n3\n\n\n\nLarge Language Diffusion Models\n\nwhere p0 is the prompt and r0 denotes the response. This\nis the simplest and most basic post-training method for\nLLMs. Technically, this requires to model the conditional\ndistribution p\u03b8(r0|p0) instead of p\u03b8(x0) in pre-training.\n\nThe implementation is similar to pre-training. As shown in\nFig. 2 (b), we leave the prompt unchanged and mask the\ntokens in the response independently, as done for x0. Then,\nwe feed both the prompt and the masked response rt to the\npre-trained mask predictor to compute the loss for SFT:\n\n\u2212Et,p0,r0,rt\n\n\uf8ee\uf8f01\n\nt\n\nL\u2032\u2211\ni=1\n\n1[rit = M] log p\u03b8(r\ni\n0|p0, rt)\n\n\uf8f9\uf8fb , (5)\n\nwhere L\u2032 denotes a dynamic length specified later, and all\nother notations remain the same as before.\n\nNote that this approach is fully compatible with pre-training.\nEssentially, the concatenation of p0 and r0 can be treated\nas clean pre-training data x0, while the concatenation of\np0 and rt serves as the masked version xt. The process is\nidentical to pre-training, with the only difference being that\nall masked tokens happen to appear in the r0 portion.\n\nThe LLaDA 8B model undergoes SFT on a dataset com-\nprising 4.5 million pairs. Consistent with the pre-training\nprocess, both data preparation and training follow the SFT\nprotocols utilized in existing LLMs (Chu et al., 2024; Yang\net al., 2024), without introducing any additional techniques\nto optimize LLaDA\u2019s performance. The dataset spans mul-\ntiple domains, including code, mathematics, instruction-\nfollowing, and structured data understanding. We append\n|EOS| tokens to the end of short pairs in each mini-batch\nto ensure equal lengths across all data. We treat |EOS| as a\nnormal token during training and remove it during sampling,\nenabling LLaDA to control the response length automati-\ncally. Please refer to Appendix B.1 for more details.\n\nWe train for 3 epochs on the SFT data using a similar sched-\nule to the pre-training phase. The learning rate is linearly\nincreased from 0 to 2.5\u00d7 10\u22125 over the first 50 iterations\nand then kept constant. During the final 10% of iterations,\nit is linearly reduced to 2.5 \u00d7 10\u22126. Additionally, we set\nthe weight decay to 0.1, the global batch size to 256, and\nthe local batch size to 2 per GPU. The SFT experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.4. Inference\n\nAs a generative model, LLaDA is capable of both sampling\nnew text and evaluating the likelihood of candidate text.\n\nWe begin with the sampling. As illustrated in Fig. 2 (c),\ngiven a prompt p0, we discretize the reverse process to sam-\nple from the model distribution p\u03b8(r0|p0), starting from a\nfully masked response. The total number of sampling steps\nis a hyperparameter, which naturally provides LLaDA with a\n\ntrade-off between efficiency and sample quality, as analyzed\nin Sec. 3.3. We employ uniformly distributed timesteps by\ndefault. In addition, the generation length is also treated as\na hyperparameter, specifying the length of the fully masked\nsentence at the beginning of the sampling process. As de-\ntailed in Appendix B.4, since both pre-training and SFT\nare conducted using datasets with variable lengths, the final\nresults are insensitive to this length hyperparameter.\n\nAt an intermediate step from time t \u2208 (0, 1] to s \u2208 [0, t), we\nfeed both p0 and rt into the mask predictor and predict all\nmasked tokens simultaneously. Subsequently, we remask s\n\nt\nof the predicted tokens in expectation to obtain rs, ensuring\nthat the transition of the reverse process aligns with the\nforward process for accurate sampling (Austin et al., 2021a).\n\nIn principle, the remasking strategy should be purely ran-\ndom. However, inspired by the annealing tricks of sampling\nin LLMs (Holtzman et al., 2019; Brown, 2020), we explore\ntwo deterministic yet effective remasking strategies. Specif-\nically, similarly to Chang et al. (2022), we remask the s\n\nt\nof predicted tokens with the lowest confidence based on\nthe predictions, called low-confidence remasking. Addition-\nally, for LLaDA after SFT, we can divide the sequence into\nseveral blocks and generate them from left to right, called\nsemi-autoregressive remasking. Within each block, we ap-\nply the reverse process to perform sampling. We provide\nmore details and ablation studies in Appendix. B.3.\n\nFor conditional likelihood evaluation, we can naturally uti-\nlize the upper bound in Eq. (5). However, we find that the\nfollowing equivalent form (Ou et al., 2024) exhibits lower\nvariance and is more stable for evaluation:\n\n\u2212El,r0,rl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[ril = M] log p\u03b8(r\ni\n0|p0, rl)\n\n]\n, (6)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and rl is\nobtained by uniformly sampling l tokens from r0 without\nreplacement for masking. In addition, we employ the unsu-\npervised classifier-free guidance (Nie et al., 2024). We refer\nthe readers to more details in Appendix A.2.\n\nWe present the training, sampling, and likelihood evaluation\nalgorithms, along with theoretical details, in Appendix A.\n\n3. Experiments\nWe evaluate the scalability, instruction-following, and in-\ncontext learning capabilities of LLaDA on standard bench-\nmarks, followed by analyses and case studies on more con-\ntrolled datasets to provide a comprehensive assessment.\n\n3.1. Scalability of LLaDA on Language Tasks\n\nWe first investigate the scalability of LLaDA on downstream\ntasks in comparison with the ARM baselines we constructed.\n\n4\n\n\n\nLarge Language Diffusion Models\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nA\nR\n\nC\n-C\n\n (0\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nC\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt) Autoregressive Baseline\n\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n50\n\n60\n\n70\n\n80\n\nPI\nQ\n\nA\n (0\n\n-s\nho\n\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n20\n\n40\n\n60\n\nG\nSM\n\n8K\n (4\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n8\n\n16\n\n24\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\nFigure 3. Scalability of LLaDA. We evaluate the performance of LLaDA and our ARM baselines trained on the same data across\nincreasing computational FLOPs. LLaDA exhibits strong scalability, matching the overall performance of ARMs on six tasks.\n\nSpecifically, at the 1B scale, we ensured that LLaDA and\nARM shared the same architecture, data, and all other config-\nurations. At larger scales, we also report results for LLaDA\nand ARM models of slightly different sizes trained on the\nsame data due to resource limit, as detailed in Appendix B.2.\nWe use the computational cost as a unified scaling metric.\nFor evaluation, we focused on six standard and diverse tasks.\n\nAs shown in Fig. 3, LLaDA demonstrates impressive scala-\nbility, with its overall trend being highly competitive with\nthat of ARM. Notably, in tasks such as MMLU and GSM8K,\nLLaDA exhibits even stronger scalability. Even on tasks like\nPIQA, where performance lags, LLaDA narrows the gap\nwith ARMs at larger scales. To account for the significant\ninfluence of outliers, we opted not to fit quantitative scaling\ncurves, avoiding potential misinterpretation. Nevertheless,\nthe results clearly demonstrate the scalability of LLaDA.\n\nNie et al. (2024) suggests that MDM requires 16 times\nmore computation than ARM to achieve the same likeli-\nhood. However, there are key differences that make the\nconclusions of this study more broadly applicable. In partic-\nular, likelihood is a relatively indirect metric for downstream\ntask performance, and diffusion optimizes a bound of the\nlikelihood, making it not directly comparable to ARM. Ad-\nditionally, we extended the scaling range from 1018 \u223c 1020\n\nin Nie et al. (2024) to 1020 \u223c 1023 in this work.\n\n3.2. Benchmark Results\n\nTo comprehensively evaluate the in-context learning and\ninstruction-following capabilities of LLaDA 8B, we con-\nducted detailed comparisons with existing LLMs (Touvron\n\net al., 2023; Dubey et al., 2024; Chu et al., 2024; Yang et al.,\n2024; Bi et al., 2024; Jiang et al., 2023) of similar scale.\nThe selection of tasks and evaluation protocols was aligned\nwith existing studies, encompassing 15 popular benchmarks\nin general tasks, mathematics, code, and Chinese. Further\ndetails are provided in Appendix B.5. For a more direct\ncomparison, we re-evaluated representative LLMs (Touvron\net al., 2023; Dubey et al., 2024) in our implementation.\n\nAs shown in Tab. 1, after pretraining on 2.3T tokens,\nLLaDA 8B demonstrates remarkable performance, surpass-\ning LLaMA2 7B on nearly all tasks, and is overall competi-\ntive with LLaMA3 8B. LLaDA shows advantages in math\nand Chinese tasks. We conjecture that the strengths stem\nfrom the same factors as its relatively weaker performance\nin some tasks\u2014differences in data quality and distribution,\nlargely due to the closed-source situation of LLM datasets.\n\nNotably, we have carefully ruled out the possibility of data\nleakage by taking GSM8K as an example. First, as shown\nin Fig. 3, LLaDA outperformed ARM baselines regarding\nGSM8K. Moreover, the conclusion remains on a fully un-\nseen GSM8K-like task (Ye et al., 2024) in Appendix B.7.\n\nFurther, Tab. 2 compares the performance of LLaDA 8B In-\nstruct with existing LLMs. We observed that SFT improved\nLLaDA\u2019s performance on most downstream tasks. A few\nmetrics, such as MMLU, showed declines, and we conjec-\nture may be due to the suboptimal quality of the SFT data.\nOverall, since we did not perform alignment with reinforce-\nment learning (RL), our results are slightly behind LLaMA3\n8B Instruct, though the gaps in many metrics remain small.\nNotably, even with only SFT, LLaDA demonstrates impres-\n\n5\n\n\n\nLarge Language Diffusion Models\n\nTable 1. Benchmark Results of Pre-trained LLMs. \u2217 indicates that LLaDA 8B Base, LLaMA2 7B Base, and LLaMA3 8B Base are\nevaluated under the same protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Chu et al. (2024); Yang et al.\n(2024) and Bi et al. (2024) respectively. The numbers in parentheses represent the number of shots used for evaluation. \u201c-\u201d indicates\nunknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Mistral 7B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T - 2T\n\nGeneral Tasks\n\nMMLU 65.9 (5) 65.4 (5) 45.9 (5) 70.3 (5) 74.2 (5) 64.2 (5) 48.2 (5)\nBBH 49.8 (3) 57.6 (3) 37.3 (3) 62.3 (3) 70.4 (3) 56.1 (3) 39.5 (3)\nARC-C 47.9 (0) 53.1 (0) 46.3 (0) 60.6 (25) 63.7 (25) 60.0 (25) 48.1 (0)\nHellaswag 72.5 (0) 79.1 (0) 76.0 (0) 80.7 (10) 80.2 (10) 83.3 (10) 75.4 (0)\nTruthfulQA 46.4 (0) 44.0 (0) 39.0 (0) 54.2 (0) 56.4 (0) 42.2 (0) -\nWinoGrande 74.8 (5) 77.3 (5) 72.5 (5) 77.0 (5) 75.9 (5) 78.4 (5) 70.5 (0)\nPIQA 74.4 (0) 80.6 (0) 79.1 (0) - - - 79.2 (0)\n\nMathematics & Science\n\nGSM8K 70.7 (4) 53.1 (4) 14.3 (4) 80.2 (4) 85.4 (4) 36.2 (4) 17.4 (8)\nMath 27.3 (4) 15.1 (4) 3.2 (4) 43.5 (4) 49.8 (4) 10.2 (4) 6.0 (4)\nGPQA 26.1 (5) 25.9 (5) 25.7 (5) 30.8 (5) 36.4 (5) 24.7 (5) -\n\nCode\n\nHumanEval 33.5 (0) 34.2 (0) 12.8 (0) 51.2 (0) 57.9 (0) 29.3 (0) 26.2 (0)\nHumanEval-FIM 73.8 (2) 73.3 (2) 26.9 (2) - - - -\nMBPP 38.2 (4) 47.4 (4) 18.4 (4) 64.2 (0) 74.9 (0) 51.1 (0) 39.0 (3)\n\nChinese\n\nCMMLU 69.9 (5) 50.7 (5) 32.5 (5) 83.9 (5) - - 47.2 (5)\nC-Eval 70.5 (5) 51.7 (5) 34.0 (5) 83.2 (5) - - 45.0 (5)\n\nTable 2. Benchmark Results of Post-trained LLMs. LLaDA only employs an SFT procedure while other models have extra reinforcement\nlearning (RL) alignment. \u2217 indicates that LLaDA 8B Instruct, LLaMA2 7B Instruct, and LLaMA3 8B Instruct are evaluated under the\nsame protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Yang et al. (2024) and Bi et al. (2024) respectively.\nThe numbers in parentheses represent the number of shots used for in-context learning. \u201c-\u201d indicates unknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Gemma2 9B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T 8T 2T\n\nPost-training SFT SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL\nAlignment pairs 4.5M - - 0.5M + - 1M + 0.15M - 1.5M + -\n\nGeneral Tasks\n\nMMLU 65.5 (5) 68.4 (5) 44.1 (5) - - - 49.4 (0)\nMMLU-pro 37.0 (0) 41.9 (0) 4.6 (0) 44.1 (5) 56.3 (5) 52.1 (5) -\nHellaswag 74.6 (0) 75.5 (0) 51.5 (0) - - - 68.5 (-)\nARC-C 88.5 (0) 82.4 (0) 57.3 (0) - - - 49.4 (-)\n\nMathematics & Science\n\nGSM8K 78.6 (4) 78.3 (4) 29.0 (4) 85.7 (0) 91.6 (0) 76.7 (0) 63.0 (0)\nMath 26.6 (0) 29.6 (0) 3.8 (0) 52.9 (0) 75.5 (0) 44.3 (0) 15.8 (0)\nGPQA 31.8 (5) 31.9 (5) 28.4 (5) 34.3 (0) 36.4 (0) 32.8 (0) -\n\nCode\n\nHumanEval 47.6 (0) 59.8 (0) 16.5 (0) 79.9 (0) 84.8 (0) 68.9 (0) 48.2 (-)\nMBPP 34.2 (4) 57.6 (4) 20.6 (4) 67.2 (0) 79.2 (0) 74.9 (0) 35.2 (-)\n\n6\n\n\n\nLarge Language Diffusion Models\n\nTable 3. Comparison in the Poem Completion Task.\n\nForward Reversal\n\nGPT-4o (2024-08-06) 82.7 34.3\nQwen2.5 7B Instruct 75.9 38.0\nLLaDA 8B Instruct 48.8 42.4\n\nsive instruction-following abilities, as detailed in Sec. 3.4.\nWe leave RL-based alignment for future work.\n\nOverall, despite the lack of data transparency, we have made\nevery effort to adopt standardized procedures and introduce\ndiverse tasks, we believe they sufficiently demonstrate the\nextraordinary capabilities of LLaDA, which is the only com-\npetitive non-autoregressive model to our knowledge.\n\n3.3. Reversal Reasoning and Analyses\n\nTo quantify the reversal reasoning ability of mod-\nels (Berglund et al., 2023), we follow the protocol estab-\nlished in Allen-Zhu & Li (2023). Specifically, we construct\na dataset of 496 famous Chinese poem sentence pairs. Given\na sentence from a poem, models are tasked with generating\nthe subsequent line (forward) or the preceding line (rever-\nsal) without additional fine-tuning. Examples can be found\nin Appendix B.8. This setting provides a straightforward\nand more realistic evaluation compared to previous stud-\nies (Nie et al., 2024; Kitouni et al., 2024).\n\nAs shown in Tab. 3, LLaDA effectively addresses the rever-\nsal curse (Berglund et al., 2023), demonstrating consistent\nzero-shot performance across both forward and reversal\ntasks. In contrast, both Qwen 2.5 and GPT-4o exhibit a\nsignificant gap between the two. The results on forward\ngeneration confirm that both ARMs are strong, benefiting\nfrom significantly larger datasets and greater computational\nresources than LLaDA. However, LLaDA outperforms both\nby a large margin in the reversal task.\n\nWe emphasize that we did not design anything special for\nreversal tasks. Intuitively, LLaDA treats tokens uniformly\nwithout inductive bias, leading to balanced performance.\nSee more details in Appendix A.2.\n\nWe also analyze the effect of remasking strategies and sam-\npling steps, as detailed in Appendix B.3 and Appendix B.6.\n\n3.4. Case Studies\n\nWe present samples generated by LLaDA 8B Instruct in\nTab. 4, showcasing its instruction-following capabilities.\nFirst, the table illustrates LLaDA\u2019s ability to generate co-\nherent, fluent, and extended text in a non-autoregressive\nmanner. Second, it highlights the model\u2019s multi-turn dia-\nlogue capability, effectively retaining conversation history\n\nand producing contextually appropriate responses across\nmultiple languages. Such chat capabilities of LLaDA are\nimpressive, as it departs from conventional ARMs for the\nfirst time, to the best of our knowledge. See more case\nstudies on remasking and reasoning tasks in Appendix B.9.\n\n4. Related Work\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020) have excelled in visual domains but\nremain unverified for LLMs despite extensive efforts.\n\nA simple approach is to continuousize text data and apply\ndiffusion models directly (Li et al., 2022; Gong et al., 2022;\nHan et al., 2022; Strudel et al., 2022; Chen et al., 2022;\nDieleman et al., 2022; Richemond et al., 2022; Wu et al.,\n2023; Mahabadi et al., 2024; Ye et al., 2023b). Alternatively,\nsome methods model continuous parameters of discrete dis-\ntributions instead (Lou & Ermon, 2023; Graves et al., 2023;\nLin et al., 2023; Xue et al., 2024). However, scalability\nremains a challenge, as a 1B parameter model requires 64\ntimes the compute of an ARM to achieve comparable per-\nformance (Gulrajani & Hashimoto, 2024).\n\nAnother approach replaces continuous diffusion with dis-\ncrete processes featuring new forward and reverse dy-\nnamics (Austin et al., 2021a), leading to numerous vari-\nants (Hoogeboom et al., 2021b;a; He et al., 2022; Campbell\net al., 2022; Meng et al., 2022; Reid et al., 2022; Sun et al.,\n2022; Kitouni et al., 2023; Zheng et al., 2023; Chen et al.,\n2023; Ye et al., 2023a; Gat et al., 2024; Zheng et al., 2024;\nSahoo et al., 2024; Shi et al., 2024). Notably, Lou et al.\n(2023) showed that masked diffusion, as a special case of\ndiscrete diffusion, achieves perplexity comparable to or sur-\npassing ARMs at GPT-2 scale. Ou et al. (2024) established\nfundamental theoretical results, which motivated our model\ndesign, training, and inference (see Appendix A). Nie et al.\n(2024) explored how MDM can be leveraged for language\ntasks such as question answering at GPT-2 scale. Gong et al.\n(2024) fine-tune ARMs in the MDM formulation. However,\nimprovements are confined to certain metrics, and it remains\nunclear whether this approach can yield a foundation model\ncomparable to strong LLMs under a comprehensive evalua-\ntion.\n\nIn comparison, this study scales MDM to an unprecedented\nsize of 8B parameters from scratch, achieving performance\ncomparable to leading LLMs such as LLaMA 3.\n\nAdditionally, a parallel line of work on image genera-\ntion (Chang et al., 2022; 2023) aligns well with the appli-\ncation of MDMs to text data. Moreover, MDMs have also\nshown promise in domains such as protein generation (Wang\net al., 2024b;c), where they have achieved promising results.\nNotably, Kou et al. (2024); Xu et al. (2025) demonstrate the\npotential of using distillation to accelerate MDMs sampling,\n\n7\n\n\n\nLarge Language Diffusion Models\n\nTable 4. Visualization of the Sampling Process and a Generated Multi-round Dialogue. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nSampling Process\n\nUser Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour.\nHow many kilometers can she run in 8 hours?\n\nLLaDA\n\nMulti-round Dialogue Case\n\nUser Could you please share with me what the initial two lines of the renowned poem \u2018The\nRoad Not Taken\u2019?\n\nLLaDA Certainly! The initial two lines of the renowned poem \u201cThe Road Not Taken\u201d by Robert\nFrost are: \u201cTwo roads diverged in a yellow wood, And sorry I could not travel both.\u201d These\nlines set the stage for the poem, introducing the idea of a speaker facing a choice between\ntwo paths.\n\nUser Please help me translate into Chinese.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nChinese as: \u201c\u4e24\u6761\u8def\u5206\u5c94\u5728\u9ec4\u8272\u7684\u6811\u6797\u4e2d\uff0c\u9057\u61be\u6211\u4e0d\u80fd\u540c\u65f6\u8d70\u201d\nUser Please translate into German.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nGerman as: \u201cZwei Wege trennten sich im gelben Wald, und leider konnte ich nicht beide\ntun.\u201d\n\nUser Please also write a poem about life choices. I require 5 sentences, and the first word of\neach sentence starts with C as the first letter.\n\nLLaDA Certainly! Here\u2019s a poem about life choices, with each sentence starting with the letter \u201cC\u201d:\nChoices cascade through life\u2019s journey, Creating connections with each decision, Careful\nconsideration leads to better paths, Courageous choices shape our destiny, Choosing wisely\nguides us through the years.\n\nfurther enhancing their efficiency.\n\n5. Conclusion and Discussion\nIn the middle of difficulty lies opportunity.\nplaceholder,placeholder,placeh\u2014Albert Einstein\n\nWe introduce LLaDA, a principled and previously unex-\nplored approach to large language modeling based on dif-\nfusion models. LLaDA demonstrates strong capabilities in\nscalability, in-context learning, and instruction-following,\nachieving performance comparable to strong LLMs. In addi-\ntion, LLaDA offers unique advantages such as bidirectional\nmodeling and enhanced robustness, effectively addressing\nseveral inherent limitations of existing LLMs. Our findings\nnot only establish diffusion models as a viable and promis-\ning alternative but also challenge the prevailing assumption\nthat these essential capabilities are inherently tied to ARMs.\n\nWhile promising, the full potential of diffusion models re-\n\nmains to be fully explored. Several limitations of this work\npresent significant opportunities for future research.\n\nDue to computational constraints, direct comparisons be-\ntween LLaDA and ARMs\u2014such as training on identical\ndatasets\u2014were restricted to a computational budget of less\nthan 1023 FLOPs. To allocate resources for training the\nlargest possible LLaDA model and showcasing its poten-\ntial, we were unable to scale the ARM baseline to the same\nextent. Moreover, no specialized attention mechanisms\nor position embeddings were designed for LLaDA, nor\nwere any system-level architectural optimizations applied.\nOn the inference side, our exploration of guidance mecha-\nnisms (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) re-\nmains preliminary, and LLaDA currently exhibits sensitivity\nto inference hyperparameters. Furthermore, LLaDA has yet\nto undergo alignment with reinforcement learning (Ouyang\net al., 2022; Rafailov et al., 2024), which is crucial for im-\nproving its performance and alignment with human intent.\n\n8\n\n\n\nLarge Language Diffusion Models\n\nLooking forward, the scale of LLaDA is still smaller than\nleading counterparts (Achiam et al., 2023; Dubey et al.,\n2024; Google, 2024; Anthropic, 2024; Yang et al., 2024;\nLiu et al., 2024), highlighting the need for further scaling\nto fully assess its capabilities. In addition, LLaDA\u2019s abil-\nity to handle multi-modal data remains unexplored. The\nimpact of LLaDA on prompt tuning techniques (Wei et al.,\n2022) and its integration into agent-based systems (Park\net al., 2023; Wang et al., 2024a) is yet to be fully under-\nstood. Finally, a systematic investigation into post-training\nfor LLaDA could facilitate the development of O1-like sys-\ntems (OpenAI, 2024; Guo et al., 2025).\n\nImpact Statement\nThis work challenges the prevailing dominance of autore-\ngressive modeling in LLMs by proposing a diffusion-based\nalternative. Our findings open new avenues for exploring\nalternative probabilistic paradigms in natural language pro-\ncessing, with potential applications in conversational AI,\ncode generation, and complex reasoning tasks.\n\nHowever, diffusion-based models, like traditional LLMs,\nraise similar societal concerns. These include the environ-\nmental impact of large-scale training, the potential misuse\nfor generating harmful or misleading content, and the ampli-\nfication of biases present in training data. Addressing these\nchallenges is critical to ensuring the responsible develop-\nment and deployment of diffusion-based language models.\n\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (No. 92470118); Beijing Natural Sci-\nence Foundation (No. L247030); Beijing Nova Program\n(No. 20220484044); Major Innovation & Planning Interdis-\nciplinary Platform for the \u201cDouble-First Class\u201d Initiative,\nRenmin University of China; the Fundamental Research\nFunds for the Central Universities, the Research Funds of\nRenmin University of China (22XNKJ13); Ant Group Re-\nsearch Fund. The work was partially done at the Engineering\nResearch Center of Next-Generation Intelligent Search and\nRecommendation, Ministry of Education.\n\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\n\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebron, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. In Proceedings of the 2023 Conference on Em-\n\npirical Methods in Natural Language Processing, pp.\n4895\u20134901, 2023.\n\nAllen-Zhu, Z. and Li, Y. Physics of Language Mod-\nels: Part 3.2, Knowledge Manipulation. ArXiv e-prints,\nabs/2309.14402, September 2023. Full version available\nat http://arxiv.org/abs/2309.14402.\n\nAnonymous. Interpolating autoregressive and discrete de-\nnoising diffusion language models. In The Thirteenth\nInternational Conference on Learning Representations,\n2025. URL https://openreview.net/forum?\nid=tyEyYT267x.\n\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.\nanthropic.com/news/claude-3-5-sonnet.\n\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van\nDen Berg, R. Structured denoising diffusion models in\ndiscrete state-spaces. Advances in Neural Information\nProcessing Systems, 34:17981\u201317993, 2021a.\n\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021b.\n\nBao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu,\nJ. All are worth words: A vit backbone for diffusion\nmodels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22669\u2013\n22679, 2023.\n\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M. Efficient training of\nlanguage models to fill in the middle. arXiv preprint\narXiv:2207.14255, 2022.\n\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O. The reversal curse:\nLlms trained on\u201d a is b\u201d fail to learn\u201d b is a\u201d. arXiv\npreprint arXiv:2309.12288, 2023.\n\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\nScaling open-source language models with longtermism.\narXiv preprint arXiv:2401.02954, 2024.\n\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\n2020.\n\nBrooks, T., Peebles, B., Holmes, C., DePue, W.,\nGuo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman,\nT., Luhman, E., Ng, C., Wang, R., and Ramesh,\nA. Video generation models as world simulators.\n2024. URL https://openai.com/research/\nvideo-generation-models-as-world-simulators.\n\n9\n\nhttp://arxiv.org/abs/2309.14402\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://openai.com/research/video-generation-models-as-world-simulators\nhttps://openai.com/research/video-generation-models-as-world-simulators\n\n\nLarge Language Diffusion Models\n\nBrown, T. B. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\n\nCampbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deli-\ngiannidis, G., and Doucet, A. A continuous time frame-\nwork for discrete denoising models. Advances in Neural\nInformation Processing Systems, 35:28266\u201328279, 2022.\n\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,\nW. T. Maskgit: Masked generative image transformer. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11315\u201311325, 2022.\n\nChang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J.,\nJiang, L., Yang, M.-H., Murphy, K., Freeman, W. T.,\nRubinstein, M., et al. Muse: Text-to-image genera-\ntion via masked generative transformers. arXiv preprint\narXiv:2301.00704, 2023.\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\n\nChen, T., Zhang, R., and Hinton, G. Analog bits: Gen-\nerating discrete data using diffusion models with self-\nconditioning. arXiv preprint arXiv:2208.04202, 2022.\n\nChen, Z., Yuan, H., Li, Y., Kou, Y., Zhang, J., and Gu, Q.\nFast sampling via de-randomization for discrete diffusion\nmodels. arXiv preprint arXiv:2312.09193, 2023.\n\nChu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng,\nY., Lv, Y., He, J., Lin, J., et al. Qwen2-audio technical\nreport. arXiv preprint arXiv:2407.10759, 2024.\n\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\n\nDeletang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Ge-\nnewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K.,\nAitchison, M., Orseau, L., et al. Language modeling is\ncompression. In The Twelfth International Conference on\nLearning Representations.\n\nDevlin, J. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021.\n\nDieleman, S., Sartran, L., Roshannai, A., Savinov, N.,\nGanin, Y., Richemond, P. H., Doucet, A., Strudel, R.,\nDyer, C., Durkan, C., et al. Continuous diffusion for\ncategorical data. arXiv preprint arXiv:2211.15089, 2022.\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nFisher, R. A. On the mathematical foundations of theoretical\nstatistics. Philosophical transactions of the Royal Society\nof London. Series A, containing papers of a mathematical\nor physical character, 222(594-604):309\u2013368, 1922.\n\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n07 2024. URL https://zenodo.org/records/\n12608602.\n\nGat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Syn-\nnaeve, G., Adi, Y., and Lipman, Y. Discrete flow match-\ning. arXiv preprint arXiv:2407.15595, 2024.\n\nGong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq:\nSequence to sequence text generation with diffusion mod-\nels. arXiv preprint arXiv:2210.08933, 2022.\n\nGong, S., Agarwal, S., Zhang, Y., Ye, J., Zheng, L., Li, M.,\nAn, C., Zhao, P., Bi, W., Han, J., et al. Scaling diffu-\nsion language models via adaptation from autoregressive\nmodels. arXiv preprint arXiv:2410.17891, 2024.\n\nGoogle. Our next-generation model: Gemini 1.5, 2024.\nURL https://blog.google/technology/ai/\ngoogle-gemini-next-generation-model-february-2024.\n\nGraves, A., Srivastava, R. K., Atkinson, T., and\nGomez, F. Bayesian flow networks. arXiv preprint\narXiv:2308.07037, 2023.\n\nGulrajani, I. and Hashimoto, T. B. Likelihood-based diffu-\nsion language models. Advances in Neural Information\nProcessing Systems, 36, 2024.\n\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\n\nHan, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semi-\nautoregressive simplex-based diffusion language model\nfor text generation and modular control. arXiv preprint\narXiv:2210.17432, 2022.\n\n10\n\nhttps://zenodo.org/records/12608602\nhttps://zenodo.org/records/12608602\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\n\n\nLarge Language Diffusion Models\n\nHe, Z., Sun, T., Wang, K., Huang, X., and Qiu, X. Diffu-\nsionbert: Improving generative masked language models\nwith diffusion models. arXiv preprint arXiv:2211.15029,\n2022.\n\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J. Measuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\n\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\n\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840\u20136851, 2020.\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\n\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The\ncurious case of neural text degeneration. arXiv preprint\narXiv:1904.09751, 2019.\n\nHoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B.,\nBerg, R. v. d., and Salimans, T. Autoregressive diffusion\nmodels. arXiv preprint arXiv:2110.02037, 2021a.\n\nHoogeboom, E., Nielsen, D., Jaini, P., Forre\u0301, P., and Welling,\nM. Argmax flows and multinomial diffusion: Learning\ncategorical distributions. Advances in Neural Information\nProcessing Systems, 34:12454\u201312465, 2021b.\n\nHu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z.,\nFang, Y., Huang, Y., Zhao, W., et al. Minicpm: Unveiling\nthe potential of small language models with scalable train-\ning strategies. arXiv preprint arXiv:2404.06395, 2024.\n\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu,\nJ., Lv, C., Zhang, Y., Fu, Y., et al. C-eval: A multi-level\nmulti-discipline chinese evaluation suite for foundation\nmodels. Advances in Neural Information Processing\nSystems, 36, 2024a.\n\nHuang, Y., Zhang, J., Shan, Z., and He, J. Compres-\nsion represents intelligence linearly. arXiv preprint\narXiv:2404.09937, 2024b.\n\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\n\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\n\nKitouni, O., Nolte, N., Hensman, J., and Mitra, B. Disk: A\ndiffusion model for structured knowledge. arXiv preprint\narXiv:2312.05253, 2023.\n\nKitouni, O., Nolte, N., Bouchacourt, D., Williams, A., Rab-\nbat, M., and Ibrahim, M. The factorization curse: Which\ntokens you predict underlie the reversal curse and more.\narXiv preprint arXiv:2406.05183, 2024.\n\nKou, S., Hu, L., He, Z., Deng, Z., and Zhang, H. Cllms:\nConsistency large language models. arXiv preprint\narXiv:2403.00835, 2024.\n\nLi, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y.,\nDuan, N., and Baldwin, T. Cmmlu: Measuring mas-\nsive multitask language understanding in chinese. arXiv\npreprint arXiv:2306.09212, 2023.\n\nLi, X., Thickstun, J., Gulrajani, I., Liang, P. S., and\nHashimoto, T. B. Diffusion-lm improves controllable\ntext generation. Advances in Neural Information Process-\ning Systems, 35:4328\u20134343, 2022.\n\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\n\nLin, Z., Gong, Y., Shen, Y., Wu, T., Fan, Z., Lin, C., Duan,\nN., and Chen, W. Text generation with diffusion language\nmodels: A pre-training approach with continuous para-\ngraph denoise. In International Conference on Machine\nLearning, pp. 21051\u201321064. PMLR, 2023.\n\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,\nC., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437, 2024.\n\nLoshchilov, I. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017.\n\nLou, A. and Ermon, S. Reflected diffusion models, 2023.\n\nLou, A., Meng, C., and Ermon, S. Discrete diffusion lan-\nguage modeling by estimating the ratios of the data distri-\nbution. arXiv preprint arXiv:2310.16834, 2023.\n\nMahabadi, R. K., Ivison, H., Tae, J., Henderson, J., Beltagy,\nI., Peters, M. E., and Cohan, A. Tess: Text-to-text self-\nconditioned simplex diffusion, 2024.\n\n11\n\n\n\nLarge Language Diffusion Models\n\nMeng, C., Choi, K., Song, J., and Ermon, S. Concrete score\nmatching: Generalized score matching for discrete data.\nAdvances in Neural Information Processing Systems, 35:\n34532\u201334545, 2022.\n\nNie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M.,\nand Li, C. Scaling up masked diffusion models on text.\narXiv preprint arXiv:2410.18514, 2024.\n\nOpenAI. ChatGPT: Optimizing Language Models for Di-\nalogue. OpenAI blog, November 2022. URL https:\n//openai.com/blog/chatgpt/.\n\nOpenAI. Learning to reason with llms, 2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n\nOu, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li,\nC. Your absorbing discrete diffusion secretly models the\nconditional distributions of clean data. arXiv preprint\narXiv:2406.03736, 2024.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems, 35:27730\u201327744, 2022.\n\nPark, J. S., O\u2019Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and\ntechnology, pp. 1\u201322, 2023.\n\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pp. 4195\u20134205,\n2023.\n\nRadford, A. Improving language understanding by genera-\ntive pre-training, 2018.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\n\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\n\nReid, M., Hellendoorn, V. J., and Neubig, G. Diffuser:\nDiscrete diffusion via edit-based reconstruction, 2022.\n\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang,\nR. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa:\nA graduate-level google-proof q&a benchmark. arXiv\npreprint arXiv:2311.12022, 2023.\n\nRichemond, P. H., Dieleman, S., and Doucet, A. Categorical\nsdes with simplex diffusion, 2022.\n\nSahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marro-\nquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. Simple\nand effective masked diffusion language models. arXiv\npreprint arXiv:2406.07524, 2024.\n\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\n\nShannon, C. E. A mathematical theory of communication.\nThe Bell system technical journal, 27(3):379\u2013423, 1948.\n\nShazeer, N. Glu variants improve transformer. arXiv\npreprint arXiv:2002.05202, 2020.\n\nShi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. K.\nSimplified and generalized masked diffusion for discrete\ndata. arXiv preprint arXiv:2406.04329, 2024.\n\nShih, A., Sadigh, D., and Ermon, S. Training and infer-\nence on any-order autoregressive models the right way.\nIn Proceedings of the 31th International Conference on\nMachine Learning, 2022.\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256\u20132265. PMLR, 2015.\n\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\n\nStrudel, R., Tallec, C., Altche\u0301, F., Du, Y., Ganin, Y., Men-\nsch, A., Grathwohl, W., Savinov, N., Dieleman, S., Sifre,\nL., et al. Self-conditioned embedding diffusion for text\ngeneration. arXiv preprint arXiv:2211.04236, 2022.\n\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, 568:127063, 2024.\n\nSun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Score-\nbased continuous-time discrete diffusion models. arXiv\npreprint arXiv:2211.16750, 2022.\n\nSuzgun, M., Scales, N., Scha\u0308rli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\n\n12\n\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/index/learning-to-reason-with-llms/\nhttps://openai.com/index/learning-to-reason-with-llms/\n\n\nLarge Language Diffusion Models\n\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n\nUria, B., Murray, I., and Larochelle, H. A deep and tractable\ndensity estimator. In Proceedings of the 31th Interna-\ntional Conference on Machine Learning, 2014.\n\nVaswani, A. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J.,\nChen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on\nlarge language model based autonomous agents. Frontiers\nof Computer Science, 18(6):186345, 2024a.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDiffusion language models are versatile protein learners.\narXiv preprint arXiv:2402.18567, 2024b.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDplm-2: A multimodal diffusion protein language model.\narXiv preprint arXiv:2410.13782, 2024c.\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\nelicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824\u201324837,\n2022.\n\nWu, T., Fan, Z., Liu, X., Gong, Y., Shen, Y., Jiao, J., Zheng,\nH.-T., Li, J., Wei, Z., Guo, J., Duan, N., and Chen, W.\nAr-diffusion: Auto-regressive diffusion model for text\ngeneration, 2023.\n\nXu, C., Wang, X., Liao, Z., Li, Y., Hou, T., and Deng,\nZ. Show-o turbo: Towards accelerated unified multi-\nmodal understanding and generation. arXiv preprint\narXiv:2502.05415, 2025.\n\nXue, K., Zhou, Y., Nie, S., Min, X., Zhang, X., Zhou, J.,\nand Li, C. Unifying bayesian flow networks and diffusion\nmodels through stochastic differential equations. arXiv\npreprint arXiv:2404.15766, 2024.\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B.,\nLi, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu,\nJ., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang,\nK., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M.,\nZhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren,\nX., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y.,\nCui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\narXiv preprint arXiv:2412.15115, 2024.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Gu, Q. Diffusion\nlanguage models can perform many tasks with scaling and\n\ninstruction-finetuning. arXiv preprint arXiv:2308.12219,\n2023a.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Wang, M. Dinoiser:\nDiffused conditional sequence learning by manipulating\nnoises. arXiv preprint arXiv:2302.10025, 2023b.\n\nYe, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics of Lan-\nguage Models: Part 2.1, Grade-School Math and the Hid-\nden Reasoning Process. ArXiv e-prints, abs/2407.20311,\nJuly 2024. Full version available at http://arxiv.\norg/abs/2407.20311.\n\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\n\nZhang, B. and Sennrich, R. Root mean square layer nor-\nmalization. Advances in Neural Information Processing\nSystems, 32, 2019.\n\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,\nMin, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of\nlarge language models. arXiv preprint arXiv:2303.18223,\n2023.\n\nZheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and\nZhang, Q. Masked diffusion models are secretly time-\nagnostic masked models and exploit inaccurate categor-\nical sampling, 2024. URL https://arxiv.org/\nabs/2409.02908.\n\nZheng, L., Yuan, J., Yu, L., and Kong, L. A reparameter-\nized discrete diffusion model for text generation. ArXiv,\nabs/2302.05737, 2023.\n\n13\n\nhttp://arxiv.org/abs/2407.20311\nhttp://arxiv.org/abs/2407.20311\nhttps://arxiv.org/abs/2409.02908\nhttps://arxiv.org/abs/2409.02908\n\n\nLarge Language Diffusion Models\n\nAlgorithm 1 Pre-training of LLaDA\n\nRequire: mask predictor p\u03b8, data distribution pdata\n1: repeat\n2: x0 \u223c pdata, t \u223c U(0, 1] # with a probability of 1%, the sequence length of x0 follows U[1, 4096]\n3: xt \u223c qt|0(xt|x0) # qt|0 is defined in Eq. (7)\n4: Calculate L = \u2212 1\n\nt\u2217L\n\u2211L\n\ni=1 1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt) # L is the sequence length of x0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 2 Supervised Fine-Tuning of LLaDA\n\nRequire: mask predictor p\u03b8, pair data distribution pdata\n1: repeat\n2: p0, r0 \u223c pdata, t \u223c U(0, 1] # please refer to Appendix B.1 for details on the SFT data processing.\n3: rt \u223c qt|0(rt|r0) # qt|0 is defined in Eq. (7)\n\n4: Calculate L = \u2212 1\nt\u2217L\u2032\n\n\u2211L\u2032\n\ni=1 1[rit = M] log p\u03b8(r\ni\n0|p0, rt) # L\u2032 is the sequence length of r0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 3 Conditional Log-likelihood Evaluation of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, response r0, the number of Monte Carlo estimations nmc\n\n1: log likelihood = 0\n2: for i\u2190 1 to nmc do\n3: l \u223c {1, 2, . . . , L} # L is the sequence length of r0\n4: Obtain rl by uniformly sampling l tokens from r0 without replacement for masking\n5: log likelihood = log likelihood + L\n\nl\n\n\u2211L\ni=1 1[ril = M] log p\u03b8(r\n\ni\n0|p0, rl)\n\n6: end for\n7: log likelihood = log likelihood/nmc\n\n8: Return log likelihood\n\nA. Formulation of Masked Diffusion Models\n\nA.1. Training\n\nMasked diffusion models (MDMs) (Austin et al., 2021a; Lou et al., 2023; Ou et al., 2024) define the model distribution\np\u03b8(x0) in a manner distinct from autoregressive models.\n\nThese models introduce a forward process {xt} indexed by a time t \u2208 [0, 1]. This process gradually and independently\nmasks all tokens in the sequence x0. At time t = 0, the data point x0 is fully observed with no masks, while for t \u2208 (0, 1],\nxt represents latent variables with varying mask ratios in expectation.\n\nFormally, the conditional distribution of xt given x0 is defined by a fully factorized form:\n\nqt|0(xt|x0) =\n\nL\u220f\ni=1\n\nqt|0(x\ni\nt|xi\n\n0), (7)\n\nwhere the conditional distribution for each token is given by:\n\nqt|0(x\ni\nt|xi\n\n0) =\n\n{\n1\u2212 t, xi\n\nt = xi\n0,\n\nt, xi\nt = M.\n\n(8)\n\nHere, M denotes the mask token. Intuitively, each token either remains unchanged or is masked, with the probability of\n\n14\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 4 Reverse Process of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: r0 = argmaxr0 p\u03b8(r0|p0, rt) # we employ greedy sampling when predicting masked tokens\n5: for i\u2190 1 to L do\n6: if rt \u0338= M then\n7: ri0 = rit\n8: else\n9: With probability s\n\nt , ri0 is set to M\n10: end if\n11: end for\n12: rs = r0\n13: end for\n14: Return r0\n\nbeing masked increasing linearly as t progresses from 0 to 1. At t = 1, all tokens are guaranteed to be masked, meaning that\nx1 follows a Dirac distribution concentrated on a sequence of fully masked tokens. Notably, the linear masking probability\nis analogous to but distinct from, the noise schedule in continuous diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020). This linearity is motivated by the assumption that the information in the text is proportional to the\nnumber of tokens on average, making it reasonable to lose information linearly during the forward process.\n\nThe forward process is not only reversible but also corresponds to a reverse process that is fully factorized across all\ntokens (Austin et al., 2021a). The reverse process, from time t = 1 to 0, generates new data from sequences of fully masked\ntokens. The conditional distribution for the reverse process, for 0 \u2264 s < t \u2264 1, is factorized as:\n\nqs|t(xs|xt) =\n\nL\u220f\ni=1\n\nqs|t(x\ni\ns|xt), (9)\n\nwhere the conditional distribution for each token is:\n\nqs|t(x\ni\ns|xt) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n1, xi\n\nt \u0338= M, xi\ns = xi\n\nt,\ns\nt , xi\n\nt = M, xi\ns = M,\n\nt\u2212s\nt q0|t(x\n\ni\ns|xt), xi\n\nt = M, xi\ns \u0338= M,\n\n0, otherwise.\n\n(10)\n\nThus, the key function to estimate is the conditional distribution q0|t(x\ni\ns|xt), which predicts the original token if it is masked\n\nin the input xt. This is analogous to the data prediction form in continuous diffusion models.\n\nAs proven in (Ou et al., 2024), an equivalent yet time-free parameterization can be derived as:\n\nq0|t(x\ni\ns|xt) = pdata(x\n\ni\n0|xUM\n\nt ), \u2200i such that xi\nt = M, (11)\n\nwhere xUM\nt denotes the collection of unmasked tokens in xt, which is identical to the corresponding tokens in the original\n\ndata x0 since unmasked tokens are solely determined by x0 and are independent of time t. Intuitively, this implies that\nestimating the data prediction function is equivalent to estimating the conditional distributions on clean data, which is\ntime-invariant. Consequently, the time t need not be provided as input to the parametric model.\n\nAlthough the development of masked diffusion is nontrivial, the implementation is straightforward. We first introduce the\nmask predictor, a parametric model p\u03b8(\u00b7|xt) (e.g., a Transformer without a causal mask), which takes xt for any t as input\nand predict all masked tokens simultaneously. Then, we define the model distribution p\u03b8(x0) as follows: starting with x1 as\na sequence of fully masked tokens, we simulate an approximate reverse process parameterized by p\u03b8(\u00b7|xt) from t = 1 to 0.\nThe marginal distribution induced at t = 0 then represents the model distribution p\u03b8(x0).\n\n15\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 5 Low-confidence Remasking Strategy of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: for i\u2190 1 to L do\n5: if rit \u0338= M then\n6: ri0 = rit, c\n\ni = 1\n7: else\n8: ri0 = argmaxri0 p\u03b8(r\n\ni\n0|p0, rt)\n\n9: ci = p\u03b8(r\ni\n0|p0, rt)ri0\n\n10: end if\n11: end for\n12: nun = \u230aL(1\u2212 s)\u230b # the number of unmasked tokens is nun in timestep s\n13: for i\u2190 1 to L do\n14: if ci \u2208 Lowest\u2212 nun\n\n(\n{ci}L1\n\n)\nthen\n\n15: ri0 = M # the nun positions with the least confidence are selected for remasking.\n16: end if\n17: end for\n18: rs = r0\n19: end for\n20: Return r0\n\nFormally, the mask predictor is trained using a cross-entropy loss with masking:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (12)\n\nwhere x0 is sampled from the training data, t is sampled uniformly from [0, 1], and xt is sampled from qt|0(xt|x0). The\nindicator function 1[\u00b7] ensures that the cross-entropy loss is computed only for masked tokens. In Ou et al. (2024), it has\nbeen proven that the loss function L(\u03b8) is an upper bound on the negative log-likelihood of the model distribution:\n\n\u2212Ex0\u223cpdata(x0)\n[log p\u03b8(x0)] \u2264 L(\u03b8). (13)\n\nIn summary, this principled approach trains a generative model by progressively masking tokens during a forward process\nand learning to recover the data distribution during a reverse process, all under the (approximate) maximum likelihood\nestimation framework.\n\nA.2. Inference\n\nThe cross-entropy loss in Eq. (12) has several equivalent forms (Ou et al., 2024). The first one is given by\n\n\u2212El\u223c{1,2,...,L},x0,xl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[xi\nl = M] log p\u03b8(x\n\ni\n0|xl)\n\n]\n, (14)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and xl is obtained by uniformly sampling l tokens from x0 without\nreplacement for masking. Despite masking exactly l tokens is different from masking each token independently with\nprobability t, these two masking methods lead to equivalent results in expectation (Ou et al., 2024).\n\nWhile Eq. (12) and Eq. (14) share the same expectation, their variances differ. Intuitively, in Eq. (12), we expect xt to\nhave a fraction of t tokens masked. However, the randomness of the forward process (i.e., Eq. (7)) often causes deviations,\nespecially when xt contains few tokens. In contrast, in Eq. (14), the fraction of masked tokens in xl is deterministically l\n\nL .\nWhile a theoretical analysis depends on the data distribution, empirical results show that Eq. (12) requires over 1000 Monte\nCarlo estimates for stable results, whereas Eq. (14) achieves stability with only 128 estimates. In addition, we can simply\nmodify Eq. (14) to its conditional version (i.e., Eq. (6)) based on Eq. (5).\n\n16\n\n\n\nLarge Language Diffusion Models\n\nPrompt Block 0\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nBlock 1 Block 2\n\nNon-mask token\n\nMask token\n\nFigure 4. A Conceptual Overview of the Semi-autoregressive Sampling.\n\nAny-order autoregressive models (AO-ARM) (Uria et al., 2014; Hoogeboom et al., 2021a; Shih et al., 2022) characterize the\njoint distribution autoregressively for all possible orders \u03c0 of the L variables. To learn such a distribution, an AO-ARM\nutilizes a weight-sharing neural network to model all univariate conditionals and employs mask tokens to represent absent\nvariables. During training, the expected negative log-likelihood over the uniform distribution of all orders U\u03c0 is minimized:\n\n\u2212Ex0,\u03c0\u223cU\u03c0\n\n[\nL\u2211\n\ni=1\n\nlog p\u03b8(x\n\u03c0(i)\n0 |x\u03c0(<i)\n\n0 ;\u03c0)\n\n]\n. (15)\n\nIntuitively, x\u03c0(<i)\n0 can be understood as a masked token xt with index in \u03c0(\u2265 i) being masked. It can be further proved that\n\nEq. (15) is equivalent to Eq. (12). This connection explains the bidirectional reasoning capabilities of LLaDA, even though\nit was never used explicitly in the inference procedure.\n\nNie et al. (2024) introduce unsupervised classifier-free guidance, a plug-and-play technique that balances alignment with\nprompts and text diversity. Specifically, unsupervised classifier-free guidance employs the following modified mask predictor\nfor inference:\n\np\u0303\u03b8(r0|p0, rt) \u221d\np\u03b8(r0|p0, rt)1+w\n\np\u03b8(r0|m, rt)w\n, (16)\n\nwhere m is a mask sequence of the same length as p0 and w is a hyperparameter that controls the strength of p0. We employ\nunsupervised classifier-free guidance in the downstream tasks, as detailed in Appendix B.5.\n\nA.3. Algorithms\n\nIn this section, we present the training and inference algorithms. Specifically, we introduce the pre-training and supervised\nfine-tuning algorithms in Algorithm 1 and Algorithm 2, respectively. In addition, the likelihood evaluation algorithm is\nprovided in Algorithm 3. Finally, we present the sampling algorithms in Algorithm 4 and Algorithm 5, which correspond to\nthe original reverse process (i.e., random remasking) and the low-confidence remasking strategy (Chang et al., 2022) of\nLLaDA, respectively.\n\nFurthermore, we apply a semi-autoregressive remasking strategy for the instruct model. Specifically, the sequence is\ndivided into several blocks and generated from left to right. Within each block, we apply the random remasking or the\nlow-confidence remasking strategy for sampling. For clarity and better visualization, we illustrate the semi-autoregressive\nsampling process in Fig. 4.\n\nOur semi-autoregressive remasking strategy is inspired by the prior work of Anonymous (2025). However, it is important to\nnote that Anonymous (2025) and LLaDA have entirely different training objectives and network architectures. Specifically,\nthe pre-training objective of Anonymous (2025) differs from Eq. (3). Moreover, their approach incorporates a causal mask\nin the Transformer attention block, whereas LLaDA employs a bidirectional Transformer.\n\n17\n\n\n\nLarge Language Diffusion Models\n\nTable 5. Model Architecture. We provide the configurations of our 1B and 7B ARM baselines, as well as the 1B and 8B LLaDA and the\n8B LLaMA3 models.\n\nOur ARM Baseline 1B LLaDA 1B Our ARM Baseline 7B LLaDA 8B LLaMA3 8B\n\nLayers 22 22 28 32 32\n\nModel dimension 2048 2048 4096 4096 4096\n\nAttention heads 32 32 32 32 32\n\nVocabulary size 126,464 126,464 126,464 126,464 128,000\n\nFFN dimension 5634 5634 13,440 12,288 14,336\n\nKey/Value heads 4 4 8 32 8\n\nTotal parameters 1.49 B 1.49 B 6.83 B 8.02 B 8.03 B\n\nNon-embedding parameters 0.97 B 0.97 B 5.80 B 6.98 B 6.98 B\n\nB. Experiments\nB.1. Details of SFT Data\n\nA dynamic sequence length is employed during supervised SFT, where |EOS| tokens are appended to the end of short pairs\nto ensure equal sequence lengths across all data in each mini-batch. Notably, the padding |EOS| tokens are treated as part of\nthe response, i.e., masked and included in the training objective. The |EOS| tokens are removed from the generated outputs\nduring sampling. This strategy is crucial for LLaDA and ensures that the model learns to control the length of its responses\nby generating |EOS|, enabling the response length to align effectively with the given prompt.\n\nIn addition, for n-turn dialogues (p00, r\n0\n0, p\n\n1\n0, r\n\n1\n0, . . . , p\n\nn\u22121\n0 , rn\u22121\n\n0 ), we treat it as n single-turn dialogue pairs, i.e.,\n(p00, r\n\n0\n0), (p\n\n0\n0r\n\n0\n0p\n\n1\n0, r\n\n1\n0), . . . , (p\n\n0\n0r\n\n0\n0p\n\n1\n0r\n\n1\n0 . . . p\n\nn\u22121\n0 , rn\u22121\n\n0 ) and randomly sample one. This data partitioning strategy not only\nequips LLaDA with multi-turn dialogue capabilities but also aligns with the padding strategy.\n\nB.2. Architectures and Scaling Experiments\n\nThis section provides detailed information about the models we trained and the scalability experiments described in Sec. 3.1.\n\nFirstly, for efficiency, we trained an ARM and a MDM, both with 1.5B parameters and identical architectures. Additionally,\nwe scaled the MDM to 8B parameters. Due to computational resource constraints, we did not train an 8B autoregressive\nmodel with the same architecture. Instead, we utilized our previously trained 7B autoregressive model for comparison.\nThese four models are utilized in the scalability analysis in Sec. 3.1.\n\nWe adopted a Transformer architecture similar to LLaMA (Touvron et al., 2023; Dubey et al., 2024) for the ARMs and MDMs\nwe trained. Specifically, we employ RMSNorm (Zhang & Sennrich, 2019) to stabilize training, used SwiGLU (Shazeer,\n2020) as the activation function to enhance non-linearity, and integrate RoPE (Su et al., 2024) for more expressive positional\nencoding. Tab. 5 provides an overview of the model architectures.\n\nAdditionally, we employ the widely used 6ND formulation (Kaplan et al., 2020; Hoffmann et al., 2022) to calculate the\ntraining FLOPs in Fig. 3, where N represents the number of non-embedding parameters, and D denotes the total number of\ntraining tokens.\n\nB.3. Details and Ablation on Remasking\n\nThis section first explains the inference methods in detail, followed by ablation studies on various remasking strategies.\n\nFor instruct model, since our training includes a large number of |EOS| tokens, directly applying the lowest confidence\nremasking strategy would result in an excessively high and unnatural occurrence of |EOS| tokens in the generated sentences.\nTherefore, we divide the entire generation length into several blocks. Each block is generated from left to right, and within\neach block, we apply the reverse process to perform sampling. However, for the base model, since |EOS| tokens are not\npadded during training, using the semi-autoregressive remasking strategy makes little difference in the results.\n\n18\n\n\n\nLarge Language Diffusion Models\n\nTable 6. Ablation on Remasking. We use GSM8K accuracy as the evaluation metric. The lowest-confidence & semi-autoregressive\nremasking strategy refers to generating each block from left to right, while applying the lowest-confidence remasking strategy within each\nblock.\n\nLLaDA 8B Base LLaDA 8B Instruct\n\nRandomly remasking 52.3 72.0\nLowest confidence remasking 64.7 12.9\nLowest confidence & semi-autoregressive remasking 64.4 73.8\n\nTable 7. Ablation on Generated Length. We use the GSM8K accuracy as the evaluation metric and set the number of sampling steps to\nhalf of the generated length.\n\nLength LLaDA 8B Base LLaDA 8B Instruct\n\n256 62.5 75.3\n512 64.7 73.8\n1024 65.9 75.3\n\nFor efficiency, we set the answer length to 512 and the number of sampling steps to 256 in our ablation studies. For the\nsemi-autoregressive remasking strategy, we set the block length to 32. To simplify the evaluation, we use GSM8K accuracy\nas the performance metric. As shown in Table 6, for the base model, applying the lowest confidence remasking strategy\nsignificantly improves performance, whereas the semi-autoregressive remasking strategy does not yield any noticeable\nbenefits. In contrast, for the instruct model, directly applying the lowest confidence remasking strategy leads to a substantial\nperformance drop. However, combining the lowest confidence remasking strategy with the semi-autoregressive remasking\nstrategy results in the best performance for the instruct model.\n\nWe provide the specific remasking strategies and hyperparameters used for each benchmark in Appendix B.5.\n\nB.4. Ablation on Generated Length\n\nIn this section, we conduct ablation studies on the generated length. For simplicity, we use GSM8K as the evaluation metric.\nTo ensure efficiency and fairness, for each setting, we set the number of sampling steps to half of the generated length,\nensuring that in each sampling step, two tokens are transferred from the mask to the text.\n\nFor the LLaDA 8B Base model, we employ the lowest-confidence remasking strategy (i.e., Algorithm 5). For the LLaDA\n8B Instruct model, we adopt semi-autoregressive sampling with a block length of 32.\n\nAs reported in Tab. 7, both the Base model and the Instruct model exhibit minimal sensitivity to the length hyperparameter.\n\nB.5. Standard Benchmarks and Evaluation\n\nIn this section, we briefly introduce the benchmarks used in our study and describe the details of our evaluation process.\n\nFollowing standard LLM (Chu et al., 2024; Yang et al., 2024) evaluation practices, we assess LLaDA across four dimensions:\n\nGeneral ability: MMLU (Hendrycks et al., 2020), BBH (Suzgun et al., 2022), ARC-C (Clark et al., 2018), Hel-\nlaswag (Zellers et al., 2019), TruthfulQA (Lin et al., 2021), WinoGrande (Sakaguchi et al., 2021) and PIQA (Bisk\net al., 2020).\n\nMath and science ability: GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021) and GPQA (Rein et al., 2023).\n\nCode generation: HumanEval (Chen et al., 2021), HumanEval-FIM (Bavarian et al., 2022) and MBPP (Austin et al.,\n2021b).\n\nChinese understanding: CMMLU (Li et al., 2023) and C-Eval (Huang et al., 2024a).\n\nFor all the aforementioned benchmarks, we follow the widely adopted evaluation process (Gao et al., 2024) used in LLM\nassessments, primarily employing conditional likelihood estimation and conditional generation. Specifically, for certain\nbenchmarks, a prompt and multiple candidate answers are provided, and the model is required to compute each candidate\u2019s\n\n19\n\n\n\nLarge Language Diffusion Models\n\nTable 8. Inference Configuration for LLaDA 8B Instruct. We apply the low-confidence remasking strategy across all datasets. A block\nlength smaller than the answer length indicates the use of the semi-autoregressive remasking strategy; otherwise, it is not applied. The\nshorter answer lengths for MMLU and HellaSwag result from their requirement to generate only a single token (i.e., A, B, C, or D) and\ntwo special end tokens (i.e., end of the model response and end of the sentence).\n\nAnswer length Block length\n\nMMLU 3 3\nHellaswag 3 3\nGPQA 128 64\nGSM8K 256 8\nHumanEval 512 32\nMBPP 512 32\nMath 256 256\nMMLU-pro 256 256\nARC-C 512 512\n\nTable 9. Comparison on iGSM Dataset.\n\n4 steps 5 steps 6 steps\n\nLLaMA3 8B Base 38.0 35.0 34.0\nLLaDA 8B Base 64.0 41.0 44.0\n\nconditional likelihood. The candidate with the highest likelihood is then selected as the model\u2019s final answer, and accuracy\nis used as the evaluation metric. For the remaining benchmarks, the model generates responses based on the given prompt,\nand performance is evaluated using metrics such as exact match and other relevant criteria.\n\nFor the base model, we use conditional likelihood estimation for MMLU, CMMLU, C-Eval, ARC-C, Hellaswag, TruthfulQA,\nWinoGrande, PIQA, and GPQA, while the remaining benchmarks are evaluated using conditional generation. For the\ninstruct model, we evaluate all benchmarks using conditional generation. For datasets evaluated with conditional likelihood\nestimation, we adopt the widely used lm-evaluation-harness (Gao et al., 2024) framework. However, for datasets evaluated\nwith conditional generation, we rely on an internal library for evaluation, as lm-evaluation-harness lacks implementations\nfor certain key metrics (e.g., HumanEval-FIM).\n\nFor benchmarks evaluated using conditional likelihood estimation, we use Monte Carlo estimation to approximate Eq. (6)\nfor conditional likelihood estimation. Since MMLU, CMMLU, and C-EVAL only require the likelihood of a single token, a\nsingle Monte Carlo estimate is sufficient for these benchmarks. For all other benchmarks, we find that 128 Monte Carlo\nsamples are adequate to produce stable results. For ARC-C, Hellaswag, TruthfulQA, WinoGrande, PIQA, and GPQA, we\nreport the results with the optimal unsupervised classifier-free guidance scale searched in {0, 0.5, 1, 1.5, 2}.\n\nFor benchmarks evaluated using conditional generation, we apply the lowest confidence remasking strategy to the base model,\nsetting both the response length and the number of sampling steps to 1024. For the instruct model, we set the sampling steps\nequal to the answer length and provide the inference hyperparameters in Table 8. Due to computational resource constraints,\nwe do not use the unsupervised classifier-free guidance for these benchmarks evaluated using conditional generation.\n\nB.6. Analysis of Sampling Steps\n\nIn this section, we analyze the impact of sampling steps. For simplicity, we use the LLaDA 8B Base model and evaluate it\non the GSM8K and HumanEval datasets. Additionally, we fix the answer length at 1024. As shown in Fig. 5, as the number\nof sampling steps increases to match the answer length, the model\u2019s accuracy improves accordingly.\n\nB.7. Evaluation on iGSM Dataset\n\nTo further assess the mathematical capabilities of LLaDA, we test its performance on iGSM (Ye et al., 2024), an infinite,\nsynthetic GSM8K-like dataset. iGSM is generated via specific rules, with parameters that control the difficulty of problems\n(i.e., the number of solution steps). For evaluation consistency, we append \u201d#### $answer\u201d to the final solution, adhering to\n\n20\n\n\n\nLarge Language Diffusion Models\n\n64 128 256 512 1,024\nNFEs\n\n20\n\n40\n\n60\nG\n\nSM\n8K\n\n (4\n-s\n\nho\nt)\n\n64 128 256 512 1,024\nNFEs\n\n10\n\n20\n\n30\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt)\n\nFigure 5. Analysis of Sampling Steps.\n\nthe GSM8K format. Below is an example with solution steps set to 4:\n\n(Question) The number of each North Star Elementary\u2019s Cultural Studies Classroom equals 1. The number of each\nWestridge Elementary\u2019s Dance Studio equals 3 times as much as the sum of each North Star Elementary\u2019s Classroom\nand each North Star Elementary\u2019s Cultural Studies Classroom. How many Dance Studio does Westridge Elementary\nhave?\n(Solution) Define North Star Elementary\u2019s Cultural Studies Classroom as x; so x = 1.\nDefine North Star Elementary\u2019s Classroom as m; so m = x = 1.\nDefine Westridge Elementary\u2019s Dance Studio as n; w = m + x = 1 + 1 = 2;\nso n = 3 * w = 3 * 2 = 1 #### 1\n\nSince there are slight differences between GSM8K and iGSM (e.g., the use of a mod 5 algorithmic system), we follow (Ye\net al., 2024) and provide a system prompt along with four-shot question-answer pairs for each problem.\n\n(Prompt) You\u2019re an expert at solving elementary math problems involving addition, subtraction, and multiplication.\nYou solve all the problems in a uniform format. All calculations are done modulo 5. For example, 4 + 4 equals 3, 2 + 4\nequals 1, 3 + 3 + 3 equals 4, 3 * 3 equals 4, and 2 * 2 equals 4. When providing your solution, please end with \u2019#### x.\u2019\nwhere x is your final answer, an integer between 0 and 4. You must solve all the problems using the same solution\nformat. Our scenarios involve up to four categories of objects: schools, classrooms, backpacks and stationeries. Each\nschool may contain classrooms, each classroom may contain backpacks, and each backpack may contain stationeries.\nWe can specify quantities, such as t\u0308he number of dance studios at each Lakeshore High.A\u0308ssume that every entity\nwith the same name has an identical configuration; for example, each Lakeshore High contains the same number of\ndance studios. Another guiding principle is that what is not mentioned does not exist: when we refer to classrooms at\nLakeshore High, we are only discussing the classrooms explicitly mentioned in our scenario. Furthermore, if Lakeshore\nHigh is not even mentioned, any classroom within it is automatically considered to be non-existent (i.e. 0).\n\nFor solution steps ranging from 4 to 6, we generate 100 questions for each case and report the corresponding accuracy in\nTable 9. As shown in the table, LLaDA Base demonstrates significant and consistent advantages over LLaMA3 on unseen\nmathematical problems, aligning with the results in Table 1.\n\nB.8. Poem Completion Tasks\n\nIn this section, we present examples from our poem completion dataset as follows.\n\nExample 1:\nPrompt: \u7a88\u7a95\u6dd1\u5973\u7684\u4e0b\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u541b\u5b50\u597d\u9011\u3002\n\n21\n\n\n\nLarge Language Diffusion Models\n\nTable 10. Visualization of the Semi-Autoregressive Sampling Process. In the response of LLaDA, darker colors indicate tokens\npredicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser What are the benefits of regular exercise for physical and mental health?\nLLaDA\n\nTable 11. Visualization of the Multi-turn Dialogue. We employ random remasking strategy. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser I want to have a pet, but I don\u2019t know whether to get a cat or a dog. Do you have any\nsuggestions?\n\nLLaDA\n\nUser I\u2019m very busy at work, so it seems like a cat would be a more suitable option. Do you have\nany suggestions for getting a cat?\n\nLLaDA\n\nExample 2:\nPrompt: \u4e0d\u62d8\u4e00\u683c\u964d\u4eba\u624d\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u6211\u529d\u5929\u516c\u91cd\u6296\u64de\u3002\n\nB.9. More Case Studies\n\nIn this section, we present additional case studies of LLaDA 8B Instruct. First, Tab. 10 illustrates the sampling process of\nsemi-autoregressive remasking, while Tab. 11 depicts the sampling process for multi-turn dialogues with random remasking.\nAdditionally, Tab. 12 and Tab. 13 provide further examples of single-turn and multi-turn dialogues. Finally, Tab. 14 presents\nexamples of poem reversal completions where the LLaDA 8B Instruct model succeeds, in contrast to the failure of GPT-4o.\n\n22\n\n\n\nLarge Language Diffusion Models\n\nTable 12. Single-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser What\u2019s a piece of advice you\u2019ve received that you\u2019ve found helpful?\nLLaDA One helpful piece of advice I\u2019ve received is to always be open to learning and growth.\n\nI\u2019ve found that being willing to explore new ideas and experiences has helped me become\nmore resilient and adaptable. By embracing a growth mindset, I\u2019ve been able to identify\nopportunities for improvement and development, both personally and professionally.\n\nDaily life\nUser Please recommend me three famous movies\nLLaDA Certainly! Here are three famous movies that have achieved widespread popularity and\n\ncritical acclaim:\n1. **The Shawshank Redemption** (1994) - Inspired by a novella and directed by Frank\nDarabont, this film tells the story of Andy Dufresne, a man convicted of murdering his\nwife, dealing with his harsh life in Shawshank Prison and forming an unlikely friendship\nwith a fellow inmate.\n2. **The Godfather** (1972) - Directed by Francis Ford Coppola, this film is based\non Mario Puzo\u2019s novel of the same name. It follows the story of the Corleone family,\nparticularly Michael Corleone, who rises to power and takes over the family\u2019s criminal\nempire.\n3. **The Empire Strikes Back** (1980) - Directed by George Lucas, this film continues\nthe story of a young Skywalker in the Star Wars saga. It follows Luke Skywalker as\nhe helps the Rebel Alliance fight against the Galactic Empire and destroy the powerful\nweapon, the Death Star.\nAll of these films are celebrated for their cinematic achievements, powerful storytelling,\nand memorable characters.\n\nTranslation\nUser Please help me translate into Chinese: \u2018What is now proved was once only imagined\u2019\nLLaDA \u73b0\u5728\u88ab\u8bc1\u660e\u7684\u66fe\u7ecf\u53ea\u662f\u60f3\u8c61\n\nTranslation\nUser Please help me translate into German: \u2018What is now proved was once only imagined\u2019\nLLaDA Wer heute besta\u0308tigt ist, war einst nur immaginiert.\n\nMath\nUser Emma decides to run 4 sprints 5 times a week. She runs 75 meters each sprint. How many\n\ntotal meters does she run a week?\nLLaDA Emma runs 4 sprints each time, so she runs a total of 4 x 5 = 20 sprints a week. Each sprint\n\nis 75 meters long, so Emma runs a total of 20 x 75 = 1500 meters a week. The final result\nis 1500\n\nCode\nUser I am writing python code, I have an array, please help me sort it from largest to smallest\nLLaDA Certainly! You can sort an array from the largest to smallest in Python using the \u2018sort()\u2019\n\nmethod or the \u2018sorted()\u2018 function. Here\u2019s an example:\n### Using \u2018sort()\u2019 method:\nmy array = [3, 1, 4, 1, 5, 2]\nmy array.sort(reverse=True)\nprint(my array)\n\n23\n\n\n\nLarge Language Diffusion Models\n\nTable 13. Multi-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser How do I prepare for a job interview?\nLLaDA To prepare for a job interview, you should research the company and the job, practice\n\ncommon interview questions, and dress appropriately.\nUser What are some common interview questions?\nLLaDA Some common interview questions include \u201cTell me about yourself\u201d and \u201cWhy do you\n\nwant to work for us\u201d?\n\nDaily life\nUser How do I start learning programming?\nLLaDA To learn programming, you can start by choosing a programming language that you are\n\ninterested in. Some popular languages include Python, Java, and C++.\nUser Which programming language should I learn first?\nLLaDA It depends on your interests and goals. Python is a great language for beginners for its\n\nsimplicity and versatility.\nUser How long does it take to learn Python?\nLLaDA It can take anywhere from a few weeks to several months to learn Python.\nUser What are some good Python projects for beginners?\nLLaDA Some good Python projects for beginners include building a simple calculator, a game, and\n\na web application.\n\nMath\nUser Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.\n\nHow many clips did Natalia sell altogether in April and May?\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n\n48/2 = 24 clips in May. Altogether, Natalia sold 48 + 24 = 72 clips in April and May. The\nfinal result is 72.\n\nUser Natalia sold three times as many items in June as she did in May. How many clips did\nNatalia sell altogether in these three months?\n\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May,\nso she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in\nthese three months. The final result is 144.\n\nTable 14. Poem Reversal Completion Cases where LLaDA 8B Instruct Succeeds but GPT-4o Fails.\n\nQuestion1\nUser \u2018\u4f46\u95fb\u4eba\u8bed\u54cd\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\u201d,\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bcd\u4eba\u674e\u6e05\u7167\u7684\uff08\u6b66\u9675\n\n\u6625\uff0c\u6625\u665a:\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\uff0c\u4f46\u95fb\u4eba\u8bed\u54cd\u3002\u8fd9\u9996\u8bcd\u8868\u8fbe\u4e86\u4f5c\u8005\u5728\u6625\u5929\u65f6\u8282\u7684\u5b64\u72ec\u4e0e\n\u611f\u4f24\uff0c\u501f\u82b1\u843d\u548c\u4eba\u8bed\u58f0\u7684\u63cf\u5199\uff0c\u53cd\u6620\u51fa\u8bcd\u4eba\u5185\u5fc3\u7684\u65e0\u4eb2\u4e0e\u5931\u843d\u3002\n\nLLaDA \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u4e0d\u89c1\u4eba\u201d\n\nQuestion2\nUser \u2018\u591c\u9759\u6625\u5c71\u7a7a\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u65b0\u96e8\u540e\u201d.\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bd7\u4eba\u738b\u7ef4\u7684\uff08\u5c71\u5c45\u79cb\n\n\u819c\uff09:\u7a7a\u5c71\u65b0\u96e8\u540e\uff0c\u591c\u9759\u6625\u5c71\u7a7a\u3002\u8fd9\u9996\u8bd7\u63cf\u7ed8\u4e86\u6625\u5929\u5c71\u4e2d\u96e8\u540e\u7684\u5bc2\u9759\u4e0e\u7a7a\u7075\uff0c\n\u8868\u73b0\u51fa\u81ea\u7136\u7684\u5b81\u9759\u4e0e\u548c\u8c10\u7f8e\u3002\n\nLLaDA \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u4eba\u95f2\u6842\u82b1\u843d\u201d\n\n24","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":10402,"end_idx":10419,"alt":null,"type":"file","name":"2502.09992v2.pdf","id":"file-EjEr99nXVn1FWaMNbWUohS","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nLarge Language Diffusion Models\n\n\nLarge Language Diffusion Models\n\nShen Nie 1 * \u2020 Fengqi Zhu 1 * \u2020 Zebin You 1 \u2020 Xiaolu Zhang 2 \u2021 Jingyang Ou 1 Jun Hu 2 \u2021 Jun Zhou 2\n\nYankai Lin 1 \u2021 Ji-Rong Wen 1 Chongxuan Li 1 \u2021 \u00b6\n\nAbstract\nAutoregressive models (ARMs) are widely re-\ngarded as the cornerstone of large language mod-\nels (LLMs). We challenge this notion by intro-\nducing LLaDA, a diffusion model trained from\nscratch under the pre-training and supervised fine-\ntuning (SFT) paradigm. LLaDA models distri-\nbutions through a forward data masking process\nand a reverse process, parameterized by a vanilla\nTransformer to predict masked tokens. By op-\ntimizing a likelihood bound, it provides a prin-\ncipled generative approach for probabilistic in-\nference. Across extensive benchmarks, LLaDA\ndemonstrates strong scalability, outperforming\nour self-constructed ARM baselines. Remark-\nably, LLaDA 8B is competitive with strong LLMs\nlike LLaMA3 8B in in-context learning and, af-\nter SFT, exhibits impressive instruction-following\nabilities in case studies such as multi-turn dia-\nlogue. Moreover, LLaDA addresses the rever-\nsal curse, surpassing GPT-4o in a reversal poem\ncompletion task. Our findings establish diffu-\nsion models as a viable and promising alternative\nto ARMs, challenging the assumption that key\nLLM capabilities discussed above are inherently\ntied to ARMs. Project page and codes: https:\n//ml-gsai.github.io/LLaDA-demo/.\n\n1. Introduction\nWhat is now proved was once only imagined.\nplaceholder,placeholder,placeho\u2014William Blake\n\nLarge language models (LLMs) (Zhao et al., 2023) fall en-\ntirely within the framework of generative modeling. Specifi-\ncally, LLMs aim to capture the true but unknown language\n\n*Equal contribution \u2020Work done during an internship at Ant\nGroup \u2021Project leaders 1Gaoling School of Artificial Intelli-\ngence, Renmin University of China; Beijing Key Laboratory\nof Big Data Management and Analysis Methods 2Ant Group.\n\u00b6Correspondence to: Chongxuan Li <chongxuanli@ruc.edu.cn>.\n\nPreprint.\n\n46\n\n33\n\n37\n50\n\n19\n\n24\n\n33\n49\n\n50\n\n56\n\n39\n\n45\n60\n\n23\n\n29\n\n40\n59\n\n60\n\n65\n\n46\n\n52\n70\n\n27\n\n34\n\n47\n69\n\n70\n\nMMLU\n\nTruthfulQA\n\nARC-C\nGSM8K\n\nMath\n\nHumanEval\n\nMBPP\nCMMLU\n\nC-Eval\n\nGeneral TasksM\nath\n\nem\nati\n\ncs\n\nCode\n\nChinese\n\nLLaDA 8B Base\nLLaMA 3 8B Base\nLLaMA 2 7B Base\n\nFigure 1. Zero/Few-Shot Benchmarks. We scale LLaDA to an\nunprecedented size of 8B parameters from scratch, achieving com-\npetitive performance with strong LLMs (Dubey et al., 2024).\n\ndistribution pdata(\u00b7) by optimizing a model distribution p\u03b8(\u00b7)\nthrough maximum likelihood estimation, or equivalently KL\ndivergence minimization between the two distributions:\n\nmax\n\u03b8\n\nEpdata(x) log p\u03b8(x)\u21d4 min\n\u03b8\n\nKL(pdata(x)||p\u03b8(x))\ufe38 \ufe37\ufe37 \ufe38\nGenerative modeling principles\n\n. (1)\n\nThe predominant approach relies on the autoregressive mod-\neling (ARM)\u2014commonly referred to as the next-token pre-\ndiction paradigm\u2014to define the model distribution:\n\np\u03b8(x) = p\u03b8(x\n1)\n\nL\u220f\ni=2\n\np\u03b8(x\ni | x1, . . . , xi\u22121)\ufe38 \ufe37\ufe37 \ufe38\n\nAutoregressive formulation\n\n, (2)\n\nwhere x is a sequence of length L, and xi is the i-th token.\n\nThis paradigm has proven remarkably effective (Radford,\n2018; Radford et al., 2019; Brown, 2020; OpenAI, 2022)\nand has become the foundation of current LLMs. Despite\nits widespread adoption, a fundamental question remains\nunanswered: Is the autoregressive paradigm the only viable\npath to achieving the intelligence exhibited by LLMs?\n\n1\n\nar\nX\n\niv\n:2\n\n50\n2.\n\n09\n99\n\n2v\n2 \n\n [\ncs\n\n.C\nL\n\n] \n 1\n\n8 \nFe\n\nb \n20\n\n25\n\nhttps://ml-gsai.github.io/LLaDA-demo/\nhttps://ml-gsai.github.io/LLaDA-demo/\n\n\nLarge Language Diffusion Models\n\nWe argue that the answer is not a simple \u201cyes\u201d. The key\ninsight overlooked previously is: it is the generative mod-\neling principles (i.e., Eq. (1)), rather than the autoregres-\nsive formulation (i.e., Eq. (2)) itself, that fundamentally\nunderpin the essential properties of LLMs, as detailed be-\nlow. However, certain inherent limitations of LLMs can be\ndirectly traced to their autoregressive nature.\n\nIn particular, we argue that scalability is primarily a con-\nsequence of the interplay between Transformers (Vaswani,\n2017), model and data size, and Fisher consistency1 (Fisher,\n1922) induced by the generative principles in Eq. (1), rather\nthan a unique result of ARM. The success of diffusion trans-\nformers (Bao et al., 2023; Peebles & Xie, 2023) on visual\ndata (Brooks et al., 2024) supports this claim.\n\nFurthermore, the capabilities of instruction-following and\nin-context learning (Brown, 2020) appear to be intrinsic\nproperties of all proper conditional generative models on\nstructurally consistent linguistic tasks, rather than exclusive\nadvantages of ARMs. In addition, while ARMs can be\ninterpreted as a lossless data compressor (Deletang et al.;\nHuang et al., 2024b), any sufficiently expressive probabilis-\ntic model can achieve similar capabilities (Shannon, 1948).\n\nNevertheless, the autoregressive nature of LLMs presents\nnotable challenges. For example, sequential token-by-token\ngeneration incurs high computational costs, and the left-\nto-right modeling limits effectiveness in reversal reasoning\ntasks (Berglund et al., 2023). These inherent limitations\nconstrain LLMs in handling longer and more complex tasks.\n\nMotivated by these insights, we introduce LLaDA, a Large\nLanguage Diffusion with mAsking, to investigate whether\nthe capabilities exhibited by LLMs can emerge from gener-\native modeling principles beyond ARMs in Eq. (2), thereby\naddressing the fundamental question posed earlier. In con-\ntrast to traditional ARMs, LLaDA leverages a masked diffu-\nsion model (MDM) (Austin et al., 2021a; Lou et al., 2023;\nShi et al., 2024; Sahoo et al., 2024; Ou et al., 2024), which\nincorporates a discrete random masking process and trains\na mask predictor to approximate its reverse process. This\ndesign enables LLaDA to construct a model distribution\nwith bidirectional dependencies and optimize a lower bound\nof its log-likelihood, offering an unexplored and principled\nalternative to existing LLMs.\n\nWe adopt the standard pipeline of data preparation, pre-\ntraining, supervised fine-tuning (SFT), and evaluation, scal-\ning LLaDA to an unprecedented language diffusion of size\n8B. In particular, LLaDA 8B was pre-trained from scratch\non 2.3 trillion tokens using 0.13 million H800 GPU hours,\nfollowed by SFT on 4.5 million pairs. Across diverse tasks,\nincluding language understanding, math, code, and Chinese,\n\n1It suggests the ability to recover the true data distribution with\ninfinite data, a sufficiently large network and optimal training.\n\nLLaDA demonstrates the following contributions:\n\nScalability. LLaDA scales effectively up to a computational\nbudget of 1023 FLOPs, achieving comparable results to self-\nconstructed ARM baselines trained on the same data across\nsix tasks, e.g., MMLU and GSM8K.\n\nIn-Context Learning. Remarkably, LLaDA 8B surpasses\nLLaMA2 7B (Touvron et al., 2023) on nearly all 15 standard\nzero/few-shot learning tasks while performing on par with\nLLaMA3 8B (Dubey et al., 2024).\n\nInstruction-Following. LLaDA significantly enhances the\nability to follow instructions after SFT, as demonstrated in\ncase studies such as multi-turn dialogue.\n\nReversal Reasoning. LLaDA effectively breaks the re-\nversal curse (Berglund et al., 2023) with consistent per-\nformance across forward and reversal tasks. Notably, it\noutperforms GPT-4o in a reversal poem completion task.\n\n2. Approach\nIn this section, we introduce the probabilistic formulation2,\nalong with the pre-training, supervised fine-tuning, and in-\nference procedures for LLaDA, as illustrated in Fig. 2.\n\n2.1. Probabilistic Formulation\n\nUnlike ARMs in Eq. (2), LLaDA defines a model distribu-\ntion p\u03b8(x0) through a forward process and a reverse pro-\ncess (Austin et al., 2021a; Ou et al., 2024). The forward\nprocess gradually masks tokens independently in x0 until\nthe sequence is fully masked at t = 1. For t \u2208 (0, 1), the\nsequence xt is partially masked, with each being masked\nwith probability t or remaining unmasked with probability\n1\u2212 t. The reverse process recovers the data distribution by\niteratively predicting masked tokens as t moves from 1 to 0.\n\nThe core of LLaDA is a mask predictor, a parametric model\np\u03b8(\u00b7|xt) that takes xt as input and predicts all masked tokens\n(denoted M) simultaneously. It is trained using a cross-\nentropy loss computed only on the masked tokens:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (3)\n\nwhere x0 is sampled from the training data, t is sampled\nuniformly from [0, 1], and xt is sampled from the forward\nprocess. The indicator function 1[\u00b7] ensures that the loss is\ncomputed only for masked tokens.\n\nOnce trained, we can simulate a reverse process (see Sec. 2.4\nfor details) parameterized by the mask predictor and define\nthe model distribution p\u03b8(x0) as the marginal distribution\n\n2Here, we focus on the approach of LLaDA. A rigorous formu-\nlation of MDM is provided in Appendix A for interested readers.\n\n2\n\n\n\nLarge Language Diffusion Models\n\nMask predictor\n\nMask all tokens independently\n\nMask predictor\n\nPrompt Response\n\nMask predictor\n\nPrompt Response\n\n... \n\n... \n\nA\nn\n\n in\nte\n\nrm\ne\n\nd\nia\n\nte\n s\n\nte\np\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nNon-mask token Random mask\n\nRemaskMask token\n\n(c)(b)(a)\n\nRemask\n\nMask ratio \ud835\udc61 \u223c \ud835\udc48(0,1)\n\nFigure 2. A Conceptual Overview of LLaDA. (a) Pre-training. LLaDA is trained on text with random masks applied independently to all\ntokens at the same ratio t \u223c U [0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion\nprocess from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies.\n\ninduced at t = 0. Notably, the loss in Eq. (3) has been\nproven to be an upper bound on the negative log-likelihood\nof the model distribution (Shi et al., 2024; Ou et al., 2024):\n\n\u2212Epdata(x0) [log p\u03b8(x0)] \u2264 L(\u03b8), (4)\n\nmaking it a principled objective for generative modeling.\n\nNotably, LLaDA employs a masking ratio that varies ran-\ndomly between 0 and 1 while masked language models (De-\nvlin, 2018) use a fixed ratio. The subtly differences have\nsignificant implications, especially at scale: as shown in\nEq. (4), LLaDA is a principled generative model with the\npotential to perform in-context learning naturally, akin to\nLLMs. Moreover, its generative perspective ensures Fisher\nconsistency (Fisher, 1922) in extreme cases, suggesting\nstrong scalability with large data and models.\n\n2.2. Pre-training\n\nLLaDA employs a Transformer (Vaswani, 2017) as the mask\npredictor, whose architecture is similar to existing LLMs.\nHowever, LLaDA does not use a causal mask, as its formu-\nlation allows it to see the entire input for predictions.\n\nWe trained two variants of LLaDA with different sizes: 1\nbillion (B) and 8B. We summarize the model architecture\nof LLaDA 8B and LLaMA3 8B (Dubey et al., 2024) here\nand details are provided in Appendix B.2. We have ensured\nconsistency in most hyperparameters while making several\nnecessary modifications. We use vanilla multi-head atten-\ntion instead of grouped query attention (Ainslie et al., 2023)\nfor simplicity, as LLaDA is incompatible with KV caching,\nresulting in a different number of key and value heads. Con-\nsequently, the attention layer has more parameters, and we\nreduce the FFN dimension to maintain a comparable model\nsize. Additionally, the vocabulary size differs slightly due\nto a tokenizer (Brown, 2020) adapted on our data.\n\nThe LLaDA model is pre-trained on a dataset comprising\n2.3 trillion (T) tokens, adhering to a data protocol that aligns\n\nclosely with existing large language models (LLMs) (Tou-\nvron et al., 2023; Chu et al., 2024), without the incorporation\nof any special techniques. The data are derived from online\ncorpora, with low-quality content filtered through manually\ndesigned rules and LLM-based approaches. Beyond general\ntext, the dataset encompasses high-quality code, math, and\nmultilingual data. The mixing of data sources and domains\nis guided by scaled-down ARMs. The pre-training process\nutilizes a fixed sequence length of 4096 tokens, incurring a\ntotal computational cost of 0.13 million H800 GPU hours,\nsimilar to ARMs of the same scale and dataset size.\n\nFor a training sequence x0, we randomly sample t \u2208 [0, 1],\nmask each token independently with the same probability\nt to obtain xt (see Fig. 2 (a)) and estimate Eq. (3) via the\nMonte Carlo method for stochastic gradient descent training.\nIn addition, following Nie et al. (2024), to enhance the\nability of LLaDA to handle variable-length data, we set 1%\nof the pre-training data to a random length that is uniformly\nsampled from the range [1, 4096].\n\nWe adopted the Warmup-Stable-Decay (Hu et al., 2024)\nlearning rate scheduler to monitor the training progress\nwithout interrupting continuous training. Specifically, we\nlinearly increased the learning rate from 0 to 4\u00d7 10\u22124 over\nthe first 2000 iterations and maintained it at 4\u00d7 10\u22124. After\nprocessing 1.2T tokens, we decayed the learning rate to\n1\u00d7 10\u22124 and held it constant for the next 0.8T tokens to en-\nsure stable training. Finally, we linearly reduced the learning\nrate from 1\u00d710\u22124 to 1\u00d710\u22125 for the last 0.3T tokens. Fur-\nthermore, we utilized the AdamW optimizer (Loshchilov,\n2017) with a weight decay of 0.1, a batch size of 1280, and\na local batch size of 4 per GPU. The 8B experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.3. Supervised Fine-Tuning\n\nWe enhance the capability of LLaDA to follow instructions\nby supervised fine-tuning (SFT) with paired data (p0, r0),\n\n3\n\n\n\nLarge Language Diffusion Models\n\nwhere p0 is the prompt and r0 denotes the response. This\nis the simplest and most basic post-training method for\nLLMs. Technically, this requires to model the conditional\ndistribution p\u03b8(r0|p0) instead of p\u03b8(x0) in pre-training.\n\nThe implementation is similar to pre-training. As shown in\nFig. 2 (b), we leave the prompt unchanged and mask the\ntokens in the response independently, as done for x0. Then,\nwe feed both the prompt and the masked response rt to the\npre-trained mask predictor to compute the loss for SFT:\n\n\u2212Et,p0,r0,rt\n\n\uf8ee\uf8f01\n\nt\n\nL\u2032\u2211\ni=1\n\n1[rit = M] log p\u03b8(r\ni\n0|p0, rt)\n\n\uf8f9\uf8fb , (5)\n\nwhere L\u2032 denotes a dynamic length specified later, and all\nother notations remain the same as before.\n\nNote that this approach is fully compatible with pre-training.\nEssentially, the concatenation of p0 and r0 can be treated\nas clean pre-training data x0, while the concatenation of\np0 and rt serves as the masked version xt. The process is\nidentical to pre-training, with the only difference being that\nall masked tokens happen to appear in the r0 portion.\n\nThe LLaDA 8B model undergoes SFT on a dataset com-\nprising 4.5 million pairs. Consistent with the pre-training\nprocess, both data preparation and training follow the SFT\nprotocols utilized in existing LLMs (Chu et al., 2024; Yang\net al., 2024), without introducing any additional techniques\nto optimize LLaDA\u2019s performance. The dataset spans mul-\ntiple domains, including code, mathematics, instruction-\nfollowing, and structured data understanding. We append\n|EOS| tokens to the end of short pairs in each mini-batch\nto ensure equal lengths across all data. We treat |EOS| as a\nnormal token during training and remove it during sampling,\nenabling LLaDA to control the response length automati-\ncally. Please refer to Appendix B.1 for more details.\n\nWe train for 3 epochs on the SFT data using a similar sched-\nule to the pre-training phase. The learning rate is linearly\nincreased from 0 to 2.5\u00d7 10\u22125 over the first 50 iterations\nand then kept constant. During the final 10% of iterations,\nit is linearly reduced to 2.5 \u00d7 10\u22126. Additionally, we set\nthe weight decay to 0.1, the global batch size to 256, and\nthe local batch size to 2 per GPU. The SFT experiment was\nexecuted once, without any hyperparameter tuning.\n\n2.4. Inference\n\nAs a generative model, LLaDA is capable of both sampling\nnew text and evaluating the likelihood of candidate text.\n\nWe begin with the sampling. As illustrated in Fig. 2 (c),\ngiven a prompt p0, we discretize the reverse process to sam-\nple from the model distribution p\u03b8(r0|p0), starting from a\nfully masked response. The total number of sampling steps\nis a hyperparameter, which naturally provides LLaDA with a\n\ntrade-off between efficiency and sample quality, as analyzed\nin Sec. 3.3. We employ uniformly distributed timesteps by\ndefault. In addition, the generation length is also treated as\na hyperparameter, specifying the length of the fully masked\nsentence at the beginning of the sampling process. As de-\ntailed in Appendix B.4, since both pre-training and SFT\nare conducted using datasets with variable lengths, the final\nresults are insensitive to this length hyperparameter.\n\nAt an intermediate step from time t \u2208 (0, 1] to s \u2208 [0, t), we\nfeed both p0 and rt into the mask predictor and predict all\nmasked tokens simultaneously. Subsequently, we remask s\n\nt\nof the predicted tokens in expectation to obtain rs, ensuring\nthat the transition of the reverse process aligns with the\nforward process for accurate sampling (Austin et al., 2021a).\n\nIn principle, the remasking strategy should be purely ran-\ndom. However, inspired by the annealing tricks of sampling\nin LLMs (Holtzman et al., 2019; Brown, 2020), we explore\ntwo deterministic yet effective remasking strategies. Specif-\nically, similarly to Chang et al. (2022), we remask the s\n\nt\nof predicted tokens with the lowest confidence based on\nthe predictions, called low-confidence remasking. Addition-\nally, for LLaDA after SFT, we can divide the sequence into\nseveral blocks and generate them from left to right, called\nsemi-autoregressive remasking. Within each block, we ap-\nply the reverse process to perform sampling. We provide\nmore details and ablation studies in Appendix. B.3.\n\nFor conditional likelihood evaluation, we can naturally uti-\nlize the upper bound in Eq. (5). However, we find that the\nfollowing equivalent form (Ou et al., 2024) exhibits lower\nvariance and is more stable for evaluation:\n\n\u2212El,r0,rl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[ril = M] log p\u03b8(r\ni\n0|p0, rl)\n\n]\n, (6)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and rl is\nobtained by uniformly sampling l tokens from r0 without\nreplacement for masking. In addition, we employ the unsu-\npervised classifier-free guidance (Nie et al., 2024). We refer\nthe readers to more details in Appendix A.2.\n\nWe present the training, sampling, and likelihood evaluation\nalgorithms, along with theoretical details, in Appendix A.\n\n3. Experiments\nWe evaluate the scalability, instruction-following, and in-\ncontext learning capabilities of LLaDA on standard bench-\nmarks, followed by analyses and case studies on more con-\ntrolled datasets to provide a comprehensive assessment.\n\n3.1. Scalability of LLaDA on Language Tasks\n\nWe first investigate the scalability of LLaDA on downstream\ntasks in comparison with the ARM baselines we constructed.\n\n4\n\n\n\nLarge Language Diffusion Models\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nA\nR\n\nC\n-C\n\n (0\n-s\n\nho\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n20\n\n30\n\n40\n\n50\n\n60\n\nC\nM\n\nM\nLU\n\n (5\n-s\n\nho\nt) Autoregressive Baseline\n\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n50\n\n60\n\n70\n\n80\n\nPI\nQ\n\nA\n (0\n\n-s\nho\n\nt)\n\nAutoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n20\n\n40\n\n60\n\nG\nSM\n\n8K\n (4\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\n1020 1021 1022 1023\n\nFLOPs\n\n0\n\n8\n\n16\n\n24\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt) Autoregressive Baseline\nLLaDA\n\nFigure 3. Scalability of LLaDA. We evaluate the performance of LLaDA and our ARM baselines trained on the same data across\nincreasing computational FLOPs. LLaDA exhibits strong scalability, matching the overall performance of ARMs on six tasks.\n\nSpecifically, at the 1B scale, we ensured that LLaDA and\nARM shared the same architecture, data, and all other config-\nurations. At larger scales, we also report results for LLaDA\nand ARM models of slightly different sizes trained on the\nsame data due to resource limit, as detailed in Appendix B.2.\nWe use the computational cost as a unified scaling metric.\nFor evaluation, we focused on six standard and diverse tasks.\n\nAs shown in Fig. 3, LLaDA demonstrates impressive scala-\nbility, with its overall trend being highly competitive with\nthat of ARM. Notably, in tasks such as MMLU and GSM8K,\nLLaDA exhibits even stronger scalability. Even on tasks like\nPIQA, where performance lags, LLaDA narrows the gap\nwith ARMs at larger scales. To account for the significant\ninfluence of outliers, we opted not to fit quantitative scaling\ncurves, avoiding potential misinterpretation. Nevertheless,\nthe results clearly demonstrate the scalability of LLaDA.\n\nNie et al. (2024) suggests that MDM requires 16 times\nmore computation than ARM to achieve the same likeli-\nhood. However, there are key differences that make the\nconclusions of this study more broadly applicable. In partic-\nular, likelihood is a relatively indirect metric for downstream\ntask performance, and diffusion optimizes a bound of the\nlikelihood, making it not directly comparable to ARM. Ad-\nditionally, we extended the scaling range from 1018 \u223c 1020\n\nin Nie et al. (2024) to 1020 \u223c 1023 in this work.\n\n3.2. Benchmark Results\n\nTo comprehensively evaluate the in-context learning and\ninstruction-following capabilities of LLaDA 8B, we con-\nducted detailed comparisons with existing LLMs (Touvron\n\net al., 2023; Dubey et al., 2024; Chu et al., 2024; Yang et al.,\n2024; Bi et al., 2024; Jiang et al., 2023) of similar scale.\nThe selection of tasks and evaluation protocols was aligned\nwith existing studies, encompassing 15 popular benchmarks\nin general tasks, mathematics, code, and Chinese. Further\ndetails are provided in Appendix B.5. For a more direct\ncomparison, we re-evaluated representative LLMs (Touvron\net al., 2023; Dubey et al., 2024) in our implementation.\n\nAs shown in Tab. 1, after pretraining on 2.3T tokens,\nLLaDA 8B demonstrates remarkable performance, surpass-\ning LLaMA2 7B on nearly all tasks, and is overall competi-\ntive with LLaMA3 8B. LLaDA shows advantages in math\nand Chinese tasks. We conjecture that the strengths stem\nfrom the same factors as its relatively weaker performance\nin some tasks\u2014differences in data quality and distribution,\nlargely due to the closed-source situation of LLM datasets.\n\nNotably, we have carefully ruled out the possibility of data\nleakage by taking GSM8K as an example. First, as shown\nin Fig. 3, LLaDA outperformed ARM baselines regarding\nGSM8K. Moreover, the conclusion remains on a fully un-\nseen GSM8K-like task (Ye et al., 2024) in Appendix B.7.\n\nFurther, Tab. 2 compares the performance of LLaDA 8B In-\nstruct with existing LLMs. We observed that SFT improved\nLLaDA\u2019s performance on most downstream tasks. A few\nmetrics, such as MMLU, showed declines, and we conjec-\nture may be due to the suboptimal quality of the SFT data.\nOverall, since we did not perform alignment with reinforce-\nment learning (RL), our results are slightly behind LLaMA3\n8B Instruct, though the gaps in many metrics remain small.\nNotably, even with only SFT, LLaDA demonstrates impres-\n\n5\n\n\n\nLarge Language Diffusion Models\n\nTable 1. Benchmark Results of Pre-trained LLMs. \u2217 indicates that LLaDA 8B Base, LLaMA2 7B Base, and LLaMA3 8B Base are\nevaluated under the same protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Chu et al. (2024); Yang et al.\n(2024) and Bi et al. (2024) respectively. The numbers in parentheses represent the number of shots used for evaluation. \u201c-\u201d indicates\nunknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Mistral 7B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T - 2T\n\nGeneral Tasks\n\nMMLU 65.9 (5) 65.4 (5) 45.9 (5) 70.3 (5) 74.2 (5) 64.2 (5) 48.2 (5)\nBBH 49.8 (3) 57.6 (3) 37.3 (3) 62.3 (3) 70.4 (3) 56.1 (3) 39.5 (3)\nARC-C 47.9 (0) 53.1 (0) 46.3 (0) 60.6 (25) 63.7 (25) 60.0 (25) 48.1 (0)\nHellaswag 72.5 (0) 79.1 (0) 76.0 (0) 80.7 (10) 80.2 (10) 83.3 (10) 75.4 (0)\nTruthfulQA 46.4 (0) 44.0 (0) 39.0 (0) 54.2 (0) 56.4 (0) 42.2 (0) -\nWinoGrande 74.8 (5) 77.3 (5) 72.5 (5) 77.0 (5) 75.9 (5) 78.4 (5) 70.5 (0)\nPIQA 74.4 (0) 80.6 (0) 79.1 (0) - - - 79.2 (0)\n\nMathematics & Science\n\nGSM8K 70.7 (4) 53.1 (4) 14.3 (4) 80.2 (4) 85.4 (4) 36.2 (4) 17.4 (8)\nMath 27.3 (4) 15.1 (4) 3.2 (4) 43.5 (4) 49.8 (4) 10.2 (4) 6.0 (4)\nGPQA 26.1 (5) 25.9 (5) 25.7 (5) 30.8 (5) 36.4 (5) 24.7 (5) -\n\nCode\n\nHumanEval 33.5 (0) 34.2 (0) 12.8 (0) 51.2 (0) 57.9 (0) 29.3 (0) 26.2 (0)\nHumanEval-FIM 73.8 (2) 73.3 (2) 26.9 (2) - - - -\nMBPP 38.2 (4) 47.4 (4) 18.4 (4) 64.2 (0) 74.9 (0) 51.1 (0) 39.0 (3)\n\nChinese\n\nCMMLU 69.9 (5) 50.7 (5) 32.5 (5) 83.9 (5) - - 47.2 (5)\nC-Eval 70.5 (5) 51.7 (5) 34.0 (5) 83.2 (5) - - 45.0 (5)\n\nTable 2. Benchmark Results of Post-trained LLMs. LLaDA only employs an SFT procedure while other models have extra reinforcement\nlearning (RL) alignment. \u2217 indicates that LLaDA 8B Instruct, LLaMA2 7B Instruct, and LLaMA3 8B Instruct are evaluated under the\nsame protocol, detailed in Appendix B.5. Results indicated by \u2020 and \u00b6 are sourced from Yang et al. (2024) and Bi et al. (2024) respectively.\nThe numbers in parentheses represent the number of shots used for in-context learning. \u201c-\u201d indicates unknown data.\n\nLLaDA 8B\u2217 LLaMA3 8B\u2217 LLaMA2 7B\u2217 Qwen2 7B\u2020 Qwen2.5 7B\u2020 Gemma2 9B\u2020 Deepseek 7B\u00b6\n\nModel Diffusion AR AR AR AR AR AR\nTraining tokens 2.3T 15T 2T 7T 18T 8T 2T\n\nPost-training SFT SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL\nAlignment pairs 4.5M - - 0.5M + - 1M + 0.15M - 1.5M + -\n\nGeneral Tasks\n\nMMLU 65.5 (5) 68.4 (5) 44.1 (5) - - - 49.4 (0)\nMMLU-pro 37.0 (0) 41.9 (0) 4.6 (0) 44.1 (5) 56.3 (5) 52.1 (5) -\nHellaswag 74.6 (0) 75.5 (0) 51.5 (0) - - - 68.5 (-)\nARC-C 88.5 (0) 82.4 (0) 57.3 (0) - - - 49.4 (-)\n\nMathematics & Science\n\nGSM8K 78.6 (4) 78.3 (4) 29.0 (4) 85.7 (0) 91.6 (0) 76.7 (0) 63.0 (0)\nMath 26.6 (0) 29.6 (0) 3.8 (0) 52.9 (0) 75.5 (0) 44.3 (0) 15.8 (0)\nGPQA 31.8 (5) 31.9 (5) 28.4 (5) 34.3 (0) 36.4 (0) 32.8 (0) -\n\nCode\n\nHumanEval 47.6 (0) 59.8 (0) 16.5 (0) 79.9 (0) 84.8 (0) 68.9 (0) 48.2 (-)\nMBPP 34.2 (4) 57.6 (4) 20.6 (4) 67.2 (0) 79.2 (0) 74.9 (0) 35.2 (-)\n\n6\n\n\n\nLarge Language Diffusion Models\n\nTable 3. Comparison in the Poem Completion Task.\n\nForward Reversal\n\nGPT-4o (2024-08-06) 82.7 34.3\nQwen2.5 7B Instruct 75.9 38.0\nLLaDA 8B Instruct 48.8 42.4\n\nsive instruction-following abilities, as detailed in Sec. 3.4.\nWe leave RL-based alignment for future work.\n\nOverall, despite the lack of data transparency, we have made\nevery effort to adopt standardized procedures and introduce\ndiverse tasks, we believe they sufficiently demonstrate the\nextraordinary capabilities of LLaDA, which is the only com-\npetitive non-autoregressive model to our knowledge.\n\n3.3. Reversal Reasoning and Analyses\n\nTo quantify the reversal reasoning ability of mod-\nels (Berglund et al., 2023), we follow the protocol estab-\nlished in Allen-Zhu & Li (2023). Specifically, we construct\na dataset of 496 famous Chinese poem sentence pairs. Given\na sentence from a poem, models are tasked with generating\nthe subsequent line (forward) or the preceding line (rever-\nsal) without additional fine-tuning. Examples can be found\nin Appendix B.8. This setting provides a straightforward\nand more realistic evaluation compared to previous stud-\nies (Nie et al., 2024; Kitouni et al., 2024).\n\nAs shown in Tab. 3, LLaDA effectively addresses the rever-\nsal curse (Berglund et al., 2023), demonstrating consistent\nzero-shot performance across both forward and reversal\ntasks. In contrast, both Qwen 2.5 and GPT-4o exhibit a\nsignificant gap between the two. The results on forward\ngeneration confirm that both ARMs are strong, benefiting\nfrom significantly larger datasets and greater computational\nresources than LLaDA. However, LLaDA outperforms both\nby a large margin in the reversal task.\n\nWe emphasize that we did not design anything special for\nreversal tasks. Intuitively, LLaDA treats tokens uniformly\nwithout inductive bias, leading to balanced performance.\nSee more details in Appendix A.2.\n\nWe also analyze the effect of remasking strategies and sam-\npling steps, as detailed in Appendix B.3 and Appendix B.6.\n\n3.4. Case Studies\n\nWe present samples generated by LLaDA 8B Instruct in\nTab. 4, showcasing its instruction-following capabilities.\nFirst, the table illustrates LLaDA\u2019s ability to generate co-\nherent, fluent, and extended text in a non-autoregressive\nmanner. Second, it highlights the model\u2019s multi-turn dia-\nlogue capability, effectively retaining conversation history\n\nand producing contextually appropriate responses across\nmultiple languages. Such chat capabilities of LLaDA are\nimpressive, as it departs from conventional ARMs for the\nfirst time, to the best of our knowledge. See more case\nstudies on remasking and reasoning tasks in Appendix B.9.\n\n4. Related Work\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020) have excelled in visual domains but\nremain unverified for LLMs despite extensive efforts.\n\nA simple approach is to continuousize text data and apply\ndiffusion models directly (Li et al., 2022; Gong et al., 2022;\nHan et al., 2022; Strudel et al., 2022; Chen et al., 2022;\nDieleman et al., 2022; Richemond et al., 2022; Wu et al.,\n2023; Mahabadi et al., 2024; Ye et al., 2023b). Alternatively,\nsome methods model continuous parameters of discrete dis-\ntributions instead (Lou & Ermon, 2023; Graves et al., 2023;\nLin et al., 2023; Xue et al., 2024). However, scalability\nremains a challenge, as a 1B parameter model requires 64\ntimes the compute of an ARM to achieve comparable per-\nformance (Gulrajani & Hashimoto, 2024).\n\nAnother approach replaces continuous diffusion with dis-\ncrete processes featuring new forward and reverse dy-\nnamics (Austin et al., 2021a), leading to numerous vari-\nants (Hoogeboom et al., 2021b;a; He et al., 2022; Campbell\net al., 2022; Meng et al., 2022; Reid et al., 2022; Sun et al.,\n2022; Kitouni et al., 2023; Zheng et al., 2023; Chen et al.,\n2023; Ye et al., 2023a; Gat et al., 2024; Zheng et al., 2024;\nSahoo et al., 2024; Shi et al., 2024). Notably, Lou et al.\n(2023) showed that masked diffusion, as a special case of\ndiscrete diffusion, achieves perplexity comparable to or sur-\npassing ARMs at GPT-2 scale. Ou et al. (2024) established\nfundamental theoretical results, which motivated our model\ndesign, training, and inference (see Appendix A). Nie et al.\n(2024) explored how MDM can be leveraged for language\ntasks such as question answering at GPT-2 scale. Gong et al.\n(2024) fine-tune ARMs in the MDM formulation. However,\nimprovements are confined to certain metrics, and it remains\nunclear whether this approach can yield a foundation model\ncomparable to strong LLMs under a comprehensive evalua-\ntion.\n\nIn comparison, this study scales MDM to an unprecedented\nsize of 8B parameters from scratch, achieving performance\ncomparable to leading LLMs such as LLaMA 3.\n\nAdditionally, a parallel line of work on image genera-\ntion (Chang et al., 2022; 2023) aligns well with the appli-\ncation of MDMs to text data. Moreover, MDMs have also\nshown promise in domains such as protein generation (Wang\net al., 2024b;c), where they have achieved promising results.\nNotably, Kou et al. (2024); Xu et al. (2025) demonstrate the\npotential of using distillation to accelerate MDMs sampling,\n\n7\n\n\n\nLarge Language Diffusion Models\n\nTable 4. Visualization of the Sampling Process and a Generated Multi-round Dialogue. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nSampling Process\n\nUser Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour.\nHow many kilometers can she run in 8 hours?\n\nLLaDA\n\nMulti-round Dialogue Case\n\nUser Could you please share with me what the initial two lines of the renowned poem \u2018The\nRoad Not Taken\u2019?\n\nLLaDA Certainly! The initial two lines of the renowned poem \u201cThe Road Not Taken\u201d by Robert\nFrost are: \u201cTwo roads diverged in a yellow wood, And sorry I could not travel both.\u201d These\nlines set the stage for the poem, introducing the idea of a speaker facing a choice between\ntwo paths.\n\nUser Please help me translate into Chinese.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nChinese as: \u201c\u4e24\u6761\u8def\u5206\u5c94\u5728\u9ec4\u8272\u7684\u6811\u6797\u4e2d\uff0c\u9057\u61be\u6211\u4e0d\u80fd\u540c\u65f6\u8d70\u201d\nUser Please translate into German.\nLLaDA Sure! The first two lines of \u201cThe Road Not Taken\u201d by Robert Frost can be translated into\n\nGerman as: \u201cZwei Wege trennten sich im gelben Wald, und leider konnte ich nicht beide\ntun.\u201d\n\nUser Please also write a poem about life choices. I require 5 sentences, and the first word of\neach sentence starts with C as the first letter.\n\nLLaDA Certainly! Here\u2019s a poem about life choices, with each sentence starting with the letter \u201cC\u201d:\nChoices cascade through life\u2019s journey, Creating connections with each decision, Careful\nconsideration leads to better paths, Courageous choices shape our destiny, Choosing wisely\nguides us through the years.\n\nfurther enhancing their efficiency.\n\n5. Conclusion and Discussion\nIn the middle of difficulty lies opportunity.\nplaceholder,placeholder,placeh\u2014Albert Einstein\n\nWe introduce LLaDA, a principled and previously unex-\nplored approach to large language modeling based on dif-\nfusion models. LLaDA demonstrates strong capabilities in\nscalability, in-context learning, and instruction-following,\nachieving performance comparable to strong LLMs. In addi-\ntion, LLaDA offers unique advantages such as bidirectional\nmodeling and enhanced robustness, effectively addressing\nseveral inherent limitations of existing LLMs. Our findings\nnot only establish diffusion models as a viable and promis-\ning alternative but also challenge the prevailing assumption\nthat these essential capabilities are inherently tied to ARMs.\n\nWhile promising, the full potential of diffusion models re-\n\nmains to be fully explored. Several limitations of this work\npresent significant opportunities for future research.\n\nDue to computational constraints, direct comparisons be-\ntween LLaDA and ARMs\u2014such as training on identical\ndatasets\u2014were restricted to a computational budget of less\nthan 1023 FLOPs. To allocate resources for training the\nlargest possible LLaDA model and showcasing its poten-\ntial, we were unable to scale the ARM baseline to the same\nextent. Moreover, no specialized attention mechanisms\nor position embeddings were designed for LLaDA, nor\nwere any system-level architectural optimizations applied.\nOn the inference side, our exploration of guidance mecha-\nnisms (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) re-\nmains preliminary, and LLaDA currently exhibits sensitivity\nto inference hyperparameters. Furthermore, LLaDA has yet\nto undergo alignment with reinforcement learning (Ouyang\net al., 2022; Rafailov et al., 2024), which is crucial for im-\nproving its performance and alignment with human intent.\n\n8\n\n\n\nLarge Language Diffusion Models\n\nLooking forward, the scale of LLaDA is still smaller than\nleading counterparts (Achiam et al., 2023; Dubey et al.,\n2024; Google, 2024; Anthropic, 2024; Yang et al., 2024;\nLiu et al., 2024), highlighting the need for further scaling\nto fully assess its capabilities. In addition, LLaDA\u2019s abil-\nity to handle multi-modal data remains unexplored. The\nimpact of LLaDA on prompt tuning techniques (Wei et al.,\n2022) and its integration into agent-based systems (Park\net al., 2023; Wang et al., 2024a) is yet to be fully under-\nstood. Finally, a systematic investigation into post-training\nfor LLaDA could facilitate the development of O1-like sys-\ntems (OpenAI, 2024; Guo et al., 2025).\n\nImpact Statement\nThis work challenges the prevailing dominance of autore-\ngressive modeling in LLMs by proposing a diffusion-based\nalternative. Our findings open new avenues for exploring\nalternative probabilistic paradigms in natural language pro-\ncessing, with potential applications in conversational AI,\ncode generation, and complex reasoning tasks.\n\nHowever, diffusion-based models, like traditional LLMs,\nraise similar societal concerns. These include the environ-\nmental impact of large-scale training, the potential misuse\nfor generating harmful or misleading content, and the ampli-\nfication of biases present in training data. Addressing these\nchallenges is critical to ensuring the responsible develop-\nment and deployment of diffusion-based language models.\n\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (No. 92470118); Beijing Natural Sci-\nence Foundation (No. L247030); Beijing Nova Program\n(No. 20220484044); Major Innovation & Planning Interdis-\nciplinary Platform for the \u201cDouble-First Class\u201d Initiative,\nRenmin University of China; the Fundamental Research\nFunds for the Central Universities, the Research Funds of\nRenmin University of China (22XNKJ13); Ant Group Re-\nsearch Fund. The work was partially done at the Engineering\nResearch Center of Next-Generation Intelligent Search and\nRecommendation, Ministry of Education.\n\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\n\nAleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y.,\nLebron, F., and Sanghai, S. Gqa: Training generalized\nmulti-query transformer models from multi-head check-\npoints. In Proceedings of the 2023 Conference on Em-\n\npirical Methods in Natural Language Processing, pp.\n4895\u20134901, 2023.\n\nAllen-Zhu, Z. and Li, Y. Physics of Language Mod-\nels: Part 3.2, Knowledge Manipulation. ArXiv e-prints,\nabs/2309.14402, September 2023. Full version available\nat http://arxiv.org/abs/2309.14402.\n\nAnonymous. Interpolating autoregressive and discrete de-\nnoising diffusion language models. In The Thirteenth\nInternational Conference on Learning Representations,\n2025. URL https://openreview.net/forum?\nid=tyEyYT267x.\n\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.\nanthropic.com/news/claude-3-5-sonnet.\n\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van\nDen Berg, R. Structured denoising diffusion models in\ndiscrete state-spaces. Advances in Neural Information\nProcessing Systems, 34:17981\u201317993, 2021a.\n\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021b.\n\nBao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu,\nJ. All are worth words: A vit backbone for diffusion\nmodels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22669\u2013\n22679, 2023.\n\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M. Efficient training of\nlanguage models to fill in the middle. arXiv preprint\narXiv:2207.14255, 2022.\n\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stick-\nland, A. C., Korbak, T., and Evans, O. The reversal curse:\nLlms trained on\u201d a is b\u201d fail to learn\u201d b is a\u201d. arXiv\npreprint arXiv:2309.12288, 2023.\n\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\nScaling open-source language models with longtermism.\narXiv preprint arXiv:2401.02954, 2024.\n\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\n2020.\n\nBrooks, T., Peebles, B., Holmes, C., DePue, W.,\nGuo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman,\nT., Luhman, E., Ng, C., Wang, R., and Ramesh,\nA. Video generation models as world simulators.\n2024. URL https://openai.com/research/\nvideo-generation-models-as-world-simulators.\n\n9\n\nhttp://arxiv.org/abs/2309.14402\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://openreview.net/forum?id=tyEyYT267x\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://www.anthropic.com/news/claude-3-5-sonnet\nhttps://openai.com/research/video-generation-models-as-world-simulators\nhttps://openai.com/research/video-generation-models-as-world-simulators\n\n\nLarge Language Diffusion Models\n\nBrown, T. B. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\n\nCampbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deli-\ngiannidis, G., and Doucet, A. A continuous time frame-\nwork for discrete denoising models. Advances in Neural\nInformation Processing Systems, 35:28266\u201328279, 2022.\n\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,\nW. T. Maskgit: Masked generative image transformer. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11315\u201311325, 2022.\n\nChang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J.,\nJiang, L., Yang, M.-H., Murphy, K., Freeman, W. T.,\nRubinstein, M., et al. Muse: Text-to-image genera-\ntion via masked generative transformers. arXiv preprint\narXiv:2301.00704, 2023.\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\n\nChen, T., Zhang, R., and Hinton, G. Analog bits: Gen-\nerating discrete data using diffusion models with self-\nconditioning. arXiv preprint arXiv:2208.04202, 2022.\n\nChen, Z., Yuan, H., Li, Y., Kou, Y., Zhang, J., and Gu, Q.\nFast sampling via de-randomization for discrete diffusion\nmodels. arXiv preprint arXiv:2312.09193, 2023.\n\nChu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng,\nY., Lv, Y., He, J., Lin, J., et al. Qwen2-audio technical\nreport. arXiv preprint arXiv:2407.10759, 2024.\n\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018.\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\nR., et al. Training verifiers to solve math word problems.\narXiv preprint arXiv:2110.14168, 2021.\n\nDeletang, G., Ruoss, A., Duquenne, P.-A., Catt, E., Ge-\nnewein, T., Mattern, C., Grau-Moya, J., Wenliang, L. K.,\nAitchison, M., Orseau, L., et al. Language modeling is\ncompression. In The Twelfth International Conference on\nLearning Representations.\n\nDevlin, J. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021.\n\nDieleman, S., Sartran, L., Roshannai, A., Savinov, N.,\nGanin, Y., Richemond, P. H., Doucet, A., Strudel, R.,\nDyer, C., Durkan, C., et al. Continuous diffusion for\ncategorical data. arXiv preprint arXiv:2211.15089, 2022.\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n\nFisher, R. A. On the mathematical foundations of theoretical\nstatistics. Philosophical transactions of the Royal Society\nof London. Series A, containing papers of a mathematical\nor physical character, 222(594-604):309\u2013368, 1922.\n\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li,\nH., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,\nL., Tang, E., Thite, A., Wang, B., Wang, K., and Zou,\nA. A framework for few-shot language model evaluation,\n07 2024. URL https://zenodo.org/records/\n12608602.\n\nGat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Syn-\nnaeve, G., Adi, Y., and Lipman, Y. Discrete flow match-\ning. arXiv preprint arXiv:2407.15595, 2024.\n\nGong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq:\nSequence to sequence text generation with diffusion mod-\nels. arXiv preprint arXiv:2210.08933, 2022.\n\nGong, S., Agarwal, S., Zhang, Y., Ye, J., Zheng, L., Li, M.,\nAn, C., Zhao, P., Bi, W., Han, J., et al. Scaling diffu-\nsion language models via adaptation from autoregressive\nmodels. arXiv preprint arXiv:2410.17891, 2024.\n\nGoogle. Our next-generation model: Gemini 1.5, 2024.\nURL https://blog.google/technology/ai/\ngoogle-gemini-next-generation-model-february-2024.\n\nGraves, A., Srivastava, R. K., Atkinson, T., and\nGomez, F. Bayesian flow networks. arXiv preprint\narXiv:2308.07037, 2023.\n\nGulrajani, I. and Hashimoto, T. B. Likelihood-based diffu-\nsion language models. Advances in Neural Information\nProcessing Systems, 36, 2024.\n\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\n\nHan, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semi-\nautoregressive simplex-based diffusion language model\nfor text generation and modular control. arXiv preprint\narXiv:2210.17432, 2022.\n\n10\n\nhttps://zenodo.org/records/12608602\nhttps://zenodo.org/records/12608602\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\nhttps://blog.google/technology/ai/google-gemini-next-generation-model-february-2024\n\n\nLarge Language Diffusion Models\n\nHe, Z., Sun, T., Wang, K., Huang, X., and Qiu, X. Diffu-\nsionbert: Improving generative masked language models\nwith diffusion models. arXiv preprint arXiv:2211.15029,\n2022.\n\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,\nM., Song, D., and Steinhardt, J. Measuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,\nS., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021.\n\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\narXiv preprint arXiv:2207.12598, 2022.\n\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840\u20136851, 2020.\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\nWelbl, J., Clark, A., et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556,\n2022.\n\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The\ncurious case of neural text degeneration. arXiv preprint\narXiv:1904.09751, 2019.\n\nHoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B.,\nBerg, R. v. d., and Salimans, T. Autoregressive diffusion\nmodels. arXiv preprint arXiv:2110.02037, 2021a.\n\nHoogeboom, E., Nielsen, D., Jaini, P., Forre\u0301, P., and Welling,\nM. Argmax flows and multinomial diffusion: Learning\ncategorical distributions. Advances in Neural Information\nProcessing Systems, 34:12454\u201312465, 2021b.\n\nHu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z.,\nFang, Y., Huang, Y., Zhao, W., et al. Minicpm: Unveiling\nthe potential of small language models with scalable train-\ning strategies. arXiv preprint arXiv:2404.06395, 2024.\n\nHuang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu,\nJ., Lv, C., Zhang, Y., Fu, Y., et al. C-eval: A multi-level\nmulti-discipline chinese evaluation suite for foundation\nmodels. Advances in Neural Information Processing\nSystems, 36, 2024a.\n\nHuang, Y., Zhang, J., Shan, Z., and He, J. Compres-\nsion represents intelligence linearly. arXiv preprint\narXiv:2404.09937, 2024b.\n\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\n\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\n\nKitouni, O., Nolte, N., Hensman, J., and Mitra, B. Disk: A\ndiffusion model for structured knowledge. arXiv preprint\narXiv:2312.05253, 2023.\n\nKitouni, O., Nolte, N., Bouchacourt, D., Williams, A., Rab-\nbat, M., and Ibrahim, M. The factorization curse: Which\ntokens you predict underlie the reversal curse and more.\narXiv preprint arXiv:2406.05183, 2024.\n\nKou, S., Hu, L., He, Z., Deng, Z., and Zhang, H. Cllms:\nConsistency large language models. arXiv preprint\narXiv:2403.00835, 2024.\n\nLi, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y.,\nDuan, N., and Baldwin, T. Cmmlu: Measuring mas-\nsive multitask language understanding in chinese. arXiv\npreprint arXiv:2306.09212, 2023.\n\nLi, X., Thickstun, J., Gulrajani, I., Liang, P. S., and\nHashimoto, T. B. Diffusion-lm improves controllable\ntext generation. Advances in Neural Information Process-\ning Systems, 35:4328\u20134343, 2022.\n\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\nhow models mimic human falsehoods. arXiv preprint\narXiv:2109.07958, 2021.\n\nLin, Z., Gong, Y., Shen, Y., Wu, T., Fan, Z., Lin, C., Duan,\nN., and Chen, W. Text generation with diffusion language\nmodels: A pre-training approach with continuous para-\ngraph denoise. In International Conference on Machine\nLearning, pp. 21051\u201321064. PMLR, 2023.\n\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,\nC., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437, 2024.\n\nLoshchilov, I. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101, 2017.\n\nLou, A. and Ermon, S. Reflected diffusion models, 2023.\n\nLou, A., Meng, C., and Ermon, S. Discrete diffusion lan-\nguage modeling by estimating the ratios of the data distri-\nbution. arXiv preprint arXiv:2310.16834, 2023.\n\nMahabadi, R. K., Ivison, H., Tae, J., Henderson, J., Beltagy,\nI., Peters, M. E., and Cohan, A. Tess: Text-to-text self-\nconditioned simplex diffusion, 2024.\n\n11\n\n\n\nLarge Language Diffusion Models\n\nMeng, C., Choi, K., Song, J., and Ermon, S. Concrete score\nmatching: Generalized score matching for discrete data.\nAdvances in Neural Information Processing Systems, 35:\n34532\u201334545, 2022.\n\nNie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M.,\nand Li, C. Scaling up masked diffusion models on text.\narXiv preprint arXiv:2410.18514, 2024.\n\nOpenAI. ChatGPT: Optimizing Language Models for Di-\nalogue. OpenAI blog, November 2022. URL https:\n//openai.com/blog/chatgpt/.\n\nOpenAI. Learning to reason with llms, 2024.\nURL https://openai.com/index/\nlearning-to-reason-with-llms/.\n\nOu, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li,\nC. Your absorbing discrete diffusion secretly models the\nconditional distributions of clean data. arXiv preprint\narXiv:2406.03736, 2024.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in neural information\nprocessing systems, 35:27730\u201327744, 2022.\n\nPark, J. S., O\u2019Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nannual acm symposium on user interface software and\ntechnology, pp. 1\u201322, 2023.\n\nPeebles, W. and Xie, S. Scalable diffusion models with\ntransformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pp. 4195\u20134205,\n2023.\n\nRadford, A. Improving language understanding by genera-\ntive pre-training, 2018.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\n\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Er-\nmon, S., and Finn, C. Direct preference optimization:\nYour language model is secretly a reward model. Ad-\nvances in Neural Information Processing Systems, 36,\n2024.\n\nReid, M., Hellendoorn, V. J., and Neubig, G. Diffuser:\nDiscrete diffusion via edit-based reconstruction, 2022.\n\nRein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang,\nR. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa:\nA graduate-level google-proof q&a benchmark. arXiv\npreprint arXiv:2311.12022, 2023.\n\nRichemond, P. H., Dieleman, S., and Doucet, A. Categorical\nsdes with simplex diffusion, 2022.\n\nSahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marro-\nquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. Simple\nand effective masked diffusion language models. arXiv\npreprint arXiv:2406.07524, 2024.\n\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: An adversarial winograd schema challenge\nat scale. Communications of the ACM, 64(9):99\u2013106,\n2021.\n\nShannon, C. E. A mathematical theory of communication.\nThe Bell system technical journal, 27(3):379\u2013423, 1948.\n\nShazeer, N. Glu variants improve transformer. arXiv\npreprint arXiv:2002.05202, 2020.\n\nShi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. K.\nSimplified and generalized masked diffusion for discrete\ndata. arXiv preprint arXiv:2406.04329, 2024.\n\nShih, A., Sadigh, D., and Ermon, S. Training and infer-\nence on any-order autoregressive models the right way.\nIn Proceedings of the 31th International Conference on\nMachine Learning, 2022.\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256\u20132265. PMLR, 2015.\n\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\n\nStrudel, R., Tallec, C., Altche\u0301, F., Du, Y., Ganin, Y., Men-\nsch, A., Grathwohl, W., Savinov, N., Dieleman, S., Sifre,\nL., et al. Self-conditioned embedding diffusion for text\ngeneration. arXiv preprint arXiv:2211.04236, 2022.\n\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.\nRoformer: Enhanced transformer with rotary position\nembedding. Neurocomputing, 568:127063, 2024.\n\nSun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Score-\nbased continuous-time discrete diffusion models. arXiv\npreprint arXiv:2211.16750, 2022.\n\nSuzgun, M., Scales, N., Scha\u0308rli, N., Gehrmann, S., Tay,\nY., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,\nE. H., Zhou, D., et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint\narXiv:2210.09261, 2022.\n\n12\n\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/blog/chatgpt/\nhttps://openai.com/index/learning-to-reason-with-llms/\nhttps://openai.com/index/learning-to-reason-with-llms/\n\n\nLarge Language Diffusion Models\n\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n\nUria, B., Murray, I., and Larochelle, H. A deep and tractable\ndensity estimator. In Proceedings of the 31th Interna-\ntional Conference on Machine Learning, 2014.\n\nVaswani, A. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J.,\nChen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on\nlarge language model based autonomous agents. Frontiers\nof Computer Science, 18(6):186345, 2024a.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDiffusion language models are versatile protein learners.\narXiv preprint arXiv:2402.18567, 2024b.\n\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q.\nDplm-2: A multimodal diffusion protein language model.\narXiv preprint arXiv:2410.13782, 2024c.\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\nelicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824\u201324837,\n2022.\n\nWu, T., Fan, Z., Liu, X., Gong, Y., Shen, Y., Jiao, J., Zheng,\nH.-T., Li, J., Wei, Z., Guo, J., Duan, N., and Chen, W.\nAr-diffusion: Auto-regressive diffusion model for text\ngeneration, 2023.\n\nXu, C., Wang, X., Liao, Z., Li, Y., Hou, T., and Deng,\nZ. Show-o turbo: Towards accelerated unified multi-\nmodal understanding and generation. arXiv preprint\narXiv:2502.05415, 2025.\n\nXue, K., Zhou, Y., Nie, S., Min, X., Zhang, X., Zhou, J.,\nand Li, C. Unifying bayesian flow networks and diffusion\nmodels through stochastic differential equations. arXiv\npreprint arXiv:2404.15766, 2024.\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B.,\nLi, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu,\nJ., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang,\nK., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M.,\nZhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren,\nX., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y.,\nCui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\narXiv preprint arXiv:2412.15115, 2024.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Gu, Q. Diffusion\nlanguage models can perform many tasks with scaling and\n\ninstruction-finetuning. arXiv preprint arXiv:2308.12219,\n2023a.\n\nYe, J., Zheng, Z., Bao, Y., Qian, L., and Wang, M. Dinoiser:\nDiffused conditional sequence learning by manipulating\nnoises. arXiv preprint arXiv:2302.10025, 2023b.\n\nYe, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics of Lan-\nguage Models: Part 2.1, Grade-School Math and the Hid-\nden Reasoning Process. ArXiv e-prints, abs/2407.20311,\nJuly 2024. Full version available at http://arxiv.\norg/abs/2407.20311.\n\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. Hellaswag: Can a machine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019.\n\nZhang, B. and Sennrich, R. Root mean square layer nor-\nmalization. Advances in Neural Information Processing\nSystems, 32, 2019.\n\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y.,\nMin, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of\nlarge language models. arXiv preprint arXiv:2303.18223,\n2023.\n\nZheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and\nZhang, Q. Masked diffusion models are secretly time-\nagnostic masked models and exploit inaccurate categor-\nical sampling, 2024. URL https://arxiv.org/\nabs/2409.02908.\n\nZheng, L., Yuan, J., Yu, L., and Kong, L. A reparameter-\nized discrete diffusion model for text generation. ArXiv,\nabs/2302.05737, 2023.\n\n13\n\nhttp://arxiv.org/abs/2407.20311\nhttp://arxiv.org/abs/2407.20311\nhttps://arxiv.org/abs/2409.02908\nhttps://arxiv.org/abs/2409.02908\n\n\nLarge Language Diffusion Models\n\nAlgorithm 1 Pre-training of LLaDA\n\nRequire: mask predictor p\u03b8, data distribution pdata\n1: repeat\n2: x0 \u223c pdata, t \u223c U(0, 1] # with a probability of 1%, the sequence length of x0 follows U[1, 4096]\n3: xt \u223c qt|0(xt|x0) # qt|0 is defined in Eq. (7)\n4: Calculate L = \u2212 1\n\nt\u2217L\n\u2211L\n\ni=1 1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt) # L is the sequence length of x0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 2 Supervised Fine-Tuning of LLaDA\n\nRequire: mask predictor p\u03b8, pair data distribution pdata\n1: repeat\n2: p0, r0 \u223c pdata, t \u223c U(0, 1] # please refer to Appendix B.1 for details on the SFT data processing.\n3: rt \u223c qt|0(rt|r0) # qt|0 is defined in Eq. (7)\n\n4: Calculate L = \u2212 1\nt\u2217L\u2032\n\n\u2211L\u2032\n\ni=1 1[rit = M] log p\u03b8(r\ni\n0|p0, rt) # L\u2032 is the sequence length of r0\n\n5: Calculate\u2207\u03b8L and run optimizer.\n6: until Converged\n7: Return p\u03b8\n\nAlgorithm 3 Conditional Log-likelihood Evaluation of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, response r0, the number of Monte Carlo estimations nmc\n\n1: log likelihood = 0\n2: for i\u2190 1 to nmc do\n3: l \u223c {1, 2, . . . , L} # L is the sequence length of r0\n4: Obtain rl by uniformly sampling l tokens from r0 without replacement for masking\n5: log likelihood = log likelihood + L\n\nl\n\n\u2211L\ni=1 1[ril = M] log p\u03b8(r\n\ni\n0|p0, rl)\n\n6: end for\n7: log likelihood = log likelihood/nmc\n\n8: Return log likelihood\n\nA. Formulation of Masked Diffusion Models\n\nA.1. Training\n\nMasked diffusion models (MDMs) (Austin et al., 2021a; Lou et al., 2023; Ou et al., 2024) define the model distribution\np\u03b8(x0) in a manner distinct from autoregressive models.\n\nThese models introduce a forward process {xt} indexed by a time t \u2208 [0, 1]. This process gradually and independently\nmasks all tokens in the sequence x0. At time t = 0, the data point x0 is fully observed with no masks, while for t \u2208 (0, 1],\nxt represents latent variables with varying mask ratios in expectation.\n\nFormally, the conditional distribution of xt given x0 is defined by a fully factorized form:\n\nqt|0(xt|x0) =\n\nL\u220f\ni=1\n\nqt|0(x\ni\nt|xi\n\n0), (7)\n\nwhere the conditional distribution for each token is given by:\n\nqt|0(x\ni\nt|xi\n\n0) =\n\n{\n1\u2212 t, xi\n\nt = xi\n0,\n\nt, xi\nt = M.\n\n(8)\n\nHere, M denotes the mask token. Intuitively, each token either remains unchanged or is masked, with the probability of\n\n14\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 4 Reverse Process of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: r0 = argmaxr0 p\u03b8(r0|p0, rt) # we employ greedy sampling when predicting masked tokens\n5: for i\u2190 1 to L do\n6: if rt \u0338= M then\n7: ri0 = rit\n8: else\n9: With probability s\n\nt , ri0 is set to M\n10: end if\n11: end for\n12: rs = r0\n13: end for\n14: Return r0\n\nbeing masked increasing linearly as t progresses from 0 to 1. At t = 1, all tokens are guaranteed to be masked, meaning that\nx1 follows a Dirac distribution concentrated on a sequence of fully masked tokens. Notably, the linear masking probability\nis analogous to but distinct from, the noise schedule in continuous diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2020). This linearity is motivated by the assumption that the information in the text is proportional to the\nnumber of tokens on average, making it reasonable to lose information linearly during the forward process.\n\nThe forward process is not only reversible but also corresponds to a reverse process that is fully factorized across all\ntokens (Austin et al., 2021a). The reverse process, from time t = 1 to 0, generates new data from sequences of fully masked\ntokens. The conditional distribution for the reverse process, for 0 \u2264 s < t \u2264 1, is factorized as:\n\nqs|t(xs|xt) =\n\nL\u220f\ni=1\n\nqs|t(x\ni\ns|xt), (9)\n\nwhere the conditional distribution for each token is:\n\nqs|t(x\ni\ns|xt) =\n\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n1, xi\n\nt \u0338= M, xi\ns = xi\n\nt,\ns\nt , xi\n\nt = M, xi\ns = M,\n\nt\u2212s\nt q0|t(x\n\ni\ns|xt), xi\n\nt = M, xi\ns \u0338= M,\n\n0, otherwise.\n\n(10)\n\nThus, the key function to estimate is the conditional distribution q0|t(x\ni\ns|xt), which predicts the original token if it is masked\n\nin the input xt. This is analogous to the data prediction form in continuous diffusion models.\n\nAs proven in (Ou et al., 2024), an equivalent yet time-free parameterization can be derived as:\n\nq0|t(x\ni\ns|xt) = pdata(x\n\ni\n0|xUM\n\nt ), \u2200i such that xi\nt = M, (11)\n\nwhere xUM\nt denotes the collection of unmasked tokens in xt, which is identical to the corresponding tokens in the original\n\ndata x0 since unmasked tokens are solely determined by x0 and are independent of time t. Intuitively, this implies that\nestimating the data prediction function is equivalent to estimating the conditional distributions on clean data, which is\ntime-invariant. Consequently, the time t need not be provided as input to the parametric model.\n\nAlthough the development of masked diffusion is nontrivial, the implementation is straightforward. We first introduce the\nmask predictor, a parametric model p\u03b8(\u00b7|xt) (e.g., a Transformer without a causal mask), which takes xt for any t as input\nand predict all masked tokens simultaneously. Then, we define the model distribution p\u03b8(x0) as follows: starting with x1 as\na sequence of fully masked tokens, we simulate an approximate reverse process parameterized by p\u03b8(\u00b7|xt) from t = 1 to 0.\nThe marginal distribution induced at t = 0 then represents the model distribution p\u03b8(x0).\n\n15\n\n\n\nLarge Language Diffusion Models\n\nAlgorithm 5 Low-confidence Remasking Strategy of LLaDA\n\nRequire: mask predictor p\u03b8, prompt p0, answer length L, sampling steps N\n1: Set r1 is a fully masked sequence of length L.\n2: for t\u2190 1 down to 1\n\nN step 1\nN do\n\n3: s = t\u2212 1\nN\n\n4: for i\u2190 1 to L do\n5: if rit \u0338= M then\n6: ri0 = rit, c\n\ni = 1\n7: else\n8: ri0 = argmaxri0 p\u03b8(r\n\ni\n0|p0, rt)\n\n9: ci = p\u03b8(r\ni\n0|p0, rt)ri0\n\n10: end if\n11: end for\n12: nun = \u230aL(1\u2212 s)\u230b # the number of unmasked tokens is nun in timestep s\n13: for i\u2190 1 to L do\n14: if ci \u2208 Lowest\u2212 nun\n\n(\n{ci}L1\n\n)\nthen\n\n15: ri0 = M # the nun positions with the least confidence are selected for remasking.\n16: end if\n17: end for\n18: rs = r0\n19: end for\n20: Return r0\n\nFormally, the mask predictor is trained using a cross-entropy loss with masking:\n\nL(\u03b8) \u225c \u2212Et,x0,xt\n\n[\n1\n\nt\n\nL\u2211\ni=1\n\n1[xi\nt = M] log p\u03b8(x\n\ni\n0|xt)\n\n]\n, (12)\n\nwhere x0 is sampled from the training data, t is sampled uniformly from [0, 1], and xt is sampled from qt|0(xt|x0). The\nindicator function 1[\u00b7] ensures that the cross-entropy loss is computed only for masked tokens. In Ou et al. (2024), it has\nbeen proven that the loss function L(\u03b8) is an upper bound on the negative log-likelihood of the model distribution:\n\n\u2212Ex0\u223cpdata(x0)\n[log p\u03b8(x0)] \u2264 L(\u03b8). (13)\n\nIn summary, this principled approach trains a generative model by progressively masking tokens during a forward process\nand learning to recover the data distribution during a reverse process, all under the (approximate) maximum likelihood\nestimation framework.\n\nA.2. Inference\n\nThe cross-entropy loss in Eq. (12) has several equivalent forms (Ou et al., 2024). The first one is given by\n\n\u2212El\u223c{1,2,...,L},x0,xl\n\n[\nL\n\nl\n\nL\u2211\ni=1\n\n1[xi\nl = M] log p\u03b8(x\n\ni\n0|xl)\n\n]\n, (14)\n\nwhere l is uniformly sampled from {1, 2, . . . , L}, and xl is obtained by uniformly sampling l tokens from x0 without\nreplacement for masking. Despite masking exactly l tokens is different from masking each token independently with\nprobability t, these two masking methods lead to equivalent results in expectation (Ou et al., 2024).\n\nWhile Eq. (12) and Eq. (14) share the same expectation, their variances differ. Intuitively, in Eq. (12), we expect xt to\nhave a fraction of t tokens masked. However, the randomness of the forward process (i.e., Eq. (7)) often causes deviations,\nespecially when xt contains few tokens. In contrast, in Eq. (14), the fraction of masked tokens in xl is deterministically l\n\nL .\nWhile a theoretical analysis depends on the data distribution, empirical results show that Eq. (12) requires over 1000 Monte\nCarlo estimates for stable results, whereas Eq. (14) achieves stability with only 128 estimates. In addition, we can simply\nmodify Eq. (14) to its conditional version (i.e., Eq. (6)) based on Eq. (5).\n\n16\n\n\n\nLarge Language Diffusion Models\n\nPrompt Block 0\n\n\ud835\udc61 = 0\n\n\ud835\udc61 = 1\n\nBlock 1 Block 2\n\nNon-mask token\n\nMask token\n\nFigure 4. A Conceptual Overview of the Semi-autoregressive Sampling.\n\nAny-order autoregressive models (AO-ARM) (Uria et al., 2014; Hoogeboom et al., 2021a; Shih et al., 2022) characterize the\njoint distribution autoregressively for all possible orders \u03c0 of the L variables. To learn such a distribution, an AO-ARM\nutilizes a weight-sharing neural network to model all univariate conditionals and employs mask tokens to represent absent\nvariables. During training, the expected negative log-likelihood over the uniform distribution of all orders U\u03c0 is minimized:\n\n\u2212Ex0,\u03c0\u223cU\u03c0\n\n[\nL\u2211\n\ni=1\n\nlog p\u03b8(x\n\u03c0(i)\n0 |x\u03c0(<i)\n\n0 ;\u03c0)\n\n]\n. (15)\n\nIntuitively, x\u03c0(<i)\n0 can be understood as a masked token xt with index in \u03c0(\u2265 i) being masked. It can be further proved that\n\nEq. (15) is equivalent to Eq. (12). This connection explains the bidirectional reasoning capabilities of LLaDA, even though\nit was never used explicitly in the inference procedure.\n\nNie et al. (2024) introduce unsupervised classifier-free guidance, a plug-and-play technique that balances alignment with\nprompts and text diversity. Specifically, unsupervised classifier-free guidance employs the following modified mask predictor\nfor inference:\n\np\u0303\u03b8(r0|p0, rt) \u221d\np\u03b8(r0|p0, rt)1+w\n\np\u03b8(r0|m, rt)w\n, (16)\n\nwhere m is a mask sequence of the same length as p0 and w is a hyperparameter that controls the strength of p0. We employ\nunsupervised classifier-free guidance in the downstream tasks, as detailed in Appendix B.5.\n\nA.3. Algorithms\n\nIn this section, we present the training and inference algorithms. Specifically, we introduce the pre-training and supervised\nfine-tuning algorithms in Algorithm 1 and Algorithm 2, respectively. In addition, the likelihood evaluation algorithm is\nprovided in Algorithm 3. Finally, we present the sampling algorithms in Algorithm 4 and Algorithm 5, which correspond to\nthe original reverse process (i.e., random remasking) and the low-confidence remasking strategy (Chang et al., 2022) of\nLLaDA, respectively.\n\nFurthermore, we apply a semi-autoregressive remasking strategy for the instruct model. Specifically, the sequence is\ndivided into several blocks and generated from left to right. Within each block, we apply the random remasking or the\nlow-confidence remasking strategy for sampling. For clarity and better visualization, we illustrate the semi-autoregressive\nsampling process in Fig. 4.\n\nOur semi-autoregressive remasking strategy is inspired by the prior work of Anonymous (2025). However, it is important to\nnote that Anonymous (2025) and LLaDA have entirely different training objectives and network architectures. Specifically,\nthe pre-training objective of Anonymous (2025) differs from Eq. (3). Moreover, their approach incorporates a causal mask\nin the Transformer attention block, whereas LLaDA employs a bidirectional Transformer.\n\n17\n\n\n\nLarge Language Diffusion Models\n\nTable 5. Model Architecture. We provide the configurations of our 1B and 7B ARM baselines, as well as the 1B and 8B LLaDA and the\n8B LLaMA3 models.\n\nOur ARM Baseline 1B LLaDA 1B Our ARM Baseline 7B LLaDA 8B LLaMA3 8B\n\nLayers 22 22 28 32 32\n\nModel dimension 2048 2048 4096 4096 4096\n\nAttention heads 32 32 32 32 32\n\nVocabulary size 126,464 126,464 126,464 126,464 128,000\n\nFFN dimension 5634 5634 13,440 12,288 14,336\n\nKey/Value heads 4 4 8 32 8\n\nTotal parameters 1.49 B 1.49 B 6.83 B 8.02 B 8.03 B\n\nNon-embedding parameters 0.97 B 0.97 B 5.80 B 6.98 B 6.98 B\n\nB. Experiments\nB.1. Details of SFT Data\n\nA dynamic sequence length is employed during supervised SFT, where |EOS| tokens are appended to the end of short pairs\nto ensure equal sequence lengths across all data in each mini-batch. Notably, the padding |EOS| tokens are treated as part of\nthe response, i.e., masked and included in the training objective. The |EOS| tokens are removed from the generated outputs\nduring sampling. This strategy is crucial for LLaDA and ensures that the model learns to control the length of its responses\nby generating |EOS|, enabling the response length to align effectively with the given prompt.\n\nIn addition, for n-turn dialogues (p00, r\n0\n0, p\n\n1\n0, r\n\n1\n0, . . . , p\n\nn\u22121\n0 , rn\u22121\n\n0 ), we treat it as n single-turn dialogue pairs, i.e.,\n(p00, r\n\n0\n0), (p\n\n0\n0r\n\n0\n0p\n\n1\n0, r\n\n1\n0), . . . , (p\n\n0\n0r\n\n0\n0p\n\n1\n0r\n\n1\n0 . . . p\n\nn\u22121\n0 , rn\u22121\n\n0 ) and randomly sample one. This data partitioning strategy not only\nequips LLaDA with multi-turn dialogue capabilities but also aligns with the padding strategy.\n\nB.2. Architectures and Scaling Experiments\n\nThis section provides detailed information about the models we trained and the scalability experiments described in Sec. 3.1.\n\nFirstly, for efficiency, we trained an ARM and a MDM, both with 1.5B parameters and identical architectures. Additionally,\nwe scaled the MDM to 8B parameters. Due to computational resource constraints, we did not train an 8B autoregressive\nmodel with the same architecture. Instead, we utilized our previously trained 7B autoregressive model for comparison.\nThese four models are utilized in the scalability analysis in Sec. 3.1.\n\nWe adopted a Transformer architecture similar to LLaMA (Touvron et al., 2023; Dubey et al., 2024) for the ARMs and MDMs\nwe trained. Specifically, we employ RMSNorm (Zhang & Sennrich, 2019) to stabilize training, used SwiGLU (Shazeer,\n2020) as the activation function to enhance non-linearity, and integrate RoPE (Su et al., 2024) for more expressive positional\nencoding. Tab. 5 provides an overview of the model architectures.\n\nAdditionally, we employ the widely used 6ND formulation (Kaplan et al., 2020; Hoffmann et al., 2022) to calculate the\ntraining FLOPs in Fig. 3, where N represents the number of non-embedding parameters, and D denotes the total number of\ntraining tokens.\n\nB.3. Details and Ablation on Remasking\n\nThis section first explains the inference methods in detail, followed by ablation studies on various remasking strategies.\n\nFor instruct model, since our training includes a large number of |EOS| tokens, directly applying the lowest confidence\nremasking strategy would result in an excessively high and unnatural occurrence of |EOS| tokens in the generated sentences.\nTherefore, we divide the entire generation length into several blocks. Each block is generated from left to right, and within\neach block, we apply the reverse process to perform sampling. However, for the base model, since |EOS| tokens are not\npadded during training, using the semi-autoregressive remasking strategy makes little difference in the results.\n\n18\n\n\n\nLarge Language Diffusion Models\n\nTable 6. Ablation on Remasking. We use GSM8K accuracy as the evaluation metric. The lowest-confidence & semi-autoregressive\nremasking strategy refers to generating each block from left to right, while applying the lowest-confidence remasking strategy within each\nblock.\n\nLLaDA 8B Base LLaDA 8B Instruct\n\nRandomly remasking 52.3 72.0\nLowest confidence remasking 64.7 12.9\nLowest confidence & semi-autoregressive remasking 64.4 73.8\n\nTable 7. Ablation on Generated Length. We use the GSM8K accuracy as the evaluation metric and set the number of sampling steps to\nhalf of the generated length.\n\nLength LLaDA 8B Base LLaDA 8B Instruct\n\n256 62.5 75.3\n512 64.7 73.8\n1024 65.9 75.3\n\nFor efficiency, we set the answer length to 512 and the number of sampling steps to 256 in our ablation studies. For the\nsemi-autoregressive remasking strategy, we set the block length to 32. To simplify the evaluation, we use GSM8K accuracy\nas the performance metric. As shown in Table 6, for the base model, applying the lowest confidence remasking strategy\nsignificantly improves performance, whereas the semi-autoregressive remasking strategy does not yield any noticeable\nbenefits. In contrast, for the instruct model, directly applying the lowest confidence remasking strategy leads to a substantial\nperformance drop. However, combining the lowest confidence remasking strategy with the semi-autoregressive remasking\nstrategy results in the best performance for the instruct model.\n\nWe provide the specific remasking strategies and hyperparameters used for each benchmark in Appendix B.5.\n\nB.4. Ablation on Generated Length\n\nIn this section, we conduct ablation studies on the generated length. For simplicity, we use GSM8K as the evaluation metric.\nTo ensure efficiency and fairness, for each setting, we set the number of sampling steps to half of the generated length,\nensuring that in each sampling step, two tokens are transferred from the mask to the text.\n\nFor the LLaDA 8B Base model, we employ the lowest-confidence remasking strategy (i.e., Algorithm 5). For the LLaDA\n8B Instruct model, we adopt semi-autoregressive sampling with a block length of 32.\n\nAs reported in Tab. 7, both the Base model and the Instruct model exhibit minimal sensitivity to the length hyperparameter.\n\nB.5. Standard Benchmarks and Evaluation\n\nIn this section, we briefly introduce the benchmarks used in our study and describe the details of our evaluation process.\n\nFollowing standard LLM (Chu et al., 2024; Yang et al., 2024) evaluation practices, we assess LLaDA across four dimensions:\n\nGeneral ability: MMLU (Hendrycks et al., 2020), BBH (Suzgun et al., 2022), ARC-C (Clark et al., 2018), Hel-\nlaswag (Zellers et al., 2019), TruthfulQA (Lin et al., 2021), WinoGrande (Sakaguchi et al., 2021) and PIQA (Bisk\net al., 2020).\n\nMath and science ability: GSM8K (Cobbe et al., 2021), Math (Hendrycks et al., 2021) and GPQA (Rein et al., 2023).\n\nCode generation: HumanEval (Chen et al., 2021), HumanEval-FIM (Bavarian et al., 2022) and MBPP (Austin et al.,\n2021b).\n\nChinese understanding: CMMLU (Li et al., 2023) and C-Eval (Huang et al., 2024a).\n\nFor all the aforementioned benchmarks, we follow the widely adopted evaluation process (Gao et al., 2024) used in LLM\nassessments, primarily employing conditional likelihood estimation and conditional generation. Specifically, for certain\nbenchmarks, a prompt and multiple candidate answers are provided, and the model is required to compute each candidate\u2019s\n\n19\n\n\n\nLarge Language Diffusion Models\n\nTable 8. Inference Configuration for LLaDA 8B Instruct. We apply the low-confidence remasking strategy across all datasets. A block\nlength smaller than the answer length indicates the use of the semi-autoregressive remasking strategy; otherwise, it is not applied. The\nshorter answer lengths for MMLU and HellaSwag result from their requirement to generate only a single token (i.e., A, B, C, or D) and\ntwo special end tokens (i.e., end of the model response and end of the sentence).\n\nAnswer length Block length\n\nMMLU 3 3\nHellaswag 3 3\nGPQA 128 64\nGSM8K 256 8\nHumanEval 512 32\nMBPP 512 32\nMath 256 256\nMMLU-pro 256 256\nARC-C 512 512\n\nTable 9. Comparison on iGSM Dataset.\n\n4 steps 5 steps 6 steps\n\nLLaMA3 8B Base 38.0 35.0 34.0\nLLaDA 8B Base 64.0 41.0 44.0\n\nconditional likelihood. The candidate with the highest likelihood is then selected as the model\u2019s final answer, and accuracy\nis used as the evaluation metric. For the remaining benchmarks, the model generates responses based on the given prompt,\nand performance is evaluated using metrics such as exact match and other relevant criteria.\n\nFor the base model, we use conditional likelihood estimation for MMLU, CMMLU, C-Eval, ARC-C, Hellaswag, TruthfulQA,\nWinoGrande, PIQA, and GPQA, while the remaining benchmarks are evaluated using conditional generation. For the\ninstruct model, we evaluate all benchmarks using conditional generation. For datasets evaluated with conditional likelihood\nestimation, we adopt the widely used lm-evaluation-harness (Gao et al., 2024) framework. However, for datasets evaluated\nwith conditional generation, we rely on an internal library for evaluation, as lm-evaluation-harness lacks implementations\nfor certain key metrics (e.g., HumanEval-FIM).\n\nFor benchmarks evaluated using conditional likelihood estimation, we use Monte Carlo estimation to approximate Eq. (6)\nfor conditional likelihood estimation. Since MMLU, CMMLU, and C-EVAL only require the likelihood of a single token, a\nsingle Monte Carlo estimate is sufficient for these benchmarks. For all other benchmarks, we find that 128 Monte Carlo\nsamples are adequate to produce stable results. For ARC-C, Hellaswag, TruthfulQA, WinoGrande, PIQA, and GPQA, we\nreport the results with the optimal unsupervised classifier-free guidance scale searched in {0, 0.5, 1, 1.5, 2}.\n\nFor benchmarks evaluated using conditional generation, we apply the lowest confidence remasking strategy to the base model,\nsetting both the response length and the number of sampling steps to 1024. For the instruct model, we set the sampling steps\nequal to the answer length and provide the inference hyperparameters in Table 8. Due to computational resource constraints,\nwe do not use the unsupervised classifier-free guidance for these benchmarks evaluated using conditional generation.\n\nB.6. Analysis of Sampling Steps\n\nIn this section, we analyze the impact of sampling steps. For simplicity, we use the LLaDA 8B Base model and evaluate it\non the GSM8K and HumanEval datasets. Additionally, we fix the answer length at 1024. As shown in Fig. 5, as the number\nof sampling steps increases to match the answer length, the model\u2019s accuracy improves accordingly.\n\nB.7. Evaluation on iGSM Dataset\n\nTo further assess the mathematical capabilities of LLaDA, we test its performance on iGSM (Ye et al., 2024), an infinite,\nsynthetic GSM8K-like dataset. iGSM is generated via specific rules, with parameters that control the difficulty of problems\n(i.e., the number of solution steps). For evaluation consistency, we append \u201d#### $answer\u201d to the final solution, adhering to\n\n20\n\n\n\nLarge Language Diffusion Models\n\n64 128 256 512 1,024\nNFEs\n\n20\n\n40\n\n60\nG\n\nSM\n8K\n\n (4\n-s\n\nho\nt)\n\n64 128 256 512 1,024\nNFEs\n\n10\n\n20\n\n30\n\nH\num\n\nan\nEv\n\nal\n (0\n\n-s\nho\n\nt)\n\nFigure 5. Analysis of Sampling Steps.\n\nthe GSM8K format. Below is an example with solution steps set to 4:\n\n(Question) The number of each North Star Elementary\u2019s Cultural Studies Classroom equals 1. The number of each\nWestridge Elementary\u2019s Dance Studio equals 3 times as much as the sum of each North Star Elementary\u2019s Classroom\nand each North Star Elementary\u2019s Cultural Studies Classroom. How many Dance Studio does Westridge Elementary\nhave?\n(Solution) Define North Star Elementary\u2019s Cultural Studies Classroom as x; so x = 1.\nDefine North Star Elementary\u2019s Classroom as m; so m = x = 1.\nDefine Westridge Elementary\u2019s Dance Studio as n; w = m + x = 1 + 1 = 2;\nso n = 3 * w = 3 * 2 = 1 #### 1\n\nSince there are slight differences between GSM8K and iGSM (e.g., the use of a mod 5 algorithmic system), we follow (Ye\net al., 2024) and provide a system prompt along with four-shot question-answer pairs for each problem.\n\n(Prompt) You\u2019re an expert at solving elementary math problems involving addition, subtraction, and multiplication.\nYou solve all the problems in a uniform format. All calculations are done modulo 5. For example, 4 + 4 equals 3, 2 + 4\nequals 1, 3 + 3 + 3 equals 4, 3 * 3 equals 4, and 2 * 2 equals 4. When providing your solution, please end with \u2019#### x.\u2019\nwhere x is your final answer, an integer between 0 and 4. You must solve all the problems using the same solution\nformat. Our scenarios involve up to four categories of objects: schools, classrooms, backpacks and stationeries. Each\nschool may contain classrooms, each classroom may contain backpacks, and each backpack may contain stationeries.\nWe can specify quantities, such as t\u0308he number of dance studios at each Lakeshore High.A\u0308ssume that every entity\nwith the same name has an identical configuration; for example, each Lakeshore High contains the same number of\ndance studios. Another guiding principle is that what is not mentioned does not exist: when we refer to classrooms at\nLakeshore High, we are only discussing the classrooms explicitly mentioned in our scenario. Furthermore, if Lakeshore\nHigh is not even mentioned, any classroom within it is automatically considered to be non-existent (i.e. 0).\n\nFor solution steps ranging from 4 to 6, we generate 100 questions for each case and report the corresponding accuracy in\nTable 9. As shown in the table, LLaDA Base demonstrates significant and consistent advantages over LLaMA3 on unseen\nmathematical problems, aligning with the results in Table 1.\n\nB.8. Poem Completion Tasks\n\nIn this section, we present examples from our poem completion dataset as follows.\n\nExample 1:\nPrompt: \u7a88\u7a95\u6dd1\u5973\u7684\u4e0b\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u541b\u5b50\u597d\u9011\u3002\n\n21\n\n\n\nLarge Language Diffusion Models\n\nTable 10. Visualization of the Semi-Autoregressive Sampling Process. In the response of LLaDA, darker colors indicate tokens\npredicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser What are the benefits of regular exercise for physical and mental health?\nLLaDA\n\nTable 11. Visualization of the Multi-turn Dialogue. We employ random remasking strategy. In the response of LLaDA, darker colors\nindicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.\n\nUser I want to have a pet, but I don\u2019t know whether to get a cat or a dog. Do you have any\nsuggestions?\n\nLLaDA\n\nUser I\u2019m very busy at work, so it seems like a cat would be a more suitable option. Do you have\nany suggestions for getting a cat?\n\nLLaDA\n\nExample 2:\nPrompt: \u4e0d\u62d8\u4e00\u683c\u964d\u4eba\u624d\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48\uff1f\u76f4\u63a5\u8f93\u51fa\u53e5\u5b50\u5373\u53ef\u3002\nAnswer: \u6211\u529d\u5929\u516c\u91cd\u6296\u64de\u3002\n\nB.9. More Case Studies\n\nIn this section, we present additional case studies of LLaDA 8B Instruct. First, Tab. 10 illustrates the sampling process of\nsemi-autoregressive remasking, while Tab. 11 depicts the sampling process for multi-turn dialogues with random remasking.\nAdditionally, Tab. 12 and Tab. 13 provide further examples of single-turn and multi-turn dialogues. Finally, Tab. 14 presents\nexamples of poem reversal completions where the LLaDA 8B Instruct model succeeds, in contrast to the failure of GPT-4o.\n\n22\n\n\n\nLarge Language Diffusion Models\n\nTable 12. Single-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser What\u2019s a piece of advice you\u2019ve received that you\u2019ve found helpful?\nLLaDA One helpful piece of advice I\u2019ve received is to always be open to learning and growth.\n\nI\u2019ve found that being willing to explore new ideas and experiences has helped me become\nmore resilient and adaptable. By embracing a growth mindset, I\u2019ve been able to identify\nopportunities for improvement and development, both personally and professionally.\n\nDaily life\nUser Please recommend me three famous movies\nLLaDA Certainly! Here are three famous movies that have achieved widespread popularity and\n\ncritical acclaim:\n1. **The Shawshank Redemption** (1994) - Inspired by a novella and directed by Frank\nDarabont, this film tells the story of Andy Dufresne, a man convicted of murdering his\nwife, dealing with his harsh life in Shawshank Prison and forming an unlikely friendship\nwith a fellow inmate.\n2. **The Godfather** (1972) - Directed by Francis Ford Coppola, this film is based\non Mario Puzo\u2019s novel of the same name. It follows the story of the Corleone family,\nparticularly Michael Corleone, who rises to power and takes over the family\u2019s criminal\nempire.\n3. **The Empire Strikes Back** (1980) - Directed by George Lucas, this film continues\nthe story of a young Skywalker in the Star Wars saga. It follows Luke Skywalker as\nhe helps the Rebel Alliance fight against the Galactic Empire and destroy the powerful\nweapon, the Death Star.\nAll of these films are celebrated for their cinematic achievements, powerful storytelling,\nand memorable characters.\n\nTranslation\nUser Please help me translate into Chinese: \u2018What is now proved was once only imagined\u2019\nLLaDA \u73b0\u5728\u88ab\u8bc1\u660e\u7684\u66fe\u7ecf\u53ea\u662f\u60f3\u8c61\n\nTranslation\nUser Please help me translate into German: \u2018What is now proved was once only imagined\u2019\nLLaDA Wer heute besta\u0308tigt ist, war einst nur immaginiert.\n\nMath\nUser Emma decides to run 4 sprints 5 times a week. She runs 75 meters each sprint. How many\n\ntotal meters does she run a week?\nLLaDA Emma runs 4 sprints each time, so she runs a total of 4 x 5 = 20 sprints a week. Each sprint\n\nis 75 meters long, so Emma runs a total of 20 x 75 = 1500 meters a week. The final result\nis 1500\n\nCode\nUser I am writing python code, I have an array, please help me sort it from largest to smallest\nLLaDA Certainly! You can sort an array from the largest to smallest in Python using the \u2018sort()\u2019\n\nmethod or the \u2018sorted()\u2018 function. Here\u2019s an example:\n### Using \u2018sort()\u2019 method:\nmy array = [3, 1, 4, 1, 5, 2]\nmy array.sort(reverse=True)\nprint(my array)\n\n23\n\n\n\nLarge Language Diffusion Models\n\nTable 13. Multi-turn Dialogue Cases of LLaDA 8B Instruct.\n\nDaily life\nUser How do I prepare for a job interview?\nLLaDA To prepare for a job interview, you should research the company and the job, practice\n\ncommon interview questions, and dress appropriately.\nUser What are some common interview questions?\nLLaDA Some common interview questions include \u201cTell me about yourself\u201d and \u201cWhy do you\n\nwant to work for us\u201d?\n\nDaily life\nUser How do I start learning programming?\nLLaDA To learn programming, you can start by choosing a programming language that you are\n\ninterested in. Some popular languages include Python, Java, and C++.\nUser Which programming language should I learn first?\nLLaDA It depends on your interests and goals. Python is a great language for beginners for its\n\nsimplicity and versatility.\nUser How long does it take to learn Python?\nLLaDA It can take anywhere from a few weeks to several months to learn Python.\nUser What are some good Python projects for beginners?\nLLaDA Some good Python projects for beginners include building a simple calculator, a game, and\n\na web application.\n\nMath\nUser Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.\n\nHow many clips did Natalia sell altogether in April and May?\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n\n48/2 = 24 clips in May. Altogether, Natalia sold 48 + 24 = 72 clips in April and May. The\nfinal result is 72.\n\nUser Natalia sold three times as many items in June as she did in May. How many clips did\nNatalia sell altogether in these three months?\n\nLLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold\n48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May,\nso she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in\nthese three months. The final result is 144.\n\nTable 14. Poem Reversal Completion Cases where LLaDA 8B Instruct Succeeds but GPT-4o Fails.\n\nQuestion1\nUser \u2018\u4f46\u95fb\u4eba\u8bed\u54cd\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\u201d,\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bcd\u4eba\u674e\u6e05\u7167\u7684\uff08\u6b66\u9675\n\n\u6625\uff0c\u6625\u665a:\u65e0\u53ef\u5948\u4f55\u82b1\u843d\u53bb\uff0c\u4f46\u95fb\u4eba\u8bed\u54cd\u3002\u8fd9\u9996\u8bcd\u8868\u8fbe\u4e86\u4f5c\u8005\u5728\u6625\u5929\u65f6\u8282\u7684\u5b64\u72ec\u4e0e\n\u611f\u4f24\uff0c\u501f\u82b1\u843d\u548c\u4eba\u8bed\u58f0\u7684\u63cf\u5199\uff0c\u53cd\u6620\u51fa\u8bcd\u4eba\u5185\u5fc3\u7684\u65e0\u4eb2\u4e0e\u5931\u843d\u3002\n\nLLaDA \u201c\u4f46\u95fb\u4eba\u8bed\u54cd\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u4e0d\u89c1\u4eba\u201d\n\nQuestion2\nUser \u2018\u591c\u9759\u6625\u5c71\u7a7a\u2019\u7684\u4e0a\u4e00\u53e5\u662f\u4ec0\u4e48?\nGPT-4o \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u7a7a\u5c71\u65b0\u96e8\u540e\u201d.\u8fd9\u4e24\u53e5\u51fa\u81ea\u5510\u4ee3\u8bd7\u4eba\u738b\u7ef4\u7684\uff08\u5c71\u5c45\u79cb\n\n\u819c\uff09:\u7a7a\u5c71\u65b0\u96e8\u540e\uff0c\u591c\u9759\u6625\u5c71\u7a7a\u3002\u8fd9\u9996\u8bd7\u63cf\u7ed8\u4e86\u6625\u5929\u5c71\u4e2d\u96e8\u540e\u7684\u5bc2\u9759\u4e0e\u7a7a\u7075\uff0c\n\u8868\u73b0\u51fa\u81ea\u7136\u7684\u5b81\u9759\u4e0e\u548c\u8c10\u7f8e\u3002\n\nLLaDA \u201c\u591c\u9759\u6625\u5c71\u7a7a\u201d\u7684\u4e0a\u4e00\u53e5\u662f\u201c\u4eba\u95f2\u6842\u82b1\u843d\u201d\n\n24","cloud_doc_url":null}],"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"249d096c-1946-4cfd-83f3-8b8a938aa1f2","request_id":"91ca6c63f9c840ba-SJC","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"249d096c-1946-4cfd-83f3-8b8a938aa1f2","children":[]}}}