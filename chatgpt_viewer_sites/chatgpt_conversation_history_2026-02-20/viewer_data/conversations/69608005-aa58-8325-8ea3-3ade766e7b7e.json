{"id":"69608005-aa58-8325-8ea3-3ade766e7b7e","title":"Designing a Python Coding Interview","create_time":1767931999.213909,"update_time":1767932704.655111,"current_node":"56be9a2f-4dec-4593-ba0e-6d8bc092f8d4","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["f958fb8e-f126-48e4-9fb8-577f074d6a68"]},"f958fb8e-f126-48e4-9fb8-577f074d6a68":{"id":"f958fb8e-f126-48e4-9fb8-577f074d6a68","message":{"id":"f958fb8e-f126-48e4-9fb8-577f074d6a68","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"client-created-root","children":["4d35aca0-d17a-4966-8633-54d8ee1ffb07"]},"4d35aca0-d17a-4966-8633-54d8ee1ffb07":{"id":"4d35aca0-d17a-4966-8633-54d8ee1ffb07","message":{"id":"4d35aca0-d17a-4966-8633-54d8ee1ffb07","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"f958fb8e-f126-48e4-9fb8-577f074d6a68","children":["cd6a71e4-1033-4d6a-8ac8-7079ff996f04"]},"cd6a71e4-1033-4d6a-8ac8-7079ff996f04":{"id":"cd6a71e4-1033-4d6a-8ac8-7079ff996f04","message":{"id":"cd6a71e4-1033-4d6a-8ac8-7079ff996f04","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Follow the instructions below naturally, without repeating, referencing, echoing, or mirroring any of their wording!\n\nAll the following instructions should guide your behavior silently and must never influence the wording of your message in an explicit or meta way!\n\nBe precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"4d35aca0-d17a-4966-8633-54d8ee1ffb07","children":["decd5235-93aa-4f53-b596-9890d5f2111b"]},"decd5235-93aa-4f53-b596-9890d5f2111b":{"id":"decd5235-93aa-4f53-b596-9890d5f2111b","message":{"id":"decd5235-93aa-4f53-b596-9890d5f2111b","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"cd6a71e4-1033-4d6a-8ac8-7079ff996f04","children":["fa8d62ff-370d-479c-bb26-1e8e5638dba0"]},"fa8d62ff-370d-479c-bb26-1e8e5638dba0":{"id":"fa8d62ff-370d-479c-bb26-1e8e5638dba0","message":{"id":"fa8d62ff-370d-479c-bb26-1e8e5638dba0","author":{"role":"user","name":null,"metadata":{}},"create_time":1767931998.248,"update_time":null,"content":{"content_type":"text","parts":["How would design a coding interview comprising in part of a take home 6 day Python coding project to be done on an individual basis, given that coding agents are widely available now? Assume that the interviewers have already thought of the obvious, such as mandating a follow up on-site, in person interview for applicants who submit a good code base as their response."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"68db1f6c-9e8b-4c27-b176-fb3fa81d9ec8","message_source":null,"turn_exchange_id":"3fb269a0-a76b-4613-a57b-f00673eb160f","can_save":true},"recipient":"all","channel":null},"parent":"decd5235-93aa-4f53-b596-9890d5f2111b","children":["4609ab51-eeca-48fc-9862-a56331c9fcdd"]},"4609ab51-eeca-48fc-9862-a56331c9fcdd":{"id":"4609ab51-eeca-48fc-9862-a56331c9fcdd","message":{"id":"4609ab51-eeca-48fc-9862-a56331c9fcdd","author":{"role":"system","name":null,"metadata":{}},"create_time":1767932000.7893858,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"68db1f6c-9e8b-4c27-b176-fb3fa81d9ec8","message_type":"next","model_slug":"gpt-5-2","default_model_slug":"gpt-5-2","parent_id":"fa8d62ff-370d-479c-bb26-1e8e5638dba0","turn_exchange_id":"3fb269a0-a76b-4613-a57b-f00673eb160f","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"fa8d62ff-370d-479c-bb26-1e8e5638dba0","children":["339f489e-f573-493d-be80-5094226daa08"]},"339f489e-f573-493d-be80-5094226daa08":{"id":"339f489e-f573-493d-be80-5094226daa08","message":{"id":"339f489e-f573-493d-be80-5094226daa08","author":{"role":"system","name":null,"metadata":{}},"create_time":1767932000.8095434,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"68db1f6c-9e8b-4c27-b176-fb3fa81d9ec8","message_type":"next","model_slug":"gpt-5-2","default_model_slug":"gpt-5-2","parent_id":"4609ab51-eeca-48fc-9862-a56331c9fcdd","turn_exchange_id":"3fb269a0-a76b-4613-a57b-f00673eb160f","can_save":false},"recipient":"all","channel":null},"parent":"4609ab51-eeca-48fc-9862-a56331c9fcdd","children":["993b353e-ca09-414e-99b9-6a323e390362"]},"993b353e-ca09-414e-99b9-6a323e390362":{"id":"993b353e-ca09-414e-99b9-6a323e390362","message":{"id":"993b353e-ca09-414e-99b9-6a323e390362","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1767932000.842168,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"model_slug":"gpt-5-2-thinking","request_id":"68db1f6c-9e8b-4c27-b176-fb3fa81d9ec8","message_type":"next","default_model_slug":"gpt-5-2","parent_id":"339f489e-f573-493d-be80-5094226daa08","turn_exchange_id":"3fb269a0-a76b-4613-a57b-f00673eb160f","can_save":false},"recipient":"all","channel":null},"parent":"339f489e-f573-493d-be80-5094226daa08","children":["cc18d810-ad49-42ad-baf9-df2a9b9cd230"]},"cc18d810-ad49-42ad-baf9-df2a9b9cd230":{"id":"cc18d810-ad49-42ad-baf9-df2a9b9cd230","message":{"id":"cc18d810-ad49-42ad-baf9-df2a9b9cd230","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1767932000.157635,"update_time":null,"content":{"content_type":"text","parts":["If you\u2019re giving someone **6 days** and **Python coding agents are ubiquitous**, the take-home can\u2019t be \u201cwrite a CRUD app\u201d or \u201cimplement X algorithm.\u201d You have to design it so that:\n\n1) **Using agents is allowed but not sufficient**, and  \n2) You can reliably evaluate **engineering judgment**, **debugging**, **tradeoffs**, **verification**, and **ownership**.\n\nBelow is a design I\u2019ve seen work well (and some twists that specifically target \u201cagent-written\u201d submissions without turning into a witch-hunt).\n\n---\n\n## Core stance: treat agents like calculators\nMake it explicit:\n\n- \u201cYou may use any tools (including coding agents).\u201d\n- \u201cWe evaluate: correctness, clarity, tests, reasoning, and your ability to explain and modify the code live.\u201d\n- \u201cIf you used an agent, document what you asked it and what you changed.\u201d\n\nThis removes the arms race and lets you evaluate the real skill: **can they drive the tool**, not just paste output.\n\n---\n\n## The take-home: build a *system* with real constraints, not a \u201csolution\u201d\nPick a project that has multiple plausible designs and unavoidable edge cases. A good template:\n\n### Project: \u201cStreaming anomaly detector + reproducible report\u201d\n**What they build**\n- A small Python package + CLI that:\n  - ingests an event stream (provided log file + a \u201ctail -f\u201d simulator),\n  - computes features incrementally (streaming),\n  - detects anomalies (rule-based baseline + one ML-ish method),\n  - emits results to an output store (SQLite or parquet),\n  - generates a final report (markdown + plots) with reproducible settings.\n\n**Constraints that force thinking**\n- Must handle **large input** (e.g., 2\u201310 GB) *without loading it all*.\n- Must be deterministic and reproducible (seed, config pinning).\n- Must include a performance budget (e.g., \u201cprocess 1M events in < 60s on your laptop\u201d).\n- Must handle corrupted rows and schema drift gracefully.\n- Must support at least two operating modes: batch and streaming.\n\n**Deliverables**\n- `README.md` with:\n  - architecture diagram (text is fine),\n  - tradeoffs considered,\n  - how to run and test,\n  - known limitations and \u201cnext steps\u201d.\n- `pyproject.toml` and clean packaging.\n- Tests: unit + one end-to-end test.\n- A short \u201cengineering log\u201d (more below).\n\nWhy this works against pure agent output: it\u2019s **integration-heavy**, full of \u201csharp edges,\u201d and demands coherent decisions.\n\n---\n\n## Embed \u201cauthorship signals\u201d without playing detective\nInstead of trying to *detect* agent use, require artifacts that are hard to fake if they didn\u2019t actually do the work.\n\n### 1) Engineering log with checkpoints\nAsk for a lightweight log (bullet points are fine):\n\n- Day 1: initial plan + risks  \n- Day 2\u20133: first prototype + failures  \n- Day 4: performance bottleneck + fix  \n- Day 5: tests + refactor  \n- Day 6: polish + known gaps  \n\nInclude at least:\n- one bug they hit and how they diagnosed it,\n- one tradeoff they debated (and why they chose it),\n- one thing they would do differently.\n\nAgents can generate prose, but in the live follow-up you\u2019ll probe it. People who didn\u2019t build it crumble quickly.\n\n### 2) Require \u201cdesign seams\u201d\nAdd explicit extension hooks that you will test later, e.g.:\n\n- detectors must implement a `Detector` protocol,\n- ingestion uses a `Parser` interface with schema negotiation,\n- output uses a repository abstraction.\n\nThis makes the code easier to interrogate live (\u201cadd a third detector,\u201d \u201cswap storage backend\u201d) and harder to ship as a monolith.\n\n### 3) Provide ambiguous requirements on purpose (but bounded)\nExample: \u201cDefine \u2018anomaly\u2019 in a way that is defensible; justify it.\u201d\n\nThat forces candidates to make\u2014and defend\u2014product/engineering judgments. Agents can propose something; only competent candidates can argue for it and adapt when challenged.\n\n---\n\n## Add \u201cbooby traps\u201d ethically: hidden tests + adversarial data\nNot gotchas, but realistic failure modes.\n\n- You give them sample data A.\n- Your internal evaluation includes:\n  - data B with missing fields,\n  - data C with extreme outliers,\n  - data D with time-order violations,\n  - data E with unicode / weird delimiters.\n\nThis exposes brittle, surface-level solutions. It also exposes candidates who didn\u2019t test beyond the happy path.\n\n---\n\n## Force verification: require property-based or metamorphic tests\nA strong anti-copy mechanism is requiring tests that reflect understanding.\n\nExamples:\n- Property-based tests (Hypothesis) for parser robustness.\n- Metamorphic tests: shuffling irrelevant fields shouldn\u2019t change results; scaling units changes thresholds predictably; etc.\n\nIf they can\u2019t write or explain these, they don\u2019t understand the system.\n\n---\n\n## The follow-up interview: don\u2019t just \u201ctalk through it\u201d\nMake it a **working session** with controlled modifications.\n\n### 60\u201390 min live \u201cmaintenance task\u201d\nGive them a small change request based on the seams you forced:\n\n1) \u201cAdd a new detector type with a config section.\u201d  \n2) \u201cWe discovered this bug: here\u2019s failing input; fix it.\u201d  \n3) \u201cPerformance regressed on big files; profile and optimize one hotspot.\u201d  \n4) \u201cAdd one additional invariant-based test that would have caught it.\u201d\n\nIf they truly own the code, they\u2019ll navigate it confidently. If it\u2019s agent-dumped, they\u2019ll flail: wrong mental model, can\u2019t predict consequences, can\u2019t locate responsibilities.\n\n### Use a fresh \u201cpatch\u201d scenario\nGive them:\n- a new dataset file they haven\u2019t seen,\n- a failing test you wrote,\n- a new feature that is small but crosses boundaries.\n\nThis is where \u201cI pasted agent code\u201d gets exposed\u2014without needing any accusation.\n\n---\n\n## Add a second stage: a short timed micro-task *after* take-home\nSince you already said \u201cyou\u2019ll do onsite,\u201d add a **30\u201345 min offline exercise** right before or during onsite:\n\n- same domain, tiny scope,\n- no internet (or limited),\n- evaluate their raw competence.\n\nExample:\n- write a streaming aggregator with exact semantics,\n- implement a small parser with tricky edge cases,\n- write tests for a provided buggy function.\n\nThis calibrates: \u201cgreat project but can\u2019t code under mild pressure\u201d is a red flag.\n\n---\n\n## Grade on a rubric that rewards real engineering, not polish\nMake scoring explicit and aligned with what you want:\n\n- Correctness on hidden tests (big weight)\n- Test quality and coverage of edge cases\n- Simplicity and clarity of architecture\n- Performance and profiling evidence\n- Error handling and observability (logging, metrics)\n- Documentation that reflects real tradeoffs\n- Live modification performance\n\nThis makes \u201cagent-generated but shallow\u201d submissions score poorly.\n\n---\n\n## Practical knobs to tune difficulty and fairness\n- Provide a **starter repo** with tooling (lint, test runner, CI config) so you\u2019re not grading \u201cwho has the best project template.\u201d\n- Limit scope with clear \u201cmust have\u201d vs \u201cnice to have.\u201d\n- Make dataset + requirements **domain-neutral** (avoid needing finance/biology knowledge).\n- Ensure it\u2019s doable in ~6\u201310 hours of real work; 6 days is just scheduling slack.\n\n---\n\n## A strong, modern framing you can literally put in the prompt\n> \u201cAssume you have access to excellent coding agents. Your job is to produce a codebase you can defend, test, extend, and debug live. We will evaluate ownership and engineering judgment as much as the final output.\u201d\n\nThis aligns incentives immediately.\n\n---\n\nIf you tell me the role (backend, data, ML engineering, infra, full-stack) and seniority, I can propose **3 concrete take-home project prompts** (with datasets, hidden test plan, and a live interview script) tuned to that level."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[],"content_references":[],"model_slug":"gpt-5-2-thinking","request_id":"68db1f6c-9e8b-4c27-b176-fb3fa81d9ec8","message_type":"next","default_model_slug":"gpt-5-2","parent_id":"993b353e-ca09-414e-99b9-6a323e390362","turn_exchange_id":"3fb269a0-a76b-4613-a57b-f00673eb160f","can_save":true},"recipient":"all","channel":"final"},"parent":"993b353e-ca09-414e-99b9-6a323e390362","children":["314f88b8-4a23-4f27-a474-f08ad54e8979"]},"314f88b8-4a23-4f27-a474-f08ad54e8979":{"id":"314f88b8-4a23-4f27-a474-f08ad54e8979","message":{"id":"314f88b8-4a23-4f27-a474-f08ad54e8979","author":{"role":"user","name":null,"metadata":{}},"create_time":1767932228.201,"update_time":null,"content":{"content_type":"text","parts":["End to end machine learning pipeline comprising of data cleaning, EDA, training and fine-turning ML models, model evaluation, deployment with data privacy meaures in place, and constructing API interface to allow users to perform inference including batch inference"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"fee67b1d-8713-4071-8123-3101bfc1004b","message_source":null,"turn_exchange_id":"c9c024c8-134e-4597-a5d2-2db002e8780d","can_save":true},"recipient":"all","channel":null},"parent":"cc18d810-ad49-42ad-baf9-df2a9b9cd230","children":["2bd2c3ea-8e6f-4dfc-83a9-a8b8abe39acf"]},"2bd2c3ea-8e6f-4dfc-83a9-a8b8abe39acf":{"id":"2bd2c3ea-8e6f-4dfc-83a9-a8b8abe39acf","message":{"id":"2bd2c3ea-8e6f-4dfc-83a9-a8b8abe39acf","author":{"role":"system","name":null,"metadata":{}},"create_time":1767932230.4146388,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"fee67b1d-8713-4071-8123-3101bfc1004b","message_type":"next","model_slug":"gpt-5-2","default_model_slug":"gpt-5-2","parent_id":"314f88b8-4a23-4f27-a474-f08ad54e8979","turn_exchange_id":"c9c024c8-134e-4597-a5d2-2db002e8780d","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"314f88b8-4a23-4f27-a474-f08ad54e8979","children":["3550ed43-3084-4fd8-b17b-a86a25a60642"]},"3550ed43-3084-4fd8-b17b-a86a25a60642":{"id":"3550ed43-3084-4fd8-b17b-a86a25a60642","message":{"id":"3550ed43-3084-4fd8-b17b-a86a25a60642","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1767932230.521216,"update_time":null,"content":{"content_type":"thoughts","thoughts":[],"source_analysis_msg_id":"d418f33f-d1d6-4156-ba7e-c45aa8d8a5c2"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-2-thinking","request_id":"fee67b1d-8713-4071-8123-3101bfc1004b","message_type":"next","default_model_slug":"gpt-5-2","parent_id":"2bd2c3ea-8e6f-4dfc-83a9-a8b8abe39acf","turn_exchange_id":"c9c024c8-134e-4597-a5d2-2db002e8780d","can_save":false},"recipient":"all","channel":null},"parent":"2bd2c3ea-8e6f-4dfc-83a9-a8b8abe39acf","children":["8d8d37eb-ead6-4d11-a788-7ffeb327bcf5"]},"8d8d37eb-ead6-4d11-a788-7ffeb327bcf5":{"id":"8d8d37eb-ead6-4d11-a788-7ffeb327bcf5","message":{"id":"8d8d37eb-ead6-4d11-a788-7ffeb327bcf5","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1767932231.360086,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for a second"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":0,"model_slug":"gpt-5-2-thinking","request_id":"fee67b1d-8713-4071-8123-3101bfc1004b","message_type":"next","default_model_slug":"gpt-5-2","parent_id":"3550ed43-3084-4fd8-b17b-a86a25a60642","turn_exchange_id":"c9c024c8-134e-4597-a5d2-2db002e8780d","can_save":true},"recipient":"all","channel":null},"parent":"3550ed43-3084-4fd8-b17b-a86a25a60642","children":["a1e0164b-c074-48c0-b178-c61439099f54"]},"a1e0164b-c074-48c0-b178-c61439099f54":{"id":"a1e0164b-c074-48c0-b178-c61439099f54","message":{"id":"a1e0164b-c074-48c0-b178-c61439099f54","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1767932229.600495,"update_time":null,"content":{"content_type":"text","parts":["Here\u2019s a robust way to design a **6-day take-home** that is an **end-to-end machine learning (ML) pipeline** (cleaning \u2192 exploratory data analysis (EDA) \u2192 training/fine-tuning \u2192 evaluation \u2192 deployment \u2192 privacy \u2192 inference API incl. batch) **in an era where coding agents exist**.\n\nThe key is: **assume agents are used**, and structure the project so success requires **judgment, verification, and ownership**\u2014things agents don\u2019t give you for free.\n\n---\n\n## 1) Project brief (what you give candidates)\n\n### Scenario\n\u201cYou are building a privacy-aware prediction service for a company. You receive messy tabular data, must train and evaluate models responsibly, and deploy an inference API that supports both real-time and batch inference.\u201d\n\n### Data (you provide)\nProvide **two datasets**:\n\n1) **Train dataset (public to candidate)**  \n   - Realistic messiness: missingness, outliers, duplicates, mixed types, time columns, free-text field, categorical drift.\n   - Includes *direct identifiers* (name/email/phone) and *quasi-identifiers* (postcode, age, etc.) to force privacy handling.\n\n2) **\u201cShadow\u201d dataset (NOT given)** for your hidden evaluation  \n   - Same schema but with:\n     - harder missingness patterns  \n     - unseen categories  \n     - distribution shift  \n     - adversarial strings (unicode, weird separators)  \n     - deliberate leakage traps (columns correlated with target only in train)\n\nAlso provide a **data dictionary** that is slightly incomplete/ambiguous to force candidate decisions.\n\n### Prediction task\nPick a task where metrics and business costs matter:\n- binary classification with imbalance (fraud/churn), or\n- regression with heavy tails, or\n- ranking-style \u201cprobability of default\u201d etc.\n\n### Constraints (force real engineering)\n- Must be **reproducible** (seed control, config, deterministic pipeline).\n- Must prevent **data leakage** (time split or group split\u2014your call).\n- Must handle **schema drift** (new categories, missing columns, extra columns).\n- Must support **batch inference** at scale (e.g., 1M rows within X minutes).\n- Must implement **privacy measures** (see below).\n- Must include tests (unit + integration + at least one \u201cdata contract\u201d test).\n\n---\n\n## 2) Required architecture (so you can interrogate it later)\n\nRequire a modular pipeline with explicit seams:\n\n### Components\n1) `data_ingest/`  \n   - schema validation (e.g., pydantic or pandera)  \n   - parsing and type coercion  \n2) `cleaning/`  \n   - missing handling, outlier strategy, dedup rules  \n3) `features/`  \n   - feature pipeline (sklearn Pipeline or equivalent)  \n   - text handling if included (simple TF-IDF is fine, or embeddings)  \n4) `training/`  \n   - baseline model + improved model  \n   - hyperparameter tuning strategy (even minimal)  \n   - calibration (if classification)  \n5) `evaluation/`  \n   - metrics, plots, slice metrics (by subgroup/time)  \n   - error analysis (top false positives/negatives etc.)  \n6) `privacy/`  \n   - PII detection & redaction policy  \n   - logging policy + sensitive-field guards  \n7) `serving/`  \n   - FastAPI (or Flask)  \n   - endpoints: single inference + batch inference + health + model info  \n8) `cli.py`  \n   - one-command flows: `train`, `evaluate`, `serve`, `batch_predict`\n\n### Deliverables\n- A repo that runs locally via:\n  - `make train` / `make evaluate` / `make serve` / `make batch_predict`\n- A `README.md` containing:\n  - system design and tradeoffs\n  - how privacy is enforced end-to-end\n  - what they would do next in production (monitoring, drift, rollback)\n- A short `ENGINEERING_LOG.md` (see below)\n\nThis structure makes it easy for you to test, and hard for someone to ship an unowned blob.\n\n---\n\n## 3) Privacy requirements (make them concrete, testable)\n\nYou want privacy measures that aren\u2019t vague. Require at least:\n\n### A) PII handling\n- Detect & classify PII:\n  - direct identifiers: email, phone, name\n  - quasi-identifiers: full postcode, full DOB, exact address\n- In the pipeline:\n  - drop or hash direct identifiers\n  - generalize quasi-identifiers (e.g., postcode sector, age bucket)\n- Ensure **no PII in artifacts**:\n  - model file\n  - feature store / cached files\n  - logs\n  - metrics exports\n\n### B) Data minimization + retention\n- Persist only:\n  - features after redaction/generalization, not raw\n  - aggregated metrics, not raw records\n- Provide a config flag to disable saving intermediate data.\n\n### C) Inference privacy\n- API input validation that rejects PII fields if the contract says so\n- Logging middleware that:\n  - redacts known sensitive keys\n  - truncates long text fields\n  - never logs request bodies by default\n\n### D) Optional \u201cadvanced\u201d privacy (nice-to-have)\n- differential privacy (DP) training for one model (even a sketch), OR\n- membership inference risk discussion, OR\n- k-anonymity style analysis for quasi-identifiers\n\n### Your evaluation trick\nYour hidden tests should include:\n- PII in unexpected fields (e.g., email embedded in \u201cnotes\u201d)\n- payloads that try to force the API to echo PII in error messages/logs\n\n---\n\n## 4) The \u201cagent-resilient\u201d parts (without accusing anyone)\n\n### Engineering log (required)\nA simple 1\u20132 page log:\n- initial plan\n- one bug encountered and how they debugged it\n- one leakage risk they investigated\n- one performance bottleneck and what they changed\n- what they would ship next\n\nIn the on-site, you\u2019ll ask them to open that log and walk you through it, then reproduce one debugging step.\n\n### \u201cAmbiguity on purpose\u201d\nInclude one requirement that needs judgment, e.g.:\n- \u201cDefine fairness slices relevant to this data and report them.\u201d\n- \u201cChoose the right split strategy and justify it.\u201d\n- \u201cChoose a baseline that is explainable.\u201d\n\nAgents can propose; good candidates can defend and modify.\n\n---\n\n## 5) Evaluation rubric (what you score)\n\nWeight heavily toward ownership and robustness:\n\n1) **Correctness & robustness** (hidden tests)  \n   - handles drift, missingness, edge cases  \n2) **Leakage prevention**  \n   - correct split strategy, no target leakage  \n3) **Reproducibility**  \n   - seeds, config, deterministic runs  \n4) **Model quality & evaluation maturity**  \n   - appropriate metrics, calibration, slices, error analysis  \n5) **Privacy enforcement**  \n   - no PII in logs/artifacts, policy implemented end-to-end  \n6) **Deployment quality**  \n   - API contracts, versioning, healthchecks, batch support  \n7) **Testing & data contracts**  \n   - tests that catch failures, not just \u201chappy path\u201d  \n8) **Maintainability**  \n   - clean structure, clear interfaces, reasonable complexity  \n\n---\n\n## 6) On-site follow-up (the part that actually reveals ownership)\n\nDo **not** just \u201ctalk through the code.\u201d Make it a working maintenance session.\n\nPick 2\u20133 tasks from:\n\n### Task A: Fix a real failing case (you provide unseen data row)\n- Their batch inference fails on a schema drift case.\n- They must patch it and add a regression test.\n\n### Task B: Add a feature end-to-end\n- Add a new categorical column and ensure:\n  - training + inference both work\n  - model artifact version increments\n  - batch job still passes\n\n### Task C: Privacy breach drill\n- You show logs that accidentally captured a request payload.\n- They must change logging middleware and prove it\u2019s fixed.\n\n### Task D: Performance bottleneck\n- Batch inference is too slow by 3\u00d7 on a 500k-row file.\n- They profile and optimize one hotspot (vectorization, I/O, caching).\n\nIf the take-home was agent-assisted but not owned, they\u2019ll struggle immediately.\n\n---\n\n## 7) Minimal but strong API spec (what you require)\n\n### Endpoints\n- `GET /health`\n- `GET /model` \u2192 version, training timestamp, feature schema hash\n- `POST /predict` \u2192 single or small batch JSON\n- `POST /batch_predict` \u2192 accepts file upload or a path; returns job id\n- `GET /batch_predict/{job_id}` \u2192 status + result location\n\n### Batch inference implementation\n- local implementation: background worker thread or simple queue  \n- or optional: Celery/RQ (nice-to-have)\n\n### Output\n- include prediction + confidence + optional top features (if explainable model)\n- never echo raw sensitive fields back\n\n---\n\n## 8) What dataset should you use?\nUse something that allows real EDA and pitfalls, without domain expertise:\n- synthetic but realistic (you generate with planted issues), or\n- adapted from a public dataset but **you inject** identifiers, drift, and leakage traps.\n\nIn practice, synthetic-with-traps is best because you can precisely test for:\n- leakage\n- brittleness\n- privacy mishandling\n\n---\n\n## 9) What you send candidates (copy-paste spec skeleton)\n\n- Problem statement + success criteria  \n- Data dictionary + schema  \n- Required commands (train/eval/serve/batch)  \n- Privacy policy checklist  \n- Rubric  \n- Time expectation: \u201c~8\u201312 hours of effort, 6 days to schedule it\u201d  \n\n---\n\nIf you tell me:\n- role level (junior/mid/senior/staff),\n- whether this is more **ML engineering** vs **data science** vs **backend**,\n- preferred stack (FastAPI? Docker? cloud allowed?),\n\n\u2026I\u2019ll produce a **complete take-home pack**: dataset schema, exact tasks, starter repo structure, hidden test plan, and the on-site exercise script."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[],"content_references":[],"model_slug":"gpt-5-2-thinking","request_id":"fee67b1d-8713-4071-8123-3101bfc1004b","message_type":"next","default_model_slug":"gpt-5-2","parent_id":"8d8d37eb-ead6-4d11-a788-7ffeb327bcf5","turn_exchange_id":"c9c024c8-134e-4597-a5d2-2db002e8780d","can_save":true},"recipient":"all","channel":"final"},"parent":"8d8d37eb-ead6-4d11-a788-7ffeb327bcf5","children":["98f0b50b-a812-42ef-9db2-d4e76c827e27"]},"98f0b50b-a812-42ef-9db2-d4e76c827e27":{"id":"98f0b50b-a812-42ef-9db2-d4e76c827e27","message":{"id":"98f0b50b-a812-42ef-9db2-d4e76c827e27","author":{"role":"user","name":null,"metadata":{}},"create_time":1767932659.64,"update_time":null,"content":{"content_type":"text","parts":["Our roles are so integrated these days I'm not sure junior/mid/senior demarcations make sense anyone. It is ML engineering focussed but deployment, reproducibility, documentation, and inference are crucial too. Preferred stack: Cloud only if absolutely necessary, prefer local training and inference on modest hardware that individual candidates may have on their personal laptops (limited or no GPU, for example)"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"1002d973-38d4-42cf-b1de-f31dec9fb5a5","message_source":null,"turn_exchange_id":"be1d1b18-e9ae-443e-8792-0805cadb042f","can_save":true},"recipient":"all","channel":null},"parent":"a1e0164b-c074-48c0-b178-c61439099f54","children":["63262e87-ef5b-49d4-b4e7-39880ace3d4e"]},"63262e87-ef5b-49d4-b4e7-39880ace3d4e":{"id":"63262e87-ef5b-49d4-b4e7-39880ace3d4e","message":{"id":"63262e87-ef5b-49d4-b4e7-39880ace3d4e","author":{"role":"system","name":null,"metadata":{}},"create_time":1767932661.331107,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"1002d973-38d4-42cf-b1de-f31dec9fb5a5","message_type":"next","model_slug":"gpt-5-2","default_model_slug":"gpt-5-2","parent_id":"98f0b50b-a812-42ef-9db2-d4e76c827e27","turn_exchange_id":"be1d1b18-e9ae-443e-8792-0805cadb042f","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"98f0b50b-a812-42ef-9db2-d4e76c827e27","children":["56be9a2f-4dec-4593-ba0e-6d8bc092f8d4"]},"56be9a2f-4dec-4593-ba0e-6d8bc092f8d4":{"id":"56be9a2f-4dec-4593-ba0e-6d8bc092f8d4","message":{"id":"56be9a2f-4dec-4593-ba0e-6d8bc092f8d4","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1767932660.793941,"update_time":null,"content":{"content_type":"text","parts":["Got it. If you want an ML-engineering\u2013heavy take-home that is **local-first**, runs on **modest CPU laptops**, and still tests **deployment, reproducibility, documentation, and inference (incl. batch)**\u2014then the project should be:\n\n- **tabular + light text** (optional), not deep learning,\n- **streaming/batch capable**,\n- **robust to drift and messy data**,\n- **privacy-aware by design**,\n- and **easy to interrogate live**.\n\nBelow is a concrete interview design you can basically ship as a take-home spec + an on-site plan + a scoring rubric.\n\n---\n\n## 1) Take-home project (6 days): \u201cPrivacy-Aware Risk Scoring Service\u201d\n\n### Candidate goal\nBuild a local-first ML pipeline that trains a model from messy data and serves predictions through an API that supports **single** and **batch** inference.\n\n### Task type (CPU-friendly, realistic)\nBinary classification (imbalanced) or regression (heavy tails). I recommend **binary** because you can evaluate calibration + thresholds + business tradeoffs.\n\nExamples:\n- \u201cdefault risk\u201d\n- \u201csubscription churn\u201d\n- \u201cfraud likelihood\u201d (simulated)\n\n### Data you provide\nGive them a dataset with:\n\n- tabular columns (numerical + categorical)\n- timestamps (to force time split / leakage thinking)\n- a free-text \u201cnotes\u201d field (to test privacy/logging, not to force large NLP)\n- PII columns (email/phone/name)\n- duplicates, missingness, inconsistent formats, outliers\n\n**Two partitions:**\n- `train.csv` and `valid_public.csv` (they see these)\n- you keep `valid_hidden.csv` for evaluation (drift + adversarial cases)\n\nKeep dataset size moderate (say 100k\u2013500k rows) but include a \u201cbig file\u201d mode:\n- either a larger file or a generator script to create 2\u20135 million rows for batch speed tests.\n\n---\n\n## 2) Requirements that matter (and are hard to fake)\n\n### A) Pipeline requirements\nMust implement:\n1) **Data validation / contracts**  \n   - schema validation at ingest\n   - type coercion + failure modes\n2) **Cleaning**  \n   - missing strategy, outliers strategy, dedup strategy\n3) **EDA report** (not a notebook dump)  \n   - a generated markdown or HTML report\n   - must include at least: missingness, drift check vs valid_public, target distribution, and at least 2 \u201cdata quality findings\u201d\n4) **Training**  \n   - baseline model (e.g., logistic regression)  \n   - improved model (e.g., LightGBM *if allowed locally*, otherwise HistGradientBoosting / XGBoost CPU / CatBoost CPU)\n   - proper split strategy (time-based or grouped; you can require time split)\n5) **Evaluation**  \n   - metrics: AUC-ROC, AUC-PR (precision-recall), calibration (Brier / reliability diagram)\n   - threshold selection strategy with justification\n   - slice metrics (at least 3 slices, one time-based)\n6) **Reproducibility**  \n   - one command trains end-to-end\n   - pinned dependencies\n   - deterministic seeds and artifact versioning\n7) **Model packaging**  \n   - save preprocessing + model together (sklearn Pipeline)\n   - include model card (short) with assumptions + limitations\n8) **Deployment + inference**\n   - FastAPI service\n   - JSON schema for request/response\n   - endpoints: `/predict` and `/batch_predict`\n   - batch inference accepts a CSV upload OR path and returns a job id + result file\n\n### B) Privacy requirements (must be testable)\nThey must:\n- drop or transform PII (email/phone/name) so it cannot leak into model artifacts\n- ensure logs redact sensitive fields\n- never persist raw PII in intermediate artifacts\n- add a \u201cprivacy audit\u201d command that scans generated artifacts/logs for PII patterns (simple but effective)\n\nThis is easy for you to evaluate with automated tests.\n\n---\n\n## 3) Local-first constraints (explicit)\nYou should say:\n\n- \u201cMust run on a CPU-only laptop (no GPU assumed).\u201d\n- \u201cTraining should complete within ~15 minutes on a typical laptop for the provided dataset.\u201d\n- \u201cBatch inference should handle 1M rows within a reasonable time (you define a target, e.g. < 3 minutes) using streaming/chunking.\u201d\n\nThis pushes candidates toward sensible model choices and efficient pipelines.\n\n---\n\n## 4) Starter repo you provide (so you\u2019re not grading boilerplate)\nGive a scaffold with:\n- `pyproject.toml`\n- `Makefile` (or `justfile`)\n- preconfigured lint/test (`ruff`, `pytest`, `mypy` optional)\n- minimal FastAPI skeleton\n- a placeholder data contract file\n- \u201ctoy baseline\u201d that barely works (optional)\n\nThis reduces noise and makes comparisons fair.\n\n---\n\n## 5) Must-have repo structure (forces seams for live probing)\nRequire something like:\n\n- `src/app/` (FastAPI service)\n- `src/pipeline/`\n  - `ingest.py`\n  - `validate.py`\n  - `clean.py`\n  - `features.py`\n  - `train.py`\n  - `evaluate.py`\n- `src/privacy/`\n  - `redact.py`\n  - `audit.py`\n- `scripts/` (CLI entrypoints)\n- `tests/`\n  - unit + 1 integration test\n- `artifacts/` (ignored in git, generated)\n- `reports/` (EDA + eval outputs)\n\nAnd require a CLI like:\n\n- `python -m app.cli train --config configs/base.yaml`\n- `python -m app.cli evaluate --config ...`\n- `python -m app.cli serve --model artifacts/model.pkl`\n- `python -m app.cli batch_predict --input file.csv --output preds.csv`\n\nThis makes it very hard to submit a tangled \u201cagent blob\u201d without understanding.\n\n---\n\n## 6) Hidden evaluation (your side) that catches shallow work\nYour hidden tests should include:\n\n### A) Data drift + schema drift\n- unseen categories\n- new columns\n- missing expected columns\n- different date formats\n\n### B) Leakage traps\n- a column that correlates with target only in train\n- time leakage via \u201cfuture aggregate\u201d style field\nYou check whether they used proper split and whether performance collapses on hidden set.\n\n### C) Privacy attacks\n- email/phone embedded in the \u201cnotes\u201d text\n- weird unicode separators\nYou scan their logs + artifacts for PII patterns.\n\n### D) Batch performance\n- run batch predict on a large file and enforce chunked reading\nIf their batch code does `pd.read_csv()` on 1M+ rows and explodes, they fail.\n\n---\n\n## 7) What you ask for (deliverables)\nMake deliverables concrete:\n\n1) `README.md`\n   - how to run train/eval/serve/batch\n   - architecture summary\n   - privacy policy implemented\n   - reproducibility notes\n2) `MODEL_CARD.md`\n   - intended use, non-use, data limitations, evaluation summary, threshold policy\n3) `ENGINEERING_LOG.md` (short but required)\n   - 5\u201310 bullets: major decisions, one bug, one bottleneck, one tradeoff\n4) Generated reports\n   - `reports/eda.md` and `reports/eval.md` (or HTML)\n\n---\n\n## 8) On-site follow-up (the real filter)\nDo a 75\u201390 minute working session:\n\n### Part 1 (15 min): code walk\nAsk them to locate:\n- where schema drift is handled\n- where privacy redaction happens\n- where feature pipeline is saved\n\n### Part 2 (45 min): maintenance task (choose 1)\n1) **Schema drift fix**: add a new categorical column and ensure training + inference work  \n2) **Privacy breach drill**: you show a log snippet leaking PII; fix middleware and add a test  \n3) **Batch perf**: speed up batch inference 2\u00d7 by profiling and optimizing one hotspot  \n4) **Metric shift**: hidden set shows calibration collapse; add calibration and update threshold policy\n\n### Part 3 (15\u201330 min): \u201cexplain and defend\u201d\n- Why this split strategy?\n- What\u2019s the biggest leakage risk remaining?\n- What would you add for production monitoring?\n\nIf they didn\u2019t truly own the code, this exposes it instantly\u2014no need to police agent usage.\n\n---\n\n## 9) Scoring rubric (weight what you care about)\nA solid weighting:\n\n- Robustness on hidden tests: 30%\n- Privacy correctness (no PII in artifacts/logs): 20%\n- Reproducibility + packaging: 15%\n- Evaluation maturity (metrics, calibration, slices): 15%\n- Deployment quality (API contracts, batch jobs): 10%\n- Code quality + docs: 10%\n\nThis prevents \u201cpretty notebook + mediocre system\u201d from winning.\n\n---\n\n## 10) Why this works in the agent era\nAgents can generate scaffolds and even plausible pipelines, but they usually fail on:\n- coherent end-to-end reproducibility\n- leakage-aware splitting and evaluation discipline\n- truly robust schema drift handling\n- privacy enforcement that survives adversarial inputs\n- performance bottlenecks in batch inference\n- live modifications under time pressure\n\nSo your process naturally selects for engineers who can *drive* tools rather than paste them.\n\n---\n\nIf you want, I can draft the actual **candidate prompt** (1\u20132 pages), including:\n- exact endpoints and schemas,\n- exact commands,\n- the privacy checklist,\n- \u201cmust / should / nice-to-have\u201d scope boundaries,\n- and a hidden-test plan you can implement in CI."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[],"content_references":[],"model_slug":"gpt-5-2-thinking","request_id":"1002d973-38d4-42cf-b1de-f31dec9fb5a5","message_type":"next","default_model_slug":"gpt-5-2","parent_id":"63262e87-ef5b-49d4-b4e7-39880ace3d4e","turn_exchange_id":"be1d1b18-e9ae-443e-8792-0805cadb042f","can_save":true},"recipient":"all","channel":"final"},"parent":"63262e87-ef5b-49d4-b4e7-39880ace3d4e","children":[]}}}