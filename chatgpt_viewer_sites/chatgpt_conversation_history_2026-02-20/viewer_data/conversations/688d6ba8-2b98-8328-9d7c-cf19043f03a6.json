{"id":"688d6ba8-2b98-8328-9d7c-cf19043f03a6","title":"Reconstructing AGI Timelines","create_time":1754098602.444226,"update_time":1754098653.210452,"current_node":"409d4cf5-ff1f-4095-bfc9-76e75faa7ae7","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["3a0e665b-ba32-401f-a321-4eb0c709f045"]},"3a0e665b-ba32-401f-a321-4eb0c709f045":{"id":"3a0e665b-ba32-401f-a321-4eb0c709f045","message":{"id":"3a0e665b-ba32-401f-a321-4eb0c709f045","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"client-created-root","children":["2bb41763-9ef8-4992-992e-acb96dab56e9"]},"2bb41763-9ef8-4992-992e-acb96dab56e9":{"id":"2bb41763-9ef8-4992-992e-acb96dab56e9","message":{"id":"2bb41763-9ef8-4992-992e-acb96dab56e9","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: I have mathematical knowledge up to the post-graduate level.\n\nCurrent research areas and interests:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: I have mathematical knowledge up to the post-graduate level.\n\nCurrent research areas and interests:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"3a0e665b-ba32-401f-a321-4eb0c709f045","children":["fe1f848b-e0cc-4741-9be3-e03c36bdad47"]},"fe1f848b-e0cc-4741-9be3-e03c36bdad47":{"id":"fe1f848b-e0cc-4741-9be3-e03c36bdad47","message":{"id":"fe1f848b-e0cc-4741-9be3-e03c36bdad47","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"2bb41763-9ef8-4992-992e-acb96dab56e9","children":["48a42df0-a10f-4aee-a32d-9526320cb5e9"]},"48a42df0-a10f-4aee-a32d-9526320cb5e9":{"id":"48a42df0-a10f-4aee-a32d-9526320cb5e9","message":{"id":"48a42df0-a10f-4aee-a32d-9526320cb5e9","author":{"role":"user","name":null,"metadata":{}},"create_time":1754098603.357,"update_time":null,"content":{"content_type":"text","parts":["Attached is transcript of a youtube video.  \r\n\r\n- Using the transcript, reconstruct the narrative as closely as possible. \r\n\r\n- The reconstruction must be a direct transcription of what the speaker is saying as close to verbatim as possible, without switching from one voice to another, nor changing the style or tone in any way.\r\n\r\n- Correct any flaws and repair any broken or truncated sentences as best as you can using the surrounding context. \r\n\r\n- Output in continuous prose that is coherent and grammatically correct.\r\n\r\n- Divide the text into appropriate sections and give a header for each section\r\n\r\n- Distinguish between different speakers and label them appropriately.\r\n\r\n---\n0:00\r\nI\u2019ve had a lot of discussions on my podcast  where we haggle out our timelines to AGI. \r\n0:04\r\nSome guests think it\u2019s 20  years away. Others 2 years. \r\n0:08\r\nHere\u2019s where my thoughts lie as of July 2025. Sometimes people say that even if all AI  \r\n0:16\r\nprogress totally stopped, the systems of  today would still be far more economically  \r\n0:20\r\ntransformative than the internet. I disagree.  I think the LLMs of today are magical. \r\n0:26\r\nBut the reason that the Fortune 500 aren\u2019t using  them to totally transform their workflows isn\u2019t  \r\n0:31\r\nbecause the management there is too stodgy. Rather, I think it\u2019s genuinely hard to get  \r\n0:36\r\nnormal humanlike labor out of these LLMs. And this has to do with some fundamental  \r\n0:41\r\ncapabilities these models lack. I like to think I\u2019m \"AI forward\"  \r\n0:46\r\nhere at the Dwarkesh Podcast. I\u2019ve probably spent over a hundred  \r\n0:49\r\nhours trying to build these little LLM  tools for my post production setup. \r\n0:53\r\nAnd the experience of trying to get them  to be useful has extended my timelines. \r\n0:58\r\nI\u2019ll try to get an LLMs to rewrite autogenerated  transcripts for me, to optimize for readability  \r\n1:03\r\nin the way a human would. Or I\u2019ll try to get them to  \r\n1:06\r\nidentify clips from a transcript that I feed in. Sometimes I\u2019ll try to get them to co-write an  \r\n1:12\r\nessay with me, passage by passage. These are simple, self contained,  \r\n1:18\r\nshort horizon, language in, language out tasks\u2014the  kinds of assignments that should be dead center  \r\n1:23\r\nin the LLMs\u2019 repertoire. And they're 5/10 at  them. Don\u2019t get me wrong, that is impressive. \r\n1:30\r\nBut the fundamental problem is that LLMs don\u2019t  get better over time the way a human would. \r\n1:37\r\nThis lack of continual learning  is a huge huge bottleneck. \r\n1:42\r\nThe LLM baseline at many tasks might  be higher than the average human's. \r\n1:47\r\nBut there\u2019s no way to give  a model high level feedback. \r\n1:50\r\nYou\u2019re stuck with the abilities  you get out of the box. \r\n1:53\r\nYou can keep messing around with the system  prompt, but in practice this just doesn\u2019t produce  \r\n1:58\r\nanything even close to the kind of learning and  improvement that human employees experience. \r\n2:04\r\nThe reason humans are so useful  is not mainly their raw intellect. \r\n2:08\r\nIt\u2019s their ability to build up context,  interrogate their own failures,  \r\n2:12\r\nand pick up small improvements and  efficiencies as they practice a task. \r\n2:17\r\nHow would you teach a kid to play the saxophone? You'd have them try to blow into one, and then  \r\n2:22\r\nthey'd see how it sounds, and they'd adjust. Now imagine if this was the way you'd have to  \r\n2:28\r\nteach saxophone instead: A  student takes one attempt. \r\n2:30\r\nAnd the moment they make a mistake, you  send them away and you write detailed  \r\n2:34\r\ninstructions about what went wrong. And you call the next student in. \r\n2:38\r\nAnd the next student reads your notes  and tries to play Charlie Parker cold. \r\n2:42\r\nAnd when they fail, you refine your instructions  and you invite the next student. This just  \r\n2:47\r\nwouldn\u2019t work. No matter how well honed your  prompt is, no kid is just going to learn how  \r\n2:52\r\nto play saxophone from reading your instructions. But this is the only modality we have to \u2018teach\u2019  \r\n2:59\r\nLLMs anything. Yes, there\u2019s RL fine tuning.  But it\u2019s not a deliberate, adaptive process  \r\n3:07\r\nin the way that human learning is. My editors have gotten extremely good. \r\n3:11\r\nAnd they wouldn\u2019t have gotten that way if  we had to build bespoke RL environments for  \r\n3:16\r\ndifferent subtasks involved in their work. They\u2019ve just noticed a lot of small things  \r\n3:21\r\nthemselves and thought hard about  what resonates with the audience,  \r\n3:25\r\nwhat kind of content I like, and how they  can improve their day to day workflows. \r\n3:30\r\nNow, it\u2019s possible to imagine ways in  which a smarter model could build a  \r\n3:34\r\ndedicated RL loop for itself which just  feels super organic from the outside. \r\n3:38\r\nI give some high level feedback, and the  model comes up with a bunch of verifiable  \r\n3:43\r\npractice problems to RL on\u2014maybe even a whole  environment in which it gets to rehearse the  \r\n3:49\r\nskills that it thinks it's lacking. But this just sounds really hard. \r\n3:52\r\nAnd I don\u2019t know how well these  techniques will generalize to  \r\n3:54\r\ndifferent kinds of tasks and feedback. Eventually the models will be able to learn  \r\n3:59\r\non the job in this organic way that humans can. But it\u2019s just hard for me to see how that could  \r\n4:05\r\nhappen within the next few years, given  there\u2019s no immediately obvious way in  \r\n4:09\r\nwhich to slot in continuous learning into  the kinds of models that these LLMs are. \r\n4:14\r\nLLMs actually do get kinda smart and  useful in the middle of a session. \r\n4:17\r\nFor example, sometimes I\u2019ll  co-write an essay with an LLM. \r\n4:20\r\nI\u2019ll give it an outline, and I\u2019ll ask it  to draft the essay passage by passage. \r\n4:24\r\nAnd all its suggestions up till  paragraph four will just be bad. \r\n4:28\r\nI'll just rewrite every single paragraph from  scratch and tell it, \"Look, your shit sucked. \r\n4:34\r\nThis is what I wrote instead.\" And at this point, it will actually start  \r\n4:38\r\ngiving good suggestions for the next paragraph. But this whole subtle understanding of my  \r\n4:42\r\npreferences and style will just  be lost by the end of the session. \r\n4:47\r\nMaybe there is an easy solution to this that  looks like a long rolling context window, like  \r\n4:52\r\nClaude Code already has, which just compacts the  session memory into a summary every 30 minutes. \r\n4:58\r\nI just think that titrating all this rich  tacit experience into a text summary will  \r\n5:03\r\nbe brittle in domains outside of software  engineering, which is very text-based,  \r\n5:07\r\nin which you already have this external scaffold  of memory that is stored in the codebase itself. \r\n5:12\r\nAgain, think about what it would be like to  teach a kid to play the saxophone just from text. \r\n5:19\r\nEven Claude Code will often reverse a hard-earned  optimization that we engineered together before I  \r\n5:25\r\nhit /compact \u2014because the explanation for why  it was made didn\u2019t make it into the summary. \r\n5:31\r\nThis is why I disagree with something that  Anthropic researchers Sholto Douglas and  \r\n5:36\r\nTrenton Bricken said on my podcast. And this quote is from Trenton: \r\n5:40\r\n\"Even if AI progress totally stalls  (and you think that the models are  \r\n5:43\r\nreally spiky, and they don't have general  intelligence), it's so economically valuable,  \r\n5:48\r\nand sufficiently easy to collect data on all  of these different white collar job tasks,  \r\n5:55\r\nsuch that to Sholto's point we should expect to  see them automated within the next five years.\" \r\n6:00\r\nIf AI progress totally stops today, I think less  than 25% of white collar employment goes away. \r\n6:06\r\nSure, many tasks will get automated. Claude 4 Opus can technically rewrite  \r\n6:12\r\nautogenerated transcripts for me. But since it\u2019s not possible for me  \r\n6:17\r\nto have it improve over time and learn my  preferences, I still hire a human for this. \r\n6:22\r\nSo even if we get more data, without progress  in continual learning, I think that we will be  \r\n6:28\r\nin a substantially similar position with  all other kinds of white collar work. \r\n6:33\r\nYes, technically AIs will be able to do  a lot of subtasks somewhat satisfactorily,  \r\n6:38\r\nbut their inability to build up context  will make it impossible to have them operate  \r\n6:44\r\nas actual employees at your firm. While this makes me bearish about  \r\n6:48\r\ntransformative AI in the next few years, it makes  me especially bullish on AI over the next decades. \r\n6:55\r\nWhen we do solve continual learning, we\u2019ll see a  huge discontinuity in the value of these models. \r\n7:01\r\nEven if there isn\u2019t a software only  singularity, where these models rapidly  \r\n7:05\r\nbuild smarter and smarter successor systems,  we might still get something that looks like  \r\n7:10\r\na broadly deployed intelligence explosion. AIs will be getting broadly deployed through the  \r\n7:14\r\neconomy, and doing different jobs and learning  while doing them in the way that humans can. \r\n7:20\r\nHowever, unlike humans, these  models can amalgamate their  \r\n7:24\r\nlearnings across all their copies. So one AI is basically learning  \r\n7:29\r\nhow to do every single job in the economy. An AI that is capable of this kind of online  \r\n7:34\r\nlearning might rapidly become a superintelligence  even if there's no further algorithmic progress. \r\n7:42\r\nHowever, I\u2019m not expecting to watch some  OpenAI livestream where they announce that  \r\n7:45\r\ncontinual learning has been totally solved. Because labs are incentivized to release any  \r\n7:50\r\ninnovations quickly, we\u2019ll see a broken early  version of continual learning (or test time  \r\n7:55\r\ntraining, or whatever you want to call it) before  we see something which truly learns like a human. \r\n8:01\r\nI expect to get lots of heads up before  this big bottleneck is totally solved. \r\nComputer Use\r\n8:08\r\nWhen I interviewed Anthropic researchers  Sholto Douglas and Trenton Bricken on my  \r\n8:12\r\npodcast, they said that they expect reliable  computer use agents by the end of next year. \r\n8:17\r\nWe already have computer use agents right  now, but they\u2019re pretty bad. They\u2019re imagining  \r\n8:23\r\nsomething quite different. Their forecast is  that by the end of next year, you should be  \r\n8:28\r\nable to tell an AI, \"Go do my taxes.\" And it'ill go through all your email,  \r\n8:32\r\nyour Amazon orders, and Slack messages, and  it will email back and forth with every single  \r\n8:37\r\nperson you need to get invoices from, it'll  compile all your receipts, decide what things  \r\n8:42\r\nare actually are business expenses, and it will  ask for your approval on all the edge cases,  \r\n8:46\r\nand then will just submit Form 1040 to the IRS.  I\u2019m skeptical. I\u2019m not an AI researcher, so far be  \r\n8:53\r\nit for me to contradict them on technical details. But given what little I know, here\u2019s why I\u2019d bet  \r\n8:58\r\nagainst this forecast: One. \r\n9:01\r\nAs horizon lengths increase,  rollouts have to become longer. \r\n9:04\r\nThe AI needs to do two hours worth of  agentic computer use tasks before we  \r\n9:09\r\ncan even see if it did it right. Not to mention that computer use  \r\n9:12\r\nrequires processing images and video,  which is already more compute intensive,  \r\n9:16\r\neven if you don\u2019t factor in the longer rollouts. This seems like it should slow down progress.  \r\n9:22\r\nTwo. We don\u2019t have a large pretraining  corpus of multimodal computer use data. \r\n9:28\r\nI like this quote from Mechanize\u2019s post  on automating software engineering:  \r\n9:33\r\n\"For the past decade of scaling, we\u2019ve been  spoiled by the enormous amount of internet  \r\n9:37\r\ndata that was freely available for us to use. This was enough to crack natural language  \r\n9:42\r\nprocessing, but not for getting models  to become reliable, competent agents. \r\n9:47\r\nImagine trying to train GPT-4 on all  the text data available in 1980\u2014the  \r\n9:53\r\ndata would have been nowhere near enough,  even if you had the necessary compute.\" \r\n9:58\r\nAgain, I\u2019m not at the labs. Maybe text only training already gives you a  \r\n10:03\r\ngreat prior over how different UIs work, and what  the relationship is between different components. \r\n10:08\r\nMaybe RL fine tuning is so sample efficient  that you don\u2019t need that much data. \r\n10:13\r\nBut I haven\u2019t seen any public evidence which  makes me think that these models have suddenly  \r\n10:18\r\ngotten less data hungry, especially in domains  where they\u2019re substantially less practiced. \r\n10:24\r\nAlternatively, maybe these models are such  good front end coders that they can just  \r\n10:29\r\ngenerate millions of toy UIs for themselves  to practice on. But, three. Even algorithmic  \r\n10:36\r\ninnovations which seem quite simple in  retrospect took a long time to iron out. \r\n10:42\r\nThe RL procedure which DeepSeek explained in  their R1 paper seems simple at a high level. \r\n10:48\r\nAnd yet it took 2 years from the development  and launch of GPT-4 to the release of o1. \r\n10:54\r\nNow of course I know that it's insanely and  hilariously arrogant to say that R1/o1 were easy\u2014a  \r\n11:00\r\nton of engineering, debugging, and pruning of  alternative ideas was required to arrive at  \r\n11:06\r\nthis solution. But that\u2019s precisely my point!  Seeing how long it took to implement the idea  \r\n11:11\r\nof \u2018We should train a model to solve verifiable  math and coding problems,\u2019 makes me think that  \r\n11:17\r\nwe\u2019re underestimating the difficulty of solving  the much gnarlier problem of computer use, where  \r\n11:22\r\nyou\u2019re operating in a totally different modality  with much less data. Okay, enough cold water. I\u2019m  \r\nReasoning\r\n11:30\r\nnot going to be like one of those spoiled children  on Hackernews who could be handed a golden-egg  \r\n11:35\r\nlaying goose and would still spend all their  time complaining about how loud its quacks are. \r\n11:39\r\nHave you read the reasoning traces from o3 or  Gemini 2.5? It\u2019s actually reasoning! It\u2019s breaking  \r\n11:46\r\ndown a problem, thinking through what the user  wants, reacting to its own internal monologue,  \r\n11:52\r\nand correcting itself when it notices that  it's pursuing an unproductive direction. \r\n11:57\r\nHow are we just like, \"Oh yeah of course a  machine is gonna go think a bunch, come up  \r\n12:02\r\nwith a bunch of ideas, and come back to me with  a smart answer. That\u2019s just what machines do.\" \r\n12:06\r\nPart of the reason some people are too  pessimistic is that they haven\u2019t played  \r\n12:10\r\naround with the smartest models in  domains where they\u2019re the most competent. \r\n12:14\r\nGiving Claude Code a vague spec and just  sitting around for 10 minutes while it  \r\n12:19\r\nzero shots a working application is a wild  experience. How did it do that? You can talk  \r\n12:25\r\nabout circuits and the training distribution  and RL or whatever, but the most proximal,  \r\n12:31\r\nconcise, and accurate explanation is simply that  it\u2019s powered by a baby general intelligence. \r\n12:38\r\nAt this point, part of you has to  be thinking, \"It\u2019s actually working. \r\n12:42\r\nWe\u2019re making machines that are intelligent.\" My probability distributions are super wide. \r\nSo what are my predictions?\r\n12:50\r\nAnd I want to emphasize that I do  believe in probability distributions. \r\n12:53\r\nWhich means that work to prepare for a  misaligned 2028 ASI still makes a ton of sense. \r\n12:59\r\nI think this is a totally plausible outcome. But here are the timelines at which  \r\n13:03\r\nI\u2019d take a 50/50 bet. An AI that can do taxes  \r\n13:07\r\nend-to-end for my small business as well as  a competent general manager could in a week:  \r\n13:12\r\nincluding chasing down all the receipts on  different websites, finding the missing pieces,  \r\n13:17\r\nemailing back and forth with anyone who we need  to hassle for invoices, filling out the form,  \r\n13:22\r\nand sending it to the IRS. This I'd say 2028. I  think we\u2019re in the GPT 2 era for computer use. \r\n13:28\r\nBut we have no pretraining corpus, and the  models are optimizing for a much sparser  \r\n13:33\r\nreward over a much longer time horizon using  action primitives they\u2019re unfamiliar with. \r\n13:39\r\nThat being said, the base model is  already decently smart and might have  \r\n13:42\r\na good prior over computer use tasks,  plus there\u2019s a lot more compute and AI  \r\n13:47\r\nresearchers in the world, so it might even out. Preparing taxes for a small business feels like  \r\n13:53\r\nfor computer use what GPT 4 was for language. It took 4 years to get from GPT 2 to GPT 4. \r\n14:00\r\nJust to clarify, I am not saying that  we won\u2019t have really cool computer use  \r\n14:04\r\ndemos in 2026 and 2027. GPT-3 was super cool,  \r\n14:08\r\nbut it was not that practically useful. I\u2019m saying that these models won\u2019t be capable  \r\n14:13\r\nof end-to-end handling a week long and quite  involved project which involves computer use. \r\n14:19\r\nOk, and as for the forecast of when AI  will be able to learn on the job as easily,  \r\n14:24\r\norganically, seamlessly, and quickly  as humans, for any white collar work. \r\n14:29\r\nFor example, if I hired an AI video editor, after  six months it would have as much actionable,  \r\n14:35\r\ndeep understanding of my preferences,  our channel, what works for the audience,  \r\n14:39\r\nas well as a human would. I'd say this would come in 2032. \r\n14:44\r\nWhile I don\u2019t see an obvious way to  slot in continuous online learning  \r\n14:47\r\ninto the kinds of models these LLMs  are, 7 years is a really long time! \r\n14:53\r\nGPT 1 had just come out this time 7 years ago. It doesn\u2019t seem implausible to me that over the  \r\n14:58\r\nnext 7 years, we\u2019ll find some way to get  these models to actually learn on the job. \r\n15:04\r\nAt this point you might be reacting,  \"Wait you made this huge fuss about  \r\n15:06\r\ncontinual learning being such a huge handicap. But then your prediction is that we\u2019re 7 years  \r\n15:11\r\naway from what, at a minimum, looks like a broadly  deployed intelligence explosion.\" And yeah, you\u2019re  \r\n15:17\r\nright. I am forecasting a pretty wild world within  a relatively short amount of time. AGI timelines  \r\n15:24\r\nare very lognormal. It's either this decade or  bust. (Not really, it's more like lower marginal  \r\n15:30\r\nprobability per year\u2014but that\u2019s less catchy). AI progress over the last decade has  \r\n15:35\r\nbeen driven by scaling training  compute on the frontier systems. \r\n15:38\r\nIt's been over 4x a year. This cannot continue beyond this decade,  \r\n15:43\r\nwhether you look at chips, power, or even the  raw fraction of GDP that is used on training. \r\n15:48\r\nAfter 2030, AI progress has to mostly  come from algorithmic progress. \r\n15:53\r\nBut even there all the low  hanging fruit will be plucked,  \r\n15:56\r\nat least under the deep learning paradigm. So the yearly probability of AGI collapses. \r\n16:02\r\nThis means that if we end up on the longer side of  my 50/50 bets, we might be looking at a relatively  \r\n16:08\r\nnormal world up till the 2030s or even the 2040s. But in all the other worlds, even if we stay sober  \r\n16:15\r\nabout the current limitations of AI, we  have to expect some truly crazy outcomes. \r\nDwarkesh.com\r\n16:21\r\nThis was originally a blog post that I  published on my website at dwarkesh.com. \r\n16:27\r\nAnd it was obviously inspired by the discussion  I had with Sholto and Trenton on my podcast,  \r\n16:32\r\nwhere I ended up disagreeing with them about  timelines but it took me a few weeks of thinking  \r\n16:36\r\nafterwards, sorting out exactly where I  disagree and why I had longer timelines. \r\n16:41\r\nAnd I do this for other episodes as well. I wrote up some thoughts I had about the many  \r\n16:46\r\nthousands of pages that Stephen Kotkin has written  about Stalin, obviously which we were not able to  \r\n16:51\r\nexhaustively cover in that one 2-hour interview. So anyways, if you want to see these additional  \r\n16:56\r\nartifacts and writing that I produce as a result  of this podcast and in preparation for episodes,  \r\n17:02\r\nyou should subscribe to my blog and newsletter. You can do that at dwarkesh.com. \r\n17:07\r\nOtherwise I will also see you next week  for a full episode with a real guest."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"9689d885dc695fab-SIN","message_source":null,"timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"fe1f848b-e0cc-4741-9be3-e03c36bdad47","children":["7594cec5-8c51-400a-b6f9-0418258c60bf"]},"7594cec5-8c51-400a-b6f9-0418258c60bf":{"id":"7594cec5-8c51-400a-b6f9-0418258c60bf","message":{"id":"7594cec5-8c51-400a-b6f9-0418258c60bf","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1754098602.169541,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"9689d885dc695fab-SIN","message_type":"next","model_slug":"gpt-4-1","default_model_slug":"gpt-4-1","parent_id":"48a42df0-a10f-4aee-a32d-9526320cb5e9","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"48a42df0-a10f-4aee-a32d-9526320cb5e9","children":["409d4cf5-ff1f-4095-bfc9-76e75faa7ae7"]},"409d4cf5-ff1f-4095-bfc9-76e75faa7ae7":{"id":"409d4cf5-ff1f-4095-bfc9-76e75faa7ae7","message":{"id":"409d4cf5-ff1f-4095-bfc9-76e75faa7ae7","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1754098602.17772,"update_time":null,"content":{"content_type":"text","parts":["Certainly. Below is a reconstructed, direct transcription of the narrative, divided into sections with speaker labels as appropriate. I have corrected broken or truncated sentences where necessary using the surrounding context. The speaker is Dwarkesh Patel, host of the Dwarkesh Podcast.\n\n---\n\n# Reflections on AGI Timelines\n\n**Dwarkesh:**  \nI\u2019ve had a lot of discussions on my podcast where we haggle out our timelines to AGI. Some guests think it\u2019s 20 years away. Others, 2 years. Here\u2019s where my thoughts lie as of July 2025. Sometimes people say that even if all AI progress totally stopped, the systems of today would still be far more economically transformative than the internet. I disagree. I think the LLMs of today are magical. But the reason that the Fortune 500 aren\u2019t using them to totally transform their workflows isn\u2019t because the management there is too stodgy. Rather, I think it\u2019s genuinely hard to get normal humanlike labor out of these LLMs. And this has to do with some fundamental capabilities these models lack.\n\n# Practical Limits of Current LLMs\n\nI like to think I\u2019m \"AI forward\" here at the Dwarkesh Podcast. I\u2019ve probably spent over a hundred hours trying to build these little LLM tools for my post-production setup. And the experience of trying to get them to be useful has actually extended my timelines. I\u2019ll try to get LLMs to rewrite autogenerated transcripts for me, to optimize for readability in the way a human would. Or I\u2019ll try to get them to identify clips from a transcript that I feed in. Sometimes I\u2019ll try to get them to co-write an essay with me, passage by passage. These are simple, self-contained, short-horizon, language-in, language-out tasks\u2014the kinds of assignments that should be dead center in the LLMs\u2019 repertoire. And they're 5/10 at them. Don\u2019t get me wrong, that is impressive.\n\nBut the fundamental problem is that LLMs don\u2019t get better over time the way a human would. This lack of continual learning is a huge, huge bottleneck. The LLM baseline at many tasks might be higher than the average human's. But there\u2019s no way to give a model high-level feedback. You\u2019re stuck with the abilities you get out of the box. You can keep messing around with the system prompt, but in practice this just doesn\u2019t produce anything even close to the kind of learning and improvement that human employees experience.\n\n# The Importance of Continual Learning\n\nThe reason humans are so useful is not mainly their raw intellect. It\u2019s their ability to build up context, interrogate their own failures, and pick up small improvements and efficiencies as they practice a task. How would you teach a kid to play the saxophone? You'd have them try to blow into one, and then they'd see how it sounds, and they'd adjust. Now imagine if this was the way you'd have to teach saxophone instead: A student takes one attempt. And the moment they make a mistake, you send them away and you write detailed instructions about what went wrong. Then you call the next student in. The next student reads your notes and tries to play Charlie Parker cold. And when they fail, you refine your instructions and you invite the next student. This just wouldn\u2019t work. No matter how well honed your prompt is, no kid is just going to learn how to play saxophone from reading your instructions.\n\nBut this is the only modality we have to \u2018teach\u2019 LLMs anything. Yes, there\u2019s RL fine-tuning. But it\u2019s not a deliberate, adaptive process in the way that human learning is. My editors have gotten extremely good. And they wouldn\u2019t have gotten that way if we had to build bespoke RL environments for different subtasks involved in their work. They\u2019ve just noticed a lot of small things themselves and thought hard about what resonates with the audience, what kind of content I like, and how they can improve their day-to-day workflows.\n\nNow, it\u2019s possible to imagine ways in which a smarter model could build a dedicated RL loop for itself which just feels super organic from the outside. I give some high-level feedback, and the model comes up with a bunch of verifiable practice problems to RL on\u2014maybe even a whole environment in which it gets to rehearse the skills that it thinks it's lacking. But this just sounds really hard. And I don\u2019t know how well these techniques will generalize to different kinds of tasks and feedback.\n\nEventually the models will be able to learn on the job in this organic way that humans can. But it\u2019s just hard for me to see how that could happen within the next few years, given there\u2019s no immediately obvious way in which to slot in continuous learning into the kinds of models that these LLMs are.\n\n# Session Memory and Context Limits\n\nLLMs actually do get kinda smart and useful in the middle of a session. For example, sometimes I\u2019ll co-write an essay with an LLM. I\u2019ll give it an outline, and I\u2019ll ask it to draft the essay passage by passage. And all its suggestions up till paragraph four will just be bad. I'll just rewrite every single paragraph from scratch and tell it, \"Look, your shit sucked. This is what I wrote instead.\" And at this point, it will actually start giving good suggestions for the next paragraph. But this whole subtle understanding of my preferences and style will just be lost by the end of the session.\n\nMaybe there is an easy solution to this that looks like a long rolling context window, like Claude Code already has, which just compacts the session memory into a summary every 30 minutes. I just think that titrating all this rich tacit experience into a text summary will be brittle in domains outside of software engineering, which is very text-based, in which you already have this external scaffold of memory that is stored in the codebase itself. Again, think about what it would be like to teach a kid to play the saxophone just from text.\n\nEven Claude Code will often reverse a hard-earned optimization that we engineered together before I hit /compact\u2014because the explanation for why it was made didn\u2019t make it into the summary.\n\n# Disagreement With AI Optimism\n\nThis is why I disagree with something that Anthropic researchers Sholto Douglas and Trenton Bricken said on my podcast. And this quote is from Trenton:  \n\"Even if AI progress totally stalls (and you think that the models are really spiky, and they don't have general intelligence), it's so economically valuable, and sufficiently easy to collect data on all of these different white collar job tasks, such that to Sholto's point we should expect to see them automated within the next five years.\"\n\nIf AI progress totally stops today, I think less than 25% of white-collar employment goes away. Sure, many tasks will get automated. Claude 4 Opus can technically rewrite autogenerated transcripts for me. But since it\u2019s not possible for me to have it improve over time and learn my preferences, I still hire a human for this. So even if we get more data, without progress in continual learning, I think that we will be in a substantially similar position with all other kinds of white-collar work.\n\nYes, technically AIs will be able to do a lot of subtasks somewhat satisfactorily, but their inability to build up context will make it impossible to have them operate as actual employees at your firm.\n\nWhile this makes me bearish about transformative AI in the next few years, it makes me especially bullish on AI over the next decades. When we do solve continual learning, we\u2019ll see a huge discontinuity in the value of these models. Even if there isn\u2019t a software-only singularity, where these models rapidly build smarter and smarter successor systems, we might still get something that looks like a broadly deployed intelligence explosion. AIs will be getting broadly deployed through the economy, and doing different jobs and learning while doing them in the way that humans can.\n\nHowever, unlike humans, these models can amalgamate their learnings across all their copies. So one AI is basically learning how to do every single job in the economy. An AI that is capable of this kind of online learning might rapidly become a superintelligence even if there's no further algorithmic progress.\n\nHowever, I\u2019m not expecting to watch some OpenAI livestream where they announce that continual learning has been totally solved. Because labs are incentivized to release any innovations quickly, we\u2019ll see a broken early version of continual learning (or test-time training, or whatever you want to call it) before we see something which truly learns like a human. I expect to get lots of heads up before this big bottleneck is totally solved.\n\n# Computer Use Agents and Skepticism\n\nWhen I interviewed Anthropic researchers Sholto Douglas and Trenton Bricken on my podcast, they said that they expect reliable computer use agents by the end of next year. We already have computer use agents right now, but they\u2019re pretty bad. They\u2019re imagining something quite different. Their forecast is that by the end of next year, you should be able to tell an AI, \"Go do my taxes.\" And it\u2019ll go through all your email, your Amazon orders, and Slack messages, and it will email back and forth with every single person you need to get invoices from, it'll compile all your receipts, decide what things are actually business expenses, and it will ask for your approval on all the edge cases, and then will just submit Form 1040 to the IRS.\n\nI\u2019m skeptical. I\u2019m not an AI researcher, so far be it for me to contradict them on technical details. But given what little I know, here\u2019s why I\u2019d bet against this forecast:\n\nOne. As horizon lengths increase, rollouts have to become longer. The AI needs to do two hours worth of agentic computer use tasks before we can even see if it did it right. Not to mention that computer use requires processing images and video, which is already more compute intensive, even if you don\u2019t factor in the longer rollouts. This seems like it should slow down progress.\n\nTwo. We don\u2019t have a large pretraining corpus of multimodal computer use data. I like this quote from Mechanize\u2019s post on automating software engineering:  \n\"For the past decade of scaling, we\u2019ve been spoiled by the enormous amount of internet data that was freely available for us to use. This was enough to crack natural language processing, but not for getting models to become reliable, competent agents. Imagine trying to train GPT-4 on all the text data available in 1980\u2014the data would have been nowhere near enough, even if you had the necessary compute.\"\n\nAgain, I\u2019m not at the labs. Maybe text-only training already gives you a great prior over how different UIs work, and what the relationship is between different components. Maybe RL fine-tuning is so sample efficient that you don\u2019t need that much data. But I haven\u2019t seen any public evidence which makes me think that these models have suddenly gotten less data hungry, especially in domains where they\u2019re substantially less practiced.\n\nAlternatively, maybe these models are such good front-end coders that they can just generate millions of toy UIs for themselves to practice on.\n\nBut, three. Even algorithmic innovations which seem quite simple in retrospect took a long time to iron out. The RL procedure which DeepSeek explained in their R1 paper seems simple at a high level. And yet it took 2 years from the development and launch of GPT-4 to the release of o1. Now, of course, I know that it's insanely and hilariously arrogant to say that R1/o1 were easy\u2014a ton of engineering, debugging, and pruning of alternative ideas was required to arrive at this solution. But that\u2019s precisely my point! Seeing how long it took to implement the idea of \u2018We should train a model to solve verifiable math and coding problems,\u2019 makes me think that we\u2019re underestimating the difficulty of solving the much gnarlier problem of computer use, where you\u2019re operating in a totally different modality with much less data.\n\n# On Reasoning and Progress\n\nOkay, enough cold water. I\u2019m not going to be like one of those spoiled children on Hackernews who could be handed a golden-egg-laying goose and would still spend all their time complaining about how loud its quacks are. Have you read the reasoning traces from o3 or Gemini 2.5? It\u2019s actually reasoning! It\u2019s breaking down a problem, thinking through what the user wants, reacting to its own internal monologue, and correcting itself when it notices that it's pursuing an unproductive direction.\n\nHow are we just like, \"Oh yeah of course a machine is gonna go think a bunch, come up with a bunch of ideas, and come back to me with a smart answer. That\u2019s just what machines do.\" Part of the reason some people are too pessimistic is that they haven\u2019t played around with the smartest models in domains where they\u2019re the most competent. Giving Claude Code a vague spec and just sitting around for 10 minutes while it zero-shots a working application is a wild experience. How did it do that? You can talk about circuits and the training distribution and RL or whatever, but the most proximal, concise, and accurate explanation is simply that it\u2019s powered by a baby general intelligence.\n\nAt this point, part of you has to be thinking, \"It\u2019s actually working. We\u2019re making machines that are intelligent.\" My probability distributions are super wide.\n\n# Concrete Predictions\n\nSo what are my predictions? And I want to emphasize that I do believe in probability distributions. Which means that work to prepare for a misaligned 2028 ASI still makes a ton of sense. I think this is a totally plausible outcome. But here are the timelines at which I\u2019d take a 50/50 bet.\n\nAn AI that can do taxes end-to-end for my small business as well as a competent general manager could in a week: including chasing down all the receipts on different websites, finding the missing pieces, emailing back and forth with anyone who we need to hassle for invoices, filling out the form, and sending it to the IRS. This I'd say 2028. I think we\u2019re in the GPT 2 era for computer use. But we have no pretraining corpus, and the models are optimizing for a much sparser reward over a much longer time horizon using action primitives they\u2019re unfamiliar with.\n\nThat being said, the base model is already decently smart and might have a good prior over computer use tasks, plus there\u2019s a lot more compute and AI researchers in the world, so it might even out. Preparing taxes for a small business feels like for computer use what GPT-4 was for language. It took 4 years to get from GPT-2 to GPT-4.\n\nJust to clarify, I am not saying that we won\u2019t have really cool computer use demos in 2026 and 2027. GPT-3 was super cool, but it was not that practically useful. I\u2019m saying that these models won\u2019t be capable of end-to-end handling a week-long and quite involved project which involves computer use.\n\nAnd as for the forecast of when AI will be able to learn on the job as easily, organically, seamlessly, and quickly as humans, for any white-collar work: For example, if I hired an AI video editor, after six months it would have as much actionable, deep understanding of my preferences, our channel, what works for the audience, as well as a human would. I'd say this would come in 2032. While I don\u2019t see an obvious way to slot in continuous online learning into the kinds of models these LLMs are, 7 years is a really long time! GPT-1 had just come out this time 7 years ago. It doesn\u2019t seem implausible to me that over the next 7 years, we\u2019ll find some way to get these models to actually learn on the job.\n\nAt this point you might be reacting, \"Wait, you made this huge fuss about continual learning being such a huge handicap. But then your prediction is that we\u2019re 7 years away from what, at a minimum, looks like a broadly deployed intelligence explosion.\" And yeah, you\u2019re right. I am forecasting a pretty wild world within a relatively short amount of time. AGI timelines are very lognormal. It's either this decade or bust. (Not really, it's more like lower marginal probability per year\u2014but that\u2019s less catchy.)\n\n# Limits to Scaling and Algorithmic Progress\n\nAI progress over the last decade has been driven by scaling training compute on the frontier systems. It's been over 4x a year. This cannot continue beyond this decade, whether you look at chips, power, or even the raw fraction of GDP that is used on training. After 2030, AI progress has to mostly come from algorithmic progress. But even there all the low-hanging fruit will be plucked, at least under the deep learning paradigm. So the yearly probability of AGI collapses. This means that if we end up on the longer side of my 50/50 bets, we might be looking at a relatively normal world up till the 2030s or even the 2040s. But in all the other worlds, even if we stay sober about the current limitations of AI, we have to expect some truly crazy outcomes.\n\n# Closing and Invitation\n\nThis was originally a blog post that I published on my website at dwarkesh.com. And it was obviously inspired by the discussion I had with Sholto and Trenton on my podcast, where I ended up disagreeing with them about timelines but it took me a few weeks of thinking afterwards, sorting out exactly where I disagree and why I had longer timelines. And I do this for other episodes as well. I wrote up some thoughts I had about the many thousands of pages that Stephen Kotkin has written about Stalin, obviously which we were not able to exhaustively cover in that one 2-hour interview. So anyways, if you want to see these additional artifacts and writing that I produce as a result of this podcast and in preparation for episodes, you should subscribe to my blog and newsletter. You can do that at dwarkesh.com. Otherwise I will also see you next week for a full episode with a real guest."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"request_id":"9689d885dc695fab-SIN","message_type":"next","model_slug":"gpt-4-1","default_model_slug":"gpt-4-1","parent_id":"7594cec5-8c51-400a-b6f9-0418258c60bf","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"7594cec5-8c51-400a-b6f9-0418258c60bf","children":[]}}}