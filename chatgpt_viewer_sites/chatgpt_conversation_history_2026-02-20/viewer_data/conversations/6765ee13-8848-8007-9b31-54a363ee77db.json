{"id":"6765ee13-8848-8007-9b31-54a363ee77db","title":"Byte Latent Transformers Overview","create_time":1734733331.668957,"update_time":1734733341.40082,"current_node":"73f50abc-2677-4a22-9714-728d6081e248","mapping":{"aaa18316-65f9-4a96-8585-eb2226b32cfd":{"id":"aaa18316-65f9-4a96-8585-eb2226b32cfd","message":null,"parent":null,"children":["257d4f75-4f04-4b03-9caa-7add80407f34"]},"257d4f75-4f04-4b03-9caa-7add80407f34":{"id":"257d4f75-4f04-4b03-9caa-7add80407f34","message":{"id":"257d4f75-4f04-4b03-9caa-7add80407f34","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"aaa18316-65f9-4a96-8585-eb2226b32cfd","children":["bf3079d8-43fb-4c7c-9dc0-3437e99acd2d"]},"bf3079d8-43fb-4c7c-9dc0-3437e99acd2d":{"id":"bf3079d8-43fb-4c7c-9dc0-3437e99acd2d","message":{"id":"bf3079d8-43fb-4c7c-9dc0-3437e99acd2d","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```About Myself: I have a very high level of mathematical knowledge up to the post-graduate level.  I also have a keen interest in graduate level theoretical physics.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python\n(2) mathematics up to graduate level and research level\n(3) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```- When writing machine learning code in Python, use Pytorch unless there are strong compelling reasons for using another library.\n\n- Make sure all Python code is compatible with Python version 3.11.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"About Myself: I have a very high level of mathematical knowledge up to the post-graduate level.  I also have a keen interest in graduate level theoretical physics.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python\n(2) mathematics up to graduate level and research level\n(3) machine learning and artificial intelligence","about_model_message":"- When writing machine learning code in Python, use Pytorch unless there are strong compelling reasons for using another library.\n\n- Make sure all Python code is compatible with Python version 3.11."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"257d4f75-4f04-4b03-9caa-7add80407f34","children":["aaa2e2a1-bc58-4b05-a803-dba022699062"]},"aaa2e2a1-bc58-4b05-a803-dba022699062":{"id":"aaa2e2a1-bc58-4b05-a803-dba022699062","message":{"id":"aaa2e2a1-bc58-4b05-a803-dba022699062","author":{"role":"user","name":null,"metadata":{}},"create_time":1734733331.673566,"update_time":null,"content":{"content_type":"text","parts":["What is the latest news on Byte Latent Transformers?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"8f530798fd529c47-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"bf3079d8-43fb-4c7c-9dc0-3437e99acd2d","children":["87f26f41-3b4a-4231-bbe7-b4e122e02be5"]},"87f26f41-3b4a-4231-bbe7-b4e122e02be5":{"id":"87f26f41-3b4a-4231-bbe7-b4e122e02be5","message":{"id":"87f26f41-3b4a-4231-bbe7-b4e122e02be5","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1734733340.372546,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"aaa2e2a1-bc58-4b05-a803-dba022699062","request_id":"8f530798fd529c47-SIN","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"aaa2e2a1-bc58-4b05-a803-dba022699062","children":["53212c1b-4156-4ecf-a690-551daf64d018"]},"53212c1b-4156-4ecf-a690-551daf64d018":{"id":"53212c1b-4156-4ecf-a690-551daf64d018","message":{"id":"53212c1b-4156-4ecf-a690-551daf64d018","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1734733340.372646,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"What is the latest news on Byte Latent Transformers?\")"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_complete":true,"finish_details":{"type":"stop"},"citations":[],"content_references":[],"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"87f26f41-3b4a-4231-bbe7-b4e122e02be5","request_id":"8f530798fd529c47-SIN","timestamp_":"absolute","can_save":true},"recipient":"web","channel":null},"parent":"87f26f41-3b4a-4231-bbe7-b4e122e02be5","children":["b7b57a6d-0a25-491a-bfcc-2056113aaabe"]},"b7b57a6d-0a25-491a-bfcc-2056113aaabe":{"id":"b7b57a6d-0a25-491a-bfcc-2056113aaabe","message":{"id":"b7b57a6d-0a25-491a-bfcc-2056113aaabe","author":{"role":"tool","name":"web","metadata":{}},"create_time":1734733340.372718,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":""},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"command":"search","status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"53212c1b-4156-4ecf-a690-551daf64d018","request_id":"8f530798fd529c47-SIN","timestamp_":"absolute","can_save":false},"recipient":"assistant","channel":null},"parent":"53212c1b-4156-4ecf-a690-551daf64d018","children":["73f50abc-2677-4a22-9714-728d6081e248"]},"73f50abc-2677-4a22-9714-728d6081e248":{"id":"73f50abc-2677-4a22-9714-728d6081e248","message":{"id":"73f50abc-2677-4a22-9714-728d6081e248","author":{"role":"assistant","name":null,"metadata":{"real_author":"tool:web"}},"create_time":1734733340.372783,"update_time":null,"content":{"content_type":"text","parts":["\ue203The Byte Latent Transformer (BLT) is a recently introduced architecture that processes raw byte sequences without relying on traditional tokenization methods.\ue204 \ue203Developed by researchers including Artidoro Pagnoni and colleagues, BLT dynamically groups bytes into variable-sized patches based on data complexity, enhancing efficiency and robustness in large language models (LLMs).\ue204 \ue200cite\ue202turn0search0\ue201\ue206\n\n\ue203Traditional LLMs depend on fixed-vocabulary tokenizers, which can introduce biases and inefficiencies, especially when handling diverse languages or noisy inputs.\ue204 \ue203BLT addresses these challenges by eliminating tokenization, allowing for more flexible and precise processing of text at the byte level.\ue204 \ue203This approach enables BLT to match or surpass the performance of tokenization-based models while improving inference efficiency.\ue204 \ue200cite\ue202turn0search3\ue201\ue206\n\n\ue203The architecture of BLT comprises three main components:\ue204\ue206\n\n\ue2031. **Local Encoder**: Encodes input bytes into patch representations, utilizing cross-attention mechanisms and n-gram hash embeddings.\ue204\ue206\n\n\ue2032. **Latent Transformer**: Processes these patch representations, focusing computational resources on complex data regions.\ue204\ue206\n\n\ue2033. **Local Decoder**: Decodes the processed representations back into byte sequences.\ue204\ue206\n\n\ue203This design allows BLT to dynamically adjust patch sizes, allocating more computational effort to intricate parts of the data, thereby enhancing both training and inference efficiency.\ue204 \ue200cite\ue202turn0search1\ue201\ue206\n\n\ue203In scaling studies, BLT has demonstrated the ability to handle models with up to 8 billion parameters and training datasets comprising 4 trillion bytes.\ue204 \ue203It achieves comparable or superior performance to models like Llama 3, using up to 50% fewer floating-point operations during inference.\ue204 \ue203Additionally, BLT exhibits improved robustness to input noise and a better understanding of sub-word structures, making it effective in multilingual contexts and tasks requiring sensitivity to orthographic details.\ue204 \ue200cite\ue202turn0search3\ue201\ue206\n\n\ue203For those interested in exploring BLT further, the research paper detailing its architecture and performance is available on arXiv.\ue204 \ue200cite\ue202turn0search0\ue201 \ue203Additionally, the codebase for BLT has been made publicly accessible on GitHub, facilitating experimentation and further development.\ue204 \ue200cite\ue202turn0search6\ue201\ue206\n\n\ue203Overall, BLT represents a significant advancement in the development of tokenizer-free models, offering a promising direction for future research and applications in natural language processing.\ue204\n\nFor a visual overview, you might find the following video informative:\n\n\ue200video\ue202Byte Latent Transformer: Patches Scale Better Than Tokens\ue202turn0search5\ue201 "]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"search_source":"composer_auto","client_reported_search_source":"composer_auto","search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2412.09871","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","snippet":"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inferen...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"content_type":null,"pub_date":1734067980.0,"attributions":null},{"type":"search_result","url":"https://arxiv.org/html/2412.09871v1","title":"Byte Latent Transformer: Patches Scale Better Than Tokens - arXiv.org","snippet":"We introduce the Byte Latent Transformer (BLT), a tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, wit...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"content_type":null,"pub_date":1734317223.0,"attributions":null},{"type":"search_result","url":"https://export.arxiv.org/abs/2412.09871","title":"Title: Byte Latent Transformer: Patches Scale Better Than Tokens","snippet":"Abstract: We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"content_type":null,"pub_date":1734067980.0,"attributions":null}]},{"type":"search_result_group","domain":"marktechpost.com","entries":[{"type":"search_result","url":"https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/","title":"Meta AI Introduces Byte Latent Transformer (BLT): A Tokenizer-Free ...","snippet":"Meta AI\u2019s Byte Latent Transformer (BLT) seeks to address these issues by eliminating tokenization altogether. BLT is a tokenizer-free architecture that processes raw byte sequences and dynamically gro...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"content_type":null,"pub_date":1734048000.0,"attributions":null}]},{"type":"search_result_group","domain":"ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=42415122","title":"Byte Latent Transformer: Patches Scale Better Than Tokens - Hacker News","snippet":"- Transformer: Takes these encodings of patches in autoregressive fashion - Decoder: Takes processed encodings by transformers and outputs bytes. Loss is on byte to byte crossentropy (Next byte predic...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"content_type":null,"pub_date":1734134400.0,"attributions":null}]},{"type":"search_result_group","domain":"youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=7_w_4jyQLcI","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","snippet":"The Byte Latent Transformer (BLT) achieves tokenization-level performance with improved efficiency and robustness by encoding bytes into dynamic patches, enh...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"content_type":null,"pub_date":1734393600.0,"attributions":null},{"type":"search_result","url":"https://www.youtube.com/watch?v=HIgSJPPAhbU","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","snippet":"BLT (Byte Latent Transformer) is a new type of large language model (LLM) that processes text directly at the byte level, unlike traditional LLMs that rely o...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"content_type":null,"pub_date":null,"attributions":null}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/krishnatray/meta_blt_transformer","title":"krishnatray/meta_blt_transformer: Code for BLT research paper - GitHub","snippet":"We introduce the Byte Latent Transformer architecture (BLTs), a new byte-level LLM architecture that for the first time, matches tokenization-based LLM performance at scale, with significant improveme...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"content_type":null,"pub_date":null,"attributions":null}]},{"type":"search_result_group","domain":"the-decoder.com","entries":[{"type":"search_result","url":"https://the-decoder.com/metas-new-llm-architecture-tackles-fundamental-flaw-in-how-language-models-process-text/","title":"Meta's new LLM architecture tackles fundamental flaw in how language ...","snippet":"Meta has developed the Byte Latent Transformer (BLT), a new language model architecture that operates directly at the byte level instead of splitting words into predefined tokens, allowing for more pr...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"content_type":null,"pub_date":1734220800.0,"attributions":null}]},{"type":"search_result_group","domain":"ajithp.com","entries":[{"type":"search_result","url":"https://ajithp.com/2024/12/15/metas-byte-latent-transformer-revolutionizing-natural-language-processing-with-dynamic-patching/","title":"Meta\u2019s Byte Latent Transformer: Revolutionizing Natural Language ...","snippet":"The Byte Latent Transformer (BLT) is not just a model\u2014it\u2019s a groundbreaking advancement that redefines the future of NLP. By shifting the paradigm from tokenization to byte-level processing, BLT intro...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"content_type":null,"pub_date":1734220800.0,"attributions":null}]},{"type":"search_result_group","domain":"paperswithcode.com","entries":[{"type":"search_result","url":"https://paperswithcode.com/paper/byte-latent-transformer-patches-scale-better","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","snippet":"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inferen...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"content_type":null,"pub_date":1734048000.0,"attributions":null}]},{"type":"search_result_group","domain":"papers.cool","entries":[{"type":"search_result","url":"https://papers.cool/arxiv/2412.09871","title":"Byte Latent Transformer: Patches Scale Better Than Tokens | Cool Papers ...","snippet":"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inferen...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"content_type":null,"pub_date":null,"attributions":null}]}],"safe_urls":["https://arxiv.org/html/2412.09871v1?utm_source=chatgpt.com","https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/?utm_source=chatgpt.com","https://github.com/krishnatray/meta_blt_transformer?utm_source=chatgpt.com","https://www.youtube.com/watch?v=7_w_4jyQLcI&utm_source=chatgpt.com","https://img.youtube.com/vi/7_w_4jyQLcI/maxresdefault.jpg","https://arxiv.org/abs/2412.09871?utm_source=chatgpt.com"],"message_locale":"en-US","message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"b7b57a6d-0a25-491a-bfcc-2056113aaabe","image_results":[],"content_references":[{"matched_text":"\ue203","start_idx":0,"end_idx":1,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":159,"end_idx":160,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":161,"end_idx":162,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":381,"end_idx":382,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0search0\ue201","start_idx":383,"end_idx":402,"alt":"([ArXiv](https://arxiv.org/abs/2412.09871?utm_source=chatgpt.com))","prompt_text":null,"type":"webpage","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","url":"https://arxiv.org/abs/2412.09871?utm_source=chatgpt.com","pub_date":1734067980.0,"snippet":"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte ...","attributions":null,"attribution":"ArXiv"},{"matched_text":"\ue206","start_idx":402,"end_idx":403,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":405,"end_idx":406,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":568,"end_idx":569,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":570,"end_idx":571,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":707,"end_idx":708,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":709,"end_idx":710,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":838,"end_idx":839,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0search3\ue201","start_idx":840,"end_idx":859,"alt":"([MarkTechPost](https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/?utm_source=chatgpt.com))","prompt_text":null,"type":"webpage","title":"Meta AI Introduces Byte Latent Transformer (BLT): A Tokenizer-Free ...","url":"https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/?utm_source=chatgpt.com","pub_date":1734048000.0,"snippet":"Meta AI\u2019s Byte Latent Transformer (BLT) seeks to address these issues by eliminating tokenization altogether. BLT is a tokenizer-free architecture that processes raw byte sequences and dynamically groups them into patches based on data complexity. This approach enables efficient scaling, matching, or exceeding the performance of tokenization ...","attributions":null,"attribution":"MarkTechPost"},{"matched_text":"\ue206","start_idx":859,"end_idx":860,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":862,"end_idx":863,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":919,"end_idx":920,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue206","start_idx":920,"end_idx":921,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":923,"end_idx":924,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1058,"end_idx":1059,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue206","start_idx":1059,"end_idx":1060,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":1062,"end_idx":1063,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1186,"end_idx":1187,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue206","start_idx":1187,"end_idx":1188,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":1190,"end_idx":1191,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1276,"end_idx":1277,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue206","start_idx":1277,"end_idx":1278,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":1280,"end_idx":1281,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1465,"end_idx":1466,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0search1\ue201","start_idx":1467,"end_idx":1486,"alt":"([ArXiv](https://arxiv.org/html/2412.09871v1?utm_source=chatgpt.com))","prompt_text":null,"type":"webpage","title":"Byte Latent Transformer: Patches Scale Better Than Tokens - arXiv.org","url":"https://arxiv.org/html/2412.09871v1?utm_source=chatgpt.com","pub_date":1734317223.0,"snippet":"We introduce the Byte Latent Transformer (BLT), a tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, with significant improvements in efficiency and robustness (\u00a7 6).Existing large language models (llm s) are trained almost entirely end-to-end, except for tokenization\u2014a heuristic pre-processing ...","attributions":null,"attribution":"ArXiv"},{"matched_text":"\ue206","start_idx":1486,"end_idx":1487,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":1489,"end_idx":1490,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1642,"end_idx":1643,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":1644,"end_idx":1645,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1781,"end_idx":1782,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":1783,"end_idx":1784,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1998,"end_idx":1999,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0search3\ue201","start_idx":2000,"end_idx":2019,"alt":"([MarkTechPost](https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/?utm_source=chatgpt.com))","prompt_text":null,"type":"webpage","title":"Meta AI Introduces Byte Latent Transformer (BLT): A Tokenizer-Free ...","url":"https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/?utm_source=chatgpt.com","pub_date":1734048000.0,"snippet":"Meta AI\u2019s Byte Latent Transformer (BLT) seeks to address these issues by eliminating tokenization altogether. BLT is a tokenizer-free architecture that processes raw byte sequences and dynamically groups them into patches based on data complexity. This approach enables efficient scaling, matching, or exceeding the performance of tokenization ...","attributions":null,"attribution":"MarkTechPost"},{"matched_text":"\ue206","start_idx":2019,"end_idx":2020,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":2022,"end_idx":2023,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":2154,"end_idx":2155,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0search0\ue201","start_idx":2156,"end_idx":2175,"alt":"([ArXiv](https://arxiv.org/abs/2412.09871?utm_source=chatgpt.com))","prompt_text":null,"type":"webpage","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","url":"https://arxiv.org/abs/2412.09871?utm_source=chatgpt.com","pub_date":1734067980.0,"snippet":"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte ...","attributions":null,"attribution":"ArXiv"},{"matched_text":"\ue203","start_idx":2176,"end_idx":2177,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":2310,"end_idx":2311,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0search6\ue201","start_idx":2312,"end_idx":2331,"alt":"([GitHub](https://github.com/krishnatray/meta_blt_transformer?utm_source=chatgpt.com))","prompt_text":null,"type":"webpage","title":"krishnatray/meta_blt_transformer: Code for BLT research paper - GitHub","url":"https://github.com/krishnatray/meta_blt_transformer?utm_source=chatgpt.com","pub_date":null,"snippet":"We introduce the Byte Latent Transformer architecture (BLTs), a new byte-level LLM architecture that for the first time, matches tokenization-based LLM performance at scale, with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.","attributions":null,"attribution":"GitHub"},{"matched_text":"\ue206","start_idx":2331,"end_idx":2332,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":2334,"end_idx":2335,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":2529,"end_idx":2530,"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200video\ue202Byte Latent Transformer: Patches Scale Better Than Tokens\ue202turn0search5\ue201","start_idx":2604,"end_idx":2682,"alt":"[Byte Latent Transformer: Patches Scale Better Than Tokens](https://www.youtube.com/watch?v=7_w_4jyQLcI&utm_source=chatgpt.com)\n","prompt_text":null,"type":"video","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","thumbnail_url":"https://img.youtube.com/vi/7_w_4jyQLcI/maxresdefault.jpg","url":"https://www.youtube.com/watch?v=7_w_4jyQLcI&utm_source=chatgpt.com","video_site":"youtube","video_id":"7_w_4jyQLcI"},{"matched_text":" ","start_idx":2682,"end_idx":2682,"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"Byte Latent Transformer: Patches Scale Better Than Tokens","url":"https://arxiv.org/abs/2412.09871?utm_source=chatgpt.com","attribution":"ArXiv"},{"title":"Meta AI Introduces Byte Latent Transformer (BLT): A Tokenizer-Free ...","url":"https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/?utm_source=chatgpt.com","attribution":"MarkTechPost"},{"title":"krishnatray/meta_blt_transformer: Code for BLT research paper - GitHub","url":"https://github.com/krishnatray/meta_blt_transformer?utm_source=chatgpt.com","attribution":"GitHub"}],"has_images":false}],"citations":[{"start_ix":0,"end_ix":1,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":159,"end_ix":160,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":161,"end_ix":162,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":381,"end_ix":382,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":383,"end_ix":402,"metadata":{"type":"webpage","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","url":"https://arxiv.org/abs/2412.09871?utm_source=chatgpt.com","text":"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte ...","pub_date":"2024-12-13T05:33:00+00:00","extra":null,"og_tags":null}},{"start_ix":402,"end_ix":403,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":405,"end_ix":406,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":568,"end_ix":569,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":570,"end_ix":571,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":707,"end_ix":708,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":709,"end_ix":710,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":838,"end_ix":839,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":840,"end_ix":859,"metadata":{"type":"webpage","title":"Meta AI Introduces Byte Latent Transformer (BLT): A Tokenizer-Free ...","url":"https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/?utm_source=chatgpt.com","text":"Meta AI\u2019s Byte Latent Transformer (BLT) seeks to address these issues by eliminating tokenization altogether. BLT is a tokenizer-free architecture that processes raw byte sequences and dynamically groups them into patches based on data complexity. This approach enables efficient scaling, matching, or exceeding the performance of tokenization ...","pub_date":"2024-12-13T00:00:00+00:00","extra":null,"og_tags":null}},{"start_ix":859,"end_ix":860,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":862,"end_ix":863,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":919,"end_ix":920,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":920,"end_ix":921,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":923,"end_ix":924,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1058,"end_ix":1059,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1059,"end_ix":1060,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1062,"end_ix":1063,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1186,"end_ix":1187,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1187,"end_ix":1188,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1190,"end_ix":1191,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1276,"end_ix":1277,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1277,"end_ix":1278,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1280,"end_ix":1281,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1465,"end_ix":1466,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1467,"end_ix":1486,"metadata":{"type":"webpage","title":"Byte Latent Transformer: Patches Scale Better Than Tokens - arXiv.org","url":"https://arxiv.org/html/2412.09871v1?utm_source=chatgpt.com","text":"We introduce the Byte Latent Transformer (BLT), a tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, with significant improvements in efficiency and robustness (\u00a7 6).Existing large language models (llm s) are trained almost entirely end-to-end, except for tokenization\u2014a heuristic pre-processing ...","pub_date":"2024-12-16T02:47:03+00:00","extra":null,"og_tags":null}},{"start_ix":1486,"end_ix":1487,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1489,"end_ix":1490,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1642,"end_ix":1643,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1644,"end_ix":1645,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1781,"end_ix":1782,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1783,"end_ix":1784,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1998,"end_ix":1999,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2000,"end_ix":2019,"metadata":{"type":"webpage","title":"Meta AI Introduces Byte Latent Transformer (BLT): A Tokenizer-Free ...","url":"https://www.marktechpost.com/2024/12/13/meta-ai-introduces-byte-latent-transformer-blt-a-tokenizer-free-model-that-scales-efficiently/?utm_source=chatgpt.com","text":"Meta AI\u2019s Byte Latent Transformer (BLT) seeks to address these issues by eliminating tokenization altogether. BLT is a tokenizer-free architecture that processes raw byte sequences and dynamically groups them into patches based on data complexity. This approach enables efficient scaling, matching, or exceeding the performance of tokenization ...","pub_date":"2024-12-13T00:00:00+00:00","extra":null,"og_tags":null}},{"start_ix":2019,"end_ix":2020,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2022,"end_ix":2023,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2154,"end_ix":2155,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2156,"end_ix":2175,"metadata":{"type":"webpage","title":"Byte Latent Transformer: Patches Scale Better Than Tokens","url":"https://arxiv.org/abs/2412.09871?utm_source=chatgpt.com","text":"We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte ...","pub_date":"2024-12-13T05:33:00+00:00","extra":null,"og_tags":null}},{"start_ix":2176,"end_ix":2177,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2310,"end_ix":2311,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2312,"end_ix":2331,"metadata":{"type":"webpage","title":"krishnatray/meta_blt_transformer: Code for BLT research paper - GitHub","url":"https://github.com/krishnatray/meta_blt_transformer?utm_source=chatgpt.com","text":"We introduce the Byte Latent Transformer architecture (BLTs), a new byte-level LLM architecture that for the first time, matches tokenization-based LLM performance at scale, with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.","pub_date":null,"extra":null,"og_tags":null}},{"start_ix":2331,"end_ix":2332,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2334,"end_ix":2335,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2529,"end_ix":2530,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2604,"end_ix":2682,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":2682,"end_ix":2682,"invalid_reason":"Unsupported, please upgrade"}],"request_id":"8f530798fd529c47-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"b7b57a6d-0a25-491a-bfcc-2056113aaabe","children":[]}}}