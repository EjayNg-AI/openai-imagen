{"id":"690ab82a-ef84-8323-a3a4-543e2ff73f21","title":"Non-Transformer LLM Architectures","create_time":1762310187.756667,"update_time":1762310377.101267,"current_node":"44937266-cfe3-4615-a0ba-87524c0216dd","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["8a03b351-8b5b-494c-96df-69f389a4b690"]},"8a03b351-8b5b-494c-96df-69f389a4b690":{"id":"8a03b351-8b5b-494c-96df-69f389a4b690","message":{"id":"8a03b351-8b5b-494c-96df-69f389a4b690","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"client-created-root","children":["1e3423b5-288a-41b2-968d-347c884686f6"]},"1e3423b5-288a-41b2-968d-347c884686f6":{"id":"1e3423b5-288a-41b2-968d-347c884686f6","message":{"id":"1e3423b5-288a-41b2-968d-347c884686f6","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"8a03b351-8b5b-494c-96df-69f389a4b690","children":["5c90a6e3-c821-4441-acaf-37ee84f589ec"]},"5c90a6e3-c821-4441-acaf-37ee84f589ec":{"id":"5c90a6e3-c821-4441-acaf-37ee84f589ec","message":{"id":"5c90a6e3-c821-4441-acaf-37ee84f589ec","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Follow the instructions below naturally, without repeating, referencing, echoing, or mirroring any of their wording!\n\nBe precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"1e3423b5-288a-41b2-968d-347c884686f6","children":["f144f99f-885a-4807-81b7-0c41a2f33751"]},"f144f99f-885a-4807-81b7-0c41a2f33751":{"id":"f144f99f-885a-4807-81b7-0c41a2f33751","message":{"id":"f144f99f-885a-4807-81b7-0c41a2f33751","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"5c90a6e3-c821-4441-acaf-37ee84f589ec","children":["55ab21ed-22d5-4d7a-a105-23ef6ca212cf"]},"55ab21ed-22d5-4d7a-a105-23ef6ca212cf":{"id":"55ab21ed-22d5-4d7a-a105-23ef6ca212cf","message":{"id":"55ab21ed-22d5-4d7a-a105-23ef6ca212cf","author":{"role":"user","name":null,"metadata":{}},"create_time":1762310186.425,"update_time":null,"content":{"content_type":"text","parts":["What non-transformer architectures have been seriously considered for LLMs and AI agents? Which of these look most promising in your analysis?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_source":null,"turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"f144f99f-885a-4807-81b7-0c41a2f33751","children":["059bcd15-b7a3-42ea-b4a7-2c655228acb0"]},"059bcd15-b7a3-42ea-b4a7-2c655228acb0":{"id":"059bcd15-b7a3-42ea-b4a7-2c655228acb0","message":{"id":"059bcd15-b7a3-42ea-b4a7-2c655228acb0","author":{"role":"system","name":null,"metadata":{}},"create_time":1762310187.598342,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"55ab21ed-22d5-4d7a-a105-23ef6ca212cf","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"55ab21ed-22d5-4d7a-a105-23ef6ca212cf","children":["f773f53c-0e95-4111-8614-fb0ec9481537"]},"f773f53c-0e95-4111-8614-fb0ec9481537":{"id":"f773f53c-0e95-4111-8614-fb0ec9481537","message":{"id":"f773f53c-0e95-4111-8614-fb0ec9481537","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310187.8369236,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"059bcd15-b7a3-42ea-b4a7-2c655228acb0","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"059bcd15-b7a3-42ea-b4a7-2c655228acb0","children":["b6ed2b5e-b3ce-478a-98f4-2ba66d88b7c3"]},"b6ed2b5e-b3ce-478a-98f4-2ba66d88b7c3":{"id":"b6ed2b5e-b3ce-478a-98f4-2ba66d88b7c3","message":{"id":"b6ed2b5e-b3ce-478a-98f4-2ba66d88b7c3","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310188.868618,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring non-transformer architectures","content":"E-Jay's question is about non-transformer architectures for LLMs and AI agents. We should focus on architectures like RNN variants (LSTM, GRU), RWKV, State Space Models (S4, DSS, GSS, SSM variants), Mamba, Hyena, and RetNet. While these are transformer-like in some ways, they may still be categorized as non-attention transformers. We need to use up-to-date sources to critique them objectively, as this field is evolving quickly.","chunks":["E-Jay's question is about non-transformer architectures for LLMs and AI agents.","We should focus on architectures like RNN variants (LSTM, GRU), RWKV, State Space Models (S4, DSS, GSS, SSM variants), Mamba, Hyena, and RetNet.","While these are transformer-like in some ways, they may still be categorized as non-attention transformers.","We need to use up-to-date sources to critique them objectively, as this field is evolving quickly."],"finished":true},{"summary":"Refining non-transformer architectures","content":"The question is about non-transformer architectures, so I should focus on SSM-based models (S4, S5, H3), Mamba, RWKV, and RetNet. While RetNet isn\u2019t using classic attention mechanisms, it introduces a retention mechanism, making it potentially new. Hyena works with long convolutions, while Mamba uses selective scans for LLM-level results.","chunks":["The question is about non-transformer architectures, so I should focus on SSM-based models (S4, S5, H3), Mamba, RWKV, and RetNet.","While RetNet isn\u2019t using classic attention mechanisms, it introduces a retention mechanism, making it potentially new.","Hyena works with long convolutions, while Mamba uses selective scans for LLM-level results."],"finished":true},{"summary":"Exploring various architectures","content":"I\u2019m reviewing architectures like Mamba, Mamba-2, and Jamba, which combine SSM with a transformer or hybrid approaches. There\u2019s also talk of \"RetNet\" with large retention capabilities and models like GNNs that aren't used for LLMs but work as planners. Other models like \"Mixture-of-Experts\" aren\u2019t alternative architectures \u2014 they\u2019re a training method.","chunks":["I\u2019m reviewing architectures like Mamba, Mamba-2, and Jamba, which combine SSM with a transformer or hybrid approaches.","There\u2019s also talk of \"RetNet\" with large retention capabilities and models like GNNs that aren't used for LLMs but work as planners. Other models like \"Mixture-of-Experts\" aren\u2019t alternative architectures \u2014 they\u2019re a training method."],"finished":true},{"summary":"Considering various agent architectures","content":"Exploring architectures like Mamba with MoE, and recurrent memory models like SSM, which provide indefinite-length memory and hardware-friendly compute. For AI agents, frameworks like MuZero, Gato, and decision transformers have been used, while non-transformer methods like Diffusion Policies for robotics and Dreamer (RSSM-based) take an alternative approach for modeling and planning.","chunks":["Exploring architectures like Mamba with MoE, and recurrent memory models like SSM, which provide indefinite-length memory and hardware-friendly compute.","For AI agents, frameworks like MuZero, Gato, and decision transformers have been used, while non-transformer methods like Diffusion Policies for robotics and Dreamer (RSSM-based) take an alternative approach for modeling and planning."],"finished":true},{"summary":"Reviewing alternative architectures","content":"I'm considering various non-transformer architectures like the SSM family (e.g., Mamba, Hyena, RetNet), along with recurrent, gated, and hybrid models. There\u2019s potential in the \"Mamba family\" and \"Jamba\" for long-context AI applications, as well as recurrent memory models. I'll need to verify benchmarks and adoption before making a final choice.","chunks":["I'm considering various non-transformer architectures like the SSM family (e.g., Mamba, Hyena, RetNet), along with recurrent, gated, and hybrid models.","There\u2019s potential in the \"Mamba family\" and \"Jamba\" for long-context AI applications, as well as recurrent memory models. I'll need to verify benchmarks and adoption before making a final choice."],"finished":true}],"source_analysis_msg_id":"50cc2c7f-cef6-4476-877e-84242ea18125"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"f773f53c-0e95-4111-8614-fb0ec9481537","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"f773f53c-0e95-4111-8614-fb0ec9481537","children":["e109dcf1-a93f-44a5-a6ed-31afe2a5f81a"]},"e109dcf1-a93f-44a5-a6ed-31afe2a5f81a":{"id":"e109dcf1-a93f-44a5-a6ed-31afe2a5f81a","message":{"id":"e109dcf1-a93f-44a5-a6ed-31afe2a5f81a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310241.968164,"update_time":1762310241.987638,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Mamba selective state space models paper arXiv 2023\"}, {\"q\": \"Retentive Networks RetNet Microsoft paper arXiv 2023\"}, {\"q\": \"Hyena Hierarchy long-range language models paper 2023\"}, {\"q\": \"RWKV paper arXiv reinventing RNNs for the Transformer era\"}, {\"q\": \"Jamba hybrid SSM transformer AI21 labs blog 2024\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Mamba selective state space models paper arXiv 2023"},{"type":"search","q":"Retentive Networks RetNet Microsoft paper arXiv 2023"},{"type":"search","q":"Hyena Hierarchy long-range language models paper 2023"},{"type":"search","q":"RWKV paper arXiv reinventing RNNs for the Transformer era"},{"type":"search","q":"Jamba hybrid SSM transformer AI21 labs blog 2024"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Browsing for non-transformer LLM and agent architectures","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"b6ed2b5e-b3ce-478a-98f4-2ba66d88b7c3","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"b6ed2b5e-b3ce-478a-98f4-2ba66d88b7c3","children":["35716b38-059c-4100-869c-95a4ff574cb3"]},"35716b38-059c-4100-869c-95a4ff574cb3":{"id":"35716b38-059c-4100-869c-95a4ff574cb3","message":{"id":"35716b38-059c-4100-869c-95a4ff574cb3","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310242.0311496,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Mamba selective state space models paper arXiv 2023","Retentive Networks RetNet Microsoft paper arXiv 2023","Hyena Hierarchy long-range language models paper 2023","RWKV paper arXiv reinventing RNNs for the Transformer era","Jamba hybrid SSM transformer AI21 labs blog 2024"]},"reasoning_status":"is_reasoning","reasoning_title":"Browsing for non-transformer LLM and agent architectures","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"e109dcf1-a93f-44a5-a6ed-31afe2a5f81a","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"e109dcf1-a93f-44a5-a6ed-31afe2a5f81a","children":["710826e8-60b8-4ed6-9e58-38408fccd821"]},"710826e8-60b8-4ed6-9e58-38408fccd821":{"id":"710826e8-60b8-4ed6-9e58-38408fccd821","message":{"id":"710826e8-60b8-4ed6-9e58-38408fccd821","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310242.028941,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2312.00752","title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2307.08621","title":"Retentive Network: A Successor to Transformer for Large Language Models","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2302.10866","title":"Hyena Hierarchy: Towards Larger Convolutional ...","snippet":"by M Poli \u00b7 2023 \u00b7 Cited by 473 \u2014 We propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2305.13048","title":"RWKV: Reinventing RNNs for the Transformer Era","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2312.00752","title":"Mamba: Linear-Time Sequence Modeling with Selective ...","snippet":"31 May 2024 \u2014 Mamba is a neural network architecture using selective state space models, integrated without attention or MLP blocks, to model sequences.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":1717375939.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2307.08621","title":"Retentive Network","snippet":"by Y Sun \u00b7 2023 \u00b7 Cited by 568 \u2014 In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long- sequence modeling, Transformer-comparable ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":1691626546.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.00752v2","title":"Linear-Time Sequence Modeling with Selective State Spaces","snippet":"We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2302.10866","title":"Hyena Hierarchy","snippet":"by M Poli \u00b7 2023 \u00b7 Cited by 469 \u2014 In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.ai21.com","entries":[{"type":"search_result","url":"https://www.ai21.com/blog/rise-of-hybrid-llms/","title":"Attention was never enough: Tracing the rise of hybrid LLMs","snippet":"5 Aug 2025 \u2014 Explore how hybrid LLMs combining Mamba and Transformer architectures are reshaping enterprise AI\u2014from Mamba's debut to state-of-the-art ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"pub_date":1754396683.0,"attribution":"www.ai21.com"},{"type":"search_result","url":"https://www.ai21.com/blog/announcing-jamba/","title":"Introducing Jamba: AI21's Groundbreaking SSM- ...","snippet":"Composed of Transformer, Mamba, and mixture-of-experts (MoE) layers, Jamba optimizes for memory, throughput, and performance\u2014all at once ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":1711618754.0,"attribution":"www.ai21.com"}]},{"type":"search_result_group","domain":"proceedings.mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v202/poli23a/poli23a.pdf","title":"Hyena Hierarchy: Towards Larger Convolutional Language ...","snippet":"by M Poli \u00b7 2023 \u00b7 Cited by 473 \u2014 In the case of Hyena, causality can be guaranteed by parametrizing causal convolutions: Proposition 3.1 (Causal Hyenas). If each filter hn, n = 1,...,N is ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":1686927721.0,"attribution":"proceedings.mlr.press"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2023.findings-emnlp.936/","title":"RWKV: Reinventing RNNs for the Transformer Era","snippet":"by B Peng \u00b7 2023 \u00b7 Cited by 807 \u2014 We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/state-spaces/mamba","title":"state-spaces/mamba: Mamba SSM architecture","snippet":"Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2307.08621","title":"Retentive Network: A Successor to Transformer for Large ...","snippet":"17 Jul 2023 \u2014 We propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"hazyresearch.stanford.edu","entries":[{"type":"search_result","url":"https://hazyresearch.stanford.edu/blog/2023-03-07-hyena","title":"Hyena Hierarchy: Towards Larger Convolutional ...","snippet":"7 Mar 2023 \u2014 The crossover point between highly optimized FlashAttention and Hyena is at about 6k sequence length. At 100k sequence length, Hyenas are 100x ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"hazyresearch.stanford.edu"}]},{"type":"search_result_group","domain":"community.openai.com","entries":[{"type":"search_result","url":"https://community.openai.com/t/paper-rwkv-reinventing-rnns-for-the-transformer-era/567110","title":"[Paper] RWKV: Reinventing RNNs for the Transformer Era","snippet":"21 Dec 2023 \u2014 We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"community.openai.com"}]},{"type":"search_result_group","domain":"aws.amazon.com","entries":[{"type":"search_result","url":"https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/","title":"Jamba 1.5 family of models by AI21 Labs is now available ...","snippet":"23 Sept 2024 \u2014 Jamba 1.5 models leverage a unique hybrid architecture that combines the transformer model architecture with Structured State Space model (SSM) ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":1727114358.0,"attribution":"aws.amazon.com"}]},{"type":"search_result_group","domain":"frenos.io","entries":[{"type":"search_result","url":"https://frenos.io/hubfs/theme-frenos/whitepaper/Frenos%20Scaling%20Retentive%20Networks.pdf?hsLang=en","title":"Whitepaper - Breakdown of Retentive Networks (RetNet)","snippet":"This paper reviews traditional neural network architectures and introduces one of the first domain-specific analyses of the. Retentive Network (RetNet) ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"frenos.io"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=7SaXczaBpG","title":"RWKV: Reinventing RNNs for the Transformer Era","snippet":"by B Peng \u00b7 Cited by 807 \u2014 Experiments suggest RWKV ad- dresses scaling and deployment challenges in AI, especially for sequential data processing, pointing towards more sustainable and ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":1698029015.0,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40yash9439/ai21-labs-introduces-jamba-the-first-production-grade-hybrid-ssm-transformer-model-7f6f500441a4","title":"The First Production-Grade Hybrid SSM-Transformer Model","snippet":"AI21 Labs has announced the release of Jamba, the world's first production-grade AI model based on the innovative Mamba architecture.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":1711713734.111,"attribution":"medium.com"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Browsing for non-transformer LLM and agent architectures","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"35716b38-059c-4100-869c-95a4ff574cb3","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"35716b38-059c-4100-869c-95a4ff574cb3","children":["b993f234-abb1-4841-baa8-b5124af2bf9b"]},"b993f234-abb1-4841-baa8-b5124af2bf9b":{"id":"b993f234-abb1-4841-baa8-b5124af2bf9b","message":{"id":"b993f234-abb1-4841-baa8-b5124af2bf9b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310249.896306,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Verifying sources and citations","content":"I have core citations for Mamba, RetNet, Hyena, RWKV, and Jamba from reliable sources like AI21 and AWS blogs. I\u2019m considering adding \"Ret2\" for RetNet as well. Also, I\u2019ll look up more on \"Liquid Neural Networks\"\u2014time to search for updated information.","chunks":["I have core citations for Mamba, RetNet, Hyena, RWKV, and Jamba from reliable sources like AI21 and AWS blogs.","I\u2019m considering adding \"Ret2\" for RetNet as well. Also, I\u2019ll look up more on \"Liquid Neural Networks\"\u2014time to search for updated information."],"finished":true}],"source_analysis_msg_id":"a9aadff3-b5d3-47d1-98cb-49823057b134"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"710826e8-60b8-4ed6-9e58-38408fccd821","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"710826e8-60b8-4ed6-9e58-38408fccd821","children":["e0694a94-a902-4238-af6c-cbfef83b9740"]},"e0694a94-a902-4238-af6c-cbfef83b9740":{"id":"e0694a94-a902-4238-af6c-cbfef83b9740","message":{"id":"e0694a94-a902-4238-af6c-cbfef83b9740","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310249.55871,"update_time":1762310249.63537,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Liquid neural networks MIT Hasani 2020 continuous-time models robotics paper\"}, {\"q\": \"Neural circuit policies explainable control MIT CSAIL liquid time constant networks 2020 Nature Machine Intelligence\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Liquid neural networks MIT Hasani 2020 continuous-time models robotics paper"},{"type":"search","q":"Neural circuit policies explainable control MIT CSAIL liquid time constant networks 2020 Nature Machine Intelligence"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for Liquid Neural Networks and related research","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"b993f234-abb1-4841-baa8-b5124af2bf9b","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"b993f234-abb1-4841-baa8-b5124af2bf9b","children":["989c1285-dd0b-402e-a4df-b8ed92d43f0d"]},"989c1285-dd0b-402e-a4df-b8ed92d43f0d":{"id":"989c1285-dd0b-402e-a4df-b8ed92d43f0d","message":{"id":"989c1285-dd0b-402e-a4df-b8ed92d43f0d","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310249.6904078,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Liquid neural networks MIT Hasani 2020 continuous-time models robotics paper","Neural circuit policies explainable control MIT CSAIL liquid time constant networks 2020 Nature Machine Intelligence"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for Liquid Neural Networks and related research","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"e0694a94-a902-4238-af6c-cbfef83b9740","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"e0694a94-a902-4238-af6c-cbfef83b9740","children":["9bf72349-1f26-42ba-b32e-e64800d74088"]},"9bf72349-1f26-42ba-b32e-e64800d74088":{"id":"9bf72349-1f26-42ba-b32e-e64800d74088","message":{"id":"9bf72349-1f26-42ba-b32e-e64800d74088","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310249.6884947,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s42256-022-00556-7","title":"Closed-form continuous-time neural networks","snippet":"by R Hasani \u00b7 2022 \u00b7 Cited by 170 \u2014 Continuous-time neural networks are a class of machine learning systems that can tackle representation learning on spatiotemporal decision-making tasks.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"ojs.aaai.org","entries":[{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/16936/16743","title":"Liquid Time-constant Networks","snippet":"by R Hasani \u00b7 2021 \u00b7 Cited by 420 \u2014 We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dy- namics by implicit nonlinearities, we ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"ojs.aaai.org"},{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/16936","title":"Liquid Time-constant Networks","snippet":"by R Hasani \u00b7 2021 \u00b7 Cited by 430 \u2014 We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dynamics by implicit nonlinearities.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"ojs.aaai.org"}]},{"type":"search_result_group","domain":"simons.berkeley.edu","entries":[{"type":"search_result","url":"https://simons.berkeley.edu/sites/default/files/docs/17404/raminhasanitfcssynthesisslides.pdf","title":"Liquid Time-Constant Network","snippet":"22 Mar 2021 \u2014 \u2713 A novel time-continuous neural networks for efficient time-series modelling. \u2022 LTCs are universal approximators. \u2022 LTCs are stable dynamical ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"simons.berkeley.edu"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40hession520/liquid-neural-nets-lnns-32ce1bfb045a","title":"Liquid Neural Nets (LNNs)","snippet":"Liquid neural nets (LNNs) are an exciting, relatively new direction in AI/ML research that promises more compact and dynamic neural nets for time series ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":3},"pub_date":1714582646.516,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2006.04439","title":"Liquid Time-constant Networks","snippet":"","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2304.08691","title":"LTC-SE: Expanding the Potential of Liquid Time-Constant Neural Networks for Scalable AI and Embedded Systems","snippet":"","ref_id":{"turn_index":1,"ref_type":"academia","ref_index":20},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2006.04418","title":"Learning Long-Term Dependencies in Irregularly-Sampled Time Series","snippet":"","ref_id":{"turn_index":1,"ref_type":"academia","ref_index":22},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.liquid.ai","entries":[{"type":"search_result","url":"https://www.liquid.ai/research/liquid-neural-networks-research","title":"From Liquid Neural Networks to Liquid Foundation Models","snippet":"30 Sept 2024 \u2014 We invented liquid neural networks, a class of brain-inspired systems that can stay adaptable and robust to changes even after training.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"www.liquid.ai"}]},{"type":"search_result_group","domain":"folk.idi.ntnu.no","entries":[{"type":"search_result","url":"https://folk.idi.ntnu.no/keithd/master-projects/2024/liquid-nns-2020.pdf","title":"arXiv:2006.04439v4 [cs.LG] 14 Dec 2020","snippet":"by R Hasani \u00b7 2021 \u00b7 Cited by 428 \u2014 We re- fer to these models as liquid time-constant recurrent neural networks (LTCs). LTCs can be implemented by an arbitrary choice of ODE solvers. In Section 2 ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"folk.idi.ntnu.no"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/342027480_Liquid_Time-constant_Networks","title":"(PDF) Liquid Time-constant Networks","snippet":"8 Jun 2020 \u2014 We introduce a new class of time-continuous recurrent neural network models. Instead of declaring the nonlinearity of a learning system by ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":1591574400.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/363386857_Liquid_Time-constant_Networks","title":"Liquid Time-constant Networks | Request PDF","snippet":"6 Aug 2025 \u2014 Hasani et al. [8] introduced liquid time-constant networks, which use differential equations to model neuron behaviours, enabling improved ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/344683434_Neural_circuit_policies_enabling_auditable_autonomy","title":"Neural circuit policies enabling auditable autonomy","snippet":"Here, we combine brain-inspired neural computation principles and scalable deep learning architectures to design compact neural controllers for task-specific ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":1601510400.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"repositum.tuwien.at","entries":[{"type":"search_result","url":"https://repositum.tuwien.at/bitstream/20.500.12708/1068/2/Hasani%20Ramin%20-%202020%20-%20Interpretable%20recurrent%20neural%20networks%20in%20continuous-time...pdf","title":"Interpretable Recurrent Neural Networks in Continuous-time ...","snippet":"by R Hasani \u00b7 2020 \u00b7 Cited by 14 \u2014 The model realizes liquid time constant dynamics, meaning that each neuron ... [Hasani et al., 2020] Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"repositum.tuwien.at"}]},{"type":"search_result_group","domain":"cbmm.mit.edu","entries":[{"type":"search_result","url":"https://cbmm.mit.edu/video/liquid-neural-networks","title":"Liquid Neural Networks | The Center for Brains, Minds ...","snippet":"So a continuous time neural network is basically when a neural network f that has certain number of layers, has certain width, it has activation ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"cbmm.mit.edu"}]},{"type":"search_result_group","domain":"scholar.google.com","entries":[{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=YarJF3QAAAAJ","title":"Ramin Hasani","snippet":"Liquid time-constant networks. R Hasani, M Lechner, A Amini, D Rus, R Grosu. Proceedings of the AAAI Conference on Artificial Intelligence 35 (9), 7657-7666, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"scholar.google.com"}]},{"type":"search_result_group","domain":"cap.csail.mit.edu","entries":[{"type":"search_result","url":"https://cap.csail.mit.edu/sites/default/files/research-pdfs/Robust%20flight%20navigation%20out%20of%20distribution%20with%20liquid%20neural%20networks.pdf","title":"Robust flight navigation out of distribution with liquid neural ...","snippet":"by M Chahine \u00b7 2023 \u00b7 Cited by 87 \u2014 Here, we present a method to create robust flight navigation agents that successfully perform vision-based fly-to-target tasks beyond their ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":1681934188.0,"attribution":"cap.csail.mit.edu"}]},{"type":"search_result_group","domain":"news.mit.edu","entries":[{"type":"search_result","url":"https://news.mit.edu/2022/solving-brain-dynamics-gives-rise-flexible-machine-learning-models-1115","title":"Solving brain dynamics gives rise to flexible machine ...","snippet":"15 Nov 2022 \u2014 The flexibility of these \u201cliquid\u201d neural nets meant boosting the bloodline to our connected world, yielding better decision-making for many ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"news.mit.edu"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching for Liquid Neural Networks and related research","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"989c1285-dd0b-402e-a4df-b8ed92d43f0d","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"989c1285-dd0b-402e-a4df-b8ed92d43f0d","children":["d979e64f-1408-43a6-8845-2cc1bdd5773e"]},"d979e64f-1408-43a6-8845-2cc1bdd5773e":{"id":"d979e64f-1408-43a6-8845-2cc1bdd5773e","message":{"id":"d979e64f-1408-43a6-8845-2cc1bdd5773e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310258.064155,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Verifying sources and expanding","content":"I\u2019ve found reliable sources for Liquid Neural Networks. Next, I\u2019ll check into SSMs like S4, S5, and H3. I\u2019ll make sure to include citations for S4 from \u201cEfficiently Modeling Long Sequences with Structured State Spaces\u201d (2021). Time to search for more!","chunks":["I\u2019ve found reliable sources for Liquid Neural Networks.","Next, I\u2019ll check into SSMs like S4, S5, and H3.","I\u2019ll make sure to include citations for S4 from \u201cEfficiently Modeling Long Sequences with Structured State Spaces\u201d (2021).","Time to search for more!"],"finished":true}],"source_analysis_msg_id":"f5e36340-2b35-4fb2-9ca0-22d2aeeb3751"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"9bf72349-1f26-42ba-b32e-e64800d74088","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"9bf72349-1f26-42ba-b32e-e64800d74088","children":["6298b056-37dc-49f4-b610-51867fb988e0"]},"6298b056-37dc-49f4-b610-51867fb988e0":{"id":"6298b056-37dc-49f4-b610-51867fb988e0","message":{"id":"6298b056-37dc-49f4-b610-51867fb988e0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310256.946059,"update_time":1762310256.961984,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"S4 Structured State Space Models paper 2021 arXiv Efficiently Modeling Long Sequences with Structured State Spaces\"}, {\"q\": \"S5 state space model paper\"}, {\"q\": \"H3 Hungry Hungry Hippos state space model\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"S4 Structured State Space Models paper 2021 arXiv Efficiently Modeling Long Sequences with Structured State Spaces"},{"type":"search","q":"S5 state space model paper"},{"type":"search","q":"H3 Hungry Hungry Hippos state space model"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for S4, S5, and H3 model papers","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"d979e64f-1408-43a6-8845-2cc1bdd5773e","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"d979e64f-1408-43a6-8845-2cc1bdd5773e","children":["55ab1928-6329-484f-9069-2dd6b7b67228"]},"55ab1928-6329-484f-9069-2dd6b7b67228":{"id":"55ab1928-6329-484f-9069-2dd6b7b67228","message":{"id":"55ab1928-6329-484f-9069-2dd6b7b67228","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310257.0102901,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["S4 Structured State Space Models paper 2021 arXiv Efficiently Modeling Long Sequences with Structured State Spaces","S5 state space model paper","H3 Hungry Hungry Hippos state space model"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for S4, S5, and H3 model papers","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"6298b056-37dc-49f4-b610-51867fb988e0","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"6298b056-37dc-49f4-b610-51867fb988e0","children":["62a425be-4424-444e-80e0-0f02aa7a7994"]},"62a425be-4424-444e-80e0-0f02aa7a7994":{"id":"62a425be-4424-444e-80e0-0f02aa7a7994","message":{"id":"62a425be-4424-444e-80e0-0f02aa7a7994","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310257.008174,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2111.00396","title":"Efficiently Modeling Long Sequences with Structured State Spaces","snippet":"","ref_id":{"turn_index":2,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2208.04933","title":"Simplified State Space Layers for Sequence Modeling","snippet":"by JTH Smith \u00b7 2022 \u00b7 Cited by 940 \u2014 We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2212.14052","title":"Towards Language Modeling with State Space Models","snippet":"by DY Fu \u00b7 2022 \u00b7 Cited by 708 \u2014 To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2503.18970","title":"A Survey on Structured State Space Sequence (S4) Models","snippet":"by S Somvanshi \u00b7 2025 \u00b7 Cited by 5 \u2014 This survey provides a comprehensive review of SSMs, tracing their evolution from the foundational. S4 model to its successors like Mamba, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2406.09477","title":"[2406.09477] Q-S5: Towards Quantized State Space Models","snippet":"by S Abreu \u00b7 2024 \u00b7 Cited by 11 \u2014 This paper investigates the effect of quantization on the S5 model to understand its impact on model performance and to facilitate its deployment to edge and ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2212.14052","title":"Towards Language Modeling with State Space Models","snippet":"by DY Fu \u00b7 2022 \u00b7 Cited by 708 \u2014 To probe how important these skills are for language modeling, we propose H3 (Hungry Hungry Hippo), a new SSM-based layer designed to solve ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":20},"pub_date":1682987114.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=uYLFoz1vlAC","title":"EFFICIENTLY MODELING LONG SEQUENCES WITH ...","snippet":"by A Gu \u00b7 Cited by 3416 \u2014 In this work, we introduce the Structured State Space (S4) sequence model based on the SSM that ... We call K the SSM convolution kernel or filter. 3 METHOD: ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":3},"pub_date":1646420925.0,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=Ai8Hw3AXqks","title":"SIMPLIFIED STATE SPACE LAYERS FOR SEQUENCE ...","snippet":"by JTH Smith \u00b7 Cited by 940 \u2014 We show in Table 2 that S5 outperforms the baselines, outperforms previous S4 methods and performs similarly to the concurrent Liquid-S4 method (Hasani et al., ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":16},"pub_date":1677619746.0,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=COZDy0WYGg","title":"Towards Language Modeling with State Space...","snippet":"by DY Fu \u00b7 Cited by 708 \u2014 To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/lindermanlab/S5","title":"lindermanlab/S5","snippet":"S5 uses a single multi-input, multi-output linear state-space model, coupled with non-linearities, to define a non-linear sequence-to-sequence transformation.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/HazyResearch/H3","title":"Language Modeling with the H3 State Space Model","snippet":"This repository provides the official implementation of H3 from the following paper. Hungry Hungry Hippos: Towards Language Modeling with State Space Models","ref_id":{"turn_index":2,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/raminmh/l-state-spaces","title":"Sequence Modeling with Structured State Spaces","snippet":"Efficiently Modeling Long Sequences with Structured State Spaces Albert Gu, Karan Goel, Christopher R\u00e9 Paper: https://arxiv.org/abs/2111.00396. LSSL (NeurIPS ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/i404788/s5-pytorch","title":"S5: Simplified State Space Layers for Sequence Modeling","snippet":"You can use variable step-size as described in the paper using a 1D tensor for step_scale however this takes a lot of memory due to a lot of intermediate values ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40kdk199604/s5-simplified-state-space-layers-for-efficient-sequence-modeling-61ef5f3386ac","title":"S5: Simplified State Space Layers for Efficient Sequence ...","snippet":"The paper presents S5, a simplified yet expressive state space layer for sequence modeling. By reformulating S4's independent subsystems ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":9},"pub_date":1759973525.032,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/autonomous-agents/comprehensive-breakdown-of-selective-structured-state-space-model-mamba-s5-441e8b94ecaf","title":"Comprehensive Breakdown of Selective Structured State ...","snippet":"Comprehensive Breakdown of Selective Structured State Space Model \u2014 Mamba (S5) ... The paper has a detailed explanation on this in section 3.3.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":13},"pub_date":1714784886.541,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/s5hajb/r_the_annotated_s4_efficiently_modeling_long/","title":"[R] The Annotated S4: Efficiently Modeling Long ...","snippet":"The Structured State Space for Sequence Modeling (S4) architecture is a new approach to very long-range sequence modeling tasks for vision, language, and audio.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":10},"pub_date":1642355680.346,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/355841686_Efficiently_Modeling_Long_Sequences_with_Structured_State_Spaces","title":"Efficiently Modeling Long Sequences with Structured State ...","snippet":"(2021) S4 demonstrated the ability of SSMs to efficiently model long-range dependencies while scaling effectively with sequence length. It introduced ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Efficiently-Modeling-Long-Sequences-with-Structured-Gu-Goel/ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51","title":"Efficiently Modeling Long Sequences with Structured State ...","snippet":"The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"ar5iv.labs.arxiv.org","entries":[{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2111.00396","title":"Efficiently Modeling Long Sequences with Structured State ...","snippet":"S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91% accuracy on sequential CIFAR-10 with no data ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"ar5iv.labs.arxiv.org"}]},{"type":"search_result_group","domain":"dataloop.ai","entries":[{"type":"search_result","url":"https://dataloop.ai/library/model/dbal0503_s5/","title":"S5 \u00b7 Models","snippet":"The S5 model is a type of artificial intelligence designed to process sequences of data, like text or images. It's built using a simplified state space layer.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"dataloop.ai"}]},{"type":"search_result_group","domain":"fanpu.io","entries":[{"type":"search_result","url":"https://fanpu.io/summaries/2023-08-25-efficiently-modeling-long-sequences-with-structured-state-spaces/","title":"Efficiently Modeling Long Sequences with Structured State ...","snippet":"25 Aug 2023 \u2014 Paper Abstract. A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"fanpu.io"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/158ac5698e36a01ee5ca9e6732685b34-Paper-Conference.pdf","title":"Slot State Space Models","snippet":"by J Jiang \u00b7 2024 \u00b7 Cited by 11 \u2014 Recent State Space Models (SSMs) such as S4, S5, and Mamba have shown re- markable computational benefits in long-range temporal dependency modeling.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":22},"pub_date":1734879150.0,"attribution":"proceedings.neurips.cc"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching for S4, S5, and H3 model papers","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"55ab1928-6329-484f-9069-2dd6b7b67228","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"55ab1928-6329-484f-9069-2dd6b7b67228","children":["db45b944-5dae-4cf9-b5b5-f18ea1d34106"]},"db45b944-5dae-4cf9-b5b5-f18ea1d34106":{"id":"db45b944-5dae-4cf9-b5b5-f18ea1d34106","message":{"id":"db45b944-5dae-4cf9-b5b5-f18ea1d34106","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310265.112837,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Confirming and sourcing details","content":"The sources confirm S4, S5, and H3, which is good. I\u2019m also considering including \u201cMega\u201d (Moving Average Equipped Gated Attention) from FAIR. However, I\u2019m not sure if it's considered transformer-like since it's based on gating with moving averages, which might make it closer to RNNs. I\u2019ll look into it.","chunks":["The sources confirm S4, S5, and H3, which is good.","I\u2019m also considering including \u201cMega\u201d (Moving Average Equipped Gated Attention) from FAIR.","However, I\u2019m not sure if it's considered transformer-like since it's based on gating with moving averages, which might make it closer to RNNs.","I\u2019ll look into it."],"finished":true}],"source_analysis_msg_id":"d6766cf1-72fa-43f5-9705-b82f87bc3d0e"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"62a425be-4424-444e-80e0-0f02aa7a7994","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"62a425be-4424-444e-80e0-0f02aa7a7994","children":["5d2cb1d7-0f05-4f8f-a883-1399bbecd507"]},"5d2cb1d7-0f05-4f8f-a883-1399bbecd507":{"id":"5d2cb1d7-0f05-4f8f-a883-1399bbecd507","message":{"id":"5d2cb1d7-0f05-4f8f-a883-1399bbecd507","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310260.921255,"update_time":1762310260.955556,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Mega moving average equipped gated attention Facebook FAIR paper 2022 language modeling\"}, {\"q\": \"RetNet vs Mega comparison long-range arena\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Mega moving average equipped gated attention Facebook FAIR paper 2022 language modeling"},{"type":"search","q":"RetNet vs Mega comparison long-range arena"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for Mega and related attention models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"db45b944-5dae-4cf9-b5b5-f18ea1d34106","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"db45b944-5dae-4cf9-b5b5-f18ea1d34106","children":["e8ffc1dd-596a-46eb-a041-862b7c6da25e"]},"e8ffc1dd-596a-46eb-a041-862b7c6da25e":{"id":"e8ffc1dd-596a-46eb-a041-862b7c6da25e","message":{"id":"e8ffc1dd-596a-46eb-a041-862b7c6da25e","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310261.0445986,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Mega moving average equipped gated attention Facebook FAIR paper 2022 language modeling","RetNet vs Mega comparison long-range arena"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for Mega and related attention models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"5d2cb1d7-0f05-4f8f-a883-1399bbecd507","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"5d2cb1d7-0f05-4f8f-a883-1399bbecd507","children":["37a10f2b-5127-4ab0-96cc-268ccf67dbfd"]},"37a10f2b-5127-4ab0-96cc-268ccf67dbfd":{"id":"37a10f2b-5127-4ab0-96cc-268ccf67dbfd","message":{"id":"37a10f2b-5127-4ab0-96cc-268ccf67dbfd","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310261.0404694,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2209.10655","title":"Mega: Moving Average Equipped Gated Attention","snippet":"","ref_id":{"turn_index":3,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2406.09062v2","title":"State-Space Modeling in Long Sequence Processing","snippet":"Long Range Arena. The LRA benchmark [35] comprises six tasks with sequence lengths ranging from 1 , 024 1{,}024 to 16 , 384 16{,}384 tokens, as described ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2403.15360v2","title":"SiMBA: Simplified Mamba-based Architecture for Vision ...","snippet":"30 Apr 2024 \u2014 This comprehensive comparative analysis highlights SiMBA's distinctive position in the evolving landscape of transformer architectures. Report ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/164n8iz/discussion_promising_alternatives_to_the_standard/","title":"[Discussion] Promising alternatives to the standard ...","snippet":"What are some promising transformer alternatives/variants that you think more folks should be aware of? They need not be new or SOTA!","ref_id":{"turn_index":3,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"www.reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/xmok0y/r_mega_moving_average_equipped_gated_attention_by/","title":"[R] Mega: Moving Average Equipped Gated Attention. By ...","snippet":"Really nice paper! Is there any public code to re-run experiments/have a play. In particular the language modelling or text classification work?","ref_id":{"turn_index":3,"ref_type":"search","ref_index":14},"pub_date":1664012929.311,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/facebookresearch/mega","title":"facebookresearch/mega: Sequence modeling with Mega.","snippet":"1 Nov 2024 \u2014 Mega: Moving Average Equipped Gated Attention. This is the PyTorch implementation of the Mega paper. This folder is based on the fairseq package v0.9.0.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=qNLe3iq2El","title":"MEGA: MOVING AVERAGE EQUIPPED GATED ATTENTION","snippet":"by X Ma \u00b7 Cited by 119 \u2014 In this paper, we introduce MEGA, a simple, theoreti- cally grounded, single-head gated attention mechanism equipped with (exponential) moving average to ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":4},"pub_date":1677639778.0,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/ea538b6258b9524c9de5b7b526fa5112819a0ab3.pdf","title":"Efficient Long Sequence Modeling and Beyond","snippet":"range arena: A benchmark for efficient transformers. 873. arXiv preprint arXiv:2011.04006. 874. Yi Tay, Mostafa Dehghani, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":9},"pub_date":1707924896.0,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"liner.com","entries":[{"type":"search_result","url":"https://liner.com/review/long-range-arena-a-benchmark-for-efficient-transformers","title":"Long Range Arena: A Benchmark for Efficient Transformers","snippet":"In this ICLR 2020 review page, we summarize the core of the paper that introduces Long Range Arena, a benchmark for efficient Transformers.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":5},"pub_date":1604793600.0,"attribution":"liner.com"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2209.10655","title":"Mega: Moving Average Equipped Gated Attention","snippet":"21 Sept 2022 \u2014 In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/machine-learns/exploring-beyond-regular-transformers-346e82fc901d","title":"Exploring Beyond Regular Transformers | by Eren G\u00f6lge","snippet":"When compared to the other Transformer alternatives, RetNet outperforms all the different models by a big margin on language modeling. Press ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":7},"pub_date":1696677104.91,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Mega%3A-Moving-Average-Equipped-Gated-Attention-Ma-Zhou/70e91e16eb321067d9402710e14a40cf28311f73","title":"[PDF] Mega: Moving Average Equipped Gated Attention","snippet":"This paper introduces Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"ar5iv.labs.arxiv.org","entries":[{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2209.10655","title":"[2209.10655] Mega: Moving Average Equipped Gated Attention","snippet":"In this work, we propose a moving average equipped gated attention mechanism (Mega) to solve the two weaknesses simultaneously. The key idea is to incorporate ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"ar5iv.labs.arxiv.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/395686915_Never_Train_from_Scratch_Fair_Comparison_of_Long-Sequence_Models_Requires_Data-Driven_Priors_Extended_Abstract","title":"Fair Comparison of Long-Sequence Models Requires Data ...","snippet":"23 Sept 2025 \u2014 We demonstrate that properly pretrained vanilla Transformers match S4 performance on Long Range Arena and improve SSM results on PathX-256 by 20 ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/363765463_Mega_Moving_Average_Equipped_Gated_Attention","title":"Mega: Moving Average Equipped Gated Attention","snippet":"In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"rizqimulki.com","entries":[{"type":"search_result","url":"https://rizqimulki.com/llm-architectures-beyond-transformers-mamba-retnet-and-alternatives-2a5963cb17d7","title":"LLM Architectures Beyond Transformers: Mamba, RetNet ...","snippet":"29 May 2025 \u2014 The self-attention mechanism allowed models to capture long-range dependencies without the sequential processing constraints of RNNs. Parallel ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":13},"pub_date":1748512062.327,"attribution":"rizqimulki.com"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/8329a45669017898bb0cc09d27f8d2bb-Paper-Conference.pdf","title":"MetaLA: Unified Optimal Linear Approximation to Softmax ...","snippet":"by Y CHOU \u00b7 2024 \u00b7 Cited by 18 \u2014 MetaLA is a linear approximation to softmax attention, designed to satisfy dynamic memory, static approximation, and least parameter approximation conditions.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":15},"pub_date":1730082345.0,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"dspace.mit.edu","entries":[{"type":"search_result","url":"https://dspace.mit.edu/bitstream/handle/1721.1/152839/sun-mmsun-meng-eecs-2023-thesis.pdf?isAllowed=y&sequence=1","title":"Melinda Sun","snippet":"by M Sun \u00b7 2023 \u2014 Several solutions have been proposed recently, including Moving. Average Equipped Gated Attention (Mega) [3] and Random Feature Attention [5]. In this work, we ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"dspace.mit.edu"}]},{"type":"search_result_group","domain":"wikidocs.net","entries":[{"type":"search_result","url":"https://wikidocs.net/236369","title":"MEGA: Moving Average Equipped Gated Attention","snippet":"In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"wikidocs.net"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching for Mega and related attention models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"e8ffc1dd-596a-46eb-a041-862b7c6da25e","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"e8ffc1dd-596a-46eb-a041-862b7c6da25e","children":["af52c41b-5a4d-49c6-9a75-a52bd103ca59"]},"af52c41b-5a4d-49c6-9a75-a52bd103ca59":{"id":"af52c41b-5a4d-49c6-9a75-a52bd103ca59","message":{"id":"af52c41b-5a4d-49c6-9a75-a52bd103ca59","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310270.711302,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Refining sources and citations","content":"I\u2019ve thought about including \u201cSiren\u2019s edible red nets,\u201d but I think it\u2019s not really relevant. Moving on, I\u2019m considering RetNet usage in production\u2014possibly in Phi-3 and LongMem. I\u2019ll skip any unconfirmed claims but can mention theoretical details from the paper. I\u2019ll also add materials like \u201cJamba 1.5\u201d and \u201cDeltaNet,\u201d checking the sources.","chunks":["I\u2019ve thought about including \u201cSiren\u2019s edible red nets,\u201d but I think it\u2019s not really relevant.","Moving on, I\u2019m considering RetNet usage in production\u2014possibly in Phi-3 and LongMem.","I\u2019ll skip any unconfirmed claims but can mention theoretical details from the paper.","I\u2019ll also add materials like \u201cJamba 1.5\u201d and \u201cDeltaNet,\u201d checking the sources."],"finished":true}],"source_analysis_msg_id":"9a6edb78-ef8a-4e3d-8dfe-23e9305b9484"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"37a10f2b-5127-4ab0-96cc-268ccf67dbfd","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"37a10f2b-5127-4ab0-96cc-268ccf67dbfd","children":["34fce5b7-effd-4d56-8e5a-98163e4442e2"]},"34fce5b7-effd-4d56-8e5a-98163e4442e2":{"id":"34fce5b7-effd-4d56-8e5a-98163e4442e2","message":{"id":"34fce5b7-effd-4d56-8e5a-98163e4442e2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310268.005219,"update_time":1762310268.028691,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"DeltaNet simple efficient attention using delta rule arXiv 2024\"}, {\"q\": \"Retention memory networks DeltaNet retentive alternative architecture 2024\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"DeltaNet simple efficient attention using delta rule arXiv 2024"},{"type":"search","q":"Retention memory networks DeltaNet retentive alternative architecture 2024"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for DeltaNet and related memory architectures","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"af52c41b-5a4d-49c6-9a75-a52bd103ca59","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"af52c41b-5a4d-49c6-9a75-a52bd103ca59","children":["025f7f52-7369-4774-999e-7d4b2bd23219"]},"025f7f52-7369-4774-999e-7d4b2bd23219":{"id":"025f7f52-7369-4774-999e-7d4b2bd23219","message":{"id":"025f7f52-7369-4774-999e-7d4b2bd23219","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310268.1421306,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["DeltaNet simple efficient attention using delta rule arXiv 2024","Retention memory networks DeltaNet retentive alternative architecture 2024"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for DeltaNet and related memory architectures","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"34fce5b7-effd-4d56-8e5a-98163e4442e2","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"34fce5b7-effd-4d56-8e5a-98163e4442e2","children":["dec1c99e-84c6-40d1-a77f-be0c4caaa6b2"]},"dec1c99e-84c6-40d1-a77f-be0c4caaa6b2":{"id":"dec1c99e-84c6-40d1-a77f-be0c4caaa6b2","message":{"id":"dec1c99e-84c6-40d1-a77f-be0c4caaa6b2","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310268.1393383,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/2406.06484","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"by S Yang \u00b7 2024 \u00b7 Cited by 136 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":0},"pub_date":1736991060.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2504.13173","title":"Attentional Bias, Retention, and Online Optimization","snippet":"by A Behrouz \u00b7 2025 \u00b7 Cited by 12 \u2014 In this framework, the memory architecture determines the model capacity to memorize; attentional bias is responsible for modeling the ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2406.06484","title":"Parallelizing Linear Transformers with the Delta Rule over Sequence Length","snippet":"","ref_id":{"turn_index":4,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2510.20787v2","title":"Alleviating Forgetfulness of Linear Attention by Hybrid ...","snippet":"24 Oct 2025 \u2014 Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.06457v1","title":"A Systematic Analysis of Hybrid Linear Attention","snippet":"8 Jul 2025 \u2014 We systematically evaluate various linear attention models across generations\u2014vector recurrences to advanced gating mechanisms\u2014both standalone ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/387671240_Titans_Learning_to_Memorize_at_Test_Time","title":"(PDF) Titans: Learning to Memorize at Test Time","snippet":"31 Dec 2024 \u2014 We present a new neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/392204297_ATLAS_Learning_to_Optimally_Memorize_the_Context_at_Test_Time","title":"(PDF) ATLAS: Learning to Optimally Memorize the Context ...","snippet":"1 Jun 2025 \u2014 To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":11},"pub_date":1748476800.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"sustcsonglin.github.io","entries":[{"type":"search_result","url":"https://sustcsonglin.github.io/blog/2024/deltanet-1/","title":"DeltaNet Explained (Part I) - Songlin Yang","snippet":"3 Dec 2024 \u2014 Linear attention is essentially a linear RNN with a matrix-valued state S that accumulates key-value outer products, enabling efficient state (size) expansion.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"sustcsonglin.github.io"},{"type":"search_result","url":"https://sustcsonglin.github.io/blog/2024/deltanet-3/","title":"DeltaNet Explained (Part III) - Songlin Yang","snippet":"3 Dec 2024 \u2014 While maintaining the core delta rule mechanism, we've introduced several architectural improvements that significantly enhance its performance.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"sustcsonglin.github.io"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=y8Rm4VNRPH&referrer=%5Bthe+profile+of+Yoon+Kim%5D%28%2Fprofile%3Fid%3D~Yoon_Kim1%29","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"25 Sept 2024 \u2014 This paper proposes a hardware-efficient algorithm for training linear transformers with a delta update (DeltaNet; SMS21). This architecture has ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=UU9Icwbhin","title":"A Successor to Transformer for Large Language Models","snippet":"by Y Sun \u00b7 Cited by 568 \u2014 We propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=y8Rm4VNRPH","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"by S Yang \u00b7 Cited by 135 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"yzhang.site","entries":[{"type":"search_result","url":"https://yzhang.site/assets/pubs/neurips/2024/gsa.pdf","title":"Gated Slot Attention for Efficient Linear-Time Sequence ...","snippet":"This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC [63]) by incorporating a gating mechanism inspired ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":7},"pub_date":1730431836.0,"attribution":"yzhang.site"},{"type":"search_result","url":"https://yzhang.site/assets/pubs/neurips/2024/deltanet.pdf","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"We similarly show that combining DeltaNet with classic attention is an effective strategy. Householder matrices. Householder matrices, known for preserving ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"yzhang.site"}]},{"type":"search_result_group","domain":"jankautz.com","entries":[{"type":"search_result","url":"https://jankautz.com/publications/GatedDeltaNet_ICLR25.pdf","title":"GATED DELTA NETWORKS: IMPROVING MAMBA2 WITH ...","snippet":"by S Yang \u00b7 Cited by 81 \u2014 Our resulting architecture, Gated DeltaNet, consistently outperforms both Mamba2 and DeltaNet across a comprehensive suite of benchmarks, including language ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":9},"pub_date":1744572002.0,"attribution":"jankautz.com"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/fla-org/flash-linear-attention","title":"fla-org/flash-linear-attention","snippet":"This repo aims at providing a collection of efficient Triton-based implementations for state-of-the-art linear attention models.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2406.06484","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"10 Jun 2024 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers?q=linear+recurrent+neural+networks","title":"Daily Papers","snippet":"Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers for long sequences ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3737916.3741584","title":"Parallelizing linear transformers with the delta rule over ...","snippet":"5 Jun 2025 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/152u5va/r_retentive_network_a_successor_to_transformer/","title":"Retentive Network: A Successor to Transformer for Large ...","snippet":"The RetNet architecture consists of multi-scale retention modules and feed-forward network modules. The retention mechanism is formulated as ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":19},"pub_date":1689674504.775,"attribution":"www.reddit.com"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching for DeltaNet and related memory architectures","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"025f7f52-7369-4774-999e-7d4b2bd23219","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"025f7f52-7369-4774-999e-7d4b2bd23219","children":["e2a90115-de03-4cf3-84d1-43d7eb5c42af"]},"e2a90115-de03-4cf3-84d1-43d7eb5c42af":{"id":"e2a90115-de03-4cf3-84d1-43d7eb5c42af","message":{"id":"e2a90115-de03-4cf3-84d1-43d7eb5c42af","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310276.185413,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Considering implicit state models","content":"Next, I\u2019m thinking about including implicit state models like \"Gated Linear Attention (GLA)\" and maybe citing \"HyenaDNA\". I need to make sure these sources are solid and relevant before proceeding, but weaving them into the broader context could definitely help round out this discussion. Let's check if GLA fits in.","chunks":["Next, I\u2019m thinking about including implicit state models like \"Gated Linear Attention (GLA)\" and maybe citing \"HyenaDNA\". I need to make sure these sources are solid and relevant before proceeding, but weaving them into the broader context could definitely help round out this discussion. Let's check if GLA fits in."],"finished":true}],"source_analysis_msg_id":"44112e42-d037-4bd6-a128-cceb5c88d2bb"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"dec1c99e-84c6-40d1-a77f-be0c4caaa6b2","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"dec1c99e-84c6-40d1-a77f-be0c4caaa6b2","children":["d4ed51c7-a0e2-4953-8e9a-12aa38ceef94"]},"d4ed51c7-a0e2-4953-8e9a-12aa38ceef94":{"id":"d4ed51c7-a0e2-4953-8e9a-12aa38ceef94","message":{"id":"d4ed51c7-a0e2-4953-8e9a-12aa38ceef94","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310273.380192,"update_time":1762310273.405039,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"HyenaDNA long-range genomics model paper 2023 Stanford\"}, {\"q\": \"Gated Linear Attention (GLA) paper\"}, {\"q\": \"RetNet long-context 1M tokens Microsoft blog\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"HyenaDNA long-range genomics model paper 2023 Stanford"},{"type":"search","q":"Gated Linear Attention (GLA) paper"},{"type":"search","q":"RetNet long-context 1M tokens Microsoft blog"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for GLA, HyenaDNA, and related models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"e2a90115-de03-4cf3-84d1-43d7eb5c42af","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"e2a90115-de03-4cf3-84d1-43d7eb5c42af","children":["fd7ac2ff-482e-4c19-a1d0-c9c6be4064f6"]},"fd7ac2ff-482e-4c19-a1d0-c9c6be4064f6":{"id":"fd7ac2ff-482e-4c19-a1d0-c9c6be4064f6","message":{"id":"fd7ac2ff-482e-4c19-a1d0-c9c6be4064f6","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310273.5312436,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["HyenaDNA long-range genomics model paper 2023 Stanford","Gated Linear Attention (GLA) paper","RetNet long-context 1M tokens Microsoft blog"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for GLA, HyenaDNA, and related models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"d4ed51c7-a0e2-4953-8e9a-12aa38ceef94","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"d4ed51c7-a0e2-4953-8e9a-12aa38ceef94","children":["037fa6bb-15a2-4631-8140-b352787c43f4"]},"037fa6bb-15a2-4631-8140-b352787c43f4":{"id":"037fa6bb-15a2-4631-8140-b352787c43f4","message":{"id":"037fa6bb-15a2-4631-8140-b352787c43f4","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310273.5280406,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"papers.neurips.cc","entries":[{"type":"search_result","url":"https://papers.neurips.cc/paper_files/paper/2023/file/86ab6927ee4ae9bde4247793c46797c7-Paper-Conference.pdf","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","snippet":"by E Nguyen \u00b7 Cited by 480 \u2014 HyenaDNA is a genomic model using Hyena's long-range capabilities, with a simple architecture of Hyena operators, and single nucleotide resolution.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":0},"pub_date":1705444742.0,"attribution":"papers.neurips.cc"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2312.06635","title":"Gated Linear Attention Transformers with Hardware-Efficient Training","snippet":"","ref_id":{"turn_index":5,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2405.18428","title":"DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention","snippet":"","ref_id":{"turn_index":5,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2306.15794","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","snippet":"by E Nguyen \u00b7 2023 \u00b7 Cited by 480 \u2014 We present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.microsoft.com","entries":[{"type":"search_result","url":"https://www.microsoft.com/en-us/research/project/minference-million-tokens-prompt-inference-for-long-context-llms/","title":"MInference: Million-Tokens Prompt Inference for Long- ...","snippet":"We propose a dynamic sparse attention method, MInference, to accelerate the pre-filling stage of long-context LLMs by up to 10x for 1M token prompts while ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"www.microsoft.com"},{"type":"search_result","url":"https://www.microsoft.com/en-us/research/publication/minference-1-0-accelerating-pre-filling-for-long-context-llms-via-dynamic-sparse-attention/","title":"MInference 1.0: Accelerating Pre-filling for Long-Context ...","snippet":"9 Dec 2024 \u2014 We introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"www.microsoft.com"}]},{"type":"search_result_group","domain":"hazyresearch.stanford.edu","entries":[{"type":"search_result","url":"https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna","title":"HyenaDNA: learning from DNA with 1 Million token context","snippet":"29 Jun 2023 \u2014 We're excited to introduce HyenaDNA, a long-range genomic foundation model with context lengths of up to 1 million tokens at single nucleotide resolution!","ref_id":{"turn_index":5,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"hazyresearch.stanford.edu"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/towardsdev/beyond-the-window-the-complete-guide-to-long-context-language-models-and-why-theyre-changing-ai-ee28f6c2b428","title":"The Complete Guide to Long Context Language Models ( ...","snippet":"We're entering the era of Long Context Language Models (LCLMs), capable of processing millions of tokens (words or parts of words) in a single ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":5},"pub_date":1744078746.623,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2025.naacl-long.147/","title":"ReGLA: Refining Gated Linear Attention","snippet":"by P Lu \u00b7 2025 \u00b7 Cited by 2 \u2014 In this work, we embarked on a comprehensive exploration of three key components that substantially impact the performance of the Gated Linear Attention module.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"mskadu.medium.com","entries":[{"type":"search_result","url":"https://mskadu.medium.com/why-large-language-model-context-windows-matter-the-significance-of-claude-sonnets-1m-token-6a0cc98b129a","title":"Why Large Language Model Context Windows Matter: The ...","snippet":"Why Context Window Size drives Adoption. The expansion to 1M tokens removes critical barriers that previously limited LLM adoption in enterprise ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":8},"pub_date":1759877623.549,"attribution":"mskadu.medium.com"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3666122.3667994","title":"HyenaDNA: long-range genomic sequence modeling at ...","snippet":"by E Nguyen \u00b7 2023 \u00b7 Cited by 480 \u2014 We present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3692070.3694403","title":"Gated linear attention transformers with hardware-efficient ...","snippet":"by S Yang \u00b7 2024 \u00b7 Cited by 270 \u2014 This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/371944243_HyenaDNA_Long-Range_Genomic_Sequence_Modeling_at_Single_Nucleotide_Resolution","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","snippet":"Leveraging Hyenas new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"openaccess.thecvf.com","entries":[{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_DiG_Scalable_and_Efficient_Diffusion_Models_with_Gated_Linear_Attention_CVPR_2025_paper.pdf","title":"DiG: Scalable and Efficient Diffusion Models with Gated Linear ...","snippet":"by L Zhu \u00b7 2025 \u00b7 Cited by 27 \u2014 In this paper, we aim to incorpo- rate the sub-quadratic modeling capability of Gated Linear. Attention (GLA) into the 2D diffusion backbone. Specifi- cally, we ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"openaccess.thecvf.com"}]},{"type":"search_result_group","domain":"uplatz.com","entries":[{"type":"search_result","url":"https://uplatz.com/blog/the-million-token-revolution-an-in-depth-analysis-of-long-context-ai-models-and-their-strategic-implications/","title":"The Million-Token Revolution: An In--Depth Analysis of ...","snippet":"23 Sept 2025 \u2014 Models like Meta's Llama 3.1 (with variants supporting up to 1M tokens), Yi, and DeepSeek are increasingly offering extended context windows in ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":14},"pub_date":1758645490.0,"attribution":"uplatz.com"}]},{"type":"search_result_group","domain":"www.researchhub.com","entries":[{"type":"search_result","url":"https://www.researchhub.com/paper/1277006/hyenadna-long-range-genomic-sequence-modeling-at-single-nucleotide-resolution","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","snippet":"28 Jun 2023 \u2014 Toward longer context models Recently, Hyena (Poli et al., 2023), a large language model based on implicit convolutions, was shown to match ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":15},"pub_date":1688547208.007562,"attribution":"www.researchhub.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=AC9FsaVIpk","title":"Understanding Gated Linear Attention through In-context ...","snippet":"by Y Li \u00b7 Cited by 5 \u2014 This paper shows that Gated Linear Attention (GLA) can implement Weighted Projected Gradient Descent (WPGD) algorithms. Furthermore, the gating mechanism in GLA ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/LocalLLaMA/comments/1f738xs/retnphi_hybrid_of_phi35_and_retnet_for_bytelevel/","title":"RetNPhi: Hybrid of Phi-3.5 and RetNet for Byte-Level ...","snippet":"Entire 1M line codebase will fit in like 8-16K context window ... long-context. If you stretch the context window to several million tokens ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":17},"pub_date":1725272171.566,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"aibusiness.com","entries":[{"type":"search_result","url":"https://aibusiness.com/ml/hyenadna-a-large-language-model-trained-on-human-genome-sequences","title":"A Large Language Model for the Human Genome","snippet":"30 Jun 2023 \u2014 Researchers from Stanford unveiled HyenaDNA, a large language model trained on one million human genomic sequences.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":18},"pub_date":1688136120.0,"attribution":"aibusiness.com"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching for GLA, HyenaDNA, and related models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"fd7ac2ff-482e-4c19-a1d0-c9c6be4064f6","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"fd7ac2ff-482e-4c19-a1d0-c9c6be4064f6","children":["b4541c4b-2997-4630-a920-c12981e8d17f"]},"b4541c4b-2997-4630-a920-c12981e8d17f":{"id":"b4541c4b-2997-4630-a920-c12981e8d17f","message":{"id":"b4541c4b-2997-4630-a920-c12981e8d17f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310283.154261,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Evaluating potential models","content":"I\u2019m considering several options: \"Monolithic memory\" structures like \"RetNet\" and \"BitNet\" \u2014 though they focus on training precision, not architecture. I could mention Spiking Neural Networks (SNNs) for AI agents, though they\u2019re not widely used for LLMs. Also, \"Diffusion-based models,\" such as \"Diffusion-LM\" may be a fit for non-transformer approaches in generative modeling. I'll finalize which ideas to use here.","chunks":["I\u2019m considering several options: \"Monolithic memory\" structures like \"RetNet\" and \"BitNet\" \u2014 though they focus on training precision, not architecture. I could mention Spiking Neural Networks (SNNs) for AI agents, though they\u2019re not widely used for LLMs. Also, \"Diffusion-based models,\" such as \"Diffusion-LM\" may be a fit for non-transformer approaches in generative modeling. I'll finalize which ideas to use here."],"finished":true}],"source_analysis_msg_id":"a0e26afc-a94f-4264-88fd-4e7319931a4d"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"037fa6bb-15a2-4631-8140-b352787c43f4","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"037fa6bb-15a2-4631-8140-b352787c43f4","children":["4ddb744b-79de-4cf9-be5b-ca1d1e7661cc"]},"4ddb744b-79de-4cf9-be5b-ca1d1e7661cc":{"id":"4ddb744b-79de-4cf9-be5b-ca1d1e7661cc","message":{"id":"4ddb744b-79de-4cf9-be5b-ca1d1e7661cc","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310282.51057,"update_time":1762310282.525924,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Diffusion language models text generation 'Diffusion-LM' 2022\"}, {\"q\": \"Flow matching language models 2024 text 'Flow matching for language modeling' 'Mamba flow? ' \"}, {\"q\": \"MaskGIT for text ' non-autoregressive ' \"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Diffusion language models text generation 'Diffusion-LM' 2022"},{"type":"search","q":"Flow matching language models 2024 text 'Flow matching for language modeling' 'Mamba flow? ' "},{"type":"search","q":"MaskGIT for text ' non-autoregressive ' "}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for non-transformer language models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"b4541c4b-2997-4630-a920-c12981e8d17f","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"b4541c4b-2997-4630-a920-c12981e8d17f","children":["30f86619-04fd-4816-85ac-788dd8e20141"]},"30f86619-04fd-4816-85ac-788dd8e20141":{"id":"30f86619-04fd-4816-85ac-788dd8e20141","message":{"id":"30f86619-04fd-4816-85ac-788dd8e20141","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310282.637496,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Diffusion language models text generation 'Diffusion-LM' 2022","Flow matching language models 2024 text 'Flow matching for language modeling' 'Mamba flow? ' ","MaskGIT for text ' non-autoregressive ' "]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for non-transformer language models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"4ddb744b-79de-4cf9-be5b-ca1d1e7661cc","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"4ddb744b-79de-4cf9-be5b-ca1d1e7661cc","children":["237ee210-fb8c-4946-a728-9ac1582e57bd"]},"237ee210-fb8c-4946-a728-9ac1582e57bd":{"id":"237ee210-fb8c-4946-a728-9ac1582e57bd","message":{"id":"237ee210-fb8c-4946-a728-9ac1582e57bd","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310282.6344285,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2205.14217","title":"Diffusion-LM Improves Controllable Text Generation","snippet":"","ref_id":{"turn_index":6,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.16907v1","title":"MambaFlow: A Novel and Flow-guided State Space Model ...","snippet":"24 Feb 2025 \u2014 In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.14988v1","title":"Efficient Text-to-Image Generation with Iterative Non- ...","snippet":"(1) MaskGIT is proposed for class-based image generation task, while our model focuses on text-based image generation. (2) MaskGIT is a decoder-only structure, ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.00290v1","title":"DLM-One: Diffusion Language Models for One-Step ...","snippet":"30 May 2025 \u2014 Diffusion-LM improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022a. Li et al. (2023a)See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.10915v1","title":"M4V: Multi-Modal Mamba for Text-to-Video Generation","snippet":"12 Jun 2025 \u2014 We propose a multi-modal diffusion Mamba (MM-DiM) block that enables seamless integration of multi-modal information and spatiotemporal modeling.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2202.04200","title":"arXiv:2202.04200v1 [cs.CV] 8 Feb 2022","snippet":"by H Chang \u00b7 2022 \u00b7 Cited by 1022 \u2014 At inference time, MaskGIT adopts a novel non-autoregressive decoding method to synthesize an image in constant number of steps ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openaccess.thecvf.com","entries":[{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf","title":"MaskGIT: Masked Generative Image Transformer","snippet":"by H Chang \u00b7 2022 \u00b7 Cited by 1024 \u2014 At inference time, MaskGIT adopts a novel non-autoregressive decoding method to synthesize an image in constant number of steps. Specifically, at each iteration ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"openaccess.thecvf.com"},{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_FlowRAM_Grounding_Flow_Matching_Policy_with_Region-Aware_Mamba_Framework_for_CVPR_2025_paper.pdf","title":"Grounding Flow Matching Policy with Region-Aware Mamba ...","snippet":"by S Wang \u00b7 2025 \u00b7 Cited by 4 \u2014 In robotics, these models are employed to learn action score functions conditioned on observations, language, and robot states, enabling the generation of ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"openaccess.thecvf.com"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3600270.3600583","title":"Diffusion-LM improves controllable text generation","snippet":"by XL Li \u00b7 2022 \u00b7 Cited by 1133 \u2014 Diffusion-LM is a non-autoregressive model using continuous diffusions to generate text, enabling complex, controllable generation tasks.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1007/978-3-031-72664-4_9","title":"ZigMa: A DiT-style Zigzag Mamba Diffusion Model","snippet":"by VT Hu \u00b7 2024 \u00b7 Cited by 153 \u2014 In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/facebookresearch/flowmm","title":"Code for \u201cFlowMM Generating Materials with Riemannian ...","snippet":"18 Sept 2025 \u2014 Code for \u201cFlowMM Generating Materials with Riemannian Flow Matching\u201d and \"FlowLLM: Flow Matching for Material Generation with Large Language ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/bansky-cl/Diffusion_NLP_Papers","title":"bansky-cl/Diffusion-LM-Papers","snippet":"24 Mar 2025 \u2014 Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference! 42, DIFFUSEMP, 2023.6, Bi ...See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/VILA-Lab/Awesome-DLMs","title":"Awesome Diffusion Language Models","snippet":"One of the most starred, comprehensive and up-to-date collections of Diffusion Language Model papers, code and resources! If you find this repository helpful, ...See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2024.findings-emnlp.497.pdf","title":"Enable Fast Sampling for Seq2Seq Text Diffusion","snippet":"by P Liu \u00b7 2024 \u00b7 Cited by 1 \u2014 In this paper, we propose an efficient model FMSeq1, which utilizes flow matching to straighten the gen- eration path, thereby enabling fast ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":7},"pub_date":1731136606.0,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=ExuBFYtCQU","title":"MaskGCT: Zero-Shot Text-to-Speech with Masked ...","snippet":"by Y Wang \u00b7 Cited by 120 \u2014 In this paper, we introduce Mask ed G enerative C odec T ransformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=3s9IrEsjLyk","title":"Diffusion-LM Improves Controllable Text Generation","snippet":"by XL Li \u00b7 2022 \u00b7 Cited by 1135 \u2014 Diffusion-LM is a non-autoregressive model using continuous diffusions, iteratively denoising Gaussian vectors to word vectors, enabling gradient-based control ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/revisions?id=3s9IrEsjLyk","title":"Revision History for Diffusion-LM Improves Controllable...","snippet":"TL;DR: We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.marktechpost.com","entries":[{"type":"search_result","url":"https://www.marktechpost.com/2024/10/16/meissonic-a-non-autoregressive-mask-image-modeling-text-to-image-synthesis-model-that-can-generate-high-resolution-images/","title":"Meissonic: A Non-Autoregressive Mask Image Modeling ...","snippet":"16 Oct 2024 \u2014 Meissonic, an innovative method to elevate non-autoregressive MIM text-to-image synthesis to a level comparable with state-of-the-art diffusion models like ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":11},"pub_date":1729146268.0,"attribution":"www.marktechpost.com"}]},{"type":"search_result_group","domain":"proceedings.iclr.cc","entries":[{"type":"search_result","url":"https://proceedings.iclr.cc/paper_files/paper/2025/file/74a31a3b862eb7f01defbbed8e5f0c69-Paper-Conference.pdf","title":"MASKGCT: ZERO-SHOT TEXT-TO-SPEECH WITH ...","snippet":"by Y Wang \u00b7 Cited by 120 \u2014 In this work, we propose MaskGCT, a fully non-autoregressive model for text-to-speech synthesis that uses masked generative transformers without requiring text- ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":14},"pub_date":1740577261.0,"attribution":"proceedings.iclr.cc"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/393045731_Flow_Matching_for_Conditional_Text_Generation_in_a_Few_Sampling_Steps","title":"Flow Matching for Conditional Text Generation in a Few ...","snippet":"28 Jun 2025 \u2014 Unlike Diffusion models, flow matching is not constrained to the Gaussian noise prior distribution, but instead supports arbitrary prior ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.linkedin.com","entries":[{"type":"search_result","url":"https://www.linkedin.com/pulse/why-diffusion-future-text-generation-mohit-sewak-ph-d--ayt6c","title":"Why Diffusion Models Are the Future of AI Text Generation","snippet":"The first team, the creators of models like Diffusion-LM (Li et al., 2022), decides not to carry the words across the chasm at all. Instead, ...See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"www.linkedin.com"}]},{"type":"search_result_group","domain":"www.ecva.net","entries":[{"type":"search_result","url":"https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07855.pdf","title":"FMBoost: Boosting Latent Diffusion with Flow Matching","snippet":"A small diffusion model can then effectively provide the necessary visual diversity, while flow matching efficiently enhances resolution and detail by mapping.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"www.ecva.net"},{"type":"search_result","url":"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830070.pdf","title":"Improved Masked Image Generation with Token-Critic","snippet":"by J Lezama \u00b7 Cited by 60 \u2014 The non-autoregressive nature of MaskGIT allows orders-of-magnitude faster sampling, generating an image typically in 8-16 steps as opposed to hundreds of steps ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"www.ecva.net"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching for non-transformer language models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"30f86619-04fd-4816-85ac-788dd8e20141","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"30f86619-04fd-4816-85ac-788dd8e20141","children":["a16a1d5f-e25e-4c0c-9af4-7295cab67ad0"]},"a16a1d5f-e25e-4c0c-9af4-7295cab67ad0":{"id":"a16a1d5f-e25e-4c0c-9af4-7295cab67ad0","message":{"id":"a16a1d5f-e25e-4c0c-9af4-7295cab67ad0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310288.128216,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring Spiking Neural Networks (SNN)","content":"I\u2019m considering Spiking Neural Networks (SNN) for their energy efficiency and event-driven nature. They\u2019re sometimes used for agents, and there\u2019s the potential \"SpikingLLM 2024\" and \"SpikeDrive.\" I'll search to see if there are any relevant advancements or models in this area for LLMs.","chunks":["I\u2019m considering Spiking Neural Networks (SNN) for their energy efficiency and event-driven nature.","They\u2019re sometimes used for agents, and there\u2019s the potential \"SpikingLLM 2024\" and \"SpikeDrive.\" I'll search to see if there are any relevant advancements or models in this area for LLMs."],"finished":true}],"source_analysis_msg_id":"1829931d-2b0a-43b3-a174-3f690feb45ce"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"237ee210-fb8c-4946-a728-9ac1582e57bd","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"237ee210-fb8c-4946-a728-9ac1582e57bd","children":["d6e2f3aa-63d9-4643-8336-f50753121c70"]},"d6e2f3aa-63d9-4643-8336-f50753121c70":{"id":"d6e2f3aa-63d9-4643-8336-f50753121c70","message":{"id":"d6e2f3aa-63d9-4643-8336-f50753121c70","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310287.038395,"update_time":1762310287.053358,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Spiking language model 'spiking LLM' 2024 arXiv\"}, {\"q\": \"SNN large language model 'SpikeGPT'\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Spiking language model 'spiking LLM' 2024 arXiv"},{"type":"search","q":"SNN large language model 'SpikeGPT'"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for Spiking Neural Networks in LLMs","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"a16a1d5f-e25e-4c0c-9af4-7295cab67ad0","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"a16a1d5f-e25e-4c0c-9af4-7295cab67ad0","children":["d0434b10-0bec-4b78-a0af-1e04f8c5b263"]},"d0434b10-0bec-4b78-a0af-1e04f8c5b263":{"id":"d0434b10-0bec-4b78-a0af-1e04f8c5b263","message":{"id":"d0434b10-0bec-4b78-a0af-1e04f8c5b263","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310287.1910348,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Spiking language model 'spiking LLM' 2024 arXiv","SNN large language model 'SpikeGPT'"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for Spiking Neural Networks in LLMs","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"d6e2f3aa-63d9-4643-8336-f50753121c70","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"d6e2f3aa-63d9-4643-8336-f50753121c70","children":["9d1cfeec-84d5-4c85-8565-292ab55c7bd1"]},"9d1cfeec-84d5-4c85-8565-292ab55c7bd1":{"id":"9d1cfeec-84d5-4c85-8565-292ab55c7bd1","message":{"id":"9d1cfeec-84d5-4c85-8565-292ab55c7bd1","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310287.18501,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2407.04752","title":"SpikeLLM: Scaling up Spiking Neural Network to Large ...","snippet":"by X Xing \u00b7 2024 \u00b7 Cited by 25 \u2014 We propose the first spiking large language model, SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spike training ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2302.13939","title":"SpikeGPT: Generative Pre-trained Language Model with ...","snippet":"by RJ Zhu \u00b7 2023 \u00b7 Cited by 169 \u2014 SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.04752v1","title":"SpikeLLM: Scaling up Spiking Neural Network to Large ...","snippet":"5 Jul 2024 \u2014 We redesign 7~70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.04752v3","title":"SpikeLLM: Scaling up Spiking Neural Network to Large ...","snippet":"10 Apr 2025 \u2014 This work aims to scale up SNNs to large language models or even part of the human brain nervous system with 86B neurons. Towards this goal, we ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2302.13939v5","title":"SpikeGPT: Generative Pre-trained Language Model with ...","snippet":"11 Jul 2024 \u2014 SpikeGPT is the largest backpropagation-trained SNN model when released, rendering it suitable for both the generation and comprehension of natural language.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2405.02543v1","title":"Exploring Extreme Quantization in Spiking Language Models","snippet":"Our proposed model represents a significant advancement as the first-of-its-kind 1/1.58-bit spiking LM, and its performance is rigorously ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2509.05276","title":"SpikingBrain Technical Report: Spiking Brain-inspired Large Models","snippet":"","ref_id":{"turn_index":7,"ref_type":"academia","ref_index":21},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2410.14687","title":"BrainTransformers: SNN-LLM","snippet":"","ref_id":{"turn_index":7,"ref_type":"academia","ref_index":22},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=gcf1anBL9e","title":"SpikeGPT: Generative Pre-trained Language Model with ...","snippet":"by RJ Zhu \u00b7 Cited by 169 \u2014 SpikeGPT is the largest backpropagation-trained SNN model when released, rendering it suitable for both the generation and comprehension of natural language.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=DEdDWSXvAP","title":"SPIKINGLLM","snippet":"In this paper, we aim to improve the energy efficiency of LLMs by leveraging the advantages of brain-inspired spiking neural networks (SNNs). We propose a novel ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":9},"pub_date":1758796063.0,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/11eqinv/spikegpt_230mparameter_spiking_neural_network/","title":"230M-parameter Spiking Neural Network trained to be a ...","snippet":"The authors train a 230M parameter SNN on an autoregressive text modeling objective. They believe this is the largest SNN ever trained with ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":6},"pub_date":1677634619.325,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"ojs.aaai.org","entries":[{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/28975/29854","title":"SpikingBERT: Distilling BERT to Train Spiking Language ...","snippet":"by M Bal \u00b7 2024 \u00b7 Cited by 78 \u2014 Spiking LM Training: We theoretically and empiri- cally verify the convergence of our proposed spiking. LM (comprising both linear and non-linear operations) to ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"ojs.aaai.org"}]},{"type":"search_result_group","domain":"news.ucsc.edu","entries":[{"type":"search_result","url":"https://news.ucsc.edu/2023/03/eshraghian-spikegpt/","title":"SpikeGPT: researcher releases code for largest-ever ...","snippet":"7 Mar 2023 \u2014 Jason Eshraghian and two students recently released the open-sourced code for the largest language-generating spiking neural network ever, named SpikeGPT.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":8},"pub_date":1678204800.0,"attribution":"news.ucsc.edu"}]},{"type":"search_result_group","domain":"syncedreview.com","entries":[{"type":"search_result","url":"https://syncedreview.com/2023/03/06/introducing-spikegpt-ucsc-kuaishous-llm-with-spiking-neural-networks-slashes-language-generation-costs/","title":"Introducing SpikeGPT: UCSC & Kuaishou's LLM With Spiking ...","snippet":"6 Mar 2023 \u2014 A research team from the University of California and Kuaishou Technology presents SpikeGPT, the first generative spiking neural network (SNN) language model.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":10},"pub_date":1678158436.0,"attribution":"syncedreview.com"}]},{"type":"search_result_group","domain":"poster-openaccess.com","entries":[{"type":"search_result","url":"https://poster-openaccess.com/files/ICIC2025/4202.pdf","title":"SpikeRWKV:Energy-efficient Large Language Model with ...","snippet":"by Y Zhang1& \u2014 Furthermore, SpikeRWKV is 3.88\u00d7more energy-efficient than its non-spiking counterpart. Keywords: Spiking neural networks \u00b7 Energy efficiency \u00b7 Spike encoding.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"poster-openaccess.com"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/SpikeGPT%3A-Generative-Pre-trained-Language-Model-Zhu-Zhao/9f52317ea9c5a6804b978987ff2a6557f98b5b2c","title":"SpikeGPT: Generative Pre-trained Language Model with ...","snippet":"This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"intel-ncl.atlassian.net","entries":[{"type":"search_result","url":"https://intel-ncl.atlassian.net/wiki/spaces/INRC/blog/2023/05/24/1933738007/INRC%2BForum%2BMay%2B30th%2BJason%2BEshraghian%2BRuijie%2BZhu","title":"Scaling up SNNs with SpikeGPT","snippet":"30 May 2023 \u2014 This presentation will be centered on SpikeGPT, the first large-scale language model (LLM) using spiking neural nets (SNNs), and possibly ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"intel-ncl.atlassian.net"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1609/aaai.v39i19.34245","title":"SpikingSSMs: learning long sequences with sparse and ...","snippet":"by S Shen \u00b7 2025 \u00b7 Cited by 16 \u2014 Known as low energy consumption networks, spiking neural networks (SNNs) have gained a lot of attention within the past decades.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0893608024004180","title":"Leveraging spiking neural networks for topic modeling","snippet":"by M Bia\u0142as \u00b7 2024 \u00b7 Cited by 2 \u2014 SpikeGPT is a back-propagation-trained SNN that incorporates several mechanisms to enhance its robustness. Initial experiments indicate that SpikeGPT maintains ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"www.alphaxiv.org","entries":[{"type":"search_result","url":"https://www.alphaxiv.org/overview/2302.13939v5","title":"Generative Pre-trained Language Model with Spiking ...","snippet":"SpikeGPT introduces the first large-scale, backpropagation-trained Spiking Neural Network (SNN) capable of generative language modeling.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"www.alphaxiv.org"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching for Spiking Neural Networks in LLMs","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"d0434b10-0bec-4b78-a0af-1e04f8c5b263","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"d0434b10-0bec-4b78-a0af-1e04f8c5b263","children":["f89568b3-2ee1-46ac-a3ee-c63234a15ec2"]},"f89568b3-2ee1-46ac-a3ee-c63234a15ec2":{"id":"f89568b3-2ee1-46ac-a3ee-c63234a15ec2","message":{"id":"f89568b3-2ee1-46ac-a3ee-c63234a15ec2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310293.285152,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Considering Neural Programming Interpreters (NPI)","content":"Next, I'll mention Neural Programming Interpreters (NPI) and Neural Reasoning, though they're not widely used. For agents, I\u2019m thinking of contrasting diffusion policies with transformer policies. I\u2019ll search for insights on decision transformers and non-transformer alternatives to explore if there are viable options out there.","chunks":["Next, I'll mention Neural Programming Interpreters (NPI) and Neural Reasoning, though they're not widely used.","For agents, I\u2019m thinking of contrasting diffusion policies with transformer policies.","I\u2019ll search for insights on decision transformers and non-transformer alternatives to explore if there are viable options out there."],"finished":true}],"source_analysis_msg_id":"f387e7d2-2eff-498f-b1ac-970077d932d1"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"9d1cfeec-84d5-4c85-8565-292ab55c7bd1","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"9d1cfeec-84d5-4c85-8565-292ab55c7bd1","children":["4fa02b04-ab78-4d2b-8c99-fde74aefa529"]},"4fa02b04-ab78-4d2b-8c99-fde74aefa529":{"id":"4fa02b04-ab78-4d2b-8c99-fde74aefa529","message":{"id":"4fa02b04-ab78-4d2b-8c99-fde74aefa529","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310293.476721,"update_time":1762310293.493484,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"DreamerV3 world model agent 2023 paper latent dynamics RSSM\"}, {\"q\": \"MuZero nature 2019 model-based RL planning architecture\"}, {\"q\": \"Diffusion Policy Robotics 2023 Berkeley paper\"}, {\"q\": \"Value Iteration Networks paper 2016\"}, {\"q\": \"Gato generalist agent 2022 DeepMind architecture\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"DreamerV3 world model agent 2023 paper latent dynamics RSSM"},{"type":"search","q":"MuZero nature 2019 model-based RL planning architecture"},{"type":"search","q":"Diffusion Policy Robotics 2023 Berkeley paper"},{"type":"search","q":"Value Iteration Networks paper 2016"},{"type":"search","q":"Gato generalist agent 2022 DeepMind architecture"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for advanced agent architectures and models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"f89568b3-2ee1-46ac-a3ee-c63234a15ec2","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"f89568b3-2ee1-46ac-a3ee-c63234a15ec2","children":["ddea68ba-e2ae-4920-b9e8-47b2fea4a1b7"]},"ddea68ba-e2ae-4920-b9e8-47b2fea4a1b7":{"id":"ddea68ba-e2ae-4920-b9e8-47b2fea4a1b7","message":{"id":"ddea68ba-e2ae-4920-b9e8-47b2fea4a1b7","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310293.6918907,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["DreamerV3 world model agent 2023 paper latent dynamics RSSM","MuZero nature 2019 model-based RL planning architecture","Diffusion Policy Robotics 2023 Berkeley paper","Value Iteration Networks paper 2016","Gato generalist agent 2022 DeepMind architecture"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for advanced agent architectures and models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"4fa02b04-ab78-4d2b-8c99-fde74aefa529","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"4fa02b04-ab78-4d2b-8c99-fde74aefa529","children":["c2fa96dc-0850-4621-b451-f57f482fa3c6"]},"c2fa96dc-0850-4621-b451-f57f482fa3c6":{"id":"c2fa96dc-0850-4621-b451-f57f482fa3c6","message":{"id":"c2fa96dc-0850-4621-b451-f57f482fa3c6","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310293.6861808,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2405.06263v1","title":"Learning Latent Dynamic Robust Representations for ...","snippet":"10 May 2024 \u2014 We develop a Hybrid Recurrent State-Space Model (HRSSM) structure, enhancing state representation robustness for effective policy learning.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.15799v1","title":"Steering Your Diffusion Policy with Latent Space ...","snippet":"18 Jun 2025 \u2014 We propose diffusion steering via reinforcement learning (Dsrl): adapting the BC policy by running RL over its latent-noise space.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1602.02867","title":"Value Iteration Networks","snippet":"","ref_id":{"turn_index":8,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2205.06175","title":"[2205.06175] A Generalist Agent","snippet":"by S Reed \u00b7 2022 \u00b7 Cited by 1332 \u2014 The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2405.15083v1","title":"Learning Predictive World Models without Reconstruction","snippet":"23 May 2024 \u2014 The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2306.00840v3","title":"What model does MuZero learn?","snippet":"18 Aug 2024 \u2014 We study MuZero, a state-of-the-art deep model-based reinforcement learning algorithm that distinguishes itself from existing algorithms by learning a value- ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/1911.08265","title":"Mastering Atari, Go, Chess and Shogi by Planning with a ...","snippet":"by J Schrittwieser \u00b7 2019 \u00b7 Cited by 3275 \u2014 In this paper, we introduce MuZero, a new approach to model-based RL that achieves state-of-the-art per- formance in Atari 2600, a visually ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":11},"pub_date":1582508940.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.23458v1","title":"Diffusion Guidance Is a Controllable Policy Improvement ...","snippet":"29 May 2025 \u2014 This paper describes work performed at UC Berkeley and is not associated with Amazon. ... In Robotics: Science and Systems (RSS), 2023. da ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/","title":"MuZero: Mastering Go, chess, shogi and Atari without rules","snippet":"23 Dec 2020 \u2014 MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/discover/blog/a-generalist-agent/","title":"A Generalist Agent","snippet":"12 May 2022 \u2014 The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"www.roboticsproceedings.org","entries":[{"type":"search_result","url":"https://www.roboticsproceedings.org/rss19/p026.pdf","title":"Diffusion Policy:","snippet":"by C Chi \u2014 Abstract\u2014This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"www.roboticsproceedings.org"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper/2016/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf","title":"Value Iteration Networks","snippet":"by A Tamar \u00b7 2016 \u00b7 Cited by 836 \u2014 We introduce the value iteration network (VIN): a fully differentiable neural net- work with a 'planning module' embedded within. VINs can learn to plan, ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":8},"pub_date":1486105073.0,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"vitalab.github.io","entries":[{"type":"search_result","url":"https://vitalab.github.io/article/2023/01/19/DreamerV3.html","title":"DreamerV3: Mastering Diverse Domains through World Models","snippet":"19 Jan 2023 \u2014 The world model, dubbed the \u201cRecurrent State-Space Model (RSSM)\u201d can be described by the following networks: Note that the encoder and the ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"vitalab.github.io"}]},{"type":"search_result_group","domain":"jxwuyi.weebly.com","entries":[{"type":"search_result","url":"https://jxwuyi.weebly.com/uploads/2/5/1/1/25111124/vin-nips16.pdf","title":"Value Iteration Networks","snippet":"We introduce the value iteration network (VIN): a fully differentiable neural net- work with a 'planning module' embedded within. VINs can learn to plan, ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":13},"pub_date":1478641002.0,"attribution":"jxwuyi.weebly.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/mlearning-ai/gato-the-generalist-agent-by-deepmind-a780a048f9bc","title":"Gato Generalist agent | DeepMind | AGI AI","snippet":"Control robot arms; Chat. The model uses just a single architecture based on Transformer. It is a 1.18B parameter model with shared embeddings.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":14},"pub_date":1652543460.519,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf/8801375407fde3d9751dbb78c2f906238f519a12.pdf","title":"MUDREAMER: LEARNING PREDICTIVE WORLD ...","snippet":"In this paper, we present MuDreamer, a reinforcement learning agent that builds upon the Dream-. erV3 (Hafner et al., 2023) algorithm by learning a predictive ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":15},"pub_date":1700359089.0,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/c39fb7a46335c23f7529dd6f9f980462fd38653a","title":"Mastering Atari, Go, chess and shogi by planning with a ...","snippet":"The MuZero algorithm is presented, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"people.eecs.berkeley.edu","entries":[{"type":"search_result","url":"https://people.eecs.berkeley.edu/~ysshao/assets/papers/diffuseloco-corl2024.pdf","title":"DiffuseLoco: Real-Time Legged Locomotion Control with ...","snippet":"To address the challenges of offline learning from diverse data sources and learning a set of diverse skills, we present DiffuseLoco, a framework leveraging ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"people.eecs.berkeley.edu"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3157096.3157337","title":"Value iteration networks | Proceedings of the 30th ...","snippet":"In this paper, we introduce a generalized value iteration network (GVIN), which is an end-to-end neural network planning module. GVIN emulates the value ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.analyticsvidhya.com","entries":[{"type":"search_result","url":"https://www.analyticsvidhya.com/blog/2022/06/gato-a-new-generalist-artificial-intelligence-agent/","title":"GATO - A New Generalist Artificial Intelligence Agent","snippet":"12 Jul 2022 \u2014 Deepmind instantiated Gato as a single, large, transformer sequence model. Another important point is that every task done by GATO uses the ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":19},"pub_date":1655882051.0,"attribution":"www.analyticsvidhya.com"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/393937568_Self-Dreaming_to_Control_Better_Boosting_Latent_Spaces_With_Self-Supervised_Learning","title":"Self-Dreaming to Control Better: Boosting Latent Spaces ...","snippet":"24 Jul 2025 \u2014 The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/371223567_What_model_does_MuZero_learn","title":"(PDF) What model does MuZero learn?","snippet":"1 Jun 2023 \u2014 In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":21},"pub_date":1685577600.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"bair.berkeley.edu","entries":[{"type":"search_result","url":"https://bair.berkeley.edu/blog/2023/07/14/ddpo/","title":"Training Diffusion Models with Reinforcement Learning","snippet":"14 Jul 2023 \u2014 In this post, we show how diffusion models can be trained on these downstream objectives directly using reinforcement learning (RL).","ref_id":{"turn_index":8,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"bair.berkeley.edu"}]},{"type":"search_result_group","domain":"www.ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2017/0700.pdf","title":"Value Iteration Networks","snippet":"by A Tamar \u00b7 Cited by 836 \u2014 Abstract. We introduce the value iteration network (VIN): a fully differentiable neural network with a 'planning module' embedded within.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":23},"pub_date":1502047434.0,"attribution":"www.ijcai.org"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching for advanced agent architectures and models","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"ddea68ba-e2ae-4920-b9e8-47b2fea4a1b7","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"ddea68ba-e2ae-4920-b9e8-47b2fea4a1b7","children":["6f488ad9-d91c-4d5a-9997-4c9c8b1a3473"]},"6f488ad9-d91c-4d5a-9997-4c9c8b1a3473":{"id":"6f488ad9-d91c-4d5a-9997-4c9c8b1a3473","message":{"id":"6f488ad9-d91c-4d5a-9997-4c9c8b1a3473","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310304.015116,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Categorizing AI Developments","content":"I\u2019ll focus on key families like neural algorithmic reasoning and kappa networks, but due to their complexity, I\u2019ll keep it concise. I'll mention important agents like tool-using agents and hybrids like SSM+Transformer, RetNet, and DeltaNet as likely winners. Later, I'll discuss memory optimizations for efficiency, including in-context retrieval, and how AI is being adopted in production, like AI21 Jamba on Bedrock.","chunks":["I\u2019ll focus on key families like neural algorithmic reasoning and kappa networks, but due to their complexity, I\u2019ll keep it concise.","I'll mention important agents like tool-using agents and hybrids like SSM+Transformer, RetNet, and DeltaNet as likely winners.","Later, I'll discuss memory optimizations for efficiency, including in-context retrieval, and how AI is being adopted in production, like AI21 Jamba on Bedrock."],"finished":true}],"source_analysis_msg_id":"82581e9d-1d9a-4c60-b7d0-c1764ce62f3b"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"c2fa96dc-0850-4621-b451-f57f482fa3c6","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"c2fa96dc-0850-4621-b451-f57f482fa3c6","children":["35edafe8-9455-4451-af49-8ee86f073676"]},"35edafe8-9455-4451-af49-8ee86f073676":{"id":"35edafe8-9455-4451-af49-8ee86f073676","message":{"id":"35edafe8-9455-4451-af49-8ee86f073676","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310303.314442,"update_time":1762310303.330634,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Mamba-2 state space model paper 2024 ' selective 2 '\"}, {\"q\": \"Mamba 2 Triton ' 2024 ' arXiv\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Mamba-2 state space model paper 2024 ' selective 2 '"},{"type":"search","q":"Mamba 2 Triton ' 2024 ' arXiv"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching insights on Mamba-2 and AI model prospects","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"6f488ad9-d91c-4d5a-9997-4c9c8b1a3473","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"web.run","channel":null},"parent":"6f488ad9-d91c-4d5a-9997-4c9c8b1a3473","children":["56f19f9b-7ee9-4faa-b052-7210a9d35f1a"]},"56f19f9b-7ee9-4faa-b052-7210a9d35f1a":{"id":"56f19f9b-7ee9-4faa-b052-7210a9d35f1a","message":{"id":"56f19f9b-7ee9-4faa-b052-7210a9d35f1a","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310303.4821148,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Mamba-2 state space model paper 2024 ' selective 2 '","Mamba 2 Triton ' 2024 ' arXiv"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching insights on Mamba-2 and AI model prospects","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"35edafe8-9455-4451-af49-8ee86f073676","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"35edafe8-9455-4451-af49-8ee86f073676","children":["2e26926d-bf2f-44a9-a7d1-2b3a0f245601"]},"2e26926d-bf2f-44a9-a7d1-2b3a0f245601":{"id":"2e26926d-bf2f-44a9-a7d1-2b3a0f245601","message":{"id":"2e26926d-bf2f-44a9-a7d1-2b3a0f245601","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1762310303.4782238,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2312.00752","title":"Linear-Time Sequence Modeling with Selective State Spaces","snippet":"by A Gu \u00b7 2023 \u00b7 Cited by 6199 \u2014 We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).","ref_id":{"turn_index":9,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2406.07887","title":"An Empirical Study of Mamba-based Language Models","snippet":"by R Waleffe \u00b7 2024 \u00b7 Cited by 163 \u2014 We present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2312.00752","title":"Mamba: Linear-Time Sequence Modeling with Selective ...","snippet":"31 May 2024 \u2014 We benchmark the speed of the SSM scan operation (state expansion N = 16), as well as the end-to-end inference throughput of Mamba, in Figure 8.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":3},"pub_date":1717375939.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2405.21060","title":"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality","snippet":"","ref_id":{"turn_index":9,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.19832v1","title":"ML-Mamba: Efficient Multi-Modal Large Language Model ...","snippet":"29 Jul 2024 \u2014 We introduce ML-Mamba, a multimodal language model that utilizes the latest and efficient Mamba-2 model for inference.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/state-spaces/mamba","title":"state-spaces/mamba: Mamba SSM architecture","snippet":"Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"tridao.me","entries":[{"type":"search_result","url":"https://tridao.me/blog/2024/mamba2-part1-model/","title":"State Space Duality (Mamba-2) Part I - The Model","snippet":"31 May 2024 \u2014 The main point of the Mamba-2 paper is what we call structured state space duality (SSD), which refers to several things.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"tridao.me"},{"type":"search_result","url":"https://tridao.me/blog/2024/mamba2-part2-theory/","title":"State Space Duality (Mamba-2) Part II - The Theory","snippet":"31 May 2024 \u2014 This framework provides a rich body of connections between state space models, attention, and structured matrices. While the SSD model can be ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"tridao.me"}]},{"type":"search_result_group","domain":"goombalab.github.io","entries":[{"type":"search_result","url":"https://goombalab.github.io/blog/2024/mamba2-part2-theory/","title":"State Space Duality (Mamba-2) Part II - The Theory","snippet":"31 May 2024 \u2014 In this post, we'll dive into the theory behind the model. We'll derive the SSD \u201cduality\u201d in two completely separate ways, one starting from the SSM ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"goombalab.github.io"},{"type":"search_result","url":"https://goombalab.github.io/blog/2024/mamba2-part3-algorithm/","title":"State Space Duality (Mamba-2) Part III - The Algorithm","snippet":"31 May 2024 \u2014 The theoretical framework of structured state space duality (see Part I and Part II of this series) connects SSMs and (linear) attention through structured ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"goombalab.github.io"}]},{"type":"search_result_group","domain":"newsletter.maartengrootendorst.com","entries":[{"type":"search_result","url":"https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state","title":"A Visual Guide to Mamba and State Space Models","snippet":"Exploring Mamba, a State Space Model, as an alternative to Transformers for Language Modeling. With more than 50 illustrations, expect an intuitive guide.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":6},"pub_date":1708354207.0,"attribution":"newsletter.maartengrootendorst.com"}]},{"type":"search_result_group","domain":"towardsai.net","entries":[{"type":"search_result","url":"https://towardsai.net/p/l/understanding-mamba-and-selective-state-space-models-ssms","title":"Understanding Mamba and Selective State Space Models ...","snippet":"24 Jun 2024 \u2014 In this blog, we'll explore a novel block architecture that aims to achieve just that: harnessing the power of large language models without the scalability ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":9},"pub_date":1719231683.0,"attribution":"towardsai.net"}]},{"type":"search_result_group","domain":"www.ibm.com","entries":[{"type":"search_result","url":"https://www.ibm.com/think/topics/mamba-model","title":"What Is A Mamba Model? | IBM","snippet":"Mamba is a neural network architecture, derived from state space models (SSMs), used for language modeling and other sequence modeling tasks.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"www.ibm.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40danieljsmit/mamba2-the-hardware-algorithm-co-design-that-unified-attention-and-state-space-models-77856d2ac4f4","title":"Mamba2: The Hardware-Algorithm Co-Design That Unified ...","snippet":"In May 2024, Tri Dao and Albert Gu unveiled Mamba2 \u2014 not just an incremental improvement, but a fundamental reconceptualization that revealed ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":12},"pub_date":1757276186.319,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science/here-comes-mamba-the-selective-state-space-model-435e5d17a451","title":"Here Comes Mamba: The Selective State Space Model","snippet":"Mamba adds selectivity to state space models which results in Transformer-like performance while maintaining the sub-quadratic work complexity of SSMs.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":13},"pub_date":1745956944.453,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"news.ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=40564067","title":"Mamba-2 \u2013 State Space Duality","snippet":"3 Jun 2024 \u2014 Mamba-2 can be trained more efficiently, ie, faster than original Mamba on GPUs. Theoretical stuff aside, Mamba-2's performance seems to scale slightly better ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"news.ycombinator.com"}]},{"type":"search_result_group","domain":"openaccess.thecvf.com","entries":[{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_Mamba-Adaptor_State_Space_Model_Adaptor_for_Visual_Recognition_CVPR_2025_paper.pdf","title":"State Space Model Adaptor for Visual Recognition","snippet":"by F Xie \u00b7 2025 \u00b7 Cited by 1 \u2014 Recent State Space Models (SSM), especially Mamba, have demonstrated impressive performance in visual modeling and possess superior model efficiency.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openaccess.thecvf.com"}]}],"debug_sonic_thread_id":"thread_690ab86212c483319aa33cb35b92553a","reasoning_status":"is_reasoning","reasoning_title":"Searching insights on Mamba-2 and AI model prospects","classifier_response":"default","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"56f19f9b-7ee9-4faa-b052-7210a9d35f1a","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"56f19f9b-7ee9-4faa-b052-7210a9d35f1a","children":["170219a7-8ce0-4864-878a-badc889d39d5"]},"170219a7-8ce0-4864-878a-badc889d39d5":{"id":"170219a7-8ce0-4864-878a-badc889d39d5","message":{"id":"170219a7-8ce0-4864-878a-badc889d39d5","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310311.268865,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring AI Architecture and Memory","content":"I'll focus on the success of RetNet in production, potentially citing OpenAI. Next, I\u2019ll present architectural alternatives for AI agents' controllers and policy networks, alongside scalable and hierarchical memory. I'll emphasize Retentive networks with O(1) memory and avoid unnecessary qualifiers, delivering clear suggestions on what to build next.","chunks":["I'll focus on the success of RetNet in production, potentially citing OpenAI.","Next, I\u2019ll present architectural alternatives for AI agents' controllers and policy networks, alongside scalable and hierarchical memory.","I'll emphasize Retentive networks with O(1) memory and avoid unnecessary qualifiers, delivering clear suggestions on what to build next."],"finished":true},{"summary":"Evaluating AI Prospects","content":"I\u2019m looking at various AI models:\n\n- **Mamba/SSM**: High potential for long contexts, widely used in Jamba & M4.\n- **RetNet**: Mature, robust, GPU-friendly, but has limited adoption.\n- **RWKV**: Open-source, effective for inference caching.\n- **Hyena**: Great scalability in genomics but not for LLMs.\n- **Linear Attn**: Promising with good performance.\n- **Spiking LLM**: Not mainstream yet, better for neuromorphic hardware.\n- **Liquid networks**: Good for robotics, less for text.\n- **World models**: Useful for agent-environment control.","chunks":["I\u2019m looking at various AI models:","**Mamba/SSM**: High potential for long contexts, widely used in Jamba & M4.","**RetNet**: Mature, robust, GPU-friendly, but has limited adoption.","**RWKV**: Open-source, effective for inference caching.","**Hyena**: Great scalability in genomics but not for LLMs.","**Linear Attn**: Promising with good performance.","**Spiking LLM**: Not mainstream yet, better for neuromorphic hardware.","**Liquid networks**: Good for robotics, less for text.","**World models**: Useful for agent-environment control."],"finished":true},{"summary":"Crafting the Response","content":"I\u2019ll start by creating a concise summary table that includes key families, examples, design principles, current scale, status, and viability. The user\u2019s instructions indicate avoiding complex UI, so I\u2019ll keep it to bullet points but include citations near the paragraphs. I\u2019ll also structure the analysis with 6-8 families, highlighting promising picks, engineering takeaways, and actionable suggestions for experiments.","chunks":["I\u2019ll start by creating a concise summary table that includes key families, examples, design principles, current scale, status, and viability.","The user\u2019s instructions indicate avoiding complex UI, so I\u2019ll keep it to bullet points but include citations near the paragraphs.","I\u2019ll also structure the analysis with 6-8 families, highlighting promising picks, engineering takeaways, and actionable suggestions for experiments."],"finished":true}],"source_analysis_msg_id":"c4f375b6-5865-48e1-9bad-1a2f56be6619"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","content_references":[],"citations":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"2e26926d-bf2f-44a9-a7d1-2b3a0f245601","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"2e26926d-bf2f-44a9-a7d1-2b3a0f245601","children":["fb93da24-ff0e-4f20-805d-37c05f230dda"]},"fb93da24-ff0e-4f20-805d-37c05f230dda":{"id":"fb93da24-ff0e-4f20-805d-37c05f230dda","message":{"id":"fb93da24-ff0e-4f20-805d-37c05f230dda","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310325.617008,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 2m 16s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":136,"request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"170219a7-8ce0-4864-878a-badc889d39d5","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"170219a7-8ce0-4864-878a-badc889d39d5","children":["44937266-cfe3-4615-a0ba-87524c0216dd"]},"44937266-cfe3-4615-a0ba-87524c0216dd":{"id":"44937266-cfe3-4615-a0ba-87524c0216dd","message":{"id":"44937266-cfe3-4615-a0ba-87524c0216dd","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1762310325.232095,"update_time":1762310374.962651,"content":{"content_type":"text","parts":["Here\u2019s a concise map of serious **non-Transformer** contenders for LLMs (text) and for **agents** (decision-making/robotics), plus which ones look most promising.\n\n# 1) Alternatives for LLM backbones (text)\n\n**A. State-Space Models (SSMs)** \u2014 *S4/S5/H3 \u2192 Mamba/Mamba-2*  \n- **What they are:** Continuous-time/state dynamical layers that scan sequences with **O(L)** compute and small memory, designed to retain long-range signal without quadratic attention. S4 (2021) kicked this off; S5 simplified/strengthened it; H3 tailored SSMs for language. Mamba adds *selectivity* (inputs modulate the SSM parameters) and a hardware-aware scan; **Mamba-2** (state-space duality) further speeds training/inference. \ue200cite\ue202turn2search0\ue202turn2search1\ue202turn2search2\ue202turn0search0\ue202turn9search7\ue201  \n- **Evidence at scale / production:** AI21\u2019s **Jamba** (hybrid SSM+Transformer+MoE) runs in production (Bedrock), supports **256K** context, and demonstrates high throughput/low memory. Independent Mamba-2 write-ups and head-to-heads show competitive perplexity and 2\u20138\u00d7 speed advantages for the core layer. \ue200cite\ue202turn0search9\ue202turn0search14\ue202turn0search4\ue202turn9search4\ue202turn9search7\ue201  \n- **Where it shines / gaps:** Excellent **long-context** efficiency and batchable inference (no KV cache); historically weaker on **content-based retrieval** vs full attention, but hybrids and improved SSMs close the gap. \ue200cite\ue202turn0search0\ue201\n\n**B. Retentive Networks (RetNet)**  \n- **What they are:** Replace attention with a **retention** operator that has **parallel**, **recurrent** (O(1) memory) and **chunk-recurrent** forms\u2014parallelizable training, cheap inference, efficient long-sequence modeling. \ue200cite\ue202turn0search1\ue202turn0search6\ue201  \n- **Why it matters:** The O(1) recurrent form gives very low-latency decoding and small memory footprints; chunk-recurrent adds linear-time long-context. Multiple reports show Transformer-comparable language modeling with better deployment properties. \ue200cite\ue202turn0search6\ue201\n\n**C. RWKV (recurrent + parallel-trainable)**  \n- **What it is:** A dense **RNN** with Transformer-style training (parallelizable) and **constant-memory** inference. Demonstrated up to **14B** parameters in the paper, on-par with Transformers of similar size. \ue200cite\ue202turn0search3\ue202turn0search18\ue202turn0search8\ue201  \n- **Trade-offs:** Practical, simple inference; ecosystem smaller than Transformers; top-tier long-context/retrieval still evolving. \ue200cite\ue202turn0search3\ue201\n\n**D. Long-convolution families (Hyena)**  \n- **What it is:** Implicit long convolutions + multiplicative gating; **subquadratic** and very fast at long L; **HyenaDNA** reaches **1M-token** genomic contexts with strong speedups. \ue200cite\ue202turn0search2\ue202turn0search7\ue202turn0search12\ue202turn5search0\ue202turn5search3\ue201  \n- **Reality check:** Extremely efficient for very long sequences; less common as a full LLM backbone vs SSM/retention, but proven in domains needing huge context. \ue200cite\ue202turn5search6\ue201\n\n**E. Linear-attention RNNs (GLA, DeltaNet, Gated-DeltaNet)**  \n- **What they are:** Linear-time \u201cattention-as-RNN\u201d with gating; **DeltaNet** (delta-rule update) scales well and, at **1.3B**/100B tokens, **outperformed Mamba/GLA** in perplexity and zero-shot in one study; **Gated-DeltaNet** improves further and works well in hybrids. \ue200cite\ue202turn5search1\ue202turn4search2\ue202turn4search0\ue202turn4search9\ue201  \n- **Caveat:** Strong mid-scale results; retrieval-heavy tasks still favor some attention or hybrid layers. \ue200cite\ue202turn4search2\ue201\n\n**F. Spiking LLMs (SNNs)**  \n- **What they are:** Event-driven neurons for extreme energy efficiency; **SpikeGPT/SpikeLLM** scale spiking models for language; promising on neuromorphic hardware but early for frontier LLM quality. \ue200cite\ue202turn7search1\ue202turn7search0\ue201\n\n**G. Liquid Neural Networks (continuous-time RNNs)**  \n- **What they are:** **LTC/Liquid** nets with provable properties, very robust in time-series/control; compelling for **on-device** and adaptive agents, less evidence for frontier text LLMs. \ue200cite\ue202turn1search4\ue202turn1search1\ue201\n\n\n# 2) Non-Transformer architectures for **agents** (decision-making/robotics)\n\nThese are about *acting* and *planning*, not just predicting the next token.\n\n- **World-model agents (RSSM / Dreamer family)**: Learn a compact latent dynamics model and plan/act in latent space; excellent **sample-efficiency** and robustness; many recent variants remove pixel reconstruction or add regularizers. \ue200cite\ue202turn8search10\ue202turn8search5\ue201  \n- **Planning with learned models (MuZero-style)**: Train a policy + value + dynamics and plan with MCTS; remains a gold standard where search is feasible. \ue200cite\ue202turn8search11\ue202turn8search1\ue201  \n- **Diffusion policies (robotics)**: Policies as diffusion processes; strong empirical performance; recent methods combine diffusion with RL or flow-matching for faster sampling. \ue200cite\ue202turn8search7\ue202turn8search22\ue202turn6search7\ue201  \n- **Differentiable planners (VIN/GVIN)**: Embed value-iteration-like operators inside nets for tasks that need explicit planning priors. \ue200cite\ue202turn8search3\ue202turn8search8\ue201  \n\n> **Note:** Agent stacks increasingly mix a text backbone (often still a Transformer) with a non-Transformer **controller** (e.g., RSSM) for action and memory. That\u2019s a different axis than \u201cLLM backbone replacement\u201d.  \n\n\n# What looks most promising (now \u2192 2 years)\n\n**1) Mamba-2 & SSM-heavy **hybrids****  \n- **Why:** Best balance of **throughput**, **memory**, and **long-context** today; actual production deployment (Jamba on Bedrock) is a strong market signal; duality theory (SSM\u2194attention) gives a healthy path to keep closing quality gaps while retaining linear-time behavior. Use SSM scan for most layers, sprinkle attention for global retrieval. \ue200cite\ue202turn0search14\ue202turn0search4\ue202turn9search7\ue201\n\n**2) RetNet (retention) for low-latency, low-RAM decoding**  \n- **Why:** True **O(1)** recurrent inference and chunkwise training make it appealing for **on-device** and **serving-cost**-constrained deployments; language modeling results are already competitive. Expect hybrids (a few attention layers) to cover retrieval. \ue200cite\ue202turn0search6\ue201\n\n**3) DeltaNet/GLA for linear-time \u201cattention-like\u201d stacks**  \n- **Why:** Recent large-token training shows **better perplexity than Mamba** at matched scale, and hybrids with a small amount of attention work very well. If you need attention-style inductive bias but must keep **O(L)** costs, this family is rising fast. \ue200cite\ue202turn4search2\ue201\n\n**4) Hyena for ultra-long context niches**  \n- **Why:** If your workload *must* operate at **100K\u20131M** tokens routinely (genomics, giant documents), Hyena-style long convolutions are extremely fast and competitive. Less common as a general LLM backbone, but strategically valuable. \ue200cite\ue202turn0search12\ue201\n\n**5) For agents:** **RSSM/Dreamer**-style world models + task-specific planners  \n- **Why:** They\u2019re architecture-agnostic to the text backbone and already deliver state-of-the-art **control** and **sample efficiency**; pair well with any perception LM. Diffusion policies are complementary where demonstrations are plentiful. \ue200cite\ue202turn8search5\ue202turn8search7\ue201\n\n\n# Practical guidance (what I\u2019d build/test)\n\n- **General-purpose LLM:** a **hybrid** stack (e.g., 70\u201380% Mamba-2/SSM layers, 20\u201330% attention), **MoE** for capacity, and retrieval. This matches what AI21 found works at scale (Jamba 1.5), and sidesteps SSMs\u2019 worst-case retrieval weaknesses. \ue200cite\ue202turn0search4\ue201  \n- **Latency-sensitive / edge:** try **pure RetNet** or **Mamba-2-heavy** with small attention islands; these minimize KV-cache and RAM while keeping quality usable. \ue200cite\ue202turn0search6\ue201  \n- **Million-token contexts:** evaluate **Hyena** blocks or SSM layers tuned for length; don\u2019t rely on attention alone unless you adopt aggressive sparsity tricks. \ue200cite\ue202turn0search12\ue201  \n- **Agents/robotics:** decouple **reasoning** (language) from **acting** (world model + planner). Start with **Dreamer/RSSM** or diffusion policy for the controller; use the LM (whatever backbone) for high-level goals/tool use. \ue200cite\ue202turn8search5\ue202turn8search7\ue201\n\n\n# Bottom line\n\n- For **LLM backbones**, the clear near-term winners are **SSM-centric designs (Mamba-2)** and **retention (RetNet)**\u2014often **hybridized with a little attention**. They deliver **linear-time** scaling, strong long-context behavior, and real-world viability. \ue200cite\ue202turn9search7\ue202turn0search6\ue201  \n- For **agents**, **world-models (RSSM/Dreamer)** and **diffusion policies** are the most robust non-Transformer choices; they solve different problems than text modeling and integrate cleanly with any LM backbone. \ue200cite\ue202turn8search5\ue202turn8search7\ue201\n\nIf you want, I can sketch concrete ablation plans (small-scale first) to compare: **pure Mamba-2 vs RetNet vs DeltaNet**, plus a **hybrid**, all trained on the same corpus with identical token budgets and long-context evals."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":"\ue200cite\ue202turn2search0\ue202turn2search1\ue202turn2search2\ue202turn0search0\ue202turn9search7\ue201","start_idx":705,"end_idx":776,"safe_urls":["https://arxiv.org/abs/2111.00396?utm_source=chatgpt.com","https://arxiv.org/abs/2208.04933?utm_source=chatgpt.com","https://arxiv.org/abs/2212.14052?utm_source=chatgpt.com","https://arxiv.org/abs/2312.00752?utm_source=chatgpt.com","https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2111.00396?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Efficiently Modeling Long Sequences with Structured State Spaces","url":"https://arxiv.org/abs/2111.00396?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Simplified State Space Layers for Sequence Modeling","url":"https://arxiv.org/abs/2208.04933?utm_source=chatgpt.com","pub_date":null,"snippet":"by JTH Smith \u00b7 2022 \u00b7 Cited by 940 \u2014 We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single ...","attribution":"arXiv"},{"title":"Towards Language Modeling with State Space Models","url":"https://arxiv.org/abs/2212.14052?utm_source=chatgpt.com","pub_date":null,"snippet":"by DY Fu \u00b7 2022 \u00b7 Cited by 708 \u2014 To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches ...","attribution":"arXiv"},{"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","url":"https://arxiv.org/abs/2312.00752?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"},{"title":"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality","url":"https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":0},{"turn_index":2,"ref_type":"search","ref_index":1},{"turn_index":2,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":0},{"turn_index":9,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search9\ue202turn0search14\ue202turn0search4\ue202turn9search4\ue202turn9search7\ue201","start_idx":1087,"end_idx":1159,"safe_urls":["https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/?utm_source=chatgpt.com","https://tridao.me/blog/2024/mamba2-part1-model/?utm_source=chatgpt.com","https://www.ai21.com/blog/announcing-jamba/?utm_source=chatgpt.com","https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com"],"refs":[],"alt":"([AI21](https://www.ai21.com/blog/announcing-jamba/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Introducing Jamba: AI21's Groundbreaking SSM- ...","url":"https://www.ai21.com/blog/announcing-jamba/?utm_source=chatgpt.com","pub_date":1711618754.0,"snippet":"Composed of Transformer, Mamba, and mixture-of-experts (MoE) layers, Jamba optimizes for memory, throughput, and performance\u2014all at once ...","attribution_segments":null,"supporting_websites":[{"title":"Jamba 1.5 family of models by AI21 Labs is now available ...","url":"https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/?utm_source=chatgpt.com","pub_date":null,"snippet":"23 Sept 2024 \u2014 Jamba 1.5 models leverage a unique hybrid architecture that combines the transformer model architecture with Structured State Space model (SSM) ...","attribution":"Amazon Web Services, Inc."},{"title":"Attention was never enough: Tracing the rise of hybrid LLMs","url":"https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com","pub_date":null,"snippet":"5 Aug 2025 \u2014 Explore how hybrid LLMs combining Mamba and Transformer architectures are reshaping enterprise AI\u2014from Mamba's debut to state-of-the-art ...","attribution":"AI21"},{"title":"State Space Duality (Mamba-2) Part I - The Model","url":"https://tridao.me/blog/2024/mamba2-part1-model/?utm_source=chatgpt.com","pub_date":null,"snippet":"31 May 2024 \u2014 The main point of the Mamba-2 paper is what we call structured state space duality (SSD), which refers to several things.","attribution":"tridao.me"},{"title":"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality","url":"https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":9},{"turn_index":0,"ref_type":"search","ref_index":14},{"turn_index":0,"ref_type":"search","ref_index":4},{"turn_index":9,"ref_type":"search","ref_index":4},{"turn_index":9,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"AI21"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue201","start_idx":1384,"end_idx":1403,"safe_urls":["https://arxiv.org/abs/2312.00752?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2312.00752?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","url":"https://arxiv.org/abs/2312.00752?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search6\ue201","start_idx":1668,"end_idx":1700,"safe_urls":["https://arxiv.org/abs/2307.08621?utm_source=chatgpt.com","https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2307.08621?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Retentive Network: A Successor to Transformer for Large Language Models","url":"https://arxiv.org/abs/2307.08621?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Retentive Network","url":"https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","pub_date":null,"snippet":"by Y Sun \u00b7 2023 \u00b7 Cited by 568 \u2014 In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long- sequence modeling, Transformer-comparable ...","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search6\ue201","start_idx":1955,"end_idx":1974,"safe_urls":["https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Retentive Network","url":"https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","pub_date":1691626546.0,"snippet":"by Y Sun \u00b7 2023 \u00b7 Cited by 568 \u2014 In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long- sequence modeling, Transformer-comparable ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search18\ue202turn0search8\ue201","start_idx":2235,"end_idx":2281,"safe_urls":["https://aclanthology.org/2023.findings-emnlp.936/?utm_source=chatgpt.com","https://arxiv.org/abs/2305.13048?utm_source=chatgpt.com","https://openreview.net/pdf?id=7SaXczaBpG&utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2305.13048?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"RWKV: Reinventing RNNs for the Transformer Era","url":"https://arxiv.org/abs/2305.13048?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"RWKV: Reinventing RNNs for the Transformer Era","url":"https://openreview.net/pdf?id=7SaXczaBpG&utm_source=chatgpt.com","pub_date":null,"snippet":"by B Peng \u00b7 Cited by 807 \u2014 Experiments suggest RWKV ad- dresses scaling and deployment challenges in AI, especially for sequential data processing, pointing towards more sustainable and ...","attribution":"OpenReview"},{"title":"RWKV: Reinventing RNNs for the Transformer Era","url":"https://aclanthology.org/2023.findings-emnlp.936/?utm_source=chatgpt.com","pub_date":null,"snippet":"by B Peng \u00b7 2023 \u00b7 Cited by 807 \u2014 We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the ...","attribution":"ACL Anthology"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":18},{"turn_index":0,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue201","start_idx":2416,"end_idx":2435,"safe_urls":["https://arxiv.org/abs/2305.13048?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2305.13048?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"RWKV: Reinventing RNNs for the Transformer Era","url":"https://arxiv.org/abs/2305.13048?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search2\ue202turn0search7\ue202turn0search12\ue202turn5search0\ue202turn5search3\ue201","start_idx":2665,"end_idx":2737,"safe_urls":["https://arxiv.org/abs/2302.10866?utm_source=chatgpt.com","https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com","https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna?utm_source=chatgpt.com","https://papers.neurips.cc/paper_files/paper/2023/file/86ab6927ee4ae9bde4247793c46797c7-Paper-Conference.pdf?utm_source=chatgpt.com","https://proceedings.mlr.press/v202/poli23a/poli23a.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2302.10866?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Hyena Hierarchy: Towards Larger Convolutional ...","url":"https://arxiv.org/abs/2302.10866?utm_source=chatgpt.com","pub_date":null,"snippet":"by M Poli \u00b7 2023 \u00b7 Cited by 473 \u2014 We propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled ...","attribution_segments":null,"supporting_websites":[{"title":"Hyena Hierarchy: Towards Larger Convolutional Language ...","url":"https://proceedings.mlr.press/v202/poli23a/poli23a.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by M Poli \u00b7 2023 \u00b7 Cited by 473 \u2014 In the case of Hyena, causality can be guaranteed by parametrizing causal convolutions: Proposition 3.1 (Causal Hyenas). If each filter hn, n = 1,...,N is ...","attribution":"Proceedings of Machine Learning Research"},{"title":"Hyena Hierarchy: Towards Larger Convolutional ...","url":"https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com","pub_date":null,"snippet":"7 Mar 2023 \u2014 The crossover point between highly optimized FlashAttention and Hyena is at about 6k sequence length. At 100k sequence length, Hyenas are 100x ...","attribution":"hazyresearch.stanford.edu"},{"title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","url":"https://papers.neurips.cc/paper_files/paper/2023/file/86ab6927ee4ae9bde4247793c46797c7-Paper-Conference.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by E Nguyen \u00b7 Cited by 480 \u2014 HyenaDNA is a genomic model using Hyena's long-range capabilities, with a simple architecture of Hyena operators, and single nucleotide resolution.","attribution":"NeurIPS Papers"},{"title":"HyenaDNA: learning from DNA with 1 Million token context","url":"https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna?utm_source=chatgpt.com","pub_date":null,"snippet":"29 Jun 2023 \u2014 We're excited to introduce HyenaDNA, a long-range genomic foundation model with context lengths of up to 1 million tokens at single nucleotide resolution!","attribution":"hazyresearch.stanford.edu"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":7},{"turn_index":0,"ref_type":"search","ref_index":12},{"turn_index":5,"ref_type":"search","ref_index":0},{"turn_index":5,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn5search6\ue201","start_idx":2903,"end_idx":2922,"safe_urls":["https://arxiv.org/abs/2306.15794?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2306.15794?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","url":"https://arxiv.org/abs/2306.15794?utm_source=chatgpt.com","pub_date":null,"snippet":"by E Nguyen \u00b7 2023 \u00b7 Cited by 480 \u2014 We present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":5,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn5search1\ue202turn4search2\ue202turn4search0\ue202turn4search9\ue201","start_idx":3259,"end_idx":3317,"safe_urls":["https://arxiv.org/abs/2312.06635?utm_source=chatgpt.com","https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com","https://arxiv.org/pdf/2406.06484?utm_source=chatgpt.com","https://jankautz.com/publications/GatedDeltaNet_ICLR25.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2312.06635?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Gated Linear Attention Transformers with Hardware-Efficient Training","url":"https://arxiv.org/abs/2312.06635?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Parallelizing Linear Transformers with the Delta Rule over Sequence Length","url":"https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"},{"title":"Parallelizing Linear Transformers with the Delta Rule over ...","url":"https://arxiv.org/pdf/2406.06484?utm_source=chatgpt.com","pub_date":null,"snippet":"by S Yang \u00b7 2024 \u00b7 Cited by 136 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient ...","attribution":"arXiv"},{"title":"GATED DELTA NETWORKS: IMPROVING MAMBA2 WITH ...","url":"https://jankautz.com/publications/GatedDeltaNet_ICLR25.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by S Yang \u00b7 Cited by 81 \u2014 Our resulting architecture, Gated DeltaNet, consistently outperforms both Mamba2 and DeltaNet across a comprehensive suite of benchmarks, including language ...","attribution":"jankautz.com"}],"refs":[{"turn_index":5,"ref_type":"search","ref_index":1},{"turn_index":4,"ref_type":"search","ref_index":2},{"turn_index":4,"ref_type":"search","ref_index":0},{"turn_index":4,"ref_type":"search","ref_index":9}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search2\ue201","start_idx":3427,"end_idx":3446,"safe_urls":["https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Parallelizing Linear Transformers with the Delta Rule over Sequence Length","url":"https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn7search1\ue202turn7search0\ue201","start_idx":3678,"end_idx":3710,"safe_urls":["https://arxiv.org/abs/2302.13939?utm_source=chatgpt.com","https://arxiv.org/abs/2407.04752?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2302.13939?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"SpikeGPT: Generative Pre-trained Language Model with ...","url":"https://arxiv.org/abs/2302.13939?utm_source=chatgpt.com","pub_date":null,"snippet":"by RJ Zhu \u00b7 2023 \u00b7 Cited by 169 \u2014 SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.","attribution_segments":null,"supporting_websites":[{"title":"SpikeLLM: Scaling up Spiking Neural Network to Large ...","url":"https://arxiv.org/abs/2407.04752?utm_source=chatgpt.com","pub_date":null,"snippet":"by X Xing \u00b7 2024 \u00b7 Cited by 25 \u2014 We propose the first spiking large language model, SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spike training ...","attribution":"arXiv"}],"refs":[{"turn_index":7,"ref_type":"search","ref_index":1},{"turn_index":7,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search4\ue202turn1search1\ue201","start_idx":3958,"end_idx":3990,"safe_urls":["https://arxiv.org/abs/2006.04439?utm_source=chatgpt.com","https://ojs.aaai.org/index.php/AAAI/article/view/16936/16743?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2006.04439?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Liquid Time-constant Networks","url":"https://arxiv.org/abs/2006.04439?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Liquid Time-constant Networks","url":"https://ojs.aaai.org/index.php/AAAI/article/view/16936/16743?utm_source=chatgpt.com","pub_date":null,"snippet":"by R Hasani \u00b7 2021 \u00b7 Cited by 420 \u2014 We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dy- namics by implicit nonlinearities, we ...","attribution":"ojs.aaai.org"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":4},{"turn_index":1,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn8search10\ue202turn8search5\ue201","start_idx":4385,"end_idx":4418,"safe_urls":["https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","https://vitalab.github.io/article/2023/01/19/DreamerV3.html?utm_source=chatgpt.com"],"refs":[],"alt":"([vitalab.github.io](https://vitalab.github.io/article/2023/01/19/DreamerV3.html?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"DreamerV3: Mastering Diverse Domains through World Models","url":"https://vitalab.github.io/article/2023/01/19/DreamerV3.html?utm_source=chatgpt.com","pub_date":null,"snippet":"19 Jan 2023 \u2014 The world model, dubbed the \u201cRecurrent State-Space Model (RSSM)\u201d can be described by the following networks: Note that the encoder and the ...","attribution_segments":null,"supporting_websites":[{"title":"Learning Predictive World Models without Reconstruction","url":"https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","pub_date":null,"snippet":"23 May 2024 \u2014 The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using ...","attribution":"arXiv"}],"refs":[{"turn_index":8,"ref_type":"search","ref_index":10},{"turn_index":8,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"vitalab.github.io"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn8search11\ue202turn8search1\ue201","start_idx":4576,"end_idx":4609,"safe_urls":["https://arxiv.org/pdf/1911.08265?utm_source=chatgpt.com","https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/pdf/1911.08265?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering Atari, Go, Chess and Shogi by Planning with a ...","url":"https://arxiv.org/pdf/1911.08265?utm_source=chatgpt.com","pub_date":1582508940.0,"snippet":"by J Schrittwieser \u00b7 2019 \u00b7 Cited by 3275 \u2014 In this paper, we introduce MuZero, a new approach to model-based RL that achieves state-of-the-art per- formance in Atari 2600, a visually ...","attribution_segments":null,"supporting_websites":[{"title":"MuZero: Mastering Go, chess, shogi and Atari without rules","url":"https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/?utm_source=chatgpt.com","pub_date":null,"snippet":"23 Dec 2020 \u2014 MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.","attribution":"Google DeepMind"}],"refs":[{"turn_index":8,"ref_type":"search","ref_index":11},{"turn_index":8,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn8search7\ue202turn8search22\ue202turn6search7\ue201","start_idx":4791,"end_idx":4837,"safe_urls":["https://aclanthology.org/2024.findings-emnlp.497.pdf?utm_source=chatgpt.com","https://bair.berkeley.edu/blog/2023/07/14/ddpo/?utm_source=chatgpt.com","https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([roboticsproceedings.org](https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Diffusion Policy:","url":"https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by C Chi \u2014 Abstract\u2014This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a ...","attribution_segments":null,"supporting_websites":[{"title":"Training Diffusion Models with Reinforcement Learning","url":"https://bair.berkeley.edu/blog/2023/07/14/ddpo/?utm_source=chatgpt.com","pub_date":null,"snippet":"14 Jul 2023 \u2014 In this post, we show how diffusion models can be trained on these downstream objectives directly using reinforcement learning (RL).","attribution":"bair.berkeley.edu"},{"title":"Enable Fast Sampling for Seq2Seq Text Diffusion","url":"https://aclanthology.org/2024.findings-emnlp.497.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by P Liu \u00b7 2024 \u00b7 Cited by 1 \u2014 In this paper, we propose an efficient model FMSeq1, which utilizes flow matching to straighten the gen- eration path, thereby enabling fast ...","attribution":"ACL Anthology"}],"refs":[{"turn_index":8,"ref_type":"search","ref_index":7},{"turn_index":8,"ref_type":"search","ref_index":22},{"turn_index":6,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"roboticsproceedings.org"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn8search3\ue202turn8search8\ue201","start_idx":4977,"end_idx":5009,"safe_urls":["https://arxiv.org/abs/1602.02867?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper/2016/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1602.02867?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Value Iteration Networks","url":"https://arxiv.org/abs/1602.02867?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Value Iteration Networks","url":"https://proceedings.neurips.cc/paper/2016/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Tamar \u00b7 2016 \u00b7 Cited by 836 \u2014 We introduce the value iteration network (VIN): a fully differentiable neural net- work with a 'planning module' embedded within. VINs can learn to plan, ...","attribution":"NeurIPS Proceedings"}],"refs":[{"turn_index":8,"ref_type":"search","ref_index":3},{"turn_index":8,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search14\ue202turn0search4\ue202turn9search7\ue201","start_idx":5668,"end_idx":5714,"safe_urls":["https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/?utm_source=chatgpt.com","https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com"],"refs":[],"alt":"([Amazon Web Services, Inc.](https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Jamba 1.5 family of models by AI21 Labs is now available ...","url":"https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/?utm_source=chatgpt.com","pub_date":1727114358.0,"snippet":"23 Sept 2024 \u2014 Jamba 1.5 models leverage a unique hybrid architecture that combines the transformer model architecture with Structured State Space model (SSM) ...","attribution_segments":null,"supporting_websites":[{"title":"Attention was never enough: Tracing the rise of hybrid LLMs","url":"https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com","pub_date":null,"snippet":"5 Aug 2025 \u2014 Explore how hybrid LLMs combining Mamba and Transformer architectures are reshaping enterprise AI\u2014from Mamba's debut to state-of-the-art ...","attribution":"AI21"},{"title":"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality","url":"https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":14},{"turn_index":0,"ref_type":"search","ref_index":4},{"turn_index":9,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"Amazon Web Services, Inc."}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search6\ue201","start_idx":6039,"end_idx":6058,"safe_urls":["https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Retentive Network","url":"https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","pub_date":1691626546.0,"snippet":"by Y Sun \u00b7 2023 \u00b7 Cited by 568 \u2014 In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long- sequence modeling, Transformer-comparable ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search2\ue201","start_idx":6380,"end_idx":6399,"safe_urls":["https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Parallelizing Linear Transformers with the Delta Rule over Sequence Length","url":"https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search12\ue201","start_idx":6683,"end_idx":6703,"safe_urls":["https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com"],"refs":[],"alt":"([hazyresearch.stanford.edu](https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Hyena Hierarchy: Towards Larger Convolutional ...","url":"https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com","pub_date":null,"snippet":"7 Mar 2023 \u2014 The crossover point between highly optimized FlashAttention and Hyena is at about 6k sequence length. At 100k sequence length, Hyenas are 100x ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"hazyresearch.stanford.edu"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn8search5\ue202turn8search7\ue201","start_idx":7032,"end_idx":7064,"safe_urls":["https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Learning Predictive World Models without Reconstruction","url":"https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","pub_date":null,"snippet":"23 May 2024 \u2014 The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using ...","attribution_segments":null,"supporting_websites":[{"title":"Diffusion Policy:","url":"https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by C Chi \u2014 Abstract\u2014This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a ...","attribution":"roboticsproceedings.org"}],"refs":[{"turn_index":8,"ref_type":"search","ref_index":5},{"turn_index":8,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search4\ue201","start_idx":7357,"end_idx":7376,"safe_urls":["https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com"],"refs":[],"alt":"([AI21](https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Attention was never enough: Tracing the rise of hybrid LLMs","url":"https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com","pub_date":1754396683.0,"snippet":"5 Aug 2025 \u2014 Explore how hybrid LLMs combining Mamba and Transformer architectures are reshaping enterprise AI\u2014from Mamba's debut to state-of-the-art ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"AI21"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search6\ue201","start_idx":7544,"end_idx":7563,"safe_urls":["https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Retentive Network","url":"https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","pub_date":1691626546.0,"snippet":"by Y Sun \u00b7 2023 \u00b7 Cited by 568 \u2014 In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long- sequence modeling, Transformer-comparable ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search12\ue201","start_idx":7729,"end_idx":7749,"safe_urls":["https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com"],"refs":[],"alt":"([hazyresearch.stanford.edu](https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Hyena Hierarchy: Towards Larger Convolutional ...","url":"https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com","pub_date":null,"snippet":"7 Mar 2023 \u2014 The crossover point between highly optimized FlashAttention and Hyena is at about 6k sequence length. At 100k sequence length, Hyenas are 100x ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"hazyresearch.stanford.edu"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn8search5\ue202turn8search7\ue201","start_idx":7980,"end_idx":8012,"safe_urls":["https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Learning Predictive World Models without Reconstruction","url":"https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","pub_date":null,"snippet":"23 May 2024 \u2014 The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using ...","attribution_segments":null,"supporting_websites":[{"title":"Diffusion Policy:","url":"https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by C Chi \u2014 Abstract\u2014This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a ...","attribution":"roboticsproceedings.org"}],"refs":[{"turn_index":8,"ref_type":"search","ref_index":5},{"turn_index":8,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn9search7\ue202turn0search6\ue201","start_idx":8288,"end_idx":8320,"safe_urls":["https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality","url":"https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Retentive Network","url":"https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","pub_date":null,"snippet":"by Y Sun \u00b7 2023 \u00b7 Cited by 568 \u2014 In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long- sequence modeling, Transformer-comparable ...","attribution":"arXiv"}],"refs":[{"turn_index":9,"ref_type":"search","ref_index":7},{"turn_index":0,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn8search5\ue202turn8search7\ue201","start_idx":8538,"end_idx":8570,"safe_urls":["https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Learning Predictive World Models without Reconstruction","url":"https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","pub_date":null,"snippet":"23 May 2024 \u2014 The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using ...","attribution_segments":null,"supporting_websites":[{"title":"Diffusion Policy:","url":"https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by C Chi \u2014 Abstract\u2014This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a ...","attribution":"roboticsproceedings.org"}],"refs":[{"turn_index":8,"ref_type":"search","ref_index":5},{"turn_index":8,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":" ","start_idx":8796,"end_idx":8796,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"Efficiently Modeling Long Sequences with Structured State Spaces","url":"https://arxiv.org/abs/2111.00396?utm_source=chatgpt.com","attribution":"arXiv"},{"title":"Introducing Jamba: AI21's Groundbreaking SSM- ...","url":"https://www.ai21.com/blog/announcing-jamba/?utm_source=chatgpt.com","attribution":"AI21"},{"title":"DreamerV3: Mastering Diverse Domains through World Models","url":"https://vitalab.github.io/article/2023/01/19/DreamerV3.html?utm_source=chatgpt.com","attribution":"vitalab.github.io"},{"title":"Diffusion Policy:","url":"https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com","attribution":"roboticsproceedings.org"},{"title":"Jamba 1.5 family of models by AI21 Labs is now available ...","url":"https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/?utm_source=chatgpt.com","attribution":"Amazon Web Services, Inc."},{"title":"Hyena Hierarchy: Towards Larger Convolutional ...","url":"https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com","attribution":"hazyresearch.stanford.edu"}],"has_images":false}],"safe_urls":["https://arxiv.org/abs/2111.00396?utm_source=chatgpt.com","https://arxiv.org/abs/2208.04933?utm_source=chatgpt.com","https://arxiv.org/abs/2212.14052?utm_source=chatgpt.com","https://arxiv.org/abs/2312.00752?utm_source=chatgpt.com","https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/?utm_source=chatgpt.com","https://tridao.me/blog/2024/mamba2-part1-model/?utm_source=chatgpt.com","https://www.ai21.com/blog/announcing-jamba/?utm_source=chatgpt.com","https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com","https://arxiv.org/abs/2312.00752?utm_source=chatgpt.com","https://arxiv.org/abs/2307.08621?utm_source=chatgpt.com","https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","https://aclanthology.org/2023.findings-emnlp.936/?utm_source=chatgpt.com","https://arxiv.org/abs/2305.13048?utm_source=chatgpt.com","https://openreview.net/pdf?id=7SaXczaBpG&utm_source=chatgpt.com","https://arxiv.org/abs/2305.13048?utm_source=chatgpt.com","https://arxiv.org/abs/2302.10866?utm_source=chatgpt.com","https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com","https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna?utm_source=chatgpt.com","https://papers.neurips.cc/paper_files/paper/2023/file/86ab6927ee4ae9bde4247793c46797c7-Paper-Conference.pdf?utm_source=chatgpt.com","https://proceedings.mlr.press/v202/poli23a/poli23a.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2306.15794?utm_source=chatgpt.com","https://arxiv.org/abs/2312.06635?utm_source=chatgpt.com","https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com","https://arxiv.org/pdf/2406.06484?utm_source=chatgpt.com","https://jankautz.com/publications/GatedDeltaNet_ICLR25.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com","https://arxiv.org/abs/2302.13939?utm_source=chatgpt.com","https://arxiv.org/abs/2407.04752?utm_source=chatgpt.com","https://arxiv.org/abs/2006.04439?utm_source=chatgpt.com","https://ojs.aaai.org/index.php/AAAI/article/view/16936/16743?utm_source=chatgpt.com","https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","https://vitalab.github.io/article/2023/01/19/DreamerV3.html?utm_source=chatgpt.com","https://arxiv.org/pdf/1911.08265?utm_source=chatgpt.com","https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/?utm_source=chatgpt.com","https://aclanthology.org/2024.findings-emnlp.497.pdf?utm_source=chatgpt.com","https://bair.berkeley.edu/blog/2023/07/14/ddpo/?utm_source=chatgpt.com","https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/1602.02867?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper/2016/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","https://aws.amazon.com/blogs/aws/jamba-1-5-family-of-models-by-ai21-labs-is-now-available-in-amazon-bedrock/?utm_source=chatgpt.com","https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com","https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","https://arxiv.org/abs/2406.06484?utm_source=chatgpt.com","https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com","https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com","https://www.ai21.com/blog/rise-of-hybrid-llms/?utm_source=chatgpt.com","https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","https://hazyresearch.stanford.edu/blog/2023-03-07-hyena?utm_source=chatgpt.com","https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2405.21060?utm_source=chatgpt.com","https://arxiv.org/pdf/2307.08621?utm_source=chatgpt.com","https://arxiv.org/html/2405.15083v1?utm_source=chatgpt.com","https://www.roboticsproceedings.org/rss19/p026.pdf?utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s42256-022-00556-7?utm_source=chatgpt.com","title":"Closed-form continuous-time neural networks","snippet":"by R Hasani \u00b7 2022 \u00b7 Cited by 170 \u2014 Continuous-time neural networks are a class of machine learning systems that can tackle representation learning on spatiotemporal decision-making tasks.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"nature.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/2312.00752?utm_source=chatgpt.com","title":"Mamba: Linear-Time Sequence Modeling with Selective ...","snippet":"31 May 2024 \u2014 Mamba is a neural network architecture using selective state space models, integrated without attention or MLP blocks, to model sequences.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.00752v2?utm_source=chatgpt.com","title":"Linear-Time Sequence Modeling with Selective State Spaces","snippet":"We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2302.10866?utm_source=chatgpt.com","title":"Hyena Hierarchy","snippet":"by M Poli \u00b7 2023 \u00b7 Cited by 469 \u2014 In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2309.11523v5?utm_source=chatgpt.com","title":"RMT: Retentive Networks Meet Vision Transformers","snippet":"We draw inspiration from the recent Retentive Network (RetNet) in the field of NLP, and propose RMT, a strong vision backbone with explicit spatial prior for ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2208.04933?utm_source=chatgpt.com","title":"Simplified State Space Layers for Sequence Modeling","snippet":"by JTH Smith \u00b7 2022 \u00b7 Cited by 940 \u2014 We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2212.14052?utm_source=chatgpt.com","title":"Towards Language Modeling with State Space Models","snippet":"by DY Fu \u00b7 2022 \u00b7 Cited by 708 \u2014 To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2503.18970?utm_source=chatgpt.com","title":"A Survey on Structured State Space Sequence (S4) Models","snippet":"by S Somvanshi \u00b7 2025 \u00b7 Cited by 5 \u2014 This survey provides a comprehensive review of SSMs, tracing their evolution from the foundational. S4 model to its successors like Mamba, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2406.09477?utm_source=chatgpt.com","title":"[2406.09477] Q-S5: Towards Quantized State Space Models","snippet":"by S Abreu \u00b7 2024 \u00b7 Cited by 11 \u2014 This paper investigates the effect of quantization on the S5 model to understand its impact on model performance and to facilitate its deployment to edge and ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2111.00396?utm_source=chatgpt.com","title":"Efficiently Modeling Long Sequences with Structured State ...","snippet":"S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91% accuracy on sequential CIFAR-10 with no data ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2212.14052?utm_source=chatgpt.com","title":"Towards Language Modeling with State Space Models","snippet":"by DY Fu \u00b7 2022 \u00b7 Cited by 708 \u2014 To probe how important these skills are for language modeling, we propose H3 (Hungry Hungry Hippo), a new SSM-based layer designed to solve ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2209.10655?utm_source=chatgpt.com","title":"Mega: Moving Average Equipped Gated Attention","snippet":"by X Ma \u00b7 2022 \u00b7 Cited by 119 \u2014 In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2406.09062v2?utm_source=chatgpt.com","title":"State-Space Modeling in Long Sequence Processing","snippet":"Long Range Arena. The LRA benchmark [35] comprises six tasks with sequence lengths ranging from 1 , 024 1{,}024 to 16 , 384 16{,}384 tokens, as described ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://ar5iv.labs.arxiv.org/html/2209.10655?utm_source=chatgpt.com","title":"[2209.10655] Mega: Moving Average Equipped Gated Attention","snippet":"In this work, we propose a moving average equipped gated attention mechanism (Mega) to solve the two weaknesses simultaneously. The key idea is to incorporate ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2403.15360v2?utm_source=chatgpt.com","title":"SiMBA: Simplified Mamba-based Architecture for Vision ...","snippet":"30 Apr 2024 \u2014 This comprehensive comparative analysis highlights SiMBA's distinctive position in the evolving landscape of transformer architectures. Report ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2406.06484?utm_source=chatgpt.com","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"by S Yang \u00b7 2024 \u00b7 Cited by 136 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2504.13173?utm_source=chatgpt.com","title":"Attentional Bias, Retention, and Online Optimization","snippet":"by A Behrouz \u00b7 2025 \u00b7 Cited by 12 \u2014 In this framework, the memory architecture determines the model capacity to memorize; attentional bias is responsible for modeling the ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2510.20787v2?utm_source=chatgpt.com","title":"Alleviating Forgetfulness of Linear Attention by Hybrid ...","snippet":"24 Oct 2025 \u2014 Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.06457v1?utm_source=chatgpt.com","title":"A Systematic Analysis of Hybrid Linear Attention","snippet":"8 Jul 2025 \u2014 We systematically evaluate various linear attention models across generations\u2014vector recurrences to advanced gating mechanisms\u2014both standalone ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2405.18428?utm_source=chatgpt.com","title":"DiG: Scalable and Efficient Diffusion Models with Gated ...","snippet":"by L Zhu \u00b7 2024 \u00b7 Cited by 27 \u2014 In this paper, we aim to incorporate the sub-quadratic modeling capability of Gated Linear Attention (GLA) into the 2D diffusion backbone.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2205.14217?utm_source=chatgpt.com","title":"Diffusion-LM Improves Controllable Text Generation","snippet":"by XL Li \u00b7 2022 \u00b7 Cited by 1133 \u2014 Diffusion-LM is a non-autoregressive language model using continuous diffusions to enable complex, controllable text generation.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.16907v1?utm_source=chatgpt.com","title":"MambaFlow: A Novel and Flow-guided State Space Model ...","snippet":"24 Feb 2025 \u2014 In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.14988v1?utm_source=chatgpt.com","title":"Efficient Text-to-Image Generation with Iterative Non- ...","snippet":"(1) MaskGIT is proposed for class-based image generation task, while our model focuses on text-based image generation. (2) MaskGIT is a decoder-only structure, ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.00290v1?utm_source=chatgpt.com","title":"DLM-One: Diffusion Language Models for One-Step ...","snippet":"30 May 2025 \u2014 Diffusion-LM improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022a. Li et al. (2023a)See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.10915v1?utm_source=chatgpt.com","title":"M4V: Multi-Modal Mamba for Text-to-Video Generation","snippet":"12 Jun 2025 \u2014 We propose a multi-modal diffusion Mamba (MM-DiM) block that enables seamless integration of multi-modal information and spatiotemporal modeling.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2202.04200?utm_source=chatgpt.com","title":"arXiv:2202.04200v1 [cs.CV] 8 Feb 2022","snippet":"by H Chang \u00b7 2022 \u00b7 Cited by 1022 \u2014 At inference time, MaskGIT adopts a novel non-autoregressive decoding method to synthesize an image in constant number of steps ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2407.04752?utm_source=chatgpt.com","title":"SpikeLLM: Scaling up Spiking Neural Network to Large ...","snippet":"by X Xing \u00b7 2024 \u00b7 Cited by 25 \u2014 We propose the first spiking large language model, SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spike training ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.04752v1?utm_source=chatgpt.com","title":"SpikeLLM: Scaling up Spiking Neural Network to Large ...","snippet":"5 Jul 2024 \u2014 We redesign 7~70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.04752v3?utm_source=chatgpt.com","title":"SpikeLLM: Scaling up Spiking Neural Network to Large ...","snippet":"10 Apr 2025 \u2014 This work aims to scale up SNNs to large language models or even part of the human brain nervous system with 86B neurons. Towards this goal, we ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2302.13939v5?utm_source=chatgpt.com","title":"SpikeGPT: Generative Pre-trained Language Model with ...","snippet":"11 Jul 2024 \u2014 SpikeGPT is the largest backpropagation-trained SNN model when released, rendering it suitable for both the generation and comprehension of natural language.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2405.02543v1?utm_source=chatgpt.com","title":"Exploring Extreme Quantization in Spiking Language Models","snippet":"Our proposed model represents a significant advancement as the first-of-its-kind 1/1.58-bit spiking LM, and its performance is rigorously ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2405.06263v1?utm_source=chatgpt.com","title":"Learning Latent Dynamic Robust Representations for ...","snippet":"10 May 2024 \u2014 We develop a Hybrid Recurrent State-Space Model (HRSSM) structure, enhancing state representation robustness for effective policy learning.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.15799v1?utm_source=chatgpt.com","title":"Steering Your Diffusion Policy with Latent Space ...","snippet":"18 Jun 2025 \u2014 We propose diffusion steering via reinforcement learning (Dsrl): adapting the BC policy by running RL over its latent-noise space.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2205.06175?utm_source=chatgpt.com","title":"[2205.06175] A Generalist Agent","snippet":"by S Reed \u00b7 2022 \u00b7 Cited by 1332 \u2014 The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2306.00840v3?utm_source=chatgpt.com","title":"What model does MuZero learn?","snippet":"18 Aug 2024 \u2014 We study MuZero, a state-of-the-art deep model-based reinforcement learning algorithm that distinguishes itself from existing algorithms by learning a value- ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.23458v1?utm_source=chatgpt.com","title":"Diffusion Guidance Is a Controllable Policy Improvement ...","snippet":"29 May 2025 \u2014 This paper describes work performed at UC Berkeley and is not associated with Amazon. ... In Robotics: Science and Systems (RSS), 2023. da ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2406.07887?utm_source=chatgpt.com","title":"An Empirical Study of Mamba-based Language Models","snippet":"by R Waleffe \u00b7 2024 \u00b7 Cited by 163 \u2014 We present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.19832v1?utm_source=chatgpt.com","title":"ML-Mamba: Efficient Multi-Modal Large Language Model ...","snippet":"29 Jul 2024 \u2014 We introduce ML-Mamba, a multimodal language model that utilizes the latest and efficient Mamba-2 model for inference.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/158ac5698e36a01ee5ca9e6732685b34-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Slot State Space Models","snippet":"by J Jiang \u00b7 2024 \u00b7 Cited by 11 \u2014 Recent State Space Models (SSMs) such as S4, S5, and Mamba have shown re- markable computational benefits in long-range temporal dependency modeling.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/8329a45669017898bb0cc09d27f8d2bb-Paper-Conference.pdf?utm_source=chatgpt.com","title":"MetaLA: Unified Optimal Linear Approximation to Softmax ...","snippet":"by Y CHOU \u00b7 2024 \u00b7 Cited by 18 \u2014 MetaLA is a linear approximation to softmax attention, designed to satisfy dynamic memory, static approximation, and least parameter approximation conditions.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://papers.neurips.cc/paper_files/paper/2023/file/86ab6927ee4ae9bde4247793c46797c7-Paper-Conference.pdf?utm_source=chatgpt.com","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","snippet":"by E Nguyen \u00b7 Cited by 480 \u2014 HyenaDNA is a genomic model using Hyena's long-range capabilities, with a simple architecture of Hyena operators, and single nucleotide resolution.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper/2016/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf?utm_source=chatgpt.com","title":"Value Iteration Networks","snippet":"by A Tamar \u00b7 2016 \u00b7 Cited by 836 \u2014 We introduce the value iteration network (VIN): a fully differentiable neural net- work with a 'planning module' embedded within. VINs can learn to plan, ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Learning Universal Policies via Text-Guided Video ...","snippet":"by Y Du \u00b7 2023 \u00b7 Cited by 338 \u2014 Specifically, we instantiate the text-conditioned video generation model using video diffusion. A set of underlying actions are then regressed from the ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://papers.neurips.cc/paper_files/paper/2021/file/e8258e5140317ff36c7f8225a3bf9590-Paper.pdf?utm_source=chatgpt.com","title":"Online and Offline Reinforcement Learning by Planning ...","snippet":"by J Schrittwieser \u00b7 Cited by 157 \u2014 In this paper we have investigated the Reanalyse algorithm and its applications to both data efficient online RL at any data budget and completely offline RL.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"aaai.org","entries":[{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/16936/16743?utm_source=chatgpt.com","title":"Liquid Time-constant Networks","snippet":"by R Hasani \u00b7 2021 \u00b7 Cited by 420 \u2014 We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dy- namics by implicit nonlinearities, we ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"aaai.org"},{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/16936?utm_source=chatgpt.com","title":"Liquid Time-constant Networks","snippet":"by R Hasani \u00b7 2021 \u00b7 Cited by 430 \u2014 We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dynamics by implicit nonlinearities.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"aaai.org"},{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/32550/34705?utm_source=chatgpt.com","title":"ViG: Linear-complexity Visual Sequence Learning with ...","snippet":"by B Liao \u00b7 2025 \u00b7 Cited by 7 \u2014 In this paper, we introduced ViG, a generic vision back- bone network that introduces Gated Linear Attention (GLA) to the vision field, achieving efficient ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"aaai.org"},{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/28975/29854?utm_source=chatgpt.com","title":"SpikingBERT: Distilling BERT to Train Spiking Language ...","snippet":"by M Bal \u00b7 2024 \u00b7 Cited by 78 \u2014 Spiking LM Training: We theoretically and empiri- cally verify the convergence of our proposed spiking. LM (comprising both linear and non-linear operations) to ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"aaai.org"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/13pbgef/r_rwkv_reinventing_rnns_for_the_transformer_era/?utm_source=chatgpt.com","title":"[R] RWKV: Reinventing RNNs for the Transformer Era","snippet":"We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/LocalLLaMA/comments/152s2ij/retentive_network_a_successor_to_transformer_for/?utm_source=chatgpt.com","title":"A Successor to Transformer for Large Language Models","snippet":"In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/11ekhni/r_hyena_hierarchy_towards_larger_convolutional/?utm_source=chatgpt.com","title":"[R] Hyena Hierarchy: Towards Larger Convolutional ...","snippet":"Hyena Hierarchy: Towards Larger Convolutional Language Models \u00b7 AI model predicts adult ADHD using virtual reality and eye movement data. \u00b7 [D] ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/s5hajb/r_the_annotated_s4_efficiently_modeling_long/?utm_source=chatgpt.com","title":"[R] The Annotated S4: Efficiently Modeling Long ...","snippet":"The Structured State Space for Sequence Modeling (S4) architecture is a new approach to very long-range sequence modeling tasks for vision, language, and audio.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/164n8iz/discussion_promising_alternatives_to_the_standard/?utm_source=chatgpt.com","title":"[Discussion] Promising alternatives to the standard ...","snippet":"What are some promising transformer alternatives/variants that you think more folks should be aware of? They need not be new or SOTA!","ref_id":{"turn_index":3,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/xmok0y/r_mega_moving_average_equipped_gated_attention_by/?utm_source=chatgpt.com","title":"[R] Mega: Moving Average Equipped Gated Attention. By ...","snippet":"Really nice paper! Is there any public code to re-run experiments/have a play. In particular the language modelling or text classification work?","ref_id":{"turn_index":3,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/152u5va/r_retentive_network_a_successor_to_transformer/?utm_source=chatgpt.com","title":"Retentive Network: A Successor to Transformer for Large ...","snippet":"The RetNet architecture consists of multi-scale retention modules and feed-forward network modules. The retention mechanism is formulated as ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/LocalLLaMA/comments/1f738xs/retnphi_hybrid_of_phi35_and_retnet_for_bytelevel/?utm_source=chatgpt.com","title":"RetNPhi: Hybrid of Phi-3.5 and RetNet for Byte-Level ...","snippet":"Entire 1M line codebase will fit in like 8-16K context window ... long-context. If you stretch the context window to several million tokens ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/11eqinv/spikegpt_230mparameter_spiking_neural_network/?utm_source=chatgpt.com","title":"230M-parameter Spiking Neural Network trained to be a ...","snippet":"The authors train a 230M parameter SNN on an autoregressive text modeling objective. They believe this is the largest SNN ever trained with ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/artificial/comments/uo47mu/introducing_gato_a_generalist_agent_from_deepmind/?utm_source=chatgpt.com","title":"Introducing Gato - a generalist agent from DeepMind","snippet":"Using a Transformer architecture, they've basically just fed through \u201cObservation\u201d and \u201cAction\u201d tokens for a bunch of tasks (ie, what's on ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/dzakrs/r_191108265_mastering_atari_go_chess_and_shogi_by/?utm_source=chatgpt.com","title":"[R] [1911.08265] Mastering Atari, Go, Chess and Shogi by ...","snippet":"So in the real world, most applications of RL based online planning are NOT operating on a causal model, but are rather operating based on a ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/?utm_source=chatgpt.com","title":"MuZero: Mastering Go, chess, shogi and Atari without rules","snippet":"23 Dec 2020 \u2014 MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/discover/blog/a-generalist-agent/?utm_source=chatgpt.com","title":"A Generalist Agent","snippet":"12 May 2022 \u2014 The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"berkeley.edu","entries":[{"type":"search_result","url":"https://simons.berkeley.edu/sites/default/files/docs/17404/raminhasanitfcssynthesisslides.pdf?utm_source=chatgpt.com","title":"Liquid Time-Constant Network","snippet":"22 Mar 2021 \u2014 \u2713 A novel time-continuous neural networks for efficient time-series modelling. \u2022 LTCs are universal approximators. \u2022 LTCs are stable dynamical ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"berkeley.edu"},{"type":"search_result","url":"https://people.eecs.berkeley.edu/~ysshao/assets/papers/diffuseloco-corl2024.pdf?utm_source=chatgpt.com","title":"DiffuseLoco: Real-Time Legged Locomotion Control with ...","snippet":"To address the challenges of offline learning from diverse data sources and learning a set of diverse skills, we present DiffuseLoco, a framework leveraging ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"berkeley.edu"},{"type":"search_result","url":"https://bair.berkeley.edu/blog/2023/07/14/ddpo/?utm_source=chatgpt.com","title":"Training Diffusion Models with Reinforcement Learning","snippet":"14 Jul 2023 \u2014 In this post, we show how diffusion models can be trained on these downstream objectives directly using reinforcement learning (RL).","ref_id":{"turn_index":8,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"berkeley.edu"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/state-spaces/mamba?utm_source=chatgpt.com","title":"state-spaces/mamba: Mamba SSM architecture","snippet":"Mamba is a new state space model architecture showing promising performance on information-dense data such as language modeling.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/Jamie-Stirling/RetNet?utm_source=chatgpt.com","title":"RetNet - An implementation of \"Retentive Network","snippet":"This is a minimal, pure pytorch implementation of RetNet. RetNet paper: Retentive Network: A Successor to Transformer for Large Language Models.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/HazyResearch/safari?utm_source=chatgpt.com","title":"HazyResearch/safari: Convolutions for Sequence Modeling","snippet":"Hyena. Hyena Hierarchy: Towards Larger Convolutional Language models Michael Poli*, Stefano Massaroli*, Eric Nguyen*, Daniel Y.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/lindermanlab/S5?utm_source=chatgpt.com","title":"lindermanlab/S5","snippet":"S5 uses a single multi-input, multi-output linear state-space model, coupled with non-linearities, to define a non-linear sequence-to-sequence transformation.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/HazyResearch/H3?utm_source=chatgpt.com","title":"Language Modeling with the H3 State Space Model","snippet":"This repository provides the official implementation of H3 from the following paper. Hungry Hungry Hippos: Towards Language Modeling with State Space Models","ref_id":{"turn_index":2,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/raminmh/l-state-spaces?utm_source=chatgpt.com","title":"Sequence Modeling with Structured State Spaces","snippet":"Efficiently Modeling Long Sequences with Structured State Spaces Albert Gu, Karan Goel, Christopher R\u00e9 Paper: https://arxiv.org/abs/2111.00396. LSSL (NeurIPS ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/i404788/s5-pytorch?utm_source=chatgpt.com","title":"S5: Simplified State Space Layers for Sequence Modeling","snippet":"You can use variable step-size as described in the paper using a 1D tensor for step_scale however this takes a lot of memory due to a lot of intermediate values ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/facebookresearch/mega?utm_source=chatgpt.com","title":"facebookresearch/mega: Sequence modeling with Mega.","snippet":"1 Nov 2024 \u2014 Mega: Moving Average Equipped Gated Attention. This is the PyTorch implementation of the Mega paper. This folder is based on the fairseq package v0.9.0.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/fla-org/flash-linear-attention?utm_source=chatgpt.com","title":"fla-org/flash-linear-attention","snippet":"This repo aims at providing a collection of efficient Triton-based implementations for state-of-the-art linear attention models.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/facebookresearch/flowmm?utm_source=chatgpt.com","title":"Code for \u201cFlowMM Generating Materials with Riemannian ...","snippet":"18 Sept 2025 \u2014 Code for \u201cFlowMM Generating Materials with Riemannian Flow Matching\u201d and \"FlowLLM: Flow Matching for Material Generation with Large Language ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/bansky-cl/Diffusion_NLP_Papers?utm_source=chatgpt.com","title":"bansky-cl/Diffusion-LM-Papers","snippet":"24 Mar 2025 \u2014 Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference! 42, DIFFUSEMP, 2023.6, Bi ...See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/VILA-Lab/Awesome-DLMs?utm_source=chatgpt.com","title":"Awesome Diffusion Language Models","snippet":"One of the most starred, comprehensive and up-to-date collections of Diffusion Language Model papers, code and resources! If you find this repository helpful, ...See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"microsoft.com","entries":[{"type":"search_result","url":"https://www.microsoft.com/en-us/research/project/minference-million-tokens-prompt-inference-for-long-context-llms/?utm_source=chatgpt.com","title":"MInference: Million-Tokens Prompt Inference for Long- ...","snippet":"We propose a dynamic sparse attention method, MInference, to accelerate the pre-filling stage of long-context LLMs by up to 10x for 1M token prompts while ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"microsoft.com"},{"type":"search_result","url":"https://www.microsoft.com/en-us/research/publication/minference-1-0-accelerating-pre-filling-for-long-context-llms-via-dynamic-sparse-attention/?utm_source=chatgpt.com","title":"MInference 1.0: Accelerating Pre-filling for Long-Context ...","snippet":"9 Dec 2024 \u2014 We introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"microsoft.com"}]},{"type":"search_result_group","domain":"thecvf.com","entries":[{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_DiG_Scalable_and_Efficient_Diffusion_Models_with_Gated_Linear_Attention_CVPR_2025_paper.pdf?utm_source=chatgpt.com","title":"DiG: Scalable and Efficient Diffusion Models with Gated Linear ...","snippet":"by L Zhu \u00b7 2025 \u00b7 Cited by 27 \u2014 In this paper, we aim to incorpo- rate the sub-quadratic modeling capability of Gated Linear. Attention (GLA) into the 2D diffusion backbone. Specifi- cally, we ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"thecvf.com"},{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf?utm_source=chatgpt.com","title":"MaskGIT: Masked Generative Image Transformer","snippet":"by H Chang \u00b7 2022 \u00b7 Cited by 1024 \u2014 At inference time, MaskGIT adopts a novel non-autoregressive decoding method to synthesize an image in constant number of steps. Specifically, at each iteration ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"thecvf.com"},{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_FlowRAM_Grounding_Flow_Matching_Policy_with_Region-Aware_Mamba_Framework_for_CVPR_2025_paper.pdf?utm_source=chatgpt.com","title":"Grounding Flow Matching Policy with Region-Aware Mamba ...","snippet":"by S Wang \u00b7 2025 \u00b7 Cited by 4 \u2014 In robotics, these models are employed to learn action score functions conditioned on observations, language, and robot states, enabling the generation of ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"thecvf.com"},{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2024/papers/Ni_Revisiting_Non-Autoregressive_Transformers_for_Efficient_Image_Synthesis_CVPR_2024_paper.pdf?utm_source=chatgpt.com","title":"Revisiting Non-Autoregressive Transformers for Efficient ...","snippet":"by Z Ni \u00b7 2024 \u00b7 Cited by 22 \u2014 As a pioneering work, MaskGIT [6] demonstrates highly- competitive fidelity and diversity on the ImageNet [40] with only 8 sampling steps. It has been further ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"thecvf.com"},{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_Mamba-Adaptor_State_Space_Model_Adaptor_for_Visual_Recognition_CVPR_2025_paper.pdf?utm_source=chatgpt.com","title":"State Space Model Adaptor for Visual Recognition","snippet":"by F Xie \u00b7 2025 \u00b7 Cited by 1 \u2014 Recent State Space Models (SSM), especially Mamba, have demonstrated impressive performance in visual modeling and possess superior model efficiency.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"thecvf.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40yash9439/ai21-labs-introduces-jamba-the-first-production-grade-hybrid-ssm-transformer-model-7f6f500441a4?utm_source=chatgpt.com","title":"The First Production-Grade Hybrid SSM-Transformer Model","snippet":"AI21 Labs has announced the release of Jamba, the world's first production-grade AI model based on the innovative Mamba architecture.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%4023subhasmukherjee/hyena-hierarchies-the-future-beyond-transformers-e1f31d42dd67?utm_source=chatgpt.com","title":"Hyena Hierarchies : The future beyond transformers ?","snippet":"Hyena Hierarchies were introduced in the year 2023 in the paper \u2014 \u201cHyena Hierarchy: Towards Larger Convolutional Language Models\u201d. It ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40hession520/liquid-neural-nets-lnns-32ce1bfb045a?utm_source=chatgpt.com","title":"Liquid Neural Nets (LNNs)","snippet":"Liquid neural nets (LNNs) are an exciting, relatively new direction in AI/ML research that promises more compact and dynamic neural nets for time series ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40kdk199604/s5-simplified-state-space-layers-for-efficient-sequence-modeling-61ef5f3386ac?utm_source=chatgpt.com","title":"S5: Simplified State Space Layers for Efficient Sequence ...","snippet":"The paper presents S5, a simplified yet expressive state space layer for sequence modeling. By reformulating S4's independent subsystems ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/autonomous-agents/comprehensive-breakdown-of-selective-structured-state-space-model-mamba-s5-441e8b94ecaf?utm_source=chatgpt.com","title":"Comprehensive Breakdown of Selective Structured State ...","snippet":"Comprehensive Breakdown of Selective Structured State Space Model \u2014 Mamba (S5) ... The paper has a detailed explanation on this in section 3.3.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/machine-learns/exploring-beyond-regular-transformers-346e82fc901d?utm_source=chatgpt.com","title":"Exploring Beyond Regular Transformers | by Eren G\u00f6lge","snippet":"When compared to the other Transformer alternatives, RetNet outperforms all the different models by a big margin on language modeling. Press ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/towardsdev/beyond-the-window-the-complete-guide-to-long-context-language-models-and-why-theyre-changing-ai-ee28f6c2b428?utm_source=chatgpt.com","title":"The Complete Guide to Long Context Language Models ( ...","snippet":"We're entering the era of Long Context Language Models (LCLMs), capable of processing millions of tokens (words or parts of words) in a single ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://mskadu.medium.com/why-large-language-model-context-windows-matter-the-significance-of-claude-sonnets-1m-token-6a0cc98b129a?utm_source=chatgpt.com","title":"Why Large Language Model Context Windows Matter: The ...","snippet":"Why Context Window Size drives Adoption. The expansion to 1M tokens removes critical barriers that previously limited LLM adoption in enterprise ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/mlearning-ai/gato-the-generalist-agent-by-deepmind-a780a048f9bc?utm_source=chatgpt.com","title":"Gato Generalist agent | DeepMind | AGI AI","snippet":"Control robot arms; Chat. The model uses just a single architecture based on Transformer. It is a 1.18B parameter model with shared embeddings.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40danieljsmit/mamba2-the-hardware-algorithm-co-design-that-unified-attention-and-state-space-models-77856d2ac4f4?utm_source=chatgpt.com","title":"Mamba2: The Hardware-Algorithm Co-Design That Unified ...","snippet":"In May 2024, Tri Dao and Albert Gu unveiled Mamba2 \u2014 not just an incremental improvement, but a fundamental reconceptualization that revealed ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science/here-comes-mamba-the-selective-state-space-model-435e5d17a451?utm_source=chatgpt.com","title":"Here Comes Mamba: The Selective State Space Model","snippet":"Mamba adds selectivity to state space models which results in Transformer-like performance while maintaining the sub-quadratic work complexity of SSMs.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=7SaXczaBpG&utm_source=chatgpt.com","title":"RWKV: Reinventing RNNs for the Transformer Era","snippet":"by B Peng \u00b7 Cited by 807 \u2014 Experiments suggest RWKV ad- dresses scaling and deployment challenges in AI, especially for sequential data processing, pointing towards more sustainable and ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=sxZlp9ZoHD&utm_source=chatgpt.com","title":"Retentive Network","snippet":"by Y Sun \u00b7 2024 \u2014 Retentive Network (RETNET) is a foundation for large language models, achieving training parallelism, low-cost inference, and good performance, with a multi- ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=uYLFoz1vlAC&utm_source=chatgpt.com","title":"EFFICIENTLY MODELING LONG SEQUENCES WITH ...","snippet":"by A Gu \u00b7 Cited by 3416 \u2014 In this work, we introduce the Structured State Space (S4) sequence model based on the SSM that ... We call K the SSM convolution kernel or filter. 3 METHOD: ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=Ai8Hw3AXqks&utm_source=chatgpt.com","title":"SIMPLIFIED STATE SPACE LAYERS FOR SEQUENCE ...","snippet":"by JTH Smith \u00b7 Cited by 940 \u2014 We show in Table 2 that S5 outperforms the baselines, outperforms previous S4 methods and performs similarly to the concurrent Liquid-S4 method (Hasani et al., ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=COZDy0WYGg&utm_source=chatgpt.com","title":"Towards Language Modeling with State Space...","snippet":"by DY Fu \u00b7 Cited by 708 \u2014 To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=qNLe3iq2El&utm_source=chatgpt.com","title":"MEGA: MOVING AVERAGE EQUIPPED GATED ATTENTION","snippet":"by X Ma \u00b7 Cited by 119 \u2014 In this paper, we introduce MEGA, a simple, theoreti- cally grounded, single-head gated attention mechanism equipped with (exponential) moving average to ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/ea538b6258b9524c9de5b7b526fa5112819a0ab3.pdf?utm_source=chatgpt.com","title":"Efficient Long Sequence Modeling and Beyond","snippet":"range arena: A benchmark for efficient transformers. 873. arXiv preprint arXiv:2011.04006. 874. Yi Tay, Mostafa Dehghani, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=y8Rm4VNRPH&referrer=%5Bthe+profile+of+Yoon+Kim%5D%28%2Fprofile%3Fid%3D~Yoon_Kim1%29&utm_source=chatgpt.com","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"25 Sept 2024 \u2014 This paper proposes a hardware-efficient algorithm for training linear transformers with a delta update (DeltaNet; SMS21). This architecture has ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=UU9Icwbhin&utm_source=chatgpt.com","title":"A Successor to Transformer for Large Language Models","snippet":"by Y Sun \u00b7 Cited by 568 \u2014 We propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=y8Rm4VNRPH&utm_source=chatgpt.com","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"by S Yang \u00b7 Cited by 135 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=AC9FsaVIpk&utm_source=chatgpt.com","title":"Understanding Gated Linear Attention through In-context ...","snippet":"by Y Li \u00b7 Cited by 5 \u2014 This paper shows that Gated Linear Attention (GLA) can implement Weighted Projected Gradient Descent (WPGD) algorithms. Furthermore, the gating mechanism in GLA ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=ExuBFYtCQU&utm_source=chatgpt.com","title":"MaskGCT: Zero-Shot Text-to-Speech with Masked ...","snippet":"by Y Wang \u00b7 Cited by 120 \u2014 In this paper, we introduce Mask ed G enerative C odec T ransformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=3s9IrEsjLyk&utm_source=chatgpt.com","title":"Diffusion-LM Improves Controllable Text Generation","snippet":"by XL Li \u00b7 2022 \u00b7 Cited by 1135 \u2014 Diffusion-LM is a non-autoregressive model using continuous diffusions, iteratively denoising Gaussian vectors to word vectors, enabling gradient-based control ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/revisions?id=3s9IrEsjLyk&utm_source=chatgpt.com","title":"Revision History for Diffusion-LM Improves Controllable...","snippet":"TL;DR: We propose a non-autoregressive language model based on continuous diffusions, which demonstrate strong performance in controllable text generation.See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/1b385f7dd441086164a71969e203ce2a272c8301.pdf?utm_source=chatgpt.com","title":"Improving Flow Matching Models For Efficient High-Quality ...","snippet":"We propose Blockwise Flow Matching (BFM), a novel framework that divides the generative trajectory into temporal segments, each modeled by smaller but ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=GJsuYHhAga&utm_source=chatgpt.com","title":"Meissonic: Revitalizing Masked Generative Transformers ...","snippet":"by J Bai \u00b7 Cited by 43 \u2014 We present Meissonic, a next generation text-to-image foundation model with non-autogressive masked image modeling method.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=gcf1anBL9e&utm_source=chatgpt.com","title":"SpikeGPT: Generative Pre-trained Language Model with ...","snippet":"by RJ Zhu \u00b7 Cited by 169 \u2014 SpikeGPT is the largest backpropagation-trained SNN model when released, rendering it suitable for both the generation and comprehension of natural language.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=DEdDWSXvAP&utm_source=chatgpt.com","title":"SPIKINGLLM","snippet":"In this paper, we aim to improve the energy efficiency of LLMs by leveraging the advantages of brain-inspired spiking neural networks (SNNs). We propose a novel ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/8801375407fde3d9751dbb78c2f906238f519a12.pdf?utm_source=chatgpt.com","title":"MUDREAMER: LEARNING PREDICTIVE WORLD ...","snippet":"In this paper, we present MuDreamer, a reinforcement learning agent that builds upon the Dream-. erV3 (Hafner et al., 2023) algorithm by learning a predictive ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=X6D9bAHhBQ1&utm_source=chatgpt.com","title":"PLANNING IN STOCHASTIC ENVIRONMENTS","snippet":"by I Antonoglou \u00b7 Cited by 102 \u2014 3.1 MuZero. MuZero is a model-based general reinforcement learning agent which combines a learned model of the environment dynamics with a Monte Carlo tree ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=1ikK0kHjvj&utm_source=chatgpt.com","title":"A Generalist Agent","snippet":"by S Reed \u00b7 2022 \u00b7 Cited by 1332 \u2014 Gato also uses an LM-like architecture for control, but with design differences chosen to support multi-modality, multi-embodiment, large scale and general ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/368688016_Hyena_Hierarchy_Towards_Larger_Convolutional_Language_Models?utm_source=chatgpt.com","title":"Towards Larger Convolutional Language Models","snippet":"[383] propose HyenaDNA, a genomic language model based on the Hyena architecture [430] , enabling modeling of genomic sequences of up to 1 million tokens.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/370949567_RWKV_Reinventing_RNNs_for_the_Transformer_Era?utm_source=chatgpt.com","title":"(PDF) RWKV: Reinventing RNNs for the Transformer Era","snippet":"22 May 2023 \u2014 We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/342027480_Liquid_Time-constant_Networks?utm_source=chatgpt.com","title":"(PDF) Liquid Time-constant Networks","snippet":"8 Jun 2020 \u2014 We introduce a new class of time-continuous recurrent neural network models. Instead of declaring the nonlinearity of a learning system by ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/363386857_Liquid_Time-constant_Networks?utm_source=chatgpt.com","title":"Liquid Time-constant Networks | Request PDF","snippet":"6 Aug 2025 \u2014 Hasani et al. [8] introduced liquid time-constant networks, which use differential equations to model neuron behaviours, enabling improved ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/344683434_Neural_circuit_policies_enabling_auditable_autonomy?utm_source=chatgpt.com","title":"Neural circuit policies enabling auditable autonomy","snippet":"Here, we combine brain-inspired neural computation principles and scalable deep learning architectures to design compact neural controllers for task-specific ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/355841686_Efficiently_Modeling_Long_Sequences_with_Structured_State_Spaces?utm_source=chatgpt.com","title":"Efficiently Modeling Long Sequences with Structured State ...","snippet":"(2021) S4 demonstrated the ability of SSMs to efficiently model long-range dependencies while scaling effectively with sequence length. It introduced ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/395686915_Never_Train_from_Scratch_Fair_Comparison_of_Long-Sequence_Models_Requires_Data-Driven_Priors_Extended_Abstract?utm_source=chatgpt.com","title":"Fair Comparison of Long-Sequence Models Requires Data ...","snippet":"23 Sept 2025 \u2014 We demonstrate that properly pretrained vanilla Transformers match S4 performance on Long Range Arena and improve SSM results on PathX-256 by 20 ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/363765463_Mega_Moving_Average_Equipped_Gated_Attention?utm_source=chatgpt.com","title":"Mega: Moving Average Equipped Gated Attention","snippet":"In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/387671240_Titans_Learning_to_Memorize_at_Test_Time?utm_source=chatgpt.com","title":"(PDF) Titans: Learning to Memorize at Test Time","snippet":"31 Dec 2024 \u2014 We present a new neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/392204297_ATLAS_Learning_to_Optimally_Memorize_the_Context_at_Test_Time?utm_source=chatgpt.com","title":"(PDF) ATLAS: Learning to Optimally Memorize the Context ...","snippet":"1 Jun 2025 \u2014 To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/371944243_HyenaDNA_Long-Range_Genomic_Sequence_Modeling_at_Single_Nucleotide_Resolution?utm_source=chatgpt.com","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","snippet":"Leveraging Hyenas new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/393045731_Flow_Matching_for_Conditional_Text_Generation_in_a_Few_Sampling_Steps?utm_source=chatgpt.com","title":"Flow Matching for Conditional Text Generation in a Few ...","snippet":"28 Jun 2025 \u2014 Unlike Diffusion models, flow matching is not constrained to the Gaussian noise prior distribution, but instead supports arbitrary prior ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/393937568_Self-Dreaming_to_Control_Better_Boosting_Latent_Spaces_With_Self-Supervised_Learning?utm_source=chatgpt.com","title":"Self-Dreaming to Control Better: Boosting Latent Spaces ...","snippet":"24 Jul 2025 \u2014 The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/371223567_What_model_does_MuZero_learn?utm_source=chatgpt.com","title":"(PDF) What model does MuZero learn?","snippet":"1 Jun 2023 \u2014 In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/395377302_DDAT_Diffusion_Policies_Enforcing_Dynamically_Admissible_Robot_Trajectories?utm_source=chatgpt.com","title":"Diffusion Policies Enforcing Dynamically Admissible Robot ...","snippet":"11 Sept 2025 \u2014 Diffusion models beat GANs on topology optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/318829926_Value_Iteration_Networks?utm_source=chatgpt.com","title":"Value Iteration Networks | Request PDF","snippet":"A recent work, the value iteration networks (VIN) (Tamar et al. 2016 ) combines recurrent convolutional neural networks and max-pooling to emulate the process ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/360559690_A_Generalist_Agent?utm_source=chatgpt.com","title":"(PDF) A Generalist Agent","snippet":"13 May 2022 \u2014 The \ufb01rst agent uses the smallest architecture introduced in Section 5.1, i.e.. 79M. parameters, and. is trained on all 50 Meta-World tasks.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"stanford.edu","entries":[{"type":"search_result","url":"https://stacks.stanford.edu/file/druid%3Amb976vf9362/gu_dissertation-augmented.pdf?utm_source=chatgpt.com","title":"modeling sequences with structured state spaces a ... - Stacks","snippet":"by A Gu \u00b7 2023 \u00b7 Cited by 87 \u2014 This thesis develops a new approach to deep sequence modeling using state space models, a flexible method that is theoretically grounded, computationally ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://hazyresearch.stanford.edu/blog/2023-01-20-h3?utm_source=chatgpt.com","title":"H3: Language Modeling with State Space ...","snippet":"23 Jan 2023 \u2014 We designed a new layer, H3 (\u201cHungry Hungry Hippos\u201d), designed explicitly to solve associative recall. There are two capabilities that we need ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna?utm_source=chatgpt.com","title":"HyenaDNA: learning from DNA with 1 Million token context","snippet":"29 Jun 2023 \u2014 We're excited to introduce HyenaDNA, a long-range genomic foundation model with context lengths of up to 1 million tokens at single nucleotide resolution!","ref_id":{"turn_index":5,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://cs224r.stanford.edu/projects/pdfs/Set_Dreamer_CS224R_Final_Project.pdf?utm_source=chatgpt.com","title":"Dynamics Modeling between Learnable State Space Subsets ...","snippet":"by HD Lee \u2014 Dreamer v3 Hafner et al. (2023) is a model-based reinforcement learning algorithm that learns a world model to simulate future trajectories and optimizes ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"stanford.edu"}]},{"type":"search_result_group","domain":"acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3637528.3672044?utm_source=chatgpt.com","title":"Towards Learning on Graphs with State Space Models","snippet":"by A Behrouz \u00b7 2024 \u00b7 Cited by 139 \u2014 We present Graph Mamba Networks (GMNs), a framework for a new class of GNNs based on selective SSMs. We discuss the new challenges when adapting SSMs to graph- ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3737916.3741584?utm_source=chatgpt.com","title":"Parallelizing linear transformers with the delta rule over ...","snippet":"5 Jun 2025 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3666122.3667994?utm_source=chatgpt.com","title":"HyenaDNA: long-range genomic sequence modeling at ...","snippet":"by E Nguyen \u00b7 2023 \u00b7 Cited by 480 \u2014 We present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3692070.3694403?utm_source=chatgpt.com","title":"Gated linear attention transformers with hardware-efficient ...","snippet":"by S Yang \u00b7 2024 \u00b7 Cited by 270 \u2014 This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3600270.3600583?utm_source=chatgpt.com","title":"Diffusion-LM improves controllable text generation","snippet":"by XL Li \u00b7 2022 \u00b7 Cited by 1133 \u2014 Diffusion-LM is a non-autoregressive model using continuous diffusions to generate text, enabling complex, controllable generation tasks.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1007/978-3-031-72664-4_9?utm_source=chatgpt.com","title":"ZigMa: A DiT-style Zigzag Mamba Diffusion Model","snippet":"by VT Hu \u00b7 2024 \u00b7 Cited by 153 \u2014 In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.5555/3618408.3619275?utm_source=chatgpt.com","title":"Text generation with diffusion language models","snippet":"by Z Lin \u00b7 2023 \u00b7 Cited by 94 \u2014 In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE.See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1609/aaai.v39i19.34245?utm_source=chatgpt.com","title":"SpikingSSMs: learning long sequences with sparse and ...","snippet":"by S Shen \u00b7 2025 \u00b7 Cited by 16 \u2014 Known as low energy consumption networks, spiking neural networks (SNNs) have gained a lot of attention within the past decades.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3157096.3157337?utm_source=chatgpt.com","title":"Value iteration networks | Proceedings of the 30th ...","snippet":"In this paper, we introduce a generalized value iteration network (GVIN), which is an end-to-end neural network planning module. GVIN emulates the value ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"acm.org"}]},{"type":"search_result_group","domain":"sustcsonglin.github.io","entries":[{"type":"search_result","url":"https://sustcsonglin.github.io/blog/2024/deltanet-1/?utm_source=chatgpt.com","title":"DeltaNet Explained (Part I) - Songlin Yang","snippet":"3 Dec 2024 \u2014 Linear attention is essentially a linear RNN with a matrix-valued state S that accumulates key-value outer products, enabling efficient state (size) expansion.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"sustcsonglin.github.io"},{"type":"search_result","url":"https://sustcsonglin.github.io/blog/2024/deltanet-3/?utm_source=chatgpt.com","title":"DeltaNet Explained (Part III) - Songlin Yang","snippet":"3 Dec 2024 \u2014 While maintaining the core delta rule mechanism, we've introduced several architectural improvements that significantly enhance its performance.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"sustcsonglin.github.io"}]},{"type":"search_result_group","domain":"tridao.me","entries":[{"type":"search_result","url":"https://tridao.me/blog/2024/mamba2-part1-model/?utm_source=chatgpt.com","title":"State Space Duality (Mamba-2) Part I - The Model","snippet":"31 May 2024 \u2014 The main point of the Mamba-2 paper is what we call structured state space duality (SSD), which refers to several things.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"tridao.me"},{"type":"search_result","url":"https://tridao.me/blog/2024/mamba2-part2-theory/?utm_source=chatgpt.com","title":"State Space Duality (Mamba-2) Part II - The Theory","snippet":"31 May 2024 \u2014 This framework provides a rich body of connections between state space models, attention, and structured matrices. While the SSD model can be ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"tridao.me"}]},{"type":"search_result_group","domain":"liquid.ai","entries":[{"type":"search_result","url":"https://www.liquid.ai/research/liquid-neural-networks-research?utm_source=chatgpt.com","title":"From Liquid Neural Networks to Liquid Foundation Models","snippet":"30 Sept 2024 \u2014 We invented liquid neural networks, a class of brain-inspired systems that can stay adaptable and robust to changes even after training.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"liquid.ai"}]},{"type":"search_result_group","domain":"liner.com","entries":[{"type":"search_result","url":"https://liner.com/review/long-range-arena-a-benchmark-for-efficient-transformers?utm_source=chatgpt.com","title":"Long Range Arena: A Benchmark for Efficient Transformers","snippet":"In this ICLR 2020 review page, we summarize the core of the paper that introduces Long Range Arena, a benchmark for efficient Transformers.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"liner.com"}]},{"type":"search_result_group","domain":"goombalab.github.io","entries":[{"type":"search_result","url":"https://goombalab.github.io/blog/2024/mamba2-part2-theory/?utm_source=chatgpt.com","title":"State Space Duality (Mamba-2) Part II - The Theory","snippet":"31 May 2024 \u2014 In this post, we'll dive into the theory behind the model. We'll derive the SSD \u201cduality\u201d in two completely separate ways, one starting from the SSM ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"goombalab.github.io"},{"type":"search_result","url":"https://goombalab.github.io/blog/2024/mamba2-part3-algorithm/?utm_source=chatgpt.com","title":"State Space Duality (Mamba-2) Part III - The Algorithm","snippet":"31 May 2024 \u2014 The theoretical framework of structured state space duality (see Part I and Part II of this series) connects SSMs and (linear) attention through structured ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"goombalab.github.io"}]},{"type":"search_result_group","domain":"ntnu.no","entries":[{"type":"search_result","url":"https://folk.idi.ntnu.no/keithd/master-projects/2024/liquid-nns-2020.pdf?utm_source=chatgpt.com","title":"arXiv:2006.04439v4 [cs.LG] 14 Dec 2020","snippet":"by R Hasani \u00b7 2021 \u00b7 Cited by 428 \u2014 We re- fer to these models as liquid time-constant recurrent neural networks (LTCs). LTCs can be implemented by an arbitrary choice of ODE solvers. In Section 2 ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"ntnu.no"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2307.08621?utm_source=chatgpt.com","title":"Retentive Network: A Successor to Transformer for Large ...","snippet":"17 Jul 2023 \u2014 We propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.5?utm_source=chatgpt.com","title":"ai21labs/AI21-Jamba-Mini-1.5","snippet":"6 Mar 2025 \u2014 The AI21 Jamba 1.5 family of models is state-of-the-art, hybrid SSM-Transformer instruction following foundation models. The Jamba models ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2305.13048?utm_source=chatgpt.com","title":"RWKV: Reinventing RNNs for the Transformer Era","snippet":"22 May 2023 \u2014 A new architecture, RWKV, combines the parallelizable training of Transformers with the efficient inference of RNNs, achieving linear scaling ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2312.00752?utm_source=chatgpt.com","title":"Linear-Time Sequence Modeling with Selective State Spaces","snippet":"1 Dec 2023 \u2014 Mamba, a novel SSM-based model, outperforms Transformers in inference speed and scalability across various modalities by selectively propagating ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2209.10655?utm_source=chatgpt.com","title":"Mega: Moving Average Equipped Gated Attention","snippet":"21 Sept 2022 \u2014 In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2406.06484?utm_source=chatgpt.com","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"10 Jun 2024 \u2014 This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers?q=linear+recurrent+neural+networks&utm_source=chatgpt.com","title":"Daily Papers","snippet":"Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers for long sequences ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2306.15794?utm_source=chatgpt.com","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","snippet":"29 Jun 2023 \u2014 HyenaDNA is a genomic model using implicit convolutions, achieving state-of-the-art performance with longer context lengths and single ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/collections?paper=2408.16725&utm_source=chatgpt.com","title":"Collections","snippet":"Explore all the collections from users and organizations and discover curated ML resources and community favorites.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v202/poli23a/poli23a.pdf?utm_source=chatgpt.com","title":"Hyena Hierarchy: Towards Larger Convolutional Language ...","snippet":"by M Poli \u00b7 2023 \u00b7 Cited by 473 \u2014 In the case of Hyena, causality can be guaranteed by parametrizing causal convolutions: Proposition 3.1 (Causal Hyenas). If each filter hn, n = 1,...,N is ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"mlr.press"},{"type":"search_result","url":"https://proceedings.mlr.press/v202/walker23a/walker23a.pdf?utm_source=chatgpt.com","title":"Investigating the role of model-based learning in exploration ...","snippet":"by JC Walker \u00b7 2023 \u00b7 Cited by 13 \u2014 The Q-Learning baseline is based on the same ar- chitecture as MuZero agent, and in particular, utilizes the same architecture for the observation encoder and ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"mlr.press"}]},{"type":"search_result_group","domain":"yzhang.site","entries":[{"type":"search_result","url":"https://yzhang.site/assets/pubs/neurips/2024/gsa.pdf?utm_source=chatgpt.com","title":"Gated Slot Attention for Efficient Linear-Time Sequence ...","snippet":"This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC [63]) by incorporating a gating mechanism inspired ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"yzhang.site"},{"type":"search_result","url":"https://yzhang.site/assets/pubs/neurips/2024/deltanet.pdf?utm_source=chatgpt.com","title":"Parallelizing Linear Transformers with the Delta Rule over ...","snippet":"We similarly show that combining DeltaNet with classic attention is an effective strategy. Householder matrices. Householder matrices, known for preserving ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"yzhang.site"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2023.findings-emnlp.936/?utm_source=chatgpt.com","title":"RWKV: Reinventing RNNs for the Transformer Era","snippet":"by B Peng \u00b7 2023 \u00b7 Cited by 807 \u2014 We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/2025.naacl-long.147/?utm_source=chatgpt.com","title":"ReGLA: Refining Gated Linear Attention","snippet":"by P Lu \u00b7 2025 \u00b7 Cited by 2 \u2014 In this work, we embarked on a comprehensive exploration of three key components that substantially impact the performance of the Gated Linear Attention module.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/2024.findings-emnlp.497.pdf?utm_source=chatgpt.com","title":"Enable Fast Sampling for Seq2Seq Text Diffusion","snippet":"by P Liu \u00b7 2024 \u00b7 Cited by 1 \u2014 In this paper, we propose an efficient model FMSeq1, which utilizes flow matching to straighten the gen- eration path, thereby enabling fast ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"tuwien.at","entries":[{"type":"search_result","url":"https://repositum.tuwien.at/bitstream/20.500.12708/1068/2/Hasani%20Ramin%20-%202020%20-%20Interpretable%20recurrent%20neural%20networks%20in%20continuous-time...pdf?utm_source=chatgpt.com","title":"Interpretable Recurrent Neural Networks in Continuous-time ...","snippet":"by R Hasani \u00b7 2020 \u00b7 Cited by 14 \u2014 The model realizes liquid time constant dynamics, meaning that each neuron ... [Hasani et al., 2020] Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"tuwien.at"}]},{"type":"search_result_group","domain":"semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Mamba%3A-Linear-Time-Sequence-Modeling-with-Selective-Gu-Dao/7bbc7595196a0606a07506c4fb1473e5e87f6082?utm_source=chatgpt.com","title":"Linear-Time Sequence Modeling with Selective State Spaces","snippet":"Mamba integrates selective SSMs into a simplified neural network for linear-time sequence modeling, without attention or MLP blocks.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/998ac3e945857cf2676ee7efdbaf443a0c6f820a?utm_source=chatgpt.com","title":"[PDF] Hyena Hierarchy: Towards Larger Convolutional ...","snippet":"21 Feb 2023 \u2014 This work proposes Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/RWKV%3A-Reinventing-RNNs-for-the-Transformer-Era-Peng-Alcaide/026b3396a63ed5772329708b7580d633bb86bec9?utm_source=chatgpt.com","title":"[PDF] RWKV: Reinventing RNNs for the Transformer Era","snippet":"This work proposes a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Efficiently-Modeling-Long-Sequences-with-Structured-Gu-Goel/ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51?utm_source=chatgpt.com","title":"Efficiently Modeling Long Sequences with Structured State ...","snippet":"The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Hungry-Hungry-Hippos%3A-Towards-Language-Modeling-Dao-Fu/5a77b508302771fc083bf24e0bcda8553c9b5421?utm_source=chatgpt.com","title":"Towards Language Modeling with State Space Models","snippet":"2024. TLDR. This paper delves into a representative SSM named Hungry Hungry Hippos (H3), examining its advantages as well as its current limitations, and ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Mega%3A-Moving-Average-Equipped-Gated-Attention-Ma-Zhou/70e91e16eb321067d9402710e14a40cf28311f73?utm_source=chatgpt.com","title":"[PDF] Mega: Moving Average Equipped Gated Attention","snippet":"This paper introduces Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Gated-Linear-Attention-Transformers-with-Training-Yang-Wang/62b18cc55dcc7ffe52c28e1086aee893b7bc4334?utm_source=chatgpt.com","title":"[PDF] Gated Linear Attention Transformers with Hardware ...","snippet":"The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/MaskGIT%3A-Masked-Generative-Image-Transformer-Chang-Zhang/7c597874535c1537d7ddff3b3723015b4dc79d30?utm_source=chatgpt.com","title":"[PDF] MaskGIT: Masked Generative Image Transformer","snippet":"The proposed MaskGIT is a novel image synthesis paradigm using a bidirectional transformer decoder that significantly outperforms the state-of-the-art ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/SpikeGPT%3A-Generative-Pre-trained-Language-Model-Zhu-Zhao/9f52317ea9c5a6804b978987ff2a6557f98b5b2c?utm_source=chatgpt.com","title":"SpikeGPT: Generative Pre-trained Language Model with ...","snippet":"This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/c39fb7a46335c23f7529dd6f9f980462fd38653a?utm_source=chatgpt.com","title":"Mastering Atari, Go, chess and shogi by planning with a ...","snippet":"The MuZero algorithm is presented, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Value-Iteration-Networks-Tamar-Levine/84680b30a20775e5d319419a7f3f2a93e57c2a61?utm_source=chatgpt.com","title":"[PDF] Value Iteration Networks","snippet":"This paper proposes a novel deep reinforcement learning architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"semanticscholar.org"}]},{"type":"search_result_group","domain":"ucsc.edu","entries":[{"type":"search_result","url":"https://news.ucsc.edu/2023/03/eshraghian-spikegpt/?utm_source=chatgpt.com","title":"SpikeGPT: researcher releases code for largest-ever ...","snippet":"7 Mar 2023 \u2014 Jason Eshraghian and two students recently released the open-sourced code for the largest language-generating spiking neural network ever, named SpikeGPT.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"ucsc.edu"}]},{"type":"search_result_group","domain":"mit.edu","entries":[{"type":"search_result","url":"https://cbmm.mit.edu/video/liquid-neural-networks?utm_source=chatgpt.com","title":"Liquid Neural Networks | The Center for Brains, Minds ...","snippet":"A class of artificial intelligence algorithms that tries to bring a little bit of neuroscience in a structured way to machine learning.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"mit.edu"},{"type":"search_result","url":"https://cap.csail.mit.edu/sites/default/files/research-pdfs/Robust%20flight%20navigation%20out%20of%20distribution%20with%20liquid%20neural%20networks.pdf?utm_source=chatgpt.com","title":"Robust flight navigation out of distribution with liquid neural ...","snippet":"by M Chahine \u00b7 2023 \u00b7 Cited by 87 \u2014 Here, we present a method to create robust flight navigation agents that successfully perform vision-based fly-to-target tasks beyond their ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"mit.edu"},{"type":"search_result","url":"https://news.mit.edu/2022/solving-brain-dynamics-gives-rise-flexible-machine-learning-models-1115?utm_source=chatgpt.com","title":"Solving brain dynamics gives rise to flexible machine ...","snippet":"15 Nov 2022 \u2014 The flexibility of these \u201cliquid\u201d neural nets meant boosting the bloodline to our connected world, yielding better decision-making for many ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"mit.edu"},{"type":"search_result","url":"https://dspace.mit.edu/bitstream/handle/1721.1/152839/sun-mmsun-meng-eecs-2023-thesis.pdf?isAllowed=y&sequence=1&utm_source=chatgpt.com","title":"Melinda Sun","snippet":"by M Sun \u00b7 2023 \u2014 Several solutions have been proposed recently, including Moving. Average Equipped Gated Attention (Mega) [3] and Random Feature Attention [5]. In this work, we ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"mit.edu"}]},{"type":"search_result_group","domain":"jankautz.com","entries":[{"type":"search_result","url":"https://jankautz.com/publications/GatedDeltaNet_ICLR25.pdf?utm_source=chatgpt.com","title":"GATED DELTA NETWORKS: IMPROVING MAMBA2 WITH ...","snippet":"by S Yang \u00b7 Cited by 81 \u2014 Our resulting architecture, Gated DeltaNet, consistently outperforms both Mamba2 and DeltaNet across a comprehensive suite of benchmarks, including language ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"jankautz.com"}]},{"type":"search_result_group","domain":"google.com","entries":[{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=YarJF3QAAAAJ&utm_source=chatgpt.com","title":"Ramin Hasani","snippet":"Liquid time-constant networks. R Hasani, M Lechner, A Amini, D Rus, R Grosu. Proceedings of the AAAI Conference on Artificial Intelligence 35 (9), 7657-7666, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"google.com"}]},{"type":"search_result_group","domain":"syncedreview.com","entries":[{"type":"search_result","url":"https://syncedreview.com/2023/03/06/introducing-spikegpt-ucsc-kuaishous-llm-with-spiking-neural-networks-slashes-language-generation-costs/?utm_source=chatgpt.com","title":"Introducing SpikeGPT: UCSC & Kuaishou's LLM With Spiking ...","snippet":"6 Mar 2023 \u2014 A research team from the University of California and Kuaishou Technology presents SpikeGPT, the first generative spiking neural network (SNN) language model.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"syncedreview.com"}]},{"type":"search_result_group","domain":"marktechpost.com","entries":[{"type":"search_result","url":"https://www.marktechpost.com/2024/03/28/ai21-labs-breaks-new-ground-with-jamba-the-pioneering-hybrid-ssm-transformer-large-language-model/?utm_source=chatgpt.com","title":"AI21 Labs Breaks New Ground with 'Jamba': The Pioneering ...","snippet":"Jamba is the first production-grade AI model that combines the Mamba Structured State Space model (SSM) with the Transformer architecture, addressing the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"marktechpost.com"},{"type":"search_result","url":"https://www.marktechpost.com/2024/10/16/meissonic-a-non-autoregressive-mask-image-modeling-text-to-image-synthesis-model-that-can-generate-high-resolution-images/?utm_source=chatgpt.com","title":"Meissonic: A Non-Autoregressive Mask Image Modeling ...","snippet":"16 Oct 2024 \u2014 Meissonic, an innovative method to elevate non-autoregressive MIM text-to-image synthesis to a level comparable with state-of-the-art diffusion models like ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"marktechpost.com"}]},{"type":"search_result_group","domain":"poster-openaccess.com","entries":[{"type":"search_result","url":"https://poster-openaccess.com/files/ICIC2025/4202.pdf?utm_source=chatgpt.com","title":"SpikeRWKV:Energy-efficient Large Language Model with ...","snippet":"by Y Zhang1& \u2014 Furthermore, SpikeRWKV is 3.88\u00d7more energy-efficient than its non-spiking counterpart. Keywords: Spiking neural networks \u00b7 Energy efficiency \u00b7 Spike encoding.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"poster-openaccess.com"}]},{"type":"search_result_group","domain":"ibm.com","entries":[{"type":"search_result","url":"https://www.ibm.com/think/topics/mamba-model?utm_source=chatgpt.com","title":"What Is A Mamba Model? | IBM","snippet":"Mamba is a neural network architecture, derived from state space models (SSMs), used for language modeling and other sequence modeling tasks.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"ibm.com"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://community.openai.com/t/paper-rwkv-reinventing-rnns-for-the-transformer-era/567110?utm_source=chatgpt.com","title":"[Paper] RWKV: Reinventing RNNs for the Transformer Era","snippet":"21 Dec 2023 \u2014 We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"rizqimulki.com","entries":[{"type":"search_result","url":"https://rizqimulki.com/llm-architectures-beyond-transformers-mamba-retnet-and-alternatives-2a5963cb17d7?utm_source=chatgpt.com","title":"LLM Architectures Beyond Transformers: Mamba, RetNet ...","snippet":"29 May 2025 \u2014 The self-attention mechanism allowed models to capture long-range dependencies without the sequential processing constraints of RNNs. Parallel ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"rizqimulki.com"}]},{"type":"search_result_group","domain":"weebly.com","entries":[{"type":"search_result","url":"https://jxwuyi.weebly.com/uploads/2/5/1/1/25111124/vin-nips16.pdf?utm_source=chatgpt.com","title":"Value Iteration Networks","snippet":"We introduce the value iteration network (VIN): a fully differentiable neural net- work with a 'planning module' embedded within. VINs can learn to plan, ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"weebly.com"}]},{"type":"search_result_group","domain":"uplatz.com","entries":[{"type":"search_result","url":"https://uplatz.com/blog/the-million-token-revolution-an-in-depth-analysis-of-long-context-ai-models-and-their-strategic-implications/?utm_source=chatgpt.com","title":"The Million-Token Revolution: An In--Depth Analysis of ...","snippet":"23 Sept 2025 \u2014 Models like Meta's Llama 3.1 (with variants supporting up to 1M tokens), Yi, and DeepSeek are increasingly offering extended context windows in ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"uplatz.com"}]},{"type":"search_result_group","domain":"iclr.cc","entries":[{"type":"search_result","url":"https://proceedings.iclr.cc/paper_files/paper/2025/file/74a31a3b862eb7f01defbbed8e5f0c69-Paper-Conference.pdf?utm_source=chatgpt.com","title":"MASKGCT: ZERO-SHOT TEXT-TO-SPEECH WITH ...","snippet":"by Y Wang \u00b7 Cited by 120 \u2014 In this work, we propose MaskGCT, a fully non-autoregressive model for text-to-speech synthesis that uses masked generative transformers without requiring text- ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"iclr.cc"}]},{"type":"search_result_group","domain":"atlassian.net","entries":[{"type":"search_result","url":"https://intel-ncl.atlassian.net/wiki/spaces/INRC/blog/2023/05/24/1933738007/INRC%2BForum%2BMay%2B30th%2BJason%2BEshraghian%2BRuijie%2BZhu?utm_source=chatgpt.com","title":"Scaling up SNNs with SpikeGPT","snippet":"30 May 2023 \u2014 This presentation will be centered on SpikeGPT, the first large-scale language model (LLM) using spiking neural nets (SNNs), and possibly ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"atlassian.net"}]},{"type":"search_result_group","domain":"ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=40564067&utm_source=chatgpt.com","title":"Mamba-2 \u2013 State Space Duality","snippet":"3 Jun 2024 \u2014 Mamba-2 can be trained more efficiently, ie, faster than original Mamba on GPUs. Theoretical stuff aside, Mamba-2's performance seems to scale slightly better ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"ycombinator.com"}]},{"type":"search_result_group","domain":"researchhub.com","entries":[{"type":"search_result","url":"https://www.researchhub.com/paper/1277006/hyenadna-long-range-genomic-sequence-modeling-at-single-nucleotide-resolution?utm_source=chatgpt.com","title":"HyenaDNA: Long-Range Genomic Sequence Modeling at ...","snippet":"28 Jun 2023 \u2014 Toward longer context models Recently, Hyena (Poli et al., 2023), a large language model based on implicit convolutions, was shown to match ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"researchhub.com"}]},{"type":"search_result_group","domain":"frenos.io","entries":[{"type":"search_result","url":"https://frenos.io/hubfs/theme-frenos/whitepaper/Frenos%20Scaling%20Retentive%20Networks.pdf?hsLang=en&utm_source=chatgpt.com","title":"Whitepaper - Breakdown of Retentive Networks (RetNet)","snippet":"This paper reviews traditional neural network architectures and introduces one of the first domain-specific analyses of the. Retentive Network (RetNet) ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"frenos.io"}]},{"type":"search_result_group","domain":"sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0893608024004180?utm_source=chatgpt.com","title":"Leveraging spiking neural networks for topic modeling","snippet":"by M Bia\u0142as \u00b7 2024 \u00b7 Cited by 2 \u2014 SpikeGPT is a back-propagation-trained SNN that incorporates several mechanisms to enhance its robustness. Initial experiments indicate that SpikeGPT maintains ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"sciencedirect.com"}]},{"type":"search_result_group","domain":"wikidocs.net","entries":[{"type":"search_result","url":"https://wikidocs.net/236369?utm_source=chatgpt.com","title":"MEGA: Moving Average Equipped Gated Attention","snippet":"In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"wikidocs.net"}]},{"type":"search_result_group","domain":"alphaxiv.org","entries":[{"type":"search_result","url":"https://www.alphaxiv.org/ko/overview/2312.06635v6?utm_source=chatgpt.com","title":"Gated Linear Attention Transformers with Hardware ...","snippet":"The evaluation demonstrates GLA's effectiveness across multiple dimensions. On synthetic recall tasks, GLA significantly outperforms other sub-quadratic models, ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"alphaxiv.org"},{"type":"search_result","url":"https://www.alphaxiv.org/overview/2302.13939v5?utm_source=chatgpt.com","title":"Generative Pre-trained Language Model with Spiking ...","snippet":"SpikeGPT introduces the first large-scale, backpropagation-trained Spiking Neural Network (SNN) capable of generative language modeling.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"alphaxiv.org"}]},{"type":"search_result_group","domain":"aibusiness.com","entries":[{"type":"search_result","url":"https://aibusiness.com/ml/hyenadna-a-large-language-model-trained-on-human-genome-sequences?utm_source=chatgpt.com","title":"A Large Language Model for the Human Genome","snippet":"30 Jun 2023 \u2014 Researchers from Stanford unveiled HyenaDNA, a large language model trained on one million human genomic sequences.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"aibusiness.com"}]},{"type":"search_result_group","domain":"dataloop.ai","entries":[{"type":"search_result","url":"https://dataloop.ai/library/model/dbal0503_s5/?utm_source=chatgpt.com","title":"S5 \u00b7 Models","snippet":"The S5 model is a type of artificial intelligence designed to process sequences of data, like text or images. It's built using a simplified state space layer.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"dataloop.ai"}]},{"type":"search_result_group","domain":"raw.githubusercontent.com","entries":[{"type":"search_result","url":"https://raw.githubusercontent.com/mlresearch/v235/main/assets/yang24ab/yang24ab.pdf?utm_source=chatgpt.com","title":"Gated Linear Attention Transformers with Hardware-Efficient ...","snippet":"by S Yang \u00b7 Cited by 270 \u2014 GLA Transformer is found to be particularly strong at length generalization and recall-intensive tasks among linear recurrent models. For training speed, the ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"raw.githubusercontent.com"},{"type":"search_result","url":"https://raw.githubusercontent.com/mlresearch/v235/main/assets/sun24n/sun24n.pdf?utm_source=chatgpt.com","title":"Learning Latent Dynamic Robust Representations for World ...","snippet":"As a notable example, Dreamer (Hafner et al., 2019a; 2020; 2023) learns recurrent state-space mod- els (RSSM) and the latent state space via reconstruction.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"raw.githubusercontent.com"},{"type":"search_result","url":"https://raw.githubusercontent.com/mlresearch/v305/main/assets/wagenmaker25a/wagenmaker25a.pdf?utm_source=chatgpt.com","title":"Steering Your Diffusion Policy with Latent Space ... - GitHub","snippet":"Abstract: Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenar-.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"raw.githubusercontent.com"}]},{"type":"search_result_group","domain":"ecva.net","entries":[{"type":"search_result","url":"https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07855.pdf?utm_source=chatgpt.com","title":"FMBoost: Boosting Latent Diffusion with Flow Matching","snippet":"A small diffusion model can then effectively provide the necessary visual diversity, while flow matching efficiently enhances resolution and detail by mapping.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"ecva.net"},{"type":"search_result","url":"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830070.pdf?utm_source=chatgpt.com","title":"Improved Masked Image Generation with Token-Critic","snippet":"by J Lezama \u00b7 Cited by 60 \u2014 The non-autoregressive nature of MaskGIT allows orders-of-magnitude faster sampling, generating an image typically in 8-16 steps as opposed to hundreds of steps ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"ecva.net"}]},{"type":"search_result_group","domain":"analyticsvidhya.com","entries":[{"type":"search_result","url":"https://www.analyticsvidhya.com/blog/2024/11/jamba-1-5/?utm_source=chatgpt.com","title":"Jamba 1.5: Featuring the Hybrid Mamba-Transformer ...","snippet":"4 Nov 2024 \u2014 Jamba 1.5 combines Mamba and Transformer models, offering 94B & 12B parameter versions with 256K token context support for NLP tasks.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"analyticsvidhya.com"},{"type":"search_result","url":"https://www.analyticsvidhya.com/blog/2022/06/gato-a-new-generalist-artificial-intelligence-agent/?utm_source=chatgpt.com","title":"GATO - A New Generalist Artificial Intelligence Agent","snippet":"12 Jul 2022 \u2014 Deepmind instantiated Gato as a single, large, transformer sequence model. Another important point is that every task done by GATO uses the ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"analyticsvidhya.com"}]},{"type":"search_result_group","domain":"maartengrootendorst.com","entries":[{"type":"search_result","url":"https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state?utm_source=chatgpt.com","title":"A Visual Guide to Mamba and State Space Models","snippet":"\"Mamba: Linear-time sequence modeling with selective state spaces.\" arXiv preprint arXiv:2312.00752 (2023). This architecture is often referred to as a ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"maartengrootendorst.com"}]},{"type":"search_result_group","domain":"emerge.haus","entries":[{"type":"search_result","url":"https://www.emerge.haus/blog/long-context-windows-in-generative-ai?utm_source=chatgpt.com","title":"Long Context Windows in Generative AI: An AI Atlas Report","snippet":"21 Apr 2025 \u2014 AI models with ultra-long context windows\u2014up to 1M+ tokens\u2014are unlocking new capabilities in reasoning, memory, and document analysis.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"emerge.haus"}]},{"type":"search_result_group","domain":"fanpu.io","entries":[{"type":"search_result","url":"https://fanpu.io/summaries/2023-08-25-efficiently-modeling-long-sequences-with-structured-state-spaces/?utm_source=chatgpt.com","title":"Efficiently Modeling Long Sequences with Structured State ...","snippet":"25 Aug 2023 \u2014 Paper Abstract. A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"fanpu.io"}]},{"type":"search_result_group","domain":"cbirt.net","entries":[{"type":"search_result","url":"https://cbirt.net/hyenadna-powers-long-range-genomic-sequence-modeling-at-single-nucleotide-resolution/?utm_source=chatgpt.com","title":"HyenaDNA Powers Long-range Genomic Sequence ...","snippet":"12 Jul 2023 \u2014 HyenaDNA is an advanced genomic foundation model offering unparalleled long-range capabilities with up to 1 million token contexts.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"cbirt.net"}]},{"type":"search_result_group","domain":"harvard.edu","entries":[{"type":"search_result","url":"https://ui.adsabs.harvard.edu/abs/2023arXiv230513048P/abstract?utm_source=chatgpt.com","title":"RWKV: Reinventing RNNs for the Transformer Era","snippet":"by B Peng \u00b7 2023 \u00b7 Cited by 807 \u2014 We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"harvard.edu"}]},{"type":"search_result_group","domain":"alignmentforum.org","entries":[{"type":"search_result","url":"https://www.alignmentforum.org/posts/75uJN3qqzyxWoknN7/interpretability-externalities-case-study-hungry-hungry?utm_source=chatgpt.com","title":"Interpretability Externalities Case Study - Hungry ...","snippet":"20 Sept 2023 \u2014 The H3 paper proposes a way to use state space models (SSMs) for language models instead of attention. With an SSM it's possible to model longer ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"alignmentforum.org"},{"type":"search_result","url":"https://www.alignmentforum.org/posts/xxvKhjpcTAJwvtbWM/deepmind-s-gato-generalist-agent?utm_source=chatgpt.com","title":"Deepmind's Gato: Generalist Agent","snippet":"12 May 2022 \u2014 The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"alignmentforum.org"}]},{"type":"search_result_group","domain":"arsturn.com","entries":[{"type":"search_result","url":"https://www.arsturn.com/blog/mastering-claudes-1-million-token-context-window-a-practical-guide?utm_source=chatgpt.com","title":"Claude's 1M Token Window: A Guide to Practical Uses","snippet":"12 Aug 2025 \u2014 Explore the power of Claude 3's 1 million token context window. Learn practical uses for code, research, & customer support to unlock new AI ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"arsturn.com"}]},{"type":"search_result_group","domain":"ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2017/0700.pdf?utm_source=chatgpt.com","title":"Value Iteration Networks","snippet":"by A Tamar \u00b7 Cited by 836 \u2014 Abstract. We introduce the value iteration network (VIN): a fully differentiable neural network with a 'planning module' embedded within.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"ijcai.org"}]},{"type":"search_result_group","domain":"emergentmind.com","entries":[{"type":"search_result","url":"https://www.emergentmind.com/articles/2306.15794?utm_source=chatgpt.com","title":"HyenaDNA: Long-Range Genomic Modeling","snippet":"27 Jun 2023 \u2014 The paper presents HyenaDNA, an advanced genomic model that leverages the capabilities of the Hyena LLM for genomic applications. It addresses ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"emergentmind.com"}]},{"type":"search_result_group","domain":"johnthickstun.com","entries":[{"type":"search_result","url":"https://johnthickstun.com/assets/pdf/diffusion-lm.pdf?utm_source=chatgpt.com","title":"Diffusion-LM Improves Controllable Text Generation","snippet":"Diffusion-LM has higher perplexity than comparably-sized AR models. ... Diffusion-LM. \u2022 Previously non-autoregressive open-ended language generation has.See more","ref_id":{"turn_index":6,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"johnthickstun.com"}]},{"type":"search_result_group","domain":"neptune.ai","entries":[{"type":"search_result","url":"https://neptune.ai/blog/state-space-models-as-natural-language-models?utm_source=chatgpt.com","title":"Introduction to State Space Models as Natural Language ...","snippet":"In this first article of the three-part series, we'll examine the core principles of SSMs, trace their evolution from Linear State Space Layers (LSSL) to the S5 ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"neptune.ai"}]},{"type":"search_result_group","domain":"nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC12510832/?utm_source=chatgpt.com","title":"DreamerNav: learning-based autonomous navigation in ...","snippet":"by S Shanks \u00b7 2025 \u2014 This paper presents DreamerNav, a robot-agnostic navigation framework that extends DreamerV3, a state-of-the-art world model\u2013based reinforcement ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"nih.gov"}]},{"type":"search_result_group","domain":"anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/news/1m-context?utm_source=chatgpt.com","title":"Claude Sonnet 4 now supports 1M tokens of context","snippet":"12 Aug 2025 \u2014 Claude Sonnet 4 now supports up to 1 million tokens of context on the Anthropic API\u2014a 5x increase that lets you process entire codebases ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"anthropic.com"}]},{"type":"search_result_group","domain":"forbes.com","entries":[{"type":"search_result","url":"https://www.forbes.com/sites/aparnadhinakaran/2023/04/10/ten-questions-with-the-researchers-behind-h3-on-state-space-models/?utm_source=chatgpt.com","title":"Ten Questions With The Researchers Behind H3 On State ...","snippet":"10 Apr 2023 \u2014 Dan and Tri discuss the major ideas and history behind H3, state space models and what make them special, what products can be built with long-context language ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"forbes.com"}]},{"type":"search_result_group","domain":"general-navigation-models.github.io","entries":[{"type":"search_result","url":"https://general-navigation-models.github.io/nomad/?utm_source=chatgpt.com","title":"NoMaD: Goal Masking Diffusion Policies for Navigation and ...","snippet":"In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"general-navigation-models.github.io"}]},{"type":"search_result_group","domain":"auto-post.io","entries":[{"type":"search_result","url":"https://auto-post.io/blog/million-token-models-transform-content-generation?utm_source=chatgpt.com","title":"Million-token models: Transforming content generation","snippet":"15 Sept 2025 \u2014 Explore how million-token (1M) context windows like Claude Sonnet 4 and Gemini reshape content generation, code analysis, costs and latency.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"auto-post.io"}]},{"type":"search_result_group","domain":"scispace.com","entries":[{"type":"search_result","url":"https://scispace.com/pdf/safe-dreamerv3-safe-reinforcement-learning-with-world-models-pvwilqy8.pdf?utm_source=chatgpt.com","title":"safedreamer: safe reinforcement learning","snippet":"Leveraging the framework above, we develop SafeDreamer, a safe model-based RL algorithm that extends the architecture of DreamerV3 (Hafner et al., 2023). World ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"scispace.com"}]},{"type":"search_result_group","domain":"facebook.com","entries":[{"type":"search_result","url":"https://www.facebook.com/groups/DeepNetGroup/posts/1944592349266971/?utm_source=chatgpt.com","title":"Artificial Intelligence & Deep Learning - Hyena Hierarchy","snippet":"In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"facebook.com"}]},{"type":"search_result_group","domain":"zr9558.com","entries":[{"type":"search_result","url":"https://zr9558.com/2017/02/09/value-iteration-networks-2/?utm_source=chatgpt.com","title":"Value iteration networks - ZHANG RONG","snippet":"9 Feb 2017 \u2014 'Value Iteration Networks' won a best paper award at NIPS 2016. It tackles two of the hot issues in reinforcement learning at the moment: ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"zr9558.com"}]},{"type":"search_result_group","domain":"prnewswire.com","entries":[{"type":"search_result","url":"https://www.prnewswire.com/news-releases/unveiling-jamba-ai21s-groundbreaking-hybrid-ssm-transformer-open-source-model-302102779.html?utm_source=chatgpt.com","title":"Unveiling Jamba: AI21's Groundbreaking Hybrid SSM- ...","snippet":"28 Mar 2024 \u2014 \"We are excited to introduce Jamba, a groundbreaking hybrid architecture that combines the best of Mamba and Transformer technologies,\" said Or ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"prnewswire.com"}]},{"type":"search_result_group","domain":"jblanier.net","entries":[{"type":"search_result","url":"https://jblanier.net/redraw_lanier_et_al.pdf?utm_source=chatgpt.com","title":"Adapting World Models with Latent-State Dynamics ...","snippet":"This work introduces a novel method for learning residual dynamics corrections directly in the latent state space of world models, eliminating the need for ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"jblanier.net"}]},{"type":"search_result_group","domain":"escholarship.org","entries":[{"type":"search_result","url":"https://escholarship.org/content/qt89j5c7j1/qt89j5c7j1.pdf?utm_source=chatgpt.com","title":"Hierarchical Reinforcement Learning with Model-Based ...","snippet":"by TD Bartley \u00b7 2023 \u00b7 Cited by 3 \u2014 The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"escholarship.org"}]},{"type":"search_result_group","domain":"substack.com","entries":[{"type":"search_result","url":"https://gonzoml.substack.com/p/jamba-15-hybrid-transformer-mamba?utm_source=chatgpt.com","title":"Jamba-1.5: Hybrid Transformer-Mamba Models at Scale","snippet":"Jamba is a hybrid model combining SSM (State Space Models) and transformer components, specifically Mamba + MoE (Mixture-of-Experts) + transformer layers.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"substack.com"}]},{"type":"search_result_group","domain":"infoq.com","entries":[{"type":"search_result","url":"https://www.infoq.com/news/2022/05/deepmind-gato-ai-agent/?utm_source=chatgpt.com","title":"DeepMind Introduces Gato, a New Generalist AI Agent","snippet":"Gato's architecture isn't that different from many of the AI systems in use today. In the sense that it's a Transformer, it's similar to ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"infoq.com"}]},{"type":"search_result_group","domain":"towardsai.net","entries":[{"type":"search_result","url":"https://towardsai.net/p/l/understanding-mamba-and-selective-state-space-models-ssms?utm_source=chatgpt.com","title":"Understanding Mamba and Selective State Space Models ...","snippet":"24 Jun 2024 \u2014 State Space Models (SSMs) model systems changing over time. Selective SSMs use a selection mechanism and selective scan, unlike typical SSMs.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"towardsai.net"}]},{"type":"search_result_group","domain":"youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=ec56a8wmfRk&utm_source=chatgpt.com","title":"Retentive Network: A Successor to Transformer for Large ...","snippet":"#ai #retnet #transformers Retention is an alternative to Attention in Transformers that can both be written in a parallel and in a recurrent ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"youtube.com"},{"type":"search_result","url":"https://www.youtube.com/watch?v=x8pW19wKfXQ&utm_source=chatgpt.com","title":"RWKV: Reinventing RNNs for the Transformer Era (Paper ...","snippet":"The RW KV model combines the efficient parallelizable training of Transformers with the efficient inference of rnns.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":48},"pub_date":null,"attribution":"youtube.com"}]},{"type":"search_result_group","domain":"iucc.ac.il","entries":[{"type":"search_result","url":"https://cris.iucc.ac.il/en/publications/value-iteration-networks?utm_source=chatgpt.com","title":"Value iteration networks","snippet":"Abstract. We introduce the value iteration network (VIN): a fully differentiable neural network with a 'planning module' embedded within. VINs can learn to ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"iucc.ac.il"}]},{"type":"search_result_group","domain":"oxen.ai","entries":[{"type":"search_result","url":"https://ghost.oxen.ai/mamba-linear-time-sequence-modeling-with-selective-state-spaces-arxiv-dives/?utm_source=chatgpt.com","title":"Mamba: Linear-Time Sequence Modeling with Selective ...","snippet":"Mamba is a recurrent neural network, a selective structured state space model, that outperforms Transformers with faster inference and scales linearly with ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"oxen.ai"}]},{"type":"search_result_group","domain":"golden.com","entries":[{"type":"search_result","url":"https://golden.com/wiki/Retentive_Network-Y3EE8PG?utm_source=chatgpt.com","title":"Retentive Network - Golden","snippet":"25 Jul 2023 \u2014 RetNet was first proposed by researchers at Microsoft Research and Tsinghua University, Beijing, in a paper submitted on July 17, 2023. The ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"golden.com"}]},{"type":"search_result_group","domain":"studocu.com","entries":[{"type":"search_result","url":"https://www.studocu.com/in/document/university-of-mumbai/social-media-analytics/nips-2016-value-iteration-networks-paper/91183309?utm_source=chatgpt.com","title":"NIPS 2016: Value Iteration Networks in Reinforcement ...","snippet":"We introduce the value iteration network (VIN): a fully differentiable neural net- work with a 'planning module' embedded within. VINs can learn to plan, and ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":48},"pub_date":null,"attribution":"studocu.com"}]},{"type":"search_result_group","domain":"jack-clark.net","entries":[{"type":"search_result","url":"https://jack-clark.net/2024/04/08/?utm_source=chatgpt.com","title":"April 8, 2024","snippet":"8 Apr 2024 \u2014 With Jamba, AI21 is trying to figure out the best of both worlds where it can develop a model with some of the computational efficiency of SSM ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":49},"pub_date":null,"attribution":"jack-clark.net"}]},{"type":"search_result_group","domain":"ironbar.github.io","entries":[{"type":"search_result","url":"https://ironbar.github.io/understanding_intelligence/artificial-intelligence/02-gato/?utm_source=chatgpt.com","title":"Gato - Understanding Intelligence - Ironbar","snippet":"21 Sept 2022 \u2014 Gato, a generalist agent, is a paper released by Deepmind short after Flamingo on May 2022. ... As described above, Gato's network architecture ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":49},"pub_date":null,"attribution":"ironbar.github.io"}]}],"citations":[],"request_id":"75088bf7-f5d1-46cd-83c8-3f343bb0e444","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","thinking_effort":"max","parent_id":"fb93da24-ff0e-4f20-805d-37c05f230dda","turn_exchange_id":"a957d85e-79ce-444c-a00b-9b81b4257a2b","timestamp_":"absolute","can_save":true},"recipient":"all","channel":"final"},"parent":"fb93da24-ff0e-4f20-805d-37c05f230dda","children":[]}}}