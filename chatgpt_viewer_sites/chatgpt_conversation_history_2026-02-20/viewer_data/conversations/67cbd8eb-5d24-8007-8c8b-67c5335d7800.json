{"id":"67cbd8eb-5d24-8007-8c8b-67c5335d7800","title":"MuZero Algorithm Deep Dive","create_time":1741412587.542292,"update_time":1743204967.807537,"current_node":"9ebc14f4-c2be-42e7-b61a-4fa13608da9b","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["6fe0e1b3-b8be-4070-aa8c-e1142611227f"]},"6fe0e1b3-b8be-4070-aa8c-e1142611227f":{"id":"6fe0e1b3-b8be-4070-aa8c-e1142611227f","message":{"id":"6fe0e1b3-b8be-4070-aa8c-e1142611227f","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"client-created-root","children":["75158c6a-a7b2-4d47-a8a3-8836126853d9"]},"75158c6a-a7b2-4d47-a8a3-8836126853d9":{"id":"75158c6a-a7b2-4d47-a8a3-8836126853d9","message":{"id":"75158c6a-a7b2-4d47-a8a3-8836126853d9","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"6fe0e1b3-b8be-4070-aa8c-e1142611227f","children":["13266d64-dce8-4cd8-81d8-5014acae3d5b"]},"13266d64-dce8-4cd8-81d8-5014acae3d5b":{"id":"13266d64-dce8-4cd8-81d8-5014acae3d5b","message":{"id":"13266d64-dce8-4cd8-81d8-5014acae3d5b","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"75158c6a-a7b2-4d47-a8a3-8836126853d9","children":["31b54bc8-d868-4fcf-ae76-de5223436c01"]},"31b54bc8-d868-4fcf-ae76-de5223436c01":{"id":"31b54bc8-d868-4fcf-ae76-de5223436c01","message":{"id":"31b54bc8-d868-4fcf-ae76-de5223436c01","author":{"role":"user","name":null,"metadata":{}},"create_time":1741412574.767,"update_time":null,"content":{"content_type":"text","parts":["Give me a deep dive into this paper"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"attachments":[{"id":"file-1bCzUqA8dc7HUfKwpvtS9r","size":1862123,"name":"10.1038@s41586-020-03051-4.pdf","mime_type":"application/pdf","file_token_size":15472}],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"91d0035e0b57fe87-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"13266d64-dce8-4cd8-81d8-5014acae3d5b","children":["b643db42-ff3e-42be-a86a-67d6217c7a08"]},"b643db42-ff3e-42be-a86a-67d6217c7a08":{"id":"b643db42-ff3e-42be-a86a-67d6217c7a08","message":{"id":"b643db42-ff3e-42be-a86a-67d6217c7a08","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1741412599.567962,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"","summary":"","assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"command":"spinner","status":"running","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"31b54bc8-d868-4fcf-ae76-de5223436c01","request_id":"91d0035e0b57fe87-SIN","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"31b54bc8-d868-4fcf-ae76-de5223436c01","children":["21756fc0-4e3a-4670-8417-8ce182aaed78"]},"21756fc0-4e3a-4670-8417-8ce182aaed78":{"id":"21756fc0-4e3a-4670-8417-8ce182aaed78","message":{"id":"21756fc0-4e3a-4670-8417-8ce182aaed78","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1741412599.568087,"update_time":null,"content":{"content_type":"tether_quote","url":"file-1bCzUqA8dc7HUfKwpvtS9r","domain":"10.1038@s41586-020-03051-4.pdf","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","title":"10.1038@s41586-020-03051-4.pdf","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"context_stuff","is_visually_hidden_from_conversation":false,"retrieval_turn_number":0,"retrieval_file_index":0,"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"b643db42-ff3e-42be-a86a-67d6217c7a08","request_id":"91d0035e0b57fe87-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"b643db42-ff3e-42be-a86a-67d6217c7a08","children":["26f2479c-90c1-4195-827b-e79a8e23f676"]},"26f2479c-90c1-4195-827b-e79a8e23f676":{"id":"26f2479c-90c1-4195-827b-e79a8e23f676","message":{"id":"26f2479c-90c1-4195-827b-e79a8e23f676","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1741412599.5682,"update_time":null,"content":{"content_type":"text","parts":["All the files uploaded by the user have been fully loaded. Searching won't provide additional information."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":false,"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"21756fc0-4e3a-4670-8417-8ce182aaed78","request_id":"91d0035e0b57fe87-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"21756fc0-4e3a-4670-8417-8ce182aaed78","children":["38f8eeb7-53cb-48c6-b0aa-040e3ce2e000"]},"38f8eeb7-53cb-48c6-b0aa-040e3ce2e000":{"id":"38f8eeb7-53cb-48c6-b0aa-040e3ce2e000","message":{"id":"38f8eeb7-53cb-48c6-b0aa-040e3ce2e000","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1741412588.608507,"update_time":null,"content":{"content_type":"text","parts":["**Deep diving into MuZero**\n\nAlright, the user wants a deep dive into the paper \u201cMastering Atari, Go, chess and shogi by planning with a learned model\u201d by DeepMind. The focus is on the MuZero algorithm, which merges model-based planning with reinforcement learning. MuZero's unique in how it learns a model predicting policy, value, and reward without knowing the environment\u2019s true dynamics. It builds on AlphaZero but is more general. What I need to do now is a detailed report on the algorithm, its experiments, results, and conclusions. Let\u2019s get started!\n\n**Analyzing MuZero\u2019s contributions**\n\nI need to dive into the key contributions of MuZero. This paper is well-known, and I'm familiar with it, so I\u2019ll break it down.\n\nFirst, I'll touch on the context, model-based reinforcement learning (RL), and how MuZero differs by learning a latent state representation without needing an environment simulator. I'll explain its architecture, including its components and the model\u2019s training process, like loss functions and training details.\n\nThen, I'll discuss experiments, comparisons with AlphaZero, and the strengths of MuZero, like its scalability and the strength of the MCTS search algorithm. Let's dig into this thoroughly!"]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned about paper analysis for 4 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","finished_duration_sec":4,"parent_id":"26f2479c-90c1-4195-827b-e79a8e23f676","request_id":"91d0035e0b57fe87-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"26f2479c-90c1-4195-827b-e79a8e23f676","children":["36c181a4-b6ea-4925-b0db-187aae758e47"]},"36c181a4-b6ea-4925-b0db-187aae758e47":{"id":"36c181a4-b6ea-4925-b0db-187aae758e47","message":{"id":"36c181a4-b6ea-4925-b0db-187aae758e47","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1741412587.773607,"update_time":null,"content":{"content_type":"text","parts":["Below is a deep dive into the paper \u201cMastering Atari, Go, chess and shogi by planning with a learned model\u201d by DeepMind. This analysis covers the key ideas, algorithmic innovations, experimental evaluations, and broader implications of the work.\n\n---\n\n## 1. Overview and Motivation\n\nThe paper tackles one of the longstanding challenges in artificial intelligence\u2014how to plan effectively in complex environments without having explicit knowledge of the underlying dynamics. Previous successes in planning (e.g. in chess or Go) relied on perfect simulators or known game rules. In contrast, many real-world problems (like robotics or industrial control) lack an exact simulator. The authors introduce **MuZero**, a model-based reinforcement learning (RL) algorithm that learns a compact, latent model of the environment dynamics and uses it for planning via Monte Carlo Tree Search (MCTS). Notably, MuZero achieves superhuman performance on board games (Go, chess, shogi) as well as visually complex domains (Atari), all without any prior knowledge of the rules or dynamics of the environments. \ue200cite\ue202turn0file0\ue201\n\n---\n\n## 2. Core Components of the MuZero Algorithm\n\n### a. Latent Model Learning\n\nUnlike traditional model-based methods that learn to predict full environment observations (e.g. pixels), MuZero focuses only on predicting what\u2019s necessary for planning. Its learned model comprises three main functions:\n\n- **Representation Function (h):**  \n  Converts the sequence of past observations into a compact hidden state \\( s_0 \\). This state does not attempt to reconstruct the raw observation but encapsulates the information necessary to predict future outcomes.\n\n- **Dynamics Function (g):**  \n  Given the current hidden state \\( s_{k-1} \\) and an action \\( a_k \\), it produces a new hidden state \\( s_k \\) and an immediate reward \\( r_k \\). This recurrent formulation mimics state transitions without needing to explicitly model the full environment.\n\n- **Prediction Function (f):**  \n  From the hidden state \\( s_k \\), it predicts a policy \\( p_k \\) (i.e. a distribution over actions) and a value \\( v_k \\) (expected cumulative future rewards).\n\nThis design allows MuZero to \u201clearn to predict\u201d only the information relevant for planning, freeing it from the burden of modeling all aspects of the observation. \ue200cite\ue202turn0file0\ue201\n\n### b. Integration with Planning\n\nMuZero integrates its learned model with MCTS to search over possible future action sequences. Key points include:\n\n- **Tree Search:**  \n  At each time step, the algorithm performs a lookahead search using MCTS, where the tree nodes are defined by hidden states from the learned model rather than explicit environment states. The search improves the policy by aggregating simulated outcomes.\n\n- **Upper Confidence Bounds:**  \n  The selection of actions during search is guided by a modified PUCT (Predictor + UCB) rule that balances the value estimates with the learned policy priors. This ensures exploration of promising moves even when the learned model might be imperfect.\n\n- **Handling Terminal States and Intermediate Rewards:**  \n  Unlike AlphaZero which deals primarily with terminal rewards in board games, MuZero is generalized to handle intermediate rewards and discounting, making it applicable to a broader range of environments (e.g. Atari). \ue200cite\ue202turn0file0\ue201\n\n---\n\n## 3. Training Objectives and Loss Function\n\nThe training of MuZero is end-to-end and jointly optimizes all three functions (representation, dynamics, prediction). The overall loss comprises:\n\n- **Policy Loss:**  \n  Minimizes the difference between the predicted policy and the improved search policy from MCTS.\n\n- **Value Loss:**  \n  Uses n-step returns (bootstrapped) to reduce the error between the predicted value and the observed cumulative reward.\n\n- **Reward Loss:**  \n  Trains the dynamics function to predict immediate rewards accurately.\n\n- **Regularization:**  \n  An L2 regularization term is added to prevent overfitting.\n\nThe paper also discusses practical choices such as using cross-entropy for policy loss and different scaling strategies during training to ensure stable gradients when unrolling the model for several hypothetical steps. \ue200cite\ue202turn0file0\ue201\n\n---\n\n## 4. Experimental Evaluation\n\n### a. Board Games: Go, Chess, and Shogi\n\n- **Performance Matching or Exceeding AlphaZero:**  \n  Even though AlphaZero was provided with the full game rules (i.e., a perfect simulator), MuZero matches or slightly exceeds its performance in these domains. This is remarkable since MuZero relies solely on its learned latent model for planning.\n  \n- **Efficiency in Computation:**  \n  MuZero uses fewer network evaluations per simulation (16 residual blocks vs. 20 in AlphaZero) and caches computation effectively through its recurrent structure.\n\n### b. Atari Games\n\n- **State-of-the-Art Performance:**  \n  In the Atari domain\u2014a setting characterized by high-dimensional visual input and complex dynamics\u2014MuZero achieves superior mean and median scores compared to previous model-free approaches (such as R2D2 and IMPALA).\n  \n- **Sample Efficiency:**  \n  The algorithm demonstrates significant sample efficiency gains, especially when extended with a variant called **MuZero Reanalyze** that revisits past trajectories to update its policy targets using the latest model parameters.\n\n### c. Scalability of Planning\n\n- **Search Depth Flexibility:**  \n  Experiments show that MuZero\u2019s learned model can generalize to longer search horizons than it was trained on. In Go, the performance scales with increased planning time, and even in Atari, modest improvements are observed with additional simulations per move. \ue200cite\ue202turn0file0\ue201\n\n---\n\n## 5. Broader Implications and Key Contributions\n\n- **Bridging Model-Based and Model-Free RL:**  \n  MuZero effectively merges the strengths of planning (traditionally associated with model-based methods) and the robust learning capabilities of model-free RL. This hybrid approach leverages lookahead search to refine policy decisions while learning a minimal model that only encodes planning-relevant information.\n\n- **General Applicability:**  \n  By not requiring a perfect simulator, MuZero opens the door to applying such planning techniques to real-world problems where the dynamics are complex or unknown\u2014ranging from robotics to industrial control systems.\n\n- **Innovation in Model Learning:**  \n  The paper challenges the assumption that accurate environment modeling (e.g., pixel-level reconstruction) is necessary. Instead, it shows that an abstract latent representation can be sufficient to drive high-quality decision-making.\n\n- **Efficiency Considerations:**  \n  Despite the heavy computational resources used (TPUs and thousands of simulations per move in board games), the approach is competitive in terms of sample efficiency and computational demands relative to pure model-free methods.\n\n---\n\n## 6. Limitations and Future Directions\n\n- **Computational Demands:**  \n  Although sample-efficient, the search and network evaluation components still require significant computational power, which may limit immediate applicability in resource-constrained settings.\n\n- **Deterministic Dynamics:**  \n  The current implementation of MuZero uses a deterministic dynamics function. Extending it to stochastic environments remains an open challenge.\n\n- **Generalization Beyond Games:**  \n  While the results are impressive in benchmark domains, applying MuZero to continuous, non-stationary, or real-world scenarios (with imperfect or partial observability) will require further research.\n\n- **Zero-Shot Transfer:**  \n  The idea of sharing or transferring the learned model across similar environments has been mentioned as future work, which could significantly broaden the algorithm\u2019s impact.\n\n---\n\n## 7. Conclusion\n\nMuZero represents a significant step forward in reinforcement learning by showing that it is possible to plan effectively without full knowledge of the environment\u2019s dynamics. Its ability to learn a latent representation tailored for planning, combined with a robust MCTS framework, allows it to excel across diverse domains\u2014from visually complex Atari games to the strategic depths of chess, shogi, and Go. While there are computational and generalization challenges ahead, the paper lays a strong foundation for future AI systems that can learn to plan and act in complex, real-world environments without relying on explicit simulators. \ue200cite\ue202turn0file0\ue201\n\n---\n\nThis analysis is intended to provide a clear, precise, and forward-looking view of the paper\u2019s contributions, strengths, and the challenges that remain."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[{"start_ix":1093,"end_ix":1110,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":2320,"end_ix":2337,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":3329,"end_ix":3346,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":4208,"end_ix":4225,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":5674,"end_ix":5691,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":8463,"end_ix":8480,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}}],"content_references":[{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":1093,"end_idx":1110,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":2320,"end_idx":2337,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":3329,"end_idx":3346,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":4208,"end_idx":4225,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":5674,"end_idx":5691,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":8463,"end_idx":8480,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null}],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"38f8eeb7-53cb-48c6-b0aa-040e3ce2e000","request_id":"91d0035e0b57fe87-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"38f8eeb7-53cb-48c6-b0aa-040e3ce2e000","children":["eaae8e80-7c71-4ef8-a140-eb18225db765"]},"eaae8e80-7c71-4ef8-a140-eb18225db765":{"id":"eaae8e80-7c71-4ef8-a140-eb18225db765","message":{"id":"eaae8e80-7c71-4ef8-a140-eb18225db765","author":{"role":"user","name":null,"metadata":{}},"create_time":1741413155.419,"update_time":null,"content":{"content_type":"text","parts":["Explore how the MuZero architecture or underlying principles can be incorporated into LLM training  and reasoning models"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"91d0118fcbba6041-ATL","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"36c181a4-b6ea-4925-b0db-187aae758e47","children":["e05b1fd2-a3e0-4fd0-be8b-e27dae8638a9"]},"e05b1fd2-a3e0-4fd0-be8b-e27dae8638a9":{"id":"e05b1fd2-a3e0-4fd0-be8b-e27dae8638a9","message":{"id":"e05b1fd2-a3e0-4fd0-be8b-e27dae8638a9","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1741413171.035539,"update_time":null,"content":{"content_type":"text","parts":["**Considering MuZero and LLMs**\n\nThe user wants to explore how MuZero's architecture or principles could apply to large language models (LLMs). MuZero, a model-based RL algorithm, learns a latent model for planning in games. LLMs and MuZero share a few conceptual similarities: both involve prediction, planning with an internal model, and reasoning with lookahead. I think LLM training could potentially benefit from incorporating planning components, or using learned latent states to guide search during reasoning processes. It's an interesting concept to think about!\n\n**Incorporating MuZero into LLMs**\n\nMuZero\u2019s model-based RL and planning principles could inform LLM training. For example, utilizing latent representations for planning, similar to MuZero, might refine LLM reasoning. Instead of predicting tokens directly, a planning module could guide logical structure predictions. Integrating search, like MCTS, into LLMs could help, especially with chain-of-thought prompting. Furthermore, MuZero\u2019s encoding of relevant information for planning could inspire more efficient LLM representations, focusing on logical structures. Lastly, incorporating iterative updates akin to recurrence might suit multi-step reasoning in LLMs. Interesting possibilities at every turn!\n\n**Exploring MuZero for LLMs**\n\nOne interesting idea is incorporating \"thinking time\" or \"internal simulation\" in LLMs, much like MuZero uses planning in decision-making. For tasks requiring multi-step reasoning or problem-solving, we could integrate a search algorithm, like MCTS, to explore different reasoning chains and select the best. MuZero\u2019s approach of joint training\u2014aligning predictions with outcomes\u2014could also help LLMs improve internal reasoning consistency. However, LLMs\u2019 discrete space and dynamics make this a challenge, though latent models could focus on reasoning structure. Interesting potential connections to recent LLM planning work!"]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned about MuZero and LLM integration for 9 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","finished_duration_sec":9,"parent_id":"eaae8e80-7c71-4ef8-a140-eb18225db765","request_id":"91d0118fcbba6041-ATL","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"eaae8e80-7c71-4ef8-a140-eb18225db765","children":["ee0b5e03-baa4-40b3-b261-e45171931592"]},"ee0b5e03-baa4-40b3-b261-e45171931592":{"id":"ee0b5e03-baa4-40b3-b261-e45171931592","message":{"id":"ee0b5e03-baa4-40b3-b261-e45171931592","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1741413169.12864,"update_time":null,"content":{"content_type":"text","parts":["Incorporating MuZero\u2019s principles into LLM training and reasoning can be thought of as adding an internal \u201cplanning layer\u201d to language models. Here\u2019s how some of the core ideas can be adapted:\n\n---\n\n### 1. Learning a Task-Relevant Latent Representation\n\n**MuZero\u2019s Approach:**  \nRather than reconstructing full observations (e.g. pixel-level details), MuZero learns a compact latent state that captures only the information necessary for decision-making\u2014namely, the predictions of future rewards, values, and policies. \ue200cite\ue202turn0file0\ue201\n\n**LLM Adaptation:**  \nFor language models, a similar latent representation could encode the \u201cessence\u201d of a reasoning or problem-solving task. Instead of merely predicting the next token, the model would learn an abstract representation of the context and intermediate reasoning steps (a chain-of-thought) that are directly relevant to arriving at a final answer. This abstraction would help the model ignore extraneous linguistic detail and focus on the logical structure of the task.\n\n---\n\n### 2. Integrating a Planning or Search Component\n\n**MuZero\u2019s Approach:**  \nMuZero uses Monte Carlo Tree Search (MCTS) over its latent space to evaluate different action trajectories. This search refines the policy and value estimates by simulating possible futures before making a move. \ue200cite\ue202turn0file0\ue201\n\n**LLM Adaptation:**  \nFor reasoning tasks, an LLM could similarly benefit from an internal planning mechanism. Instead of a single autoregressive pass, the model could generate multiple \u201creasoning trajectories\u201d (or chains-of-thought) and then use a search algorithm\u2014akin to MCTS or even the recently proposed Tree-of-Thoughts\u2014to evaluate which trajectory leads to the most coherent or highest-scoring answer. This would help in tasks where multi-step reasoning is essential, such as complex problem-solving or multi-hop question answering.\n\n---\n\n### 3. Joint Training with Multi-Objective Losses\n\n**MuZero\u2019s Approach:**  \nThe network is trained end-to-end with losses that encourage accurate predictions of the policy, value, and immediate rewards over several hypothetical steps. This joint optimization ensures that the latent state is directly useful for planning. \ue200cite\ue202turn0file0\ue201\n\n**LLM Adaptation:**  \nA similar multi-objective training regime could be designed for LLMs. In addition to the standard language modeling objective (next-token prediction), the model could be trained to:\n- **Predict Intermediate Reasoning Outcomes:** Align predicted intermediate states (or reasoning summaries) with targets derived from a supervised chain-of-thought.\n- **Estimate a \u201cValue\u201d of the Reasoning Path:** Use an auxiliary head to assess the quality or coherence of a generated reasoning trajectory.\n- **Guide Future Predictions:** Adjust the policy head (which might control the generation process) based on feedback from the planning search.\n\nThis multi-task setup could lead to a model that not only generates fluent text but also organizes its internal reasoning in a way that is optimal for reaching correct conclusions.\n\n---\n\n### 4. Iterative Refinement and Reanalysis\n\n**MuZero\u2019s Approach:**  \nThe \u201cMuZero Reanalyze\u201d variant revisits past trajectories with the latest model parameters to improve its policy targets. This continual refinement enhances sample efficiency and consistency. \ue200cite\ue202turn0file0\ue201\n\n**LLM Adaptation:**  \nAn LLM could incorporate a similar mechanism where generated reasoning paths are periodically re-evaluated or refined using updated internal representations. For instance, after an initial answer is generated, a secondary process could \u201creanalyze\u201d the reasoning steps\u2014possibly using updated parameters or additional context\u2014to ensure that the final output is coherent and well-supported.\n\n---\n\n### 5. Challenges and Future Directions\n\n- **Discrete vs. Continuous Spaces:**  \n  LLMs operate in a vast discrete token space, which poses unique challenges compared to the well-defined action spaces in games. Designing a latent planning mechanism that efficiently navigates this space will require novel approaches.\n\n- **Computational Overheads:**  \n  Incorporating an internal search (e.g., MCTS) or reanalysis loop adds computation. Finding the right balance between planning depth and efficiency is crucial, especially for real-time or large-scale applications.\n\n- **Training Signal for Reasoning:**  \n  Unlike game outcomes (win/loss or score), evaluating intermediate reasoning quality is less straightforward. It may require new forms of supervision, perhaps derived from human feedback (akin to RLHF), to effectively guide the planning component.\n\n- **Interpretability and Control:**  \n  An added benefit of this approach is that the planning mechanism can make the model\u2019s internal reasoning more transparent, potentially allowing for better debugging and control over its outputs.\n\n---\n\n### In Summary\n\nAdapting MuZero\u2019s architecture to LLMs involves teaching the model not just to predict the next word, but to build an internal, planning-oriented representation of the task at hand. By incorporating a learned latent state, an integrated search or planning mechanism, and multi-objective training that rewards coherent reasoning, we could create language models that reason more effectively on complex, multi-step tasks. This synthesis of planning and language modeling opens exciting avenues for creating AI systems that not only generate fluent language but also exhibit robust, interpretable reasoning capabilities."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[{"start_ix":519,"end_ix":536,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":1317,"end_ix":1334,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":2204,"end_ix":2221,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":3327,"end_ix":3344,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","text":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}}],"content_references":[{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":519,"end_idx":536,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":1317,"end_idx":1334,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":2204,"end_idx":2221,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":3327,"end_idx":3344,"alt":null,"type":"file","name":"10.1038@s41586-020-03051-4.pdf","id":"file-1bCzUqA8dc7HUfKwpvtS9r","source":"my_files","snippet":"\ue200cite\ue202turn0file0\ue201\n\nMastering Atari, Go, chess and shogi by planning with a learned model\n\n\n604 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nMastering Atari, Go, chess and shogi by \nplanning with a learned model\n\nJulian Schrittwieser1,3, Ioannis Antonoglou1,2,3, Thomas Hubert1,3, Karen Simonyan1, \nLaurent Sifre1, Simon Schmitt1, Arthur Guez1, Edward Lockhart1, Demis Hassabis1, \nThore Graepel1,2, Timothy Lillicrap1 & David Silver1,2,3\u2009\u2709\n\nConstructing agents with planning capabilities has long been one of the main \nchallenges in the pursuit of artificial intelligence. Tree-based planning methods have \nenjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect \nsimulator is available. However, in real-world problems, the dynamics governing the \nenvironment are often complex and unknown. Here we present the MuZero \nalgorithm, which, by combining a tree-based search with a learned model, achieves \nsuperhuman performance in a range of challenging and visually complex domains, \nwithout any knowledge of their underlying dynamics. The MuZero algorithm learns \nan iterable model that produces predictions relevant to planning: the action-selection \npolicy, the value function and the reward. When evaluated on 57 different Atari \ngames3\u2014the canonical video game environment for testing artificial intelligence \ntechniques, in which model-based planning approaches have historically struggled4\u2014\nthe MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, \nchess and shogi\u2014canonical environments for high-performance planning\u2014the \nMuZero algorithm matched, without any knowledge of the game dynamics, the \nsuperhuman performance of the AlphaZero algorithm5 that was supplied with the \nrules of the game.\n\nPlanning algorithms based on lookahead search have achieved remark-\nable successes in artificial intelligence. Human world champions have \nbeen defeated in classic games such as checkers6, chess1, Go2 and \npoker7,8, and planning algorithms have had real-world impact in applica-\ntions from logistics9 to chemical synthesis10. However, these planning \nalgorithms all rely on knowledge of the environment\u2019s dynamics, such \nas the rules of the game or an accurate simulator, preventing their direct \napplication to real-world domains such as robotics, industrial control \nor intelligent assistants, where the dynamics are normally unknown.\n\nModel-based reinforcement learning (RL)11 aims to address this issue \nby first learning a model of the environment\u2019s dynamics and then plan-\nning with respect to the learned model. Typically, these models have \neither focused on reconstructing the true environmental state12\u201314 \nor the sequence of full observations15,16. However, previous work15\u201317 \nremains far from the state of the art in visually rich domains, such as \nAtari 2600 games3. Instead, the most successful methods are based \non model-free RL18\u201320\u2014that is, they estimate the optimal policy and/\nor value function directly from interactions with the environment. \nHowever, model-free algorithms are in turn far from the state of the \nart in domains that require precise and sophisticated lookahead, such \nas chess and Go.\n\nHere we introduce MuZero, a new approach to model-based RL that \nachieves both state-of-the-art performance in Atari 2600 games\u2014a \nvisually complex set of domains\u2014and superhuman performance in \nprecision planning tasks such as chess, shogi and Go, without prior \n\nknowledge of the game dynamics. MuZero builds on AlphaZero\u2019s5 pow-\nerful search and policy iteration algorithms, but incorporates a learned \nmodel into the training procedure. MuZero also extends AlphaZero \nto a broader set of environments, including single agent domains and \nnon-zero rewards at intermediate time steps.\n\nThe main idea of the algorithm (summarized in Fig.\u00a01) is to predict \nthose aspects of the future that are directly relevant for planning. The \nmodel receives the observation (for example, an image of the Go board \nor the Atari screen) as an input and transforms it into a hidden state. \nThe hidden state is then updated iteratively by a recurrent process that \nreceives the previous hidden state and a hypothetical next action. At \nevery one of these steps, the model produces a policy (predicting the \nmove to play), value function (predicting the cumulative reward, for \nexample, the eventual winner) and immediate reward prediction (for \nexample, the points scored by playing a move). The model is trained \nend to end, with the sole objective of accurately estimating these three \nimportant quantities, to match the improved policy and value function \ngenerated by search, as well as the observed reward. There is no direct \nrequirement or constraint on the hidden state to capture all informa-\ntion necessary to reconstruct the original observation, drastically \nreducing the amount of information the model has to maintain and \npredict. Neither is there any requirement for the hidden state to match \nthe unknown, true state of the environment; nor any other constraints \non the semantics of state. Instead, the hidden states are free to repre-\nsent any state that correctly estimates the policy, value function and \n\nhttps://doi.org/10.1038/s41586-020-03051-4\n\nReceived: 3 April 2020\n\nAccepted: 7 October 2020\n\nPublished online: 23 December 2020\n\n Check for updates\n\n1DeepMind, London, UK. 2University College London, London, UK. 3These authors contributed equally: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver.  \n\u2709e-mail: davidsilver@google.com\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://crossmark.crossref.org/dialog/?doi=10.1038/s41586-020-03051-4&domain=pdf\nmailto:davidsilver@google.com\n\n\nNature | Vol 588 | 24/31 December 2020 | 605\n\nreward. Intuitively, the agent can invent, internally, any dynamics that \nlead to accurate planning.\n\nPrevious work\nRL can be subdivided into two principal categories: model based and \nmodel free11. Model-based RL constructs, as an intermediate step, a \nmodel of the environment. Classically, this model is represented by \na Markov decision process (MDP)21 consisting of two components: a \nstate transition model, predicting the next state given the selected \naction, and a reward model, predicting the expected reward during that \ntransition. Once a model has been constructed, it is straightforward \nto apply MDP planning algorithms, such as value iteration21 or Monte \nCarlo tree search (MCTS)22, to compute the optimal value function or \noptimal policy for the MDP. In large or partially observed environments, \nthe algorithm must first construct the state representation that the \nmodel should predict. This tripartite separation between representa-\ntion learning, model learning and planning is potentially problematic, \nas the agent is not able to optimize its representation or model for the \npurpose of effective planning, so, for example, modelling errors may \ncompound during planning.\n\nA common approach to model-based RL focuses on directly mod-\nelling the observation stream at the pixel level. It has been hypoth-\nesized that deep, stochastic models may mitigate the problems of \ncompounding error15,16. However, planning at pixel-level granularity \nis not computationally tractable in large-scale problems. Other meth-\nods build a latent state-space model that is sufficient to reconstruct \nthe observation stream at the pixel level23,24 or to predict its future \nlatent states25,26, which facilitates more efficient planning but still \nfocuses the majority of the model capacity on potentially irrelevant \ndetail. None of these previous methods have constructed a model \nthat facilitates effective planning in visually complex domains such \nas Atari; results lag behind well tuned, model-free methods, even in \nterms of data efficiency27.\n\nA quite different approach to model-based RL has recently been \ndeveloped, focused end to end on predicting the value function28\u201333. \nThe main idea of these methods is to construct an abstract MDP model \n\nsuch that planning in the abstract MDP is equivalent to planning in the \nreal environment. This is achieved by ensuring value equivalence, that \nis, that, starting from the same real state, the cumulative reward of a \ntrajectory through the abstract MDP matches the cumulative reward \nof a trajectory in the real environment.\n\nThe predictron29 introduced value equivalent models for predicting \nvalue\u00a0functions (without actions). Although the underlying model still \ntakes the form of an MDP, there is no requirement for its transition \nmodel to match real states in the environment. Instead the MDP model \nis viewed as a hidden layer of a deep neural network. The unrolled MDP \nis trained such that the expected cumulative sum of rewards matches \nthe expected value with respect to the real environment, for example, \nby temporal-difference learning.\n\nValue equivalent models have also been applied to optimizing \nvalue (with actions). Value-aware model learning30,31 constructs an \nMDP model, such that a step of value iteration using the model pro-\nduces the same outcome as the real environment. TreeQN32 learns \nan abstract MDP model, such that a tree search over that model \n(represented by a tree-structured neural network) approximates \nthe optimal value function. Value iteration networks28 learn a local \nMDP model, such that many steps of value iteration over that model \n(represented by a convolutional neural network) approximates the \noptimal value function.\n\nValue prediction networks33 are perhaps the closest precursor to \nMuZero: they learn an MDP model grounded in real actions; the unrolled \nMDP is trained such that the cumulative sum of rewards, conditioned \non the actual sequence of actions generated by a simple lookahead \nsearch, matches the real environment. Unlike MuZero there is no policy \nprediction, and the search utilizes only value prediction.\n\nMuZero algorithm\nWe now describe the MuZero algorithm in more detail. Predictions are \nmade at each time step t, for each of k\u00a0=\u00a00, \u2026, K steps, by a model \u03bc\u03b8, \nwith parameters \u03b8, conditioned on past observations o1, ..., ot and for \nk\u00a0>\u00a00 on future actions at+1, ..., at+k. The model predicts three future \nquantities: the policy p \u03c0 a o o a a\u2248 ( | , \u2026, , , \u2026, )t\n\nk\nt k t t t k+ +1 1 +1 + , the value \n\np0,v0 p1,v1 p2,v2\n\nf f f\n\nf\n\np2,v2\n\np3,v3\n\np1,v1\n\np0,v0\n\nf\n\nf\n\nf\n\na b\n\ncs1\n\ns2\n\ns3\n\nr2\n\nr3\n\nr1\n\na2\n\na3\n\na1\n\nh\n\ng gr1 r2\n\nvt+1 vt+2vt\nat+1 at+2 at+3\n\nat+1 at+2 at+3\n\nut+1 ut+2 ut+3\n\nt t+1 t+2\n\ns0\n\ng\n\ng\n\ng\n\nh\n\ns0 s2s1\n\nFig. 1 | Planning, acting and training with a learned model. a, How MuZero \nuses its model to plan. The model consists of three connected components for \nrepresentation, dynamics and prediction. Given a previous hidden state sk\u22121 and \na candidate action ak, the dynamics function g produces an immediate reward \nrk and a new hidden state sk. The policy pk and value function vk are computed \nfrom the hidden state sk by a prediction function f. The initial hidden state s0 is \nobtained by passing the past observations (for example, the Go board or  \nAtari screen) into a representation function h. b, How MuZero acts in the \nenvironment. An MCTS is performed at each timestep t, as described in a. An \naction at+1 is sampled from the search policy \u03c0t, which is proportional to the visit \ncount for each action from the root node. The environment receives the action \nand generates a new observation ot+1 and reward ut+1. At the end of the episode, \n\nthe trajectory data are stored into a replay buffer. c, How MuZero trains its \nmodel. A trajectory is sampled from the replay buffer. For the initial step, the \nrepresentation function h receives as input the past observations o1, ..., ot from \nthe selected trajectory. The model is subsequently unrolled recurrently for K \nsteps. At each step k, the dynamics function g receives as input the hidden state \nsk\u22121 from the previous step and the real action at+k. The parameters of the \nrepresentation, dynamics and prediction functions are jointly trained, end to \nend, by backpropagation through time, to predict three quantities: the policy \npk \u2248 \u03c0t+k, value function vk \u2248 zt+k and reward rk \u2248 ut+k, where zt+k is a sample return: \neither the final reward (board games) or n-step return (Atari). Schematic Go \nboards at the top of the figure represent the sequence of observations.\n\n\n\n606 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\n\nfunction Ev u \u03b3u o o a a\u2248 [ + + \u2026| , \u2026, , , \u2026, ]t\nk\n\nt k t k t t t k+ +1 + +2 1 +1 +  and, for k \n> 0, also the immediate reward r u\u2248t\n\nk\nt k+ , where u. is the true, observed \n\nreward, \u03c0 is the policy used to select real actions and \u03b3 is the discount \nfunction of the environment.\n\nInternally, at each time step t (subscripts t are suppressed for simplic-\nity), the model is represented by the combination of a representation \nfunction, a dynamics function and a prediction function. The dynamics \nfunction g\u03b8, is a recurrent process, rk, sk\u00a0=\u00a0g\u03b8(sk\u22121, ak), that computes, at \neach hypothetical step k, an immediate reward rk and an internal state \nsk. It mirrors the structure of an MDP model that computes the expected \nreward and state transition for a given state and action21. However, \nunlike traditional approaches to model-based RL11, this internal state \nsk has no semantics of environment state attached to it\u2014it is simply the \nhidden state of the overall model and its sole purpose is to accurately \npredict relevant, future quantities: policies, values and rewards. In this \npaper, the dynamics function is represented deterministically; the \nextension to stochastic transitions is left for future work. A prediction \nfunction f\u03b8 computes the policy and value functions from the internal \nstate sk, pk, vk\u00a0=\u00a0f\u03b8(sk), akin to the joint policy and value network of Alp-\nhaZero. A representation function h\u03b8 initializes the \u2018root\u2019 state s0 by \nencoding past observations, s0\u00a0=\u00a0h\u03b8(o1, ..., ot); again, this has no special \nsemantics beyond its support for future predictions.\n\nGiven such a model, it is possible to search over hypothetical future \ntrajectories a1, ..., ak given past observations o1, ..., ot. For example, a \nnaive search could simply select the k-step action sequence that max-\nimizes the value function. More generally, we may apply any MDP plan-\nning algorithm to the internal rewards and state space induced by the \ndynamics function. Specifically, we use an MCTS algorithm similar to \nAlphaZero\u2019s search, generalized to allow for single-agent domains and \nintermediate rewards (Methods). The MCTS algorithm may be viewed \nas a search policy \u03c0t\u00a0=\u00a0P[at+1|o1, ..., ot] and search value function \u03bdt \u2248 E\n[ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+...|o1, ..., ot] that both selects an action and predicts cumu-\nlative reward given past observations o1, ..., ot. At each internal node, \nit makes use of the policy, value function and reward estimate produced \n\nby the current model parameters \u03b8, and combines these values together \nusing lookahead search to produce an improved policy \u03c0t and improved \nvalue function \u03bdt at the root of the search tree. The next action at+1 \u2248 \u03c0t \nis then chosen by the search policy.\n\nAll parameters of the model are trained jointly to accurately match \nthe policy, value function and reward prediction, for every hypo-\nthetical step k, to three corresponding targets observed after k actual \ntime steps have elapsed. Similarly to AlphaZero, the first objective is \nto minimize the error between the actions predicted by the policy  \npt\n\nk  and by the search policy \u03c0t+k. Also like AlphaZero, value targets  \nare generated by playing out the game or MDP using the search  \npolicy. However, unlike AlphaZero, we allow for long episodes with \ndiscounting and intermediate rewards by computing an n-step return \nzt that bootstraps n steps into the future from the search value,  \nzt\u00a0=\u00a0ut+1\u00a0+\u00a0\u03b3ut+2\u00a0+\u00a0...\u00a0+\u00a0\u03b3n\u22121ut+n\u00a0+\u00a0\u03b3n\u03bdt+n. Final outcomes {lose, draw, win} in \nboard games are treated as rewards ut\u00a0\u2208\u00a0{\u22121, 0, +1} occurring at the \nfinal step of the episode. Specifically, the second objective is to min-\nimize the error between the value function vt\n\nk and the value target, \nzt+k. The third objective is to minimize the error between the predicted \nimmediate reward r t\n\nk and the observed immediate reward ut+k. Finally, \nan L2 regularization term is also added, scaled by a constant c, leading \nto the overall loss\n\n\u2211 \u2211 \u2211l \u03b8 l \u03c0 p l z v l u r c \u03b8( ) = ( , ) + ( , ) + ( , ) + || || , (1)t\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\nk\n\nK\n\nt k t\nk\n\n=0\n\np\n+\n\n=0\n\nv\n+\n\n=1\n\nr\n+\n\n2\n\nwhere lp, lv and lr are loss functions for policy, value and reward, respec-\ntively. Supplementary Fig.\u00a02 summarizes the equations governing \nhow the MuZero algorithm plans, acts and learns. We note that for \nchess, Go and shogi, the same squared error loss as AlphaZero is used \nfor rewards and values. A cross-entropy loss was found to be more \nstable than a squared error when encountering rewards and values \nof variable scale in Atari. Cross-entropy was used for the policy loss \nin both cases.\n\nChess Shogi Go Atari\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n0 0.2 0.4 0.6 0.8 1.0\n\nMillions of training steps\n\nE\nlo\n\n5,000\n\n4,000\n\n3,000\n\n2,000\n\n1,000\n\n0\n\nR\new\n\nar\nd\n\nFig. 2 | Evaluation of MuZero throughout training in chess, shogi, Go and \nAtari. The x axis shows millions of training steps. For chess, shogi and Go, the y \naxis shows Elo rating, established by playing games against AlphaZero using \n800 simulations per move for both players. MuZero\u2019s Elo is indicated by the \nblue line and AlphaZero\u2019s Elo is indicated by the horizontal orange line. For \nAtari, mean (full line) and median (dashed line) human normalized scores \n\nacross all 57 games are shown on the y axis. The scores for R2D219 (the previous \nstate of the art in this domain, based on model-free RL) are indicated by the \nhorizontal orange lines. Performance in Atari was evaluated using 50 \nsimulations every fourth time step, and then repeating the chosen action four \ntimes, as in previous work39. Supplementary Fig.\u00a01 studies the repeatability of \ntraining in Atari.\n\n\n\nNature | Vol 588 | 24/31 December 2020 | 607\n\nResults\nWe applied the MuZero algorithm to the classic board games Go, chess \nand shogi, as benchmarks for challenging planning problems, and to \nall 57 games in the Atari learning environment3, as benchmarks for \nvisually complex RL domains.\n\nIn each case, we trained MuZero for K\u00a0=\u00a05 hypothetical steps. Train-\ning proceeded for one million mini-batches of size 2,048 in board \ngames and of size 1,024 in Atari. During both training and evalua-\ntion, MuZero used 800 simulations for each search in board games \nand 50 simulations for each search in Atari. The representation \n\nfunction uses the same convolutional34 and residual35 architecture as  \nAlphaZero, but with 16 residual blocks instead of 20. The dynam-\nics function uses the same architecture as the representation  \nfunction and the prediction function uses the same architecture \nas AlphaZero. All networks use 256 hidden planes (see\u00a0Methods \nfor further details).\n\nFigure\u00a02 shows the performance throughout training in each game. In \nGo, MuZero slightly exceeded the performance of AlphaZero, despite \nusing less computation per node in the search tree (16 residual blocks \nper evaluation in MuZero compared with 20 blocks in AlphaZero). This \nsuggests that MuZero may be caching its computation in the search \n\nTable 1 | Comparison of MuZero against previous agents in Atari\n\nAgent Median (%) Mean (%) Environment frames Training time Training steps\n\nApe-X20 434.1 1,695.6 22.8 billion 5 days 8.64\u00a0million\n\nR2D219 1,920.6 4,024.9 37.5 billion 5 days 2.16\u00a0million\n\nMuZero 2,041.1 4,999.2 20.0 billion 12 hours 1\u00a0million\n\nIMPALA18 191.8 957.6 200 million \u2013 \u2013\n\nRainbow36 231.1 \u2013 200 million 10 days \u2013\n\nUNREALa 42 250a 880a 250 million \u2013 \u2013\n\nLASER37 431 \u2013 200 million \u2013 \u2013\n\nMuZero Reanalyze 731.1 2,168.9 200 million 12 hours 1\u00a0million\n\nWe compare separately against agents trained in large (top) and small (bottom) data settings; all agents other than MuZero used model-free RL techniques. Mean and median scores are given, \ncompared with human testers. The best results are highlighted in bold. MuZero shows state-of-the-art performance in both settings. aHyperparameters were tuned per game.\n\n0.1 0.2 0.5 1 2 5 10 20 50\nSearch time (s)\n\n4,000\n\n4,200\n\n4,400\n\n4,600\n\n4,800\n\n5,000\n\n5,200\n\n5,400\n\nE\nlo\n\na\n\n1 10 25 50 100 200 500\nNumber of simulations per move\n\n3,500\n\n4,000\n\n4,500\n\n5,000\n\n5,500\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nb\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\n0\n\n50,000\n\n100,000\n\n150,000\n\n200,000\n\n250,000\n\nM\nea\n\nn \nre\n\nw\nar\n\nd\n\nc\n\n0 0.2 0.4 0.6 0.8 1.0\nMillions of training steps\n\nd\n\nLearned model\nReal simulator\n\nMuZero\nR2D2\n\nMuZero\n\nR2D2\nQ-learning\n\nTrain 50 sims\nTrain 25 sims\nTrain 10 sims\nTrain 7 sims\nTrain 6 sims\nTrain 5 sims\n\nFig. 3 | Evaluations of MuZero on Go, all 57 Atari games and Ms. Pac-Man.  \na, Scaling with search time per move in Go, comparing the learned model with \nthe ground truth simulator. Both networks were trained at 800 simulations \nper\u00a0search, equivalent to 0.1\u00a0s per\u00a0search. Remarkably, the learned model is able \nto scale well to up to two orders of magnitude longer searches than seen during \ntraining. b, Scaling of final human normalized mean score in Atari with the \nnumber of simulations per\u00a0search. The network was trained at 50 simulations \nper\u00a0search. Dark line indicates mean score and the shaded regions indicate the \n25th to 75th and 5th to 95th percentiles. The learned model\u2019s performance \nincreases up to 100 simulations per\u00a0search. Beyond, even when scaling to  \nmuch longer searches than during training, the learned model\u2019s performance \nremains stable and decreases only slightly. This contrasts with the much better \nscaling in Go (a), presumably due to greater model inaccuracy in Atari than Go. \n\nc, Comparison of MCTS-based training with Q-learning in the MuZero \nframework on Ms. Pac-Man, keeping network size and amount of training \nconstant. The state-of-the-art Q-learning algorithm R2D2 is shown as a \nbaseline. Our Q-learning implementation reaches the same final score as R2D2, \nbut improves slower and results in much lower final performance compared \nwith MCTS-based training. d, Different networks trained at different numbers \nof simulations (sims) per move, but all evaluated at 50 simulations per move. \nNetworks trained with more simulations per move improve faster, consistent \nwith ablation (b), where the policy improvement is larger when using more \nsimulations per move. Surprisingly, MuZero can learn effectively even when \ntraining with less simulations per move than are enough to cover all eight \npossible actions in Ms. Pac-Man.\n\n\n\n608 | Nature | Vol 588 | 24/31 December 2020\n\nArticle\ntree and using each additional application of the dynamics model to \ngain a deeper understanding of the position.\n\nIn Atari, MuZero achieved state-of-the-art performance for both \nmean and median normalized score across the 57 games of the arcade \nlearning environment, outperforming the previous state-of-the-art \nmethod R2D219 (a model-free approach) in 42 out of 57 games, and \noutperforming the previous best model-based approach SimPLe16 in \nall games (Table\u00a01 and Supplementary Table\u00a01).\n\nWe also evaluated a second version of MuZero that was optimized \nfor greater sample efficiency. Specifically, it reanalyses old trajec-\ntories by re-running the MCTS using the latest network parameters \nto provide fresh targets (see \u2018MuZero Reanalyze\u2019 in Methods). When \napplied to 57 Atari games, using 200 million frames of experience per \ngame, MuZero Reanalyze achieved 731% median normalized score, \ncompared with 192%, 231% and 431% for previous state-of-the-art \nmodel-free approaches IMPALA18, Rainbow36 and LASER37,  \nrespectively.\n\nTo understand the role of the model in MuZero, we also ran several \nexperiments, focusing on the board game of Go and the Atari game \nof Ms. Pac-Man.\n\nFirst, we tested the scalability of planning (Fig.\u00a03a), in the canonical \nplanning problem of Go. We compared the performance of search in \nAlphaZero, using a perfect model, to the performance of search in \nMuZero, using a learned model. Specifically, the fully trained AlphaZero \nor MuZero was evaluated by comparing MCTS with different thinking \ntimes. MuZero matched the performance of a perfect model, even when \ndoing much larger searches (thinking time\u00a0of\u00a0up to 10\u00a0s) than those \nfrom which the model was trained (thinking time of\u00a0around 0.1\u00a0s; see \nalso Supplementary Fig.\u00a03a).\n\nWe also investigated the scalability of planning across all Atari games \n(Fig.\u00a03b). We compared MCTS with different numbers of simulations, \nusing the fully trained MuZero. The improvements due to planning are \nmuch less marked than in Go, perhaps because of greater model inac-\ncuracy; performance improved slightly with search time, but plateaued \nat around 100 simulations. Even with a single simulation\u2014that is, when \nselecting moves solely according to the policy network\u2014MuZero per-\nformed well, suggesting that, by the end of training, the raw policy has \nlearned to internalize the benefits of search (see also Supplementary \nFig.\u00a03b).\n\nNext, we tested our model-based learning algorithm against a compa-\nrable model-free learning algorithm (Fig.\u00a03c). We replaced the training \nobjective of MuZero (equation\u00a0(1)) with a model-free Q-learning objec-\ntive (as used by R2D2), and the dual policy and value heads with a single \nhead representing the action-value function Q(\u22c5|st). Subsequently, we \ntrained and evaluated the new model without using any search. When \nevaluated on Ms. Pac-Man, our model-free algorithm achieved identical \nresults to R2D2, but learned much slower than MuZero and converged \nto a much lower final score. We conjecture that the search-based policy \nimprovement step of MuZero provides a stronger learning signal than \nthe high-bias, high-variance targets used by Q-learning.\n\nTo better understand the nature of MuZero\u2019s learning algorithm, we \nmeasured how MuZero\u2019s training scales with respect to the amount \nof search it uses during training. Figure\u00a03d shows the performance in \nMs. Pac-Man, using an MCTS of different simulation counts per move \nthroughout training. Surprisingly, and in contrast to previous work38, \neven with only six simulations per move\u2014fewer than the number of \nactions\u2014MuZero learned an effective policy and improved rapidly. \nWith more simulations, the performance jumped much higher. For \nanalysis of the policy improvement during each individual iteration, \nsee also Supplementary Fig.\u00a03c, d.\n\nConclusions\nMany of the breakthroughs in artificial intelligence have been based on \neither high-performance planning1,2,5 or model-free RL methods39\u201341. \n\nHere we have introduced a method that combines the benefits of both \napproaches. Our algorithm, MuZero, has both matched the superhu-\nman performance of high-performance planning algorithms in their \nfavoured domains (logically complex board games such as chess and \nGo) and outperformed state-of-the-art model-free RL algorithms in \ntheir favoured domains (visually complex Atari games). Crucially, our \nmethod does not require any knowledge of the environment dynamics, \npotentially paving the way towards the application of powerful learning \nand planning methods to a host of real-world domains for which there \nexists no perfect simulator.\n\nOnline content\nAny methods, additional references, Nature Research reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41586-020-03051-4.\n\n1. Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell. 134, 57\u201383 (2002).\n2. Silver, D. et\u00a0al. Mastering the game of Go with deep neural networks and tree search. \n\nNature 529, 484\u2013489 (2016).\n3. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning \n\nenvironment: an evaluation platform for general agents. J. Artif. Intell. Res. 47,  \n253\u2013279 (2013).\n\n4. Machado, M. et\u00a0al. Revisiting the arcade learning environment: evaluation protocols and \nopen problems for general agents. J. Artif. Intell. Res. 61, 523\u2013562 (2018).\n\n5. Silver, D. et\u00a0al. A general reinforcement learning algorithm that masters chess, shogi, and \nGo through self-play. Science 362, 1140\u20131144 (2018).\n\n6. Schaeffer, J. et\u00a0al. A world championship caliber checkers program. Artif. Intell. 53,  \n273\u2013289 (1992).\n\n7. Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top \nprofessionals. Science 359, 418\u2013424 (2018).\n\n8. Morav\u010d\u00edk, M. et\u00a0al. Deepstack: expert-level artificial intelligence in heads-up no-limit \npoker. Science 356, 508\u2013513 (2017).\n\n9. Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report (EETN, 2013).\n10. Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses with deep neural \n\nnetworks and symbolic AI. Nature 555, 604\u2013610 (2018).\n11. Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction 2nd edn (MIT Press, \n\n2018).\n12. Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient approach to \n\npolicy search. In Proc. 28th International Conference on Machine Learning, ICML 2011 \n465\u2013472 (Omnipress, 2011).\n\n13. Heess, N. et\u00a0al. Learning continuous control policies by stochastic value gradients. In \nNIPS\u201915: Proc. 28th International Conference on Neural Information Processing Systems \nVol. 2 (eds Cortes, C. et\u00a0al.) 2944\u20132952 (MIT Press, 2015).\n\n14. Levine, S. & Abbeel, P. Learning neural network policies with guided policy search under \nunknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071\u20131079 (2014).\n\n15. Hafner, D. et\u00a0al. Learning latent dynamics for planning from pixels. Preprint at https://\narxiv.org/abs/1811.04551 (2018).\n\n16. Kaiser, L. et\u00a0al. Model-based reinforcement learning for atari. Preprint at https://arxiv.org/\nabs/1903.00374 (2019).\n\n17. Buesing, L. et\u00a0al. Learning and querying fast generative models for reinforcement \nlearning. Preprint at https://arxiv.org/abs/1802.03006 (2018).\n\n18. Espeholt, L. et\u00a0al. IMPALA: scalable distributed deep-RL with importance weighted \nactor-learner architectures. In Proc. International Conference on Machine Learning, ICML \nVol. 80 (eds Dy, J. & Krause, A.) 1407\u20131416 (2018).\n\n19. Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent experience \nreplay in distributed reinforcement learning. In International Conference on Learning \nRepresentations (2019).\n\n20. Horgan, D. et\u00a0al. Distributed prioritized experience replay. In International Conference on \nLearning Representations (2018).\n\n21. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming \n1st edn (John Wiley & Sons, 1994).\n\n22. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In \nInternational Conference on Computers and Games 72\u201383 (Springer, 2006).\n\n23. Wahlstr\u00f6m, N., Sch\u00f6n, T. B. & Deisenroth, M. P. From pixels to torques: policy \nlearning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251 \n(2015).\n\n24. Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed to control: a locally \nlinear latent dynamics model for control from raw images. In NIPS\u201915: Proc. 28th \nInternational Conference on Neural Information Processing Systems Vol. 2 (eds Cortes, C. \net\u00a0al.) 2746\u20132754 (MIT Press, 2015).\n\n25. Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy evolution. In NIPS\u201918: \nProc. 32nd International Conference on Neural Information Processing Systems  \n(eds Bengio, S. et\u00a0al.) 2455\u20132467 (Curran Associates, 2018).\n\n26. Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G. DeepMDP: learning \ncontinuous latent space models for representation learning. Proc. 36th International \nConference on Machine Learning: Volume 97 of Proc. Machine Learning Research  \n(eds Chaudhuri, K. & Salakhutdinov, R.) 2170\u20132179 (PMLR, 2019).\n\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1811.04551\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1903.00374\nhttps://arxiv.org/abs/1802.03006\nhttp://arxiv.org/abs/1502.02251\n\n\nNature | Vol 588 | 24/31 December 2020 | 609\n\n27. van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric models in \nreinforcement learning? Preprint at https://arxiv.org/abs/1906.05243 (2019).\n\n28. Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration networks. Adv. Neural \nInf. Process. Syst. 29, 2154\u20132162 (2016).\n\n29. Silver, D. et\u00a0al. The predictron: end-to-end learning and planning. In Proc. 34th \nInternational Conference on Machine Learning Vol. 70 (eds Precup, D. & Teh, Y. W.)  \n3191\u20133199 (JMLR, 2017).\n\n30. Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function for model-based \nreinforcement learning. In Proc. 20th International Conference on Artificial Intelligence \nand Statistics: Volume 54 of Proc. Machine Learning Research (eds Singh, A. & Zhu, J) \n1486\u20131494 (PMLR, 2017).\n\n31. Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf. Process. Syst. 31, \n9090\u20139101 (2018).\n\n32. Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and ATreeC: differentiable \ntree planning for deep reinforcement learning. In International Conference on Learning \nRepresentations (2018).\n\n33. Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural Inf. Process. Syst. 30, \n6118\u20136128 (2017).\n\n34. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097\u20131105 (2012).\n\n35. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In 14th \nEuropean Conference on Computer Vision 630\u2013645 (2016).\n\n36. Hessel, M. et\u00a0al. Rainbow: combining improvements in deep reinforcement learning. In \nThirty-Second AAAI Conference on Artificial Intelligence (2018).\n\n37. Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with shared experience \nreplay. Preprint at https://arxiv.org/abs/1909.11583 (2019).\n\n38. Azizzadenesheli, K. et\u00a0al. Surprising negative results for generative adversarial tree \nsearch. Preprint at http://arxiv.org/abs/1806.05780 (2018).\n\n39. Mnih, V. et\u00a0al. Human-level control through deep reinforcement learning. Nature 518, \n529\u2013533 (2015).\n\n40. Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/ (2018).\n41. Vinyals, O. et\u00a0al. Grandmaster level in StarCraft II using multi-agent reinforcement \n\nlearning. Nature 575, 350\u2013354 (2019).\n42. Jaderberg, M. et\u00a0al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at \n\nhttps://arxiv.org/abs/1611.05397 (2016).\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\n\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020\n\nhttps://arxiv.org/abs/1906.05243\nhttps://arxiv.org/abs/1909.11583\nhttp://arxiv.org/abs/1806.05780\nhttps://blog.openai.com/openai-five/\nhttps://arxiv.org/abs/1611.05397\n\n\nArticle\nMethods\n\nComparison to AlphaZero\nMuZero is designed for a more general setting than AlphaGo Zero43 \nand AlphaZero5.\n\nIn AlphaGo Zero and AlphaZero, the planning process makes use \nof a simulator that samples the next state and reward (for example, \naccording to the environment\u2019s dynamics, or the rules of the game). \nThe simulator updates the state of the game while traversing the search \ntree (Fig.\u00a01a). The simulator is used to provide three important pieces of \nknowledge: (1) state transitions in the search tree, (2) actions available \nat each node of the search tree and (3) episode termination within the \nsearch tree. In MuZero, all of these have been replaced with the use of \na single implicit model learned by a neural network (Fig.\u00a01b).\n\n(1) State transitions. AlphaZero had access to a perfect simulator of \nthe environment\u2019s dynamics. In contrast, MuZero employs a learned \ndynamics model within its search. Under this model, each node in the \ntree is represented by a corresponding hidden state; by providing a \nhidden state sk\u22121 and an action ak to the model, the search algorithm \ncan transition to a new node sk\u00a0=\u00a0g(sk\u22121, ak).\n\n(2) Actions available. We consider a standard problem formulation \nwhere the set of available actions is provided at each time step along-\nside the observation. During search, however, it could be helpful to \nspecify the available actions at each interior node\u2014which would require \nknowledge of how the available actions change over time. AlphaZero \nused the set of legal actions obtained from the simulator to mask the \npolicy network at interior nodes. MuZero does not perform any masking \nwithin the search tree, but only masks legal actions at the root of the \nsearch tree where the set of available actions is directly observed. The \npolicy network rapidly learns to exclude actions that are unavailable, \nsimply because they are never selected.\n\n(3) Terminal states. AlphaZero stopped the search at tree nodes rep-\nresenting terminal states and used the terminal value provided by the \nsimulator instead of the value produced by the network. MuZero does \nnot give special treatment to terminal states and always uses the value \npredicted by the network. Inside the tree, the search can proceed past \na state that would terminate the simulator. In this case, the network is \nexpected to always predict the same value, which may be achieved by \nmodelling terminal states as absorbing states during training.\n\nIn addition, MuZero is designed to operate in the general RL setting: \nsingle-agent domains with discounted intermediate rewards of arbitrary \nmagnitude. In contrast, AlphaGo Zero and AlphaZero were designed to \noperate in two-player games with undiscounted terminal rewards of\u00a0\u00b11.\n\nMany other generalizations of MuZero may be possible, for example, \nto stochastic, continuous, non-stationary or temporally extended \nenvironments, or to imperfect information or general sum games. \nThese generalizations are left for future work.\n\nSearch\nWe now describe the search algorithm used by MuZero. Our approach \nis based on MCTS with upper confidence bounds, an approach to plan-\nning that converges asymptotically to the optimal policy in single agent \ndomains and to the minimax value function in zero sum games44.\n\nEvery node of the search tree is associated with an internal state s. For \neach action a from s there is an edge (s, a) that stores a set of statistics \n{N(s, a), P(s, a), Q(s, a), R(s, a), S(s, a)}, respectively representing visit \ncounts N, policy P, mean value Q, reward R and state transition S.\n\nSimilar to AlphaZero, the search is divided into three stages, repeated \nfor a number of simulations.\n\nSelection. Each simulation starts from the internal root state s0, and \nfinishes when the simulation reaches a leaf node sl. For each hypotheti-\ncal time step k\u00a0=\u00a01 ... l of the simulation, an action ak is selected accord-\ning to the stored statistics for internal state sk\u22121, by maximizing over a \nprobabilistic upper confidence tree (PUCT)\u00a0bound5,45\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f4\n\n\uf8f1\n\uf8f2\n\uf8f3\n\n\uf8ee\n\n\uf8f0\n\uf8ef\n\n\uf8eb\n\n\uf8ed\n\uf8ec\n\n\uf8f6\n\n\uf8f8\n\uf8f7\n\uf8f9\n\n\uf8fb\n\uf8fa\n\uf8fc\n\uf8fd\n\uf8fe\n\na Q s a\n\nP s a\nN s b\n\nN s a\nc\n\nN s b c\nc\n\n= arg max ( , )\n\n+ ( , )\n\u2211 ( , )\n\n1 + ( , )\n+ log\n\n\u2211 ( , ) + + 1\n,\n\n(2)\n\nk\n\na\n\nb b\n1\n\n2\n\n2\n\nwhere a and b are possible actions. The constants c1 and c2 are used to \ncontrol the influence of the policy P(s, a) relative to the value Q(s, a) as \nnodes are visited more often. In our experiments, c1\u00a0=\u00a01.25 and c2\u00a0=\u00a019,652.\n\nFor k\u00a0<\u00a0l, the next state and reward are looked up in the state transi-\ntion and reward table sk\u00a0=\u00a0S(sk\u22121, ak), rk\u00a0=\u00a0R(sk\u22121, ak).\n\nExpansion. At the final time step l of the simulation, the reward and state \nare computed by the dynamics function, rl, sl\u00a0=\u00a0g\u03b8(sl\u22121, al), and stored in \nthe corresponding tables, R(sl\u22121, al)\u00a0=\u00a0rl, S(sl\u22121, al)\u00a0=\u00a0sl. The policy and value \nfunction are computed by the prediction function, pl, vl\u00a0=\u00a0f\u03b8 (sl). A new \nnode, corresponding to state sl is added to the search tree. Each edge (sl, \na) from the newly expanded node is initialized to {N(sl, a)\u00a0=\u00a00, Q(sl, a)\u00a0=\u00a00, \nP(sl, a)\u00a0=\u00a0pl}. Note that the search algorithm makes at most one call to the \ndynamics function and prediction function respectively per simulation; \nthe computational cost is of the same order as in AlphaZero.\n\nBackup. At the end of the simulation, the statistics along the trajectory \nare updated. The backup is generalized to the case where the environ-\nment can emit intermediate rewards, have a discount \u03b3 different from 1 \nand the value estimates are unbounded. (We note that in board games, \nthe discount is assumed to be 1 and there are no intermediate rewards.) \nFor k\u00a0=\u00a0l ... 0, we form an l\u00a0\u2212\u00a0k-step estimate of the cumulative discounted \nreward, bootstrapping from the value function vl\n\n\u2211G \u03b3 r \u03b3 v= + . (3)k\n\n\u03c4\n\nl k\n\u03c4\n\nk \u03c4\nl k l\n\n=0\n\n\u22121\u2212\n\n+1+\n\u2212\n\nFor k\u00a0=\u00a0l ... 1, we update the statistics for each edge (sk\u22121, ak) in the \nsimulation path as follows\n\nQ s a\nN s a Q s a G\n\nN s a\n\nN s a N s a\n\n( , ):=\n( , ) \u00d7 ( , ) +\n\n( , ) + 1\n,\n\n( , ):= ( , ) + 1.\n\n(4)\nk k\n\nk k k k k\n\nk k\n\nk k k k\n\n\u22121\n\u22121 \u22121\n\n\u22121\n\n\u22121 \u22121\n\nIn two-player zero sum games, the value functions are assumed to \nbe bounded within the [0, 1] interval. This choice allows us to combine \nvalue estimates with probabilities using a variant of the PUCT rule45 \n(equation\u00a0 (2)). However, as in many environments the value is \nunbounded, it is necessary to adjust the PUCT rule. A simple solution \nwould be to use the maximum score that can be observed in the envi-\nronment to either rescale the value or set the PUCT constants appro-\npriately46. However, both solutions are game specific and require \nadding prior knowledge to the MuZero algorithm. To avoid this, MuZero \ncomputes normalized Q-value estimates Q \u2208 [0, 1] by using the mini-\nmum\u2013maximum values observed in the search tree up to that point. \nWhen a node is reached during the selection stage, the algorithm com-\nputes the normalized Q  values of its edges to be used in place of the Q \nvalues\u00a0in the PUCT rule using the equation\n\nQ s a\nQ s a Q s a\n\nQ s a Q s a\n( , ) =\n\n( , ) \u2212 min ( , )\n\nmax ( , ) \u2212 min ( , )\n. (5)k k\n\nk k\n\ns a\n\ns a s a\n\n\u22121\n\n\u22121\n\n, \u2208Tree\n\n, \u2208Tree , \u2208Tree\n\nHyperparameters\nFor simplicity we preferentially use the same architectural choices \nand hyperparameters as in previous work. Specifically, we started with \nthe network architecture and search choices of AlphaZero5. For board \n\n\n\ngames, we use the same PUCT constants, Dirichlet exploration noise \nand the same 800 simulations per\u00a0search as in AlphaZero.\n\nOwing to the much smaller branching factor and simpler policies in \nAtari, we used only 50 simulations per\u00a0search to speed up experiments. \nAs shown in Fig.\u00a03b, the algorithm is not very sensitive to this choice. \nWe also use the same discount (0.997) and value transformation (see \n\u2018Network architecture\u2019) as R2D219.\n\nFor parameter values not mentioned in the text, please refer to the \npseudocode (see \u2018Code availability\u2019).\n\nData generation\nTo generate training data, the latest checkpoint of the network (updated \nevery 1,000 training steps) is used to play games with MCTS. In the \nboard games Go, chess and shogi, the search is run for 800 simulations \nper move to pick an action; in Atari, due to the much smaller action \nspace 50 simulations per move are sufficient.\n\nFor board games, games are sent to the training job as soon as they \nfinish. Owing to the much larger length of Atari games (up to 30\u00a0min or \n108,000 frames), intermediate sequences are sent every 200 moves. \nIn board games, the training job keeps an in-memory replay buffer of \nthe most recent one million games received; in Atari, where the visual \nobservations are larger, the most recent 125,000 sequences of length \n200 are kept.\n\nDuring the generation of experience in the board game domains, \nthe same exploration scheme as the one described in AlphaZero5 is \nused. Using a variation of this scheme, in the Atari domain, actions are \nsampled from the visit count distribution throughout the duration of \neach game, instead of just the first k moves. Moreover, the visit count \ndistribution is parametrized using a temperature parameter T\n\n\u03c0 a s\nN s a\n\nN s b\n( | ) =\n\n( , )\n\u2211 ( , )\n\n. (6)\nT\n\nb\nT\n\n1/\n\n1/\n\nT is decayed as a function of the number of training steps of the net-\nwork. Specifically, for the first 500,000 training steps a temperature of  \n1.0 is used, for the next 250,000 steps a temperature of 0.5 and for the \nremaining 250,000 a temperature of 0.25. This ensures that the action \nselection becomes greedier as training progresses.\n\nObservation and action encoding\nRepresentation function. The history over board states used as input \nto the representation function for Go, chess and shogi is represented \nsimilarly to AlphaZero5. In Go and shogi, we encode the last eight board \nstates as in AlphaZero; in chess, we increased the history to the last 100 \nboard states to allow correct prediction of draws.\n\nFor Atari, the input of the representation function includes the last \n32 RGB frames at resolution 96\u00a0\u00d7\u00a096 along with the last 32 actions that \nled to each of those frames. We encode the historical actions because \nunlike board games, an action in Atari does not necessarily have a visible \neffect on the observation. RGB frames are encoded as one plane per \ncolour, rescaled to the range [0, 1], for red, green and blue, respectively. \nWe perform no other normalization, whitening or other preprocessing \nof the RGB input. Historical actions are encoded as simple bias planes, \nscaled as a/18 (there are 18 total actions in Atari).\n\nDynamics function. The input to the dynamics function is the hidden \nstate produced by the representation function or previous application of \nthe dynamics function, concatenated with a representation of the action \nfor the transition. Actions are encoded spatially in planes of the same \nresolution as the hidden state. In Atari, this resolution is 6\u00a0\u00d7\u00a06 (see descrip-\ntion of downsampling in \u2018Network architecture\u2019), in board games, this is \nthe same as the board size (19\u00a0\u00d7\u00a019 for Go, 8\u00a0\u00d7\u00a08 for chess, 9\u00a0\u00d7\u00a09 for shogi).\n\nIn Go, a normal action (playing a stone on the board) is encoded as \nan all-zero plane, with a single one in the position of the played stone. \nA pass is encoded as an all-zero plane.\n\nIn chess, eight planes are used to encode the action. The first one-hot \nplane encodes which position the piece was moved from. The next two \nplanes encode which position the piece was moved to: a one-hot plane \nto encode the target position, if on the board, and a second binary plane \nto indicate whether the target was valid (on the board) or not. This is \nnecessary because for simplicity, our policy action space enumerates \na superset of all possible actions, not all of which are legal, and we use \nthe same action space for policy prediction and to encode the dynamics \nfunction input. The remaining five binary planes are used to indicate \nthe type of promotion, if any (queen, knight, bishop, rook, none).\n\nThe encoding for shogi is similar, with a total of 11 planes. We use \nthe first eight planes to indicate where the piece moved from\u2014either \na board position (first one-hot plane) or the drop of one of the seven \ntypes of prisoner (remaining seven binary planes). The next two planes \nare used to encode the target as in chess. The remaining binary plane \nindicates whether the move was a promotion or not.\n\nIn Atari, an action is encoded as a one-hot vector that is tiled appro-\npriately into planes.\n\nNetwork architecture. The prediction function pk, vk\u00a0=\u00a0f\u03b8(sk) uses the \nsame architecture as AlphaZero: one or two convolutional layers that \npreserve the resolution but reduce the number of planes, followed by \na fully connected layer to the size of the output.\n\nFor value and reward prediction in Atari, we follow ref. 47 in scaling \ntargets using an invertible transform h x x x \u03b5x( ) = sign( )( | | + 1 \u2212 1) + , \nwhere \u03b5\u00a0=\u00a00.001 in all our experiments. We then apply a transformation \n\u03d5 to the scalar reward and value targets to obtain equivalent categor-\nical representations. We use a discrete support set of size 601 with one \nsupport for every integer between \u2212300 and 300. Under this transfor-\nmation, each scalar is represented as the linear combination of its two \nadjacent supports, such that the original value can be recovered by  \nx\u00a0=\u00a0xlow\u00a0\u00d7\u00a0plow\u00a0+\u00a0xhigh\u00a0\u00d7\u00a0phigh. As an example, a target of 3.7 would be rep-\nresented as a weight of 0.3 on the support for 3 and a weight of 0.7 on \nthe support for 4. The value and reward outputs of the network are also \nmodelled using a softmax output of size 601. During inference, the \nactual value and rewards are obtained by first computing their expected \nvalue under their respective softmax distribution and subsequently \nby inverting the scaling transformation. Scaling and transformation \nof the value and reward happens transparently on the network side \nand is not visible to the rest of the algorithm.\n\nBoth the representation and dynamics function use the same archi-\ntecture as AlphaZero, but with 16 instead of 20 residual blocks35. We use \n3\u00a0\u00d7\u00a03 kernels and 256 hidden planes for each convolution.\n\nFor Atari, where observations have large spatial resolution, the rep-\nresentation function starts with a sequence of convolutions with stride \n2 to reduce the spatial resolution. Specifically, starting with an input \nobservation of resolution 96\u00a0\u00d7\u00a096 and 128 planes (32 history frames \nof 3 colour channels each, concatenated with the corresponding 32 \nactions broadcast to planes), we downsample as follows: 1 convolu-\ntion with stride 2 and 128 output planes, output resolution 48\u00a0\u00d7\u00a048; \n2 residual blocks with 128 planes; 1 convolution with stride 2 and 256 \noutput planes, output resolution 24\u00a0\u00d7\u00a024; 3 residual blocks with 256 \nplanes; average pooling with stride 2, output resolution 12\u00a0\u00d7\u00a012; 3 \nresidual blocks with 256 planes; average pooling with stride 2, output \nresolution 6\u00a0\u00d7\u00a06. The kernel size is 3\u00a0\u00d7\u00a03 for all operations.\n\nFor the dynamics function (which always operates at the downsam-\npled resolution of 6\u00a0\u00d7\u00a06), the action is first encoded as an image, then \nstacked with the hidden state of the previous step along the plane \ndimension.\n\nTraining. During training, the MuZero network is unrolled for K hypo-\nthetical steps and aligned to sequences sampled from the trajectories \ngenerated by the MCTS actors. Sequences are selected by sampling a \nstate from any game in the replay buffer, then unrolling for K steps from \nthat state. In Atari, samples are drawn according to prioritized replay48, \n\n\n\nArticle\nwith priority P i( ) =\n\np\n\np\u2211 k\n\ni\n\u03b1\n\nk\n\u03b1 , where pi\u00a0=\u00a0|\u03bdi\u00a0\u2212\u00a0zi|, \u03bd is the search value and z the \n\nobserved n-step return. To correct for sampling bias introduced by the \nprioritized sampling, we scale the loss using the importance sampling \n\nratio ( )w = , \u00d7 ,i N P i\n\n\u03b21 1\n( )\n\n. In all our experiments, we set \u03b1\u00a0=\u00a0\u03b2\u00a0=\u00a01. For board \ngames, states are sampled uniformly.\n\nEach observation ot along the sequence also has a corresponding \nsearch policy \u03c0t, search value function \u03bdt and environment reward ut. \nAt each unrolled step k, the network has a loss to the policy, value and \nreward target for that step, summed to produce the total loss for the \nMuZero network (see equation\u00a0(1)). Note that, in board games without \nintermediate rewards, we omit the reward prediction loss. For board \ngames, we bootstrap directly to the end of the game, equivalent to \npredicting the final outcome; for Atari we bootstrap for n\u00a0=\u00a010 steps \ninto the future.\n\nTo maintain roughly similar magnitude of gradient across different \nunroll steps, we scale the gradient in two separate locations. (1) We scale \nthe loss of each head by 1/K, where K is the number of unroll steps. This \nensures that the total gradient has similar magnitude irrespective of \nhow many steps we unroll for. (2) We also scale the gradient at the start \nof the dynamics function by 1/2. This ensures that the total gradient \napplied to the dynamics function stays constant.\n\nIn the experiments reported in this paper, we always unroll for K\u00a0=\u00a05 \nsteps. For a detailed illustration, see Fig.\u00a01.\n\nTo improve the learning process and bound the activations, we also \nscale the hidden state to the same range as the action input ([0,1]): \ns = s s\n\ns sscaled\n\u2212 min( )\n\nmax( ) \u2212 min( )\n.\n\nAll experiments were run using third-generation Google Cloud ten-\nsor processing units (TPUs)49. For each board game, we used 16 TPUs \nfor training and 1,000 TPUs for self-play. For each game in Atari, in the \n20 billion frame setting\u00a0we used 8 TPUs for training and 32 TPUs for \nself-play. In the smaller 200 million frame setting, we used only four \nTPUs for training and two TPUs for self-play, equivalent to two weeks \nof training on 1 GPU.\u00a0The much smaller proportion of TPUs used for \nacting in Atari is due to the smaller number of simulations per move  \n(50 instead of 800) and the smaller size of the dynamics function com-\npared with the representation function.\n\nNote that the network is trained separately for each environment \n(that is, one model for each different Atari game or board game). \nHowever, in principle, the same model could be shared between \ndifferent environments during training, or could be tested in new \nenvironments (that is, zero-shot generalization); this approach is \nleft to future work.\n\nMuZero Reanalyze. To improve the sample efficiency of MuZero, we \nintroduced a second variant of the algorithm, MuZero Reanalyze. Mu-\nZero Reanalyze revisits its past time steps and re-executes its search \nusing the latest model parameters, potentially resulting in a better-quality \npolicy than the original search. This fresh policy is used as the policy \ntarget for 80% of updates during MuZero training. Furthermore, a target \nnetwork39 v f s\u22c5, = ( )\u03b8\n\n\u2212 0\n\u2212 , based on recent parameters \u03b8\u2212, is used to provide \n\na fresher, stable n-step bootstrapped target for the value function, \nz u \u03b3u \u03b3 u \u03b3 v= + + \u2026 + +t t t\n\nn\nt n\n\nn\nt n+1 +2\n\n\u22121\n+ +\n\n\u2212 . In addition, several other hyper-\nparameters were adjusted\u2014 primarily to increase sample reuse and avoid \noverfitting of the value function. Specifically, 2.0 samples were drawn \nper state, instead of 0.1; the value target was weighted down to 0.25 com-\npared with weights of 1.0 for policy and reward targets; and the n-step \nreturn was reduced to n\u00a0=\u00a05 steps instead of n\u00a0=\u00a010 steps.\n\nEvaluation. We evaluated the relative strength of MuZero (Fig.\u00a02) in \nboard games by measuring the Elo rating of each player. We estimate \nthe probability that player a will defeat player b by a logistic function \np a b( defeats ) = (1 + 10 )c e b e a[ ( )\u2212 ( )] \u22121elo , and estimate the ratings e(\u22c5) by \nBayesian logistic regression, computed by the BayesElo program50 \nusing the standard constant celo\u00a0=\u00a01/400.\n\nElo ratings were computed from the results of an 800-simulations- \nper-move tournament between iterations of MuZero during training, \nand also a baseline player: either Stockfish, Elmo or AlphaZero, respec-\ntively. Baseline players used an equivalent search time of 100\u00a0ms per \nmove. The Elo rating of the baseline players was anchored to publicly \navailable values5.\n\nIn Atari, we computed mean reward over 1,000 episodes per game, \nlimited to the standard 30\u00a0min or 108,000 frames per episode51, using \n50 simulations per move unless indicated otherwise. To mitigate the \neffects of the deterministic nature of the Atari simulator, we employed \ntwo different evaluation strategies: 30 noop random starts and human \nstarts. For the former, at the beginning of each episode, a random num-\nber of between 0 and 30 noop actions are applied to the simulator \nbefore handing control to the agent. For the latter, start positions are \nsampled from human expert play to initialize the Atari simulator before \nhanding the control to the agent51.\n\nData availability\nMuZero is trained only on data generated by MuZero itself; no external \ndata were used to produce the results presented in the article. Data for \nall figures and tables presented are available in JSON format in the\u00a0Sup-\nplementary Information.\n\nCode availability\nThe Arcade Learning Environment3 is available open source at https://\ngithub.com/mgbellemare/Arcade-Learning-Environment. The Go and \nchess environments are available open source in OpenSpiel52 at https://\ngithub.com/deepmind/open_spiel. The pseudocode for the MuZero \nalgorithm can be found in the file pseudocode.py in the\u00a0Supplementary \nInformation. All the neural architecture details and hyperparameters \nare described in Methods.\n \n43. Silver, D. et\u00a0al. Mastering the game of Go without human knowledge. Nature 550,  \n\n354\u2013359 (2017).\n44. Kocsis, L. & Szepesv\u00e1ri, C. Bandit based Monte-Carlo planning. In European Conference \n\non Machine Learning 282\u2013293 (Springer, 2006).\n45. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203\u2013230 \n\n(2011).\n46. Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B. & Uiterwijk, J. W. \n\nSingle-player Monte-Carlo tree search. In International Conference on Computers and \nGames 1\u201312 (Springer, 2008).\n\n47. Pohlen, T. et\u00a0al. Observe and look further: achieving consistent performance on Atari. \nPreprint at https://arxiv.org/abs/1805.11593 (2018).\n\n48. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. In \nInternational Conference on Learning Representations (2016).\n\n49. Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).\n50. Coulom, R. Whole-history rating: a Bayesian rating system for players of time-varying \n\nstrength. In International Conference on Computers and Games 113\u2013124 (2008).\n51. Nair, A. et\u00a0al. Massively parallel methods for deep reinforcement learning. Preprint at \n\nhttps://arxiv.org/abs/1507.04296 (2015).\n52. Lanctot, M. et\u00a0al. OpenSpiel: a framework for reinforcement learning in games. Preprint at \n\nhttp://arxiv.org/abs/1908.09453 (2019).\n\nAcknowledgements We thank L. Bennett, O. Smith and C. Apps for organizational assistance; \nK. Kavukcuoglu for reviewing the paper; T. Anthony, M. Lai, N. Tomasev, U. Paquet, S. Ghaisas \nfor many discussions; and the rest of the DeepMind team for their support.\n\nAuthor contributions J.S., I.A., T.H. and D.S. designed the MuZero algorithm with advice from \nA.G., K.S., L.S., E.L., T.L. and T.G.; J.S., I.A., T.H. and S.S. implemented the MuZero program, ran \nexperiments and analysed data. D.S., J.S., I.A. and T.H. wrote the paper with contributions from \nA.G., K.S., L.S., E.L., T.L., T.G. and D.H.\n\nCompeting interests DeepMind filed Greek\u00a0patent GR20200100037 on 28 January 2020, \ncovering the MuZero algorithm described in this paper, listing the authors J.S., I.A. and T.H. as \ninventors. The other authors declare no competing interests.\n\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41586-020-\n03051-4.\nCorrespondence and requests for materials should be addressed to D.S.\nPeer review information Nature thanks Jaap van den Herik and the other, anonymous, \nreviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\n\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/mgbellemare/Arcade-Learning-Environment\nhttps://github.com/deepmind/open_spiel\nhttps://github.com/deepmind/open_spiel\nhttps://arxiv.org/abs/1805.11593\nhttps://cloud.google.com/tpu/\nhttps://arxiv.org/abs/1507.04296\nhttp://arxiv.org/abs/1908.09453\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttps://doi.org/10.1038/s41586-020-03051-4\nhttp://www.nature.com/reprints\n\n\tMastering Atari, Go, chess and shogi by planning with a learned model\r\n\tPrevious work\r\n\tMuZero algorithm\r\n\tResults\r\n\tConclusions\r\n\tOnline content\r\n\tFig. 1 Planning, acting and training with a learned model.\n\tFig. 2 Evaluation of MuZero throughout training in chess, shogi, Go and Atari.\n\tFig. 3 Evaluations of MuZero on Go, all 57 Atari games and Ms.\n\tTable 1 Comparison of MuZero against previous agents in Atari.","cloud_doc_url":null}],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"e05b1fd2-a3e0-4fd0-be8b-e27dae8638a9","request_id":"91d0118fcbba6041-ATL","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"e05b1fd2-a3e0-4fd0-be8b-e27dae8638a9","children":["fe18f831-3134-42f7-8a8f-ac2847a6a93c"]},"fe18f831-3134-42f7-8a8f-ac2847a6a93c":{"id":"fe18f831-3134-42f7-8a8f-ac2847a6a93c","message":{"id":"fe18f831-3134-42f7-8a8f-ac2847a6a93c","author":{"role":"user","name":null,"metadata":{}},"create_time":1741413427.03,"update_time":null,"content":{"content_type":"text","parts":["> Unlike game outcomes (win/loss or score), evaluating intermediate reasoning quality is less straightforward. It may require new forms of supervision, perhaps derived from human feedback (akin to RLHF), to effectively guide the planning component.\r\n\r\nHow about reinforcement learning with verifiable rewards?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"91d0182c9f2f6041-ATL","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"ee0b5e03-baa4-40b3-b261-e45171931592","children":["a3afadef-0773-4900-b082-56c45be3b1be"]},"a3afadef-0773-4900-b082-56c45be3b1be":{"id":"a3afadef-0773-4900-b082-56c45be3b1be","message":{"id":"a3afadef-0773-4900-b082-56c45be3b1be","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1741413440.66427,"update_time":null,"content":{"content_type":"text","parts":["**Explaining reinforcement learning with verifiable rewards**\n\nThe user is asking about incorporating reinforcement learning with verifiable rewards in the context of LLM (large language model) training. Verifiable rewards can be useful for evaluating intermediate reasoning quality. Instead of relying solely on human feedback, tasks can be set up where the correctness of intermediate steps is verified by external criteria. This measurable feedback can then be used as a reward signal to improve the training process. It's a way to make the reinforcement signal more objective and useful.\n\n**Exploring reinforcement learning with verifiable rewards**\n\nReinforcement learning with verifiable rewards can help mitigate the challenge of specifying reward functions, especially in RLHF. A potential approach is automating verification, such as through consistency checks or comparing intermediate reasoning with a ground truth. The key idea is to design tasks where rewards for intermediate steps can be objectively evaluated, such as in math, code generation, or logical puzzles. This approach aligns well with MuZero\u2019s planning methods, where rewards reflect measurable correctness, guiding the model towards better reasoning and refined internal planning."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned for 5 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"fe18f831-3134-42f7-8a8f-ac2847a6a93c","request_id":"91d0182c9f2f6041-ATL","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"fe18f831-3134-42f7-8a8f-ac2847a6a93c","children":["5fe9f364-8678-48b2-9130-6c60ed8e912c"]},"5fe9f364-8678-48b2-9130-6c60ed8e912c":{"id":"5fe9f364-8678-48b2-9130-6c60ed8e912c","message":{"id":"5fe9f364-8678-48b2-9130-6c60ed8e912c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1741413439.936918,"update_time":null,"content":{"content_type":"text","parts":["Reinforcement learning (RL) with verifiable rewards offers a promising alternative to relying solely on human feedback for guiding intermediate reasoning. Here\u2019s how this approach might work when combined with MuZero-inspired principles in LLM training:\n\n---\n\n### 1. Concept of Verifiable Rewards\n\n- **Objective, Measurable Feedback:**  \n  Unlike human feedback\u2014which can be subjective and sparse\u2014verifiable rewards are derived from clear, objective criteria. For instance, in a math problem, the correct final answer and the logical consistency of intermediate steps can be automatically checked.\n\n- **Intermediate Signal:**  \n  By designing verifiable reward signals for each intermediate reasoning step, the model can receive a dense learning signal. This means that even if a full answer is long or complex, each step that contributes to the final answer is rewarded based on its correctness or logical soundness.\n\n---\n\n### 2. Integration into the LLM Framework\n\n- **Latent Planning with Verification:**  \n  Borrowing from MuZero\u2019s architecture, the LLM could develop a latent state that captures intermediate reasoning steps. An integrated planning component (e.g., a search mechanism like MCTS or Tree-of-Thoughts) could generate multiple reasoning paths. Each path would then be evaluated using verifiable rewards to assess not only the final answer but also the correctness of the intermediate inferences.\n\n- **Multi-Objective Training:**  \n  In addition to the traditional next-token prediction loss, the model would optimize for:\n  \n  - **Intermediate Verification:** A loss component that compares the model\u2019s predicted intermediate steps against verifiable criteria (e.g., consistency with known logical rules or problem constraints).\n  - **Final Outcome Reward:** Reinforcing the entire chain-of-thought by providing a cumulative reward that reflects both the intermediate verifiability and the final answer accuracy.\n\n- **Feedback Loop:**  \n  Much like the \u201cMuZero Reanalyze\u201d variant, the model could periodically re-run its reasoning paths with updated parameters. The verifiable rewards would then refine the internal planning process, leading to more reliable and transparent reasoning over time.\n\n---\n\n### 3. Benefits and Challenges\n\n- **Benefits:**  \n  - **Objective Guidance:** Verifiable rewards remove much of the ambiguity inherent in subjective human evaluations, providing a clear signal for learning correct reasoning.\n  - **Fine-Grained Optimization:** By rewarding correct intermediate steps, the model can learn to structure its reasoning more effectively rather than relying on a \u201cblack-box\u201d final answer.\n  - **Scalability in Structured Domains:** In domains where rules are well-defined (e.g., math, code generation, logical puzzles), this approach can lead to significant improvements in both accuracy and interpretability.\n\n- **Challenges:**  \n  - **Reward Design:** Not every domain offers an obvious mechanism for generating verifiable rewards. Designing these rewards might require domain-specific tools or oracles.\n  - **Sparse vs. Dense Feedback:** In many real-world tasks, intermediate rewards might still be sparse. Techniques like reward shaping or leveraging auxiliary tasks may be necessary.\n  - **Integration Complexity:** Merging an RL framework with verifiable rewards into LLM training adds computational overhead and architectural complexity. Balancing the cost of additional planning and evaluation with the benefits in reasoning performance is a key engineering challenge.\n\n---\n\n### 4. Practical Scenarios\n\n- **Mathematical Problem Solving:**  \n  An LLM solving a math problem could verify each step using symbolic computation or consistency checks, assigning rewards when a derivation matches known mathematical rules.\n\n- **Code Generation:**  \n  For generating code, intermediate steps (such as variable declarations, function calls, and syntactical structures) can be verified using compilers or static analysis tools, providing immediate feedback on correctness.\n\n- **Logical Reasoning Tasks:**  \n  In tasks like theorem proving or logic puzzles, an automated checker could validate each inference, allowing the model to adjust its internal planning to favor logically coherent chains of thought.\n\n---\n\nIn summary, integrating reinforcement learning with verifiable rewards into LLM training is a forward-thinking strategy. It aligns well with the principles behind MuZero by focusing on learning a planning-relevant latent representation and then iteratively refining the model using concrete, measurable signals. While challenges remain\u2014especially in designing universal verification mechanisms\u2014this approach could significantly enhance the reliability and interpretability of LLM reasoning."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"a3afadef-0773-4900-b082-56c45be3b1be","request_id":"91d0182c9f2f6041-ATL","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"a3afadef-0773-4900-b082-56c45be3b1be","children":["e04225ff-d919-432f-bde7-251a03a66843"]},"e04225ff-d919-432f-bde7-251a03a66843":{"id":"e04225ff-d919-432f-bde7-251a03a66843","message":{"id":"e04225ff-d919-432f-bde7-251a03a66843","author":{"role":"user","name":null,"metadata":{}},"create_time":1741416570.122,"update_time":null,"content":{"content_type":"text","parts":["### **1. Modeling Language Generation as a Sequential Decision Process**\r\nOne way to incorporate MuZero\u2019s principles is to reframe language generation as a Markov Decision Process (MDP), akin to a game:\r\n- **State**: The current partial text.\r\n- **Action**: The next token to append.\r\n- **Reward**: A measure of text quality (e.g., coherence, relevance, or task-specific success).\r\n- **Goal**: Maximize the quality of the final text.\r\n\r\n#### **Adaptation**\r\nAn LLM could be extended to predict not only the policy (probability distribution over next tokens, which it already does) but also:\r\n- **Value**: The expected quality of the final text if generation continues from the current state.\r\n- **Reward**: Immediate feedback for adding a specific token.\r\n\r\n#### **Benefits**\r\nThis approach enables the model to consider long-term outcomes, potentially improving coherence and goal alignment over the token-by-token generation LLMs currently use.\r\n\r\n#### **Challenges**\r\n- **Reward Definition**: Unlike games with clear win/loss signals, text quality is subjective. Possible solutions include using automated metrics (e.g., BLEU for translation) or training a reward model from human feedback.\r\n- **Large Action Space**: With vocabularies of 50,000+ tokens, the number of possible actions is vast compared to games like Go (361 moves). This could be mitigated by limiting predictions to high-probability tokens or using hierarchical generation (e.g., predicting phrases first).\r\n\r\n---\r\n\r\n### **2. Incorporating Planning with MCTS**\r\nMuZero uses MCTS to explore possible future actions and select the best path. In LLMs, we could adapt MCTS to plan ahead during text generation:\r\n- **Process**: Simulate multiple token sequences, evaluate their outcomes, and choose the sequence that maximizes a reward metric.\r\n- **Example**: For a query like \"Write a summary,\" the model could explore different continuations (e.g., concise vs. detailed) and select the most coherent and informative one.\r\n\r\n#### **Benefits**\r\nMCTS could replace simpler methods like beam search, offering a more principled way to balance exploration (trying novel sequences) and exploitation (favoring high-probability tokens).\r\n\r\n#### **Challenges**\r\n- **Branching Factor**: The enormous vocabulary size makes full MCTS computationally infeasible.\r\n- **Evaluation**: Assessing partial text requires a reliable value or reward function.\r\n\r\n#### **Solutions**\r\n- Prune the action space by considering only the top-k tokens at each step.\r\n- Use a learned value function to estimate the promise of partial sequences, reducing the need for exhaustive simulation.\r\n\r\n---\r\n\r\n### **3. Learning a Latent Dynamics Model**\r\nMuZero learns a dynamics model in a latent space, predicting how the state evolves with each action and the associated reward. For LLMs:\r\n- **State**: The transformer\u2019s hidden state, representing the current text context.\r\n- **Action**: The next token.\r\n- **Prediction**: The next hidden state and immediate reward.\r\n\r\n#### **Adaptation**\r\nTrain the LLM to simulate future text generation steps in its latent space, enabling planning without generating full sequences in token space.\r\n\r\n#### **Benefits**\r\nThis could allow the model to \"think ahead\" internally, refining its predictions based on simulated outcomes, much like MuZero plans moves in games.\r\n\r\n#### **Challenges**\r\n- **Latent Space Design**: The hidden states must encode enough information to predict both tokens and rewards.\r\n- **Complexity**: Training a dynamics model adds computational overhead.\r\n\r\n#### **Solutions**\r\n- Augment training with auxiliary tasks (e.g., predicting final text quality) to enrich hidden states.\r\n- Use efficient approximations, like sampling a small number of future steps.\r\n\r\n---\r\n\r\n### **4. Applying MuZero to Structured Language Tasks**\r\nMuZero\u2019s approach may be most directly applicable to tasks with clear goals and evaluation criteria, resembling game-like environments:\r\n- **Math Problem-Solving**: Each step (e.g., applying a formula) is an action, with a reward based on reaching the correct answer.\r\n- **Code Generation**: Each token or line is an action, with a reward tied to code functionality (e.g., passing tests).\r\n\r\n#### **Adaptation**\r\nTrain a model to predict policy (next step), value (likelihood of success), and reward (correctness of intermediate steps), using planning to explore solution paths.\r\n\r\n#### **Benefits**\r\nThese tasks have smaller, more manageable action spaces and clearer rewards, making MuZero\u2019s framework more feasible.\r\n\r\n#### **Challenges**\r\n- Intermediate rewards may be sparse (e.g., only the final answer matters).\r\n- Action spaces, while smaller than open-ended text, can still be large (e.g., all possible code tokens).\r\n\r\n#### **Solutions**\r\n- Use value predictions to guide intermediate steps when rewards are sparse.\r\n- Employ hierarchical planning (e.g., plan high-level solution steps, then fill in details).\r\n\r\n---\r\n\r\n### **5. Enhancing Reasoning with Planning**\r\nRecent efforts to improve LLMs\u2019 reasoning, such as chain-of-thought prompting, suggest a role for planning:\r\n- **Process**: Model reasoning as a sequence of decisions (e.g., generating intermediate steps to answer a query), using a MuZero-like approach to predict and evaluate each step\u2019s value and reward.\r\n- **Example**: For \"What is 15% of 80?\", the model could plan steps (e.g., \"convert percentage to decimal,\" \"multiply\") and select the best path.\r\n\r\n#### **Benefits**\r\nPlanning could produce more logical, step-by-step reasoning, addressing LLMs\u2019 tendency to hallucinate or skip steps.\r\n\r\n#### **Challenges**\r\n- Defining rewards for reasoning steps is complex (e.g., how to score \"convert to decimal\"?).\r\n- Computational cost increases with planning depth.\r\n\r\n#### **Solutions**\r\n- Use self-consistency checks or learned critics to evaluate steps.\r\n- Limit planning to a few key decisions, leveraging the LLM\u2019s existing knowledge for details.\r\n\r\n\r\n\r"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"91d064e9698ece29-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"5fe9f364-8678-48b2-9130-6c60ed8e912c","children":["2eddcb18-ed09-405b-bcb0-a81162d549a6"]},"2eddcb18-ed09-405b-bcb0-a81162d549a6":{"id":"2eddcb18-ed09-405b-bcb0-a81162d549a6","message":{"id":"2eddcb18-ed09-405b-bcb0-a81162d549a6","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1741416584.087015,"update_time":null,"content":{"content_type":"text","parts":["**Analyzing MuZero integration**\n\nThe user's message outlines how MuZero could be incorporated into LLM training. It seems like they are suggesting a framework with five main points: modeling language generation, using MCTS, learning latent dynamics, applying MuZero to structured tasks, and enhancing reasoning. I think this is more of an organized proposal rather than a direct question. It may be asking for me to verify or expand upon these ideas, so I\u2019ll need to look further into how these components would mesh with LLM approaches.\n\n**Planning response to user\u2019s outline**\n\nIt seems like the user provided a detailed outline suggesting how MuZero's principles could be applied to LLM training and reasoning models. They mentioned framing LLM generation as an MDP, which is interesting! I now need to carefully plan my response and refine their outline by adding analyses, exploring improvements, and expanding on each point. I'll respond by acknowledging how well the outline is structured and offer valuable insights that could support further research or enhance the approach suggested. Time for a precise and thorough reply!"]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned for 6 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"e04225ff-d919-432f-bde7-251a03a66843","request_id":"91d064e9698ece29-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"e04225ff-d919-432f-bde7-251a03a66843","children":["5b7b6746-562b-424d-b40c-b5d48b939e35"]},"5b7b6746-562b-424d-b40c-b5d48b939e35":{"id":"5b7b6746-562b-424d-b40c-b5d48b939e35","message":{"id":"5b7b6746-562b-424d-b40c-b5d48b939e35","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1741416583.298722,"update_time":null,"content":{"content_type":"text","parts":["Your outline provides an excellent roadmap. Here\u2019s an expanded exploration that builds on those ideas, offering additional insights, potential solutions, and avenues for research:\n\n---\n\n### **1. Modeling Language Generation as a Sequential Decision Process**\n\n**Your Concept:**  \nReframe language generation as an MDP, where the state is the current partial text, the action is the next token, and the reward reflects text quality. The model predicts both a policy (next-token probabilities) and a value (expected future quality).\n\n**Expanded Insights:**\n\n- **Dynamic Long-Term Evaluation:**  \n  By estimating the value of a partial sequence, the model can better gauge whether a high-probability token might lead to long-term incoherence. For example, it might avoid locally optimal but globally suboptimal continuations by \u201clooking ahead\u201d in the latent space.\n\n- **Reward Signal Design:**  \n  Developing robust, verifiable reward functions is key. For structured tasks (like translation), automated metrics (BLEU, METEOR) could serve as proxies, while for creative or open-ended generation, auxiliary models trained on human feedback (or even unsupervised quality signals) could provide more nuanced evaluations.\n\n- **Handling the Vast Action Space:**  \n  Techniques like top\u2011k filtering, nucleus sampling, or even hierarchical approaches (first generating phrases or semantic tokens, then decoding them into words) could reduce the effective action space, making planning computationally tractable.\n\n---\n\n### **2. Incorporating Planning with MCTS**\n\n**Your Concept:**  \nAdapt MCTS to explore future token sequences, effectively simulating multiple reasoning trajectories before committing to a final output. This replaces or augments beam search with a more principled exploration\u2013exploitation strategy.\n\n**Expanded Insights:**\n\n- **Selective Expansion:**  \n  Given the enormous branching factor, you could integrate a two-level approach where the first level prunes the action space (e.g., considering only the top\u2011k tokens), and the second level uses MCTS to explore variations. This layered approach reduces computational load while maintaining search quality.\n\n- **Learned Evaluation Functions:**  \n  A learned value function could rapidly estimate the quality of partial sequences without fully simulating each branch. This mirrors how MuZero predicts future rewards in latent space and would be essential to keep MCTS practical within language models.\n\n- **Balancing Exploration and Exploitation:**  \n  The adapted PUCT formula for language tasks could include parameters that adjust for the uncertainty in language generation, possibly guided by uncertainty estimates derived from dropout or ensemble methods.\n\n---\n\n### **3. Learning a Latent Dynamics Model**\n\n**Your Concept:**  \nTrain the model to simulate future text generation in a latent space. The hidden state of the transformer acts as the \u201cstate\u201d in the MDP, and the dynamics function predicts the evolution of this state along with immediate rewards for actions.\n\n**Expanded Insights:**\n\n- **Abstract Reasoning:**  \n  Learning a latent dynamics model can allow the model to \u201cmentally simulate\u201d several steps ahead without the full cost of token-level generation. This internal simulation might capture abstract narrative or logical structure better than token-by-token autoregression.\n\n- **Auxiliary Tasks and Regularization:**  \n  To ensure that latent representations are rich enough to predict both future tokens and rewards, auxiliary tasks (like predicting summary statistics of the final text or coherence scores) can be added during training. This regularizes the latent space to be more planning-relevant.\n\n- **Model Complexity Trade-Offs:**  \n  While a richer dynamics model can yield better planning, it adds training complexity. Research could explore lightweight architectures or approximations (e.g., lower-dimensional projections) to balance planning benefits with computational efficiency.\n\n---\n\n### **4. Applying MuZero to Structured Language Tasks**\n\n**Your Concept:**  \nFocus on tasks with well-defined objectives\u2014such as math problem solving or code generation\u2014where intermediate steps are verifiable and rewards can be defined more objectively.\n\n**Expanded Insights:**\n\n- **Structured Feedback Loops:**  \n  In math or code, you can incorporate external verifiers (e.g., symbolic solvers, unit test frameworks) to provide immediate rewards for each step. This not only guides the generation process but also offers interpretability into the model\u2019s reasoning process.\n\n- **Hierarchical Planning:**  \n  For tasks like code generation, hierarchical planning might involve first planning a high-level structure (e.g., algorithmic steps) and then filling in detailed code. This reduces the effective action space and makes planning more manageable.\n\n- **Domain-Specific Reward Models:**  \n  Tailor reward models to specific domains. For example, in math, use step-by-step correctness checks; in code, integrate static analysis or even dynamic execution to verify intermediate outputs.\n\n---\n\n### **5. Enhancing Reasoning with Planning**\n\n**Your Concept:**  \nLeverage planning to improve chain-of-thought reasoning by generating and evaluating multiple reasoning paths before selecting the most coherent one.\n\n**Expanded Insights:**\n\n- **Self-Consistency and Ensemble Methods:**  \n  Similar to recent chain-of-thought prompting methods that generate multiple answers and choose the most consistent one, planning can be integrated to guide intermediate steps. A critic or secondary model can assess coherence at each decision point.\n\n- **Adaptive Planning Depth:**  \n  Not every query requires deep planning. An adaptive approach could decide, based on the complexity of the input, whether to use extensive planning or rely on a direct, autoregressive generation. This could help balance quality and computational cost.\n\n- **Integration with RL and Verifiable Rewards:**  \n  For intermediate reasoning, verifiable rewards (e.g., consistency with known facts, logical coherence) can be derived through external evaluators. Reinforcement learning can then use these rewards to fine-tune the planning module, reinforcing the correct reasoning steps and penalizing hallucinated or logically inconsistent paths.\n\n---\n\n### **Final Thoughts**\n\nIncorporating MuZero\u2019s architecture into LLM training means shifting from a purely reactive, token-by-token generation process to one where the model actively plans, simulates future outcomes in a latent space, and optimizes for long-term quality. This fusion of planning, latent dynamics modeling, and reinforcement learning with both verifiable and human-aligned rewards offers a promising path to enhancing the reasoning and coherence of language models.\n\nThe primary challenges remain in designing effective reward signals, managing the large action spaces inherent in language, and ensuring that the computational overhead of planning remains feasible. Nonetheless, structured tasks like math problem solving and code generation provide ideal testbeds where these ideas can be trialed and refined, potentially paving the way for more robust, reliable, and interpretable language models.\n\nFeel free to discuss any specific aspects further or dive deeper into implementation details!"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"2eddcb18-ed09-405b-bcb0-a81162d549a6","request_id":"91d064e9698ece29-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"2eddcb18-ed09-405b-bcb0-a81162d549a6","children":["5d1e1957-9ad5-4437-a939-f85c33762969"]},"5d1e1957-9ad5-4437-a939-f85c33762969":{"id":"5d1e1957-9ad5-4437-a939-f85c33762969","message":{"id":"5d1e1957-9ad5-4437-a939-f85c33762969","author":{"role":"user","name":null,"metadata":{}},"create_time":1742184046.733,"update_time":null,"content":{"content_type":"text","parts":["Examine the applicability of A-star search and Q-learning to the MuZero architecture"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"921995e32d14f916-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"5b7b6746-562b-424d-b40c-b5d48b939e35","children":["a717a250-1f5b-4199-b3c2-999bc95108d3"]},"a717a250-1f5b-4199-b3c2-999bc95108d3":{"id":"a717a250-1f5b-4199-b3c2-999bc95108d3","message":{"id":"a717a250-1f5b-4199-b3c2-999bc95108d3","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1742184050.927654,"update_time":null,"content":{"content_type":"text","parts":["**Evaluating integration with MuZero**\n\nThe user\u2019s asking about the applicability of A* search and Q-learning in the MuZero architecture. MuZero incorporates planning to predict actions, so I need to examine whether A* search or Q-learning could fit into its principles. Based on the \u201cMuZero\u201d paper, I know that replacing its planning component with Q-learning leads to less sample efficiency. I\u2019ll need to carefully assess how these methods align with MuZero\u2019s architecture, keeping in mind the balance of planning and learning. I'll ensure my answer is clear and accurate.\n\n**Considering A* and Q-learning for MuZero**\n\nA* search is a heuristic algorithm for pathfinding in deterministic environments, but MuZero uses MCTS for planning in more uncertain settings. While an A* search could theoretically be used with a strong heuristic in some cases, MuZero's latent state and non-Euclidean dynamics make it less straightforward. Q-learning, by contrast, is model-free, and though MuZero has planning with MCTS, it provides better efficiency than Q-learning. They\u2019re not directly interchangeable, as replacing MCTS with Q-learning lowers convergence and performance."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned for 6 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"5d1e1957-9ad5-4437-a939-f85c33762969","request_id":"921995e32d14f916-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"5d1e1957-9ad5-4437-a939-f85c33762969","children":["afcb71cc-0988-42c5-808f-31f21b3eeae3"]},"afcb71cc-0988-42c5-808f-31f21b3eeae3":{"id":"afcb71cc-0988-42c5-808f-31f21b3eeae3","message":{"id":"afcb71cc-0988-42c5-808f-31f21b3eeae3","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1742184049.713576,"update_time":null,"content":{"content_type":"text","parts":["Below is an analysis of how A\u2011star search and Q\u2011learning might be applied\u2014or why they may not be ideal\u2014in a MuZero\u2010like architecture.\n\n---\n\n## A\u2011star Search in the Context of MuZero\n\n**A\u2011star Search Overview:**\n- **Nature:** A\u2011star is a heuristic, best-first search algorithm designed to find the shortest path in a well\u2010defined, static state space.\n- **Requirement:** It needs a cost function and a strong heuristic to estimate the \u201ccost-to-go\u201d from any state to the goal.\n\n**Challenges for Integration:**\n- **Heuristic in a Learned Latent Space:**  \n  MuZero learns an abstract latent state that doesn\u2019t correspond directly to a physical or Euclidean space. To apply A\u2011star, you\u2019d need a heuristic that reliably estimates the \u201cremaining cost\u201d (or, in an RL context, the negative expected reward) in this latent space. Designing such a heuristic is nontrivial given the learned and often non\u2010interpretable nature of the latent representations.\n- **Stochastic and Dynamic Environments:**  \n  A\u2011star excels in deterministic settings. MuZero, however, operates in domains like Atari or board games where dynamics are uncertain and the environment is stochastic. MCTS, as used by MuZero, naturally accommodates these uncertainties by simulating multiple possible futures.\n- **Branching Factor and Complexity:**  \n  The vast action spaces in language or complex games make an A\u2011star approach less practical, since A\u2011star relies on exploring neighbors in a well-structured graph. MCTS, by contrast, can efficiently handle high branching factors by sampling and prioritizing promising actions.\n\n**Conclusion on A\u2011star:**  \nWhile A\u2011star is a powerful search algorithm in classical planning problems, its applicability within a MuZero framework is limited. The requirement for a strong, domain-specific heuristic and the challenges posed by stochastic, abstract latent spaces make it less suitable compared to MCTS, which MuZero uses to balance exploration and exploitation in uncertain environments.\n\n---\n\n## Q\u2011learning in the Context of MuZero\n\n**Q\u2011learning Overview:**\n- **Nature:** Q\u2011learning is a model\u2010free reinforcement learning algorithm that estimates the expected cumulative reward (the Q\u2011value) for state\u2013action pairs.\n- **Approach:** It updates Q\u2011values using temporal-difference learning without explicitly planning over future actions.\n\n**Challenges for Integration:**\n- **Model-Free vs. Model-Based:**  \n  MuZero is fundamentally a model\u2010based approach that learns a latent dynamics model and uses planning (via MCTS) to improve its policy and value estimates. Q\u2011learning, being model\u2010free, does not incorporate a planning phase. In fact, the MuZero paper presents experiments where replacing the planning component with a Q\u2011learning objective results in slower learning and lower performance.\n- **High Variance and Bias:**  \n  Q\u2011learning typically suffers from high variance and bias, particularly in environments with sparse or delayed rewards. MuZero\u2019s use of search (i.e., lookahead planning) provides more robust learning signals by refining predictions through simulated trajectories.\n- **Lack of Explicit Policy Improvement:**  \n  While Q\u2011learning only provides value estimates, MuZero jointly learns a policy and value function. The planning step in MuZero effectively improves the policy by selecting actions based on simulated outcomes. This combined approach is a core reason for MuZero\u2019s strong performance compared to pure model-free methods like Q\u2011learning.\n\n**Conclusion on Q\u2011learning:**  \nIncorporating Q\u2011learning principles into MuZero is challenging because it would strip away the benefits of its planning module. Although one could imagine a hybrid scheme where Q\u2011learning updates are used in conjunction with planning, the experimental evidence suggests that the planning-based policy improvement in MuZero provides a much stronger learning signal than a model-free Q\u2011learning update would.\n\n---\n\n## Summary\n\n- **A\u2011star Search:**  \n  Its dependency on a well\u2011defined heuristic and its design for deterministic, structured spaces limit its usefulness in the MuZero framework, where learned latent spaces and stochastic dynamics are prevalent. MCTS remains more adaptable to these conditions.\n\n- **Q\u2011learning:**  \n  As a model\u2011free algorithm, Q\u2011learning lacks the planning and lookahead capabilities that give MuZero its edge. The integration of Q\u2011learning would likely diminish the performance gains obtained from using a latent dynamics model with planning.\n\nIn essence, while both A\u2011star and Q\u2011learning have their merits in classical planning and model-free RL, respectively, the success of MuZero relies on its ability to integrate learned dynamics with search-based planning\u2014a combination that neither A\u2011star nor Q\u2011learning, in their standard forms, readily provide."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"a717a250-1f5b-4199-b3c2-999bc95108d3","request_id":"921995e32d14f916-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"a717a250-1f5b-4199-b3c2-999bc95108d3","children":["e0a6402f-e990-471b-93a9-5ef17d180385"]},"e0a6402f-e990-471b-93a9-5ef17d180385":{"id":"e0a6402f-e990-471b-93a9-5ef17d180385","message":{"id":"e0a6402f-e990-471b-93a9-5ef17d180385","author":{"role":"user","name":null,"metadata":{}},"create_time":1742184182.591,"update_time":null,"content":{"content_type":"text","parts":["Design a toy example to illustrate the concept of MuZero."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"9219993489e3f916-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"afcb71cc-0988-42c5-808f-31f21b3eeae3","children":["71aad860-5ace-4b9f-b0b3-963b4b022ab1"]},"71aad860-5ace-4b9f-b0b3-963b4b022ab1":{"id":"71aad860-5ace-4b9f-b0b3-963b4b022ab1","message":{"id":"71aad860-5ace-4b9f-b0b3-963b4b022ab1","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1742184186.40995,"update_time":null,"content":{"content_type":"text","parts":["**Designing a MuZero toy example**\n\nThe user wants a toy example that illustrates MuZero, which is about planning with a learned model. I could design something like \"Navigating a simple grid world,\" though that\u2019s common for reinforcement learning. MuZero focuses on planning, so a simple game like \"Toy Grid Maze\" or a \"2x2 Tic-Tac-Toe variant\" might fit in. I\u2019m also thinking something lighter, like \"Rock-Paper-Scissors\" or maybe even a \"coin flip\" game could work amidst straightforward planning!\n\n**Designing MuZero toy example**\n\nThe goal is to show how MuZero uses a learned latent model for planning, but it's tricky! I could go with a simple board game\u2014like a one-dimensional grid of 5 positions, where you step left or right to reach a goal, but that\u2019s too simple compared to MuZero\u2019s latent model. Another idea is a game called \"Token Catch\" in a small grid, where you work to collect rewards. Or, I could try a \"number game,\" where the agent transforms a number to reach a target, using a latent model to predict rewards\u2014maybe \u201cThe Maze of Numbers.\u201d"]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned for 6 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"e0a6402f-e990-471b-93a9-5ef17d180385","request_id":"9219993489e3f916-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"e0a6402f-e990-471b-93a9-5ef17d180385","children":["40dfbb89-d931-425e-ba48-7f09a727f273"]},"40dfbb89-d931-425e-ba48-7f09a727f273":{"id":"40dfbb89-d931-425e-ba48-7f09a727f273","message":{"id":"40dfbb89-d931-425e-ba48-7f09a727f273","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1742184185.435273,"update_time":null,"content":{"content_type":"text","parts":["Imagine a simple \u201cNumber Game\u201d where the goal is to transform a starting number into a target number using a sequence of operations. This toy example captures MuZero\u2019s core ideas\u2014learning a latent model, planning via search, and jointly predicting value, reward, and policy\u2014all in a very small, understandable setting.\n\n---\n\n### **The Number Game:**\n\n#### **Game Rules**\n- **State:** A single integer, starting at 1.\n- **Actions:**  \n  - **Add 1:** Increases the number by 1.  \n  - **Multiply by 2:** Doubles the number.\n- **Goal:** Reach the target number 10 exactly.\n- **Rewards:**  \n  - Intermediate moves: 0 reward.  \n  - If the number equals 10: +1 reward (win).  \n  - If the number exceeds 10: \u20131 reward (penalty) and the episode terminates.\n\n#### **How MuZero Would Tackle the Game**\n\n1. **Representation Function (h):**  \n   Instead of working directly with the raw number, the model learns to encode the current number into a latent state. In our toy example, the latent state might simply capture the value \u201c1\u201d, \u201c2\u201d, etc., but it could also learn more abstract features (like proximity to 10).\n\n2. **Dynamics Function (g):**  \n   Given the latent state and an action, the dynamics function predicts:\n   - The next latent state (i.e. the new number after applying the action).\n   - The immediate reward (0 for non-terminal states, +1 if the number becomes 10, or \u20131 if it overshoots).\n   \n   For example, if the latent state corresponds to 3 and the agent chooses \u201cMultiply by 2,\u201d then \\( g \\) predicts a new latent state for 6 with a reward of 0.\n\n3. **Prediction Function (f):**  \n   From the latent state, the prediction function outputs:\n   - **Policy:** A probability distribution over the two possible actions (add 1, multiply by 2).\n   - **Value:** The estimated cumulative reward (e.g., how likely this sequence is to eventually reach 10).\n\n4. **Planning with MCTS:**  \n   Using these learned functions, the agent performs a lookahead search:\n   - **Simulated Trajectories:** It simulates various sequences of actions starting from the current number. For instance, starting at 1:\n     - Option A: multiply \u2192 add \u2192 multiply \u2192 add \u2026  \n       \\(1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 6 \\rightarrow 7 \\rightarrow 8 \\rightarrow 9 \\rightarrow 10\\)\n     - Option B: add repeatedly: \\(1 \\rightarrow 2 \\rightarrow 3 \\rightarrow \\ldots \\rightarrow 10\\)\n   - **Selection:** The Monte Carlo Tree Search (MCTS) uses the predicted values and rewards to evaluate which action sequence is most promising. It balances exploring new sequences with exploiting known good ones.\n   - **Action Choice:** After the search, the action corresponding to the highest value (or the one visited most in the search) is selected.\n\n5. **Learning Process:**  \n   During training, the network updates its parameters by comparing:\n   - The predicted policy with the improved policy derived from MCTS.\n   - The predicted value with the actual outcome (reward of +1 if 10 was reached, or \u20131 if overshot).\n   - The predicted reward at each step with the observed immediate reward.\n\n---\n\n### **Illustrative Example Walkthrough**\n\n- **Starting State:** The game starts at 1.\n- **Step 1:**  \n  - **Observation:** \u201c1\u201d is encoded into a latent state \\( s_0 \\).\n  - **Prediction:** The model outputs a policy over the two actions and a value (e.g., predicting that \u201cmultiply by 2\u201d might lead to a better long-term outcome).\n  - **MCTS Planning:** Simulates future steps. One promising branch might be:\n    - Multiply: \\(1 \\rightarrow 2\\)\n    - Add: \\(2 \\rightarrow 3\\)\n    - Multiply: \\(3 \\rightarrow 6\\)\n    - Add: \\(6 \\rightarrow 7\\)\n    - Add: \\(7 \\rightarrow 8\\)\n    - Add: \\(8 \\rightarrow 9\\)\n    - Add: \\(9 \\rightarrow 10\\) (win, reward +1)\n  - **Action:** Based on the search, \u201cmultiply by 2\u201d is chosen.\n  \n- **Subsequent Steps:**  \n  The process repeats. At each step, the model uses its latent dynamics to simulate future sequences, refining its choice until it eventually reaches 10.\n\n---\n\n### **Why This Toy Example Works**\n\n- **Simplicity:**  \n  With just two operations and a small state space, it\u2019s easy to see how each component of MuZero functions.\n  \n- **Latent Dynamics Learning:**  \n  Even in this simple game, the model learns a latent representation (the number itself or an abstracted form of it) that is sufficient to predict the outcome of actions.\n  \n- **Planning:**  \n  The use of MCTS illustrates how planning helps evaluate long-term outcomes rather than making purely greedy, one-step decisions.\n  \n- **Reward and Value Prediction:**  \n  The model\u2019s task of predicting immediate rewards (0, +1, or \u20131) and the eventual outcome teaches it to focus on the sequence of actions leading to success.\n\n---\n\nThis toy example of the \u201cNumber Game\u201d captures the essence of MuZero: learning a compact representation of the environment, using a learned dynamics model to simulate future states, and planning with MCTS to make decisions that maximize long-term rewards. It provides an accessible framework for understanding how MuZero can be applied even in more complex scenarios."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"71aad860-5ace-4b9f-b0b3-963b4b022ab1","request_id":"9219993489e3f916-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"71aad860-5ace-4b9f-b0b3-963b4b022ab1","children":["11525ec6-8874-46cd-95e4-e3c29eef9517"]},"11525ec6-8874-46cd-95e4-e3c29eef9517":{"id":"11525ec6-8874-46cd-95e4-e3c29eef9517","message":{"id":"11525ec6-8874-46cd-95e4-e3c29eef9517","author":{"role":"user","name":null,"metadata":{}},"create_time":1742184936.092,"update_time":null,"content":{"content_type":"text","parts":["Can the Joint Embedding Predictive Architecture (JEPA), proposed by Yann LeCun, be incorporated or exploited by MuZero?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"9219ab99a88b4088-DFW","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"40dfbb89-d931-425e-ba48-7f09a727f273","children":["f929c461-3bfb-4c00-82a8-d8cc616ca37d"]},"f929c461-3bfb-4c00-82a8-d8cc616ca37d":{"id":"f929c461-3bfb-4c00-82a8-d8cc616ca37d","message":{"id":"f929c461-3bfb-4c00-82a8-d8cc616ca37d","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1742184940.103099,"update_time":null,"content":{"content_type":"text","parts":["**Exploring JEPA and MuZero**\n\nThe user asks about incorporating JEPA (Joint Embedding Predictive Architecture) into MuZero. JEPA is a self-supervised learning method introduced by Yann LeCun, aiming to predict embeddings of future states without needing reconstruction. It learns joint embeddings from different input representations. MuZero, on the other hand, learns a model of its environment without needing access to the environment's dynamics. I wonder if there could be an interesting synergy between how JEPA predicts future states and how MuZero models them.\n\n**Integrating JEPA with MuZero**\n\nMuZero uses a latent dynamics model to predict states and then uses these to predict policy, value, and reward. JEPA, on the other hand, focuses on predicting future state embeddings from current context without needing full observation reconstruction. They both avoid reconstruction, which is interesting, as JEPA might guide MuZero\u2019s representation or dynamics function. Using JEPA as an auxiliary loss could help improve prediction accuracy, sample efficiency, and generalization by encouraging predictive embeddings in MuZero's latent state."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned for 4 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"11525ec6-8874-46cd-95e4-e3c29eef9517","request_id":"9219ab99a88b4088-DFW","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"11525ec6-8874-46cd-95e4-e3c29eef9517","children":["d1306f3b-f785-46c8-85df-01264f040929"]},"d1306f3b-f785-46c8-85df-01264f040929":{"id":"d1306f3b-f785-46c8-85df-01264f040929","message":{"id":"d1306f3b-f785-46c8-85df-01264f040929","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1742184939.088162,"update_time":null,"content":{"content_type":"text","parts":["Yes, there\u2019s a promising synergy between JEPA and MuZero\u2019s framework. Both approaches aim to learn task-relevant latent representations without the burden of full reconstruction, but they come from slightly different angles. Here\u2019s how JEPA could be incorporated or exploited by MuZero:\n\n---\n\n### **1. Complementary Objectives**\n\n- **JEPA\u2019s Focus:**  \n  JEPA (Joint Embedding Predictive Architecture) is designed to learn representations by predicting the embedding of future observations directly, rather than reconstructing the raw data. Its objective is to capture the essence of future states in a low-dimensional space, typically using contrastive or regression-based losses.\n\n- **MuZero\u2019s Focus:**  \n  MuZero learns a latent dynamics model that outputs a representation used to predict three key quantities: a policy (next-action probabilities), a value (expected cumulative reward), and immediate rewards. The latent state in MuZero is not forced to reconstruct the full observation; it only needs to be \u201cgood enough\u201d for planning.\n\n- **Synergy:**  \n  Incorporating JEPA\u2019s predictive loss into MuZero could reinforce the quality of the latent space. By adding an auxiliary JEPA objective, the model could be encouraged to learn embeddings that are not only useful for predicting rewards and values but also for capturing the underlying dynamics of the observation space in a more semantically rich manner.\n\n---\n\n### **2. Potential Integration Strategies**\n\n- **Pre-training the Representation Network:**  \n  JEPA can be used to pre-train the representation function \\( h \\) in MuZero. By learning to predict future embeddings in a self-supervised manner, the model could start with a robust latent space that already captures important aspects of the environment dynamics. After pre-training, the full MuZero architecture (including the dynamics \\( g \\) and prediction \\( f \\) functions) can be fine-tuned with the planning objective.\n\n- **Joint Multi-Task Learning:**  \n  Instead of pre-training, one could incorporate the JEPA objective as an additional loss term during the MuZero training. Here, alongside the standard losses for policy, value, and reward prediction, the model is also trained to predict the joint embedding of a future observation. This encourages the latent state to be informative for both planning and general representation purposes.\n\n- **Improving Latent Dynamics:**  \n  JEPA\u2019s approach may help mitigate compounding errors in the dynamics model. By explicitly training the latent state to be predictive of future observations (in embedding space), the dynamics function \\( g \\) might produce more stable and semantically meaningful state transitions, which in turn can lead to better planning outcomes.\n\n---\n\n### **3. Benefits and Challenges**\n\n- **Benefits:**\n  - **Enhanced Representations:** The auxiliary JEPA loss can push the latent space to capture richer semantic and predictive features, which might improve the quality of the planning process.\n  - **Sample Efficiency:** Better representations could lead to more accurate predictions with fewer samples, an important consideration in environments with sparse rewards.\n  - **Robustness:** By aligning the latent state with both planning-specific signals (rewards, values, policies) and a broader predictive objective, the overall model might be more robust to variations in the environment.\n\n- **Challenges:**\n  - **Balancing Losses:** Jointly training with multiple objectives requires careful tuning to ensure that the JEPA loss complements rather than conflicts with the planning objectives.\n  - **Complexity:** Adding an extra objective increases the computational and architectural complexity of the model.\n  - **Design Choices:** JEPA involves its own design choices\u2014such as the structure of the target encoder and the specific form of the loss function\u2014that need to be harmonized with the MuZero setup.\n\n---\n\n### **Conclusion**\n\nIncorporating JEPA into MuZero is not only feasible but could also enhance the learned latent representations by leveraging a self-supervised predictive signal. This integration might yield a more robust and semantically enriched latent space, ultimately improving the planning and decision-making process. The key is to balance the JEPA objective with MuZero\u2019s existing goals, ensuring that both contribute positively to the model\u2019s performance in complex environments.\n\nFeel free to ask if you\u2019d like to explore more specific implementation ideas or further details on how the loss balancing might work!"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"f929c461-3bfb-4c00-82a8-d8cc616ca37d","request_id":"9219ab99a88b4088-DFW","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"f929c461-3bfb-4c00-82a8-d8cc616ca37d","children":["a015bc77-bd0e-49bc-9fcd-1390f006d63d"]},"a015bc77-bd0e-49bc-9fcd-1390f006d63d":{"id":"a015bc77-bd0e-49bc-9fcd-1390f006d63d","message":{"id":"a015bc77-bd0e-49bc-9fcd-1390f006d63d","author":{"role":"user","name":null,"metadata":{}},"create_time":1743204958.101,"update_time":null,"content":{"content_type":"text","parts":["If MuZero  did not know the rules of the Board Games, how was it trained to respond with legal moves?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"927af26b69f140ea-SJC","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"d1306f3b-f785-46c8-85df-01264f040929","children":["1ac5f9cf-0fa3-4082-9df8-6751677bb392"]},"1ac5f9cf-0fa3-4082-9df8-6751677bb392":{"id":"1ac5f9cf-0fa3-4082-9df8-6751677bb392","message":{"id":"1ac5f9cf-0fa3-4082-9df8-6751677bb392","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1743204960.414874,"update_time":null,"content":{"content_type":"text","parts":["**Exploring MuZero's move training**\n\nMuZero doesn't know the rules of the board games, but it learns to play by generating data from self-play. The paper explains that it\u2019s trained to produce legal moves without explicit rule encoding \u2014 this is done by observing available legal actions at the root of its search tree. As the model deepens, illegal moves aren't selected because they haven't been learned through training. The policy network helps too, predicting only valid moves during simulation.\n\n**Clarifying MuZero's training on legal moves**\n\nMuZero doesn't mask actions during search, except for the root where the available actions are observed. The policy network is trained to predict legal moves, but it doesn't explicitly output all of them. Instead, it learns to give near-zero probability to illegal moves because they don't reward the model. This is part of the training process, as the model simply doesn\u2019t select illegal actions during self-play, allowing it to be trained on legal moves."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned for 5 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"a015bc77-bd0e-49bc-9fcd-1390f006d63d","request_id":"927af26b69f140ea-SJC","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"a015bc77-bd0e-49bc-9fcd-1390f006d63d","children":["9ebc14f4-c2be-42e7-b61a-4fa13608da9b"]},"9ebc14f4-c2be-42e7-b61a-4fa13608da9b":{"id":"9ebc14f4-c2be-42e7-b61a-4fa13608da9b","message":{"id":"9ebc14f4-c2be-42e7-b61a-4fa13608da9b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1743204958.790439,"update_time":null,"content":{"content_type":"text","parts":["MuZero is trained entirely through self-play without being given the explicit rules, yet it still learns to output only legal moves. Here's how it achieves that:\n\n- **Observations at the Root:**  \n  At the beginning of each move, the game provides the set of legal actions. MuZero uses this information to mask the policy at the root of the search tree so that only legal moves are considered when planning.\n\n- **Learning Through Self-Play:**  \n  During self-play, the network is trained on trajectories where only legal moves lead to valid game outcomes. Illegal moves are never executed because they either result in a terminal (loss) state or are filtered out at the root. Over time, the policy network learns to assign near-zero probability to illegal moves since they don\u2019t contribute to winning.\n\n- **Implicit Learning of Game Dynamics:**  \n  The network\u2019s latent state and dynamics model are trained to predict rewards, values, and policies based solely on the outcomes observed during self-play. Because only legal moves produce meaningful rewards (or avoid penalties), the network naturally converges on strategies that only include legal moves.\n\nIn summary, by using legal moves as part of the observation at the root and through the feedback provided during self-play, MuZero learns to internalize the rules of the game without ever being explicitly told them."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"1ac5f9cf-0fa3-4082-9df8-6751677bb392","request_id":"927af26b69f140ea-SJC","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"1ac5f9cf-0fa3-4082-9df8-6751677bb392","children":[]}}}