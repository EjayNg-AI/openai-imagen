{"id":"4e621752-b105-4f5e-b105-abc288efb47a","title":"Understanding xLSTM","create_time":1719125588.778178,"update_time":1719126499.739297,"current_node":"cadab95b-f83f-4462-b2f5-0c0d2e5627de","mapping":{"aaa1c663-91ac-4b51-8013-fa9177cbd27e":{"id":"aaa1c663-91ac-4b51-8013-fa9177cbd27e","message":null,"parent":null,"children":["9744602e-be62-4532-b583-b5a84e8cdadd"]},"9744602e-be62-4532-b583-b5a84e8cdadd":{"id":"9744602e-be62-4532-b583-b5a84e8cdadd","message":{"id":"9744602e-be62-4532-b583-b5a84e8cdadd","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"aaa1c663-91ac-4b51-8013-fa9177cbd27e","children":["1affb8c0-7a27-47f5-baca-7a031aad64c7"]},"1affb8c0-7a27-47f5-baca-7a031aad64c7":{"id":"1affb8c0-7a27-47f5-baca-7a031aad64c7","message":{"id":"1affb8c0-7a27-47f5-baca-7a031aad64c7","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_user_system_message":true,"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"About Myself: I have a very high level of mathematical knowledge up to the post-graduate level. \n\nThese are my main areas of interest:\n(1) coding: mainly Python\n(2) mathematics up to graduate level and research level\n(3) machine learning and artificial intelligence","about_model_message":"In coding, use Pytorch wherever appropriate."},"can_save":false},"recipient":"all","channel":null},"parent":"9744602e-be62-4532-b583-b5a84e8cdadd","children":["aaa2de2e-1a2b-4a5d-8d0a-25605b7a3995"]},"aaa2de2e-1a2b-4a5d-8d0a-25605b7a3995":{"id":"aaa2de2e-1a2b-4a5d-8d0a-25605b7a3995","message":{"id":"aaa2de2e-1a2b-4a5d-8d0a-25605b7a3995","author":{"role":"user","name":null,"metadata":{}},"create_time":1719125588.786034,"update_time":null,"content":{"content_type":"text","parts":["what is xLTSM?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"89828f2fcd333d95-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"1affb8c0-7a27-47f5-baca-7a031aad64c7","children":["5c916647-cf88-4c51-8d1a-7c801d903bdd"]},"5c916647-cf88-4c51-8d1a-7c801d903bdd":{"id":"5c916647-cf88-4c51-8d1a-7c801d903bdd","message":{"id":"5c916647-cf88-4c51-8d1a-7c801d903bdd","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125600.998204,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"aaa2de2e-1a2b-4a5d-8d0a-25605b7a3995","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"aaa2de2e-1a2b-4a5d-8d0a-25605b7a3995","children":["34e25596-3dce-4e90-997a-d7d077060661"]},"34e25596-3dce-4e90-997a-d7d077060661":{"id":"34e25596-3dce-4e90-997a-d7d077060661","message":{"id":"34e25596-3dce-4e90-997a-d7d077060661","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125600.998265,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"xLTSM\")"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"5c916647-cf88-4c51-8d1a-7c801d903bdd","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"browser","channel":null},"parent":"5c916647-cf88-4c51-8d1a-7c801d903bdd","children":["ad42b002-ec93-4ad0-802a-db82a25bdad2"]},"ad42b002-ec93-4ad0-802a-db82a25bdad2":{"id":"ad42b002-ec93-4ad0-802a-db82a25bdad2","message":{"id":"ad42b002-ec93-4ad0-802a-db82a25bdad2","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125600.998307,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020How to Open an XLSM File: A Step-by-Step Guide\u2020www.solveyourtech.com\u3011\nStep 2: Open the File. Locate the XLSM file on your computer and double-click it to open. Double-clicking the file should launch your preferred spreadsheet program, and the file should open. If it doesn\u2019t, you might need to right-click the file, select \u201cOpen with,\u201d and choose the appropriate program.\n# \u30101\u2020Reports, Forms and Dashboards for Industry | XLReporter\u2020www.sytech.com\u3011\nThe Design Studio is provided standalone or built into Microsoft Excel. Take full advantage of charts, formulas, formatting, pagination, headers/footers plus the XLReporter functions specifically designed for industry. Use your existing Excel skills and avoid the steep learning curve usually associated with new software.\n# \u30102\u2020XLReporter - SmartSights\u2020smartsights.com\u3011\nXLReporter isn\u2019t just a reporting tool. It provides clear insights for effective decision-making and action, empowering you with operational intelligence to elevate performance. It offers easy access to data from PLCs, historians, alarm history, and IoT devices, coupled with built-in analytics that transform raw data into actionable insights.\n# \u30103\u2020Excel File Extensions: XLSX, XLSM, XLS, XLTX, and XLTM - Lifewire\u2020www.lifewire.com\u3011\nPrior to Excel 2007, the default file extension was XLS. The main difference between the two is that XLSX is an XML-based open file format and XLS is a proprietary Microsoft format. But, the newer versions of Excel save and open XLS files for the sake of compatibility with earlier versions of the program. Determine if a file contains macros ...\n# \u30104\u2020XLSM File - What is an .xlsm file and how do I open it? - FileInfo.com\u2020fileinfo.com\u3011\nAn XLSM file is a macro-enabled spreadsheet created by Microsoft Excel, a widely-used spreadsheet program included in the Microsoft Office suite. It contains worksheets of cells arranged by rows and columns as well as embedded macros programmed in the Visual Basic for Applications (VBA) language. XLSM files are saved in the Open XML format ...\n# \u30105\u2020File Types in Excel: When to use XLSX, XLSM, XLSB, XLS etc.?\u2020professor-excel.com\u3011\nExcel provides 29 different file types when you save your Excel workbook. When you save an Excel workbook for the first time or use the \u201cSave As\u201d function, you are asked for a folder, file name as well as the file type. By default, Excel suggests the XLSX file type (unless your file has VBA macros). The window looks similar to the ...\n# \u30106\u2020What Is an XLSM File? - Lifewire\u2020www.lifewire.com\u3011\nA file with the XLSM file extension is an Excel macro-enabled workbook file created in Excel 2007 or newer. These files are identical to Microsoft Excel Open XML Format Spreadsheet ( XLSX) files, with the only difference being that XLSM files will execute embedded macros that are programmed in the Visual Basic for Applications (VBA) language ...\n# \u30107\u2020XLSM File: How to open XLSM file (and what it is)\u2020file.org\u3011\nAn .XLSM file is a Microsoft Excel Open XML Format Spreadsheet file. Files that contain the .xlsm file extension are files that contain macro-enabled spreadsheet files that have been created with the Microsoft Excel spreadsheet application. XLSM worksheets contain cells that are arranged in columns and rows.\n# \u30108\u2020What is an XLSM file? - File Format Docs\u2020docs.fileformat.com\u3011\nWhat is an XLSM file? Files with XLSM extension is a type of Spreadsheet files that support Macros. From application point of view, a Macro is set of instructions that are used for automating processes. A macro is used to record the steps that are performed repeatedly and facilitates performing the actions by running the macro again.\n# \u30109\u2020XLSM Converter | CloudConvert\u2020cloudconvert.com\u3011\nXLSM Converter. CloudConvert is an online document and spreadsheet converter. Amongst many others, we support PDF, XLSX, XLS, CSV, ODS. Thanks to our advanced conversion technology the quality of the output will be as good as if the file was saved through the latest Microsoft Office 2021 suite. convert. XLSM.\n# \u301010\u2020Saving xls to xlsx xlsm - Microsoft Support\u2020support.microsoft.com\u3011\nSteps to convert .xls (macros workbooks) to .xlsm. Open the identified .xls workbook which contains macros. Navigate to File > Save As > Browse \u2013 which will open the below window. Make sure you are in the right path to save the workbook > Select Excel Macro-Enabled Workbook (*.xlsm) as shown below and select Save.\n# \u301011\u2020XLReporter V14 Go Beyond Excel - sytech.com\u2020www.sytech.com\u3011\nGo Beyond Excel. Microsoft Excel\u00ae is indisputably one of the most widely used software applications in the world. XLReporter\u00ae takes the best of this product and delivers it to an industrial environment, without needing Excel! The result is a secure, efficient reporting solution with around-the-clock robustness.\n# \u301012\u2020how to unblock macro enabled excel files .xlsm?\u2020answers.microsoft.com\u3011\nMethod 1: Trust Center Settings. Open Microsoft Excel on the computer where you want to unblock the .xlsm file. Click on the \"File\" tab in the Excel ribbon. Choose \"Options\" at the bottom of the left sidebar. This opens the Excel Options window. In the Excel Options window, select \"Trust Center\" from the left sidebar.\n# \u301013\u2020Excel File Formats: XLSX, XLSM, XLS, XLTX, XLTM\u2020blog.fileformat.com\u3011\nA file with XLSX, XLSM, XLS, XLTX or XLTM extension is a Microsoft Excel file that uses specific standard file format. You can show or display file extension on Windows OS from Folder Options. MS Excel lets you save files in any of these file formats using the Save As option. These Excel file formats serve different purposes for working with Spreadsheet files as explained in this article.\n# \u301014\u2020View XLSM Files Online for Free - Aspose\u2020products.aspose.app\u3011\nOpen Excel online and access your files from any device, anywhere, without the need for editing capabilities. Our platform provides a reliable, user-friendly way to view your Excel files with crystal clear clarity. Share, view, and navigate through your worksheets seamlessly while ensuring the top-quality viewing experience.\n# \u301015\u2020XLSM Malware with MacroSheets - McAfee\u2020www.mcafee.com\u3011\nExcel 4.0 and XLSM are both known to download other malware payloads like ZLoader, Trickbot, Qakbot, Ursnif, IcedID, etc. Field hits for XLSM macrosheet malware detection. The above figure shows the Number of samples weekly detected by the detected name \u201cDownloader-FCEI\u201d which specifically targets XLSM macrosheet based malware.\n# \u301016\u2020Difference Between Excel Files- XLSM Vs. XLSX\u2020www.reliablecounter.com\u3011\nXLSM is a file extension in Excel that was introduced in 2007. This will allow you to create macro-enabled workbook file. It is similar to the default extension XLSX but has a slight difference. The distinctive feature of XLSM is that is able to save a file with embedded macros. These are programmed in the VBA language.\n# \u301017\u2020What is f_xltsm.dll from Fasoo.com? (id:2088701) - System Explorer\u2020systemexplorer.net\u3011\nOur database contains 35 different files for filename f_xltsm.dll but this page contains information about single file with specific attributes. If you want to see general information about the f_xltsm.dll then visit General Information Page .You can also check most distributed file variants with name f_xltsm.dll.This file belongs to product f_xltsm and was developed by company Fasoo.com.\n# \u301018\u2020Excel xlsm files not opening - Microsoft Community\u2020answers.microsoft.com\u3011\nSolution 3: Reset File Associations. Try resetting the file associations this worked for many Excel users to fix xlsm file not opening issue. Follow the steps: Go to Control Panel > click Default Programs. Reset File Associations 1. Now click on Set your default programs > click Excel. Reset File Associations 2.\n# \u301019\u2020Differences between xls, xlsx, xlsm, xlsb and csv - WiseCleaner\u2020www.wisecleaner.com\u3011\nIn addition to some functional differences, the biggest difference between these types of tables is the file size. For the same table content, the \".xlsb\" format has the smallest file size, followed by the \".xlsx\" and \".xlsm\" formats, then the \".csv\", and finally the xls format. Generally speaking, for the content of the same table, the table ...\nVisible: 0% - 100%","summary":null,"assets":[],"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"cloud_doc_urls":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"How to Open an XLSM File: A Step-by-Step Guide","url":"https://www.solveyourtech.com/how-to-open-an-xlsm-file-a-step-by-step-guide/","text":"\nStep 2: Open the File. Locate the XLSM file on your computer and double-click it to open. Double-clicking the file should launch your preferred spreadsheet program, and the file should open. If it doesn\u2019t, you might need to right-click the file, select \u201cOpen with,\u201d and choose the appropriate program.\n","pub_date":"2024-06-20T05:27:00.0000000Z","extra":null},{"type":"webpage","title":"Reports, Forms and Dashboards for Industry | XLReporter","url":"https://www.sytech.com/","text":"\nThe Design Studio is provided standalone or built into Microsoft Excel. Take full advantage of charts, formulas, formatting, pagination, headers/footers plus the XLReporter functions specifically designed for industry. Use your existing Excel skills and avoid the steep learning curve usually associated with new software.\n","pub_date":"2024-06-19T20:32:00.0000000Z","extra":null},{"type":"webpage","title":"XLReporter - SmartSights","url":"https://smartsights.com/solutions/xlreporter/","text":"\nXLReporter isn\u2019t just a reporting tool. It provides clear insights for effective decision-making and action, empowering you with operational intelligence to elevate performance. It offers easy access to data from PLCs, historians, alarm history, and IoT devices, coupled with built-in analytics that transform raw data into actionable insights.\n","pub_date":"2024-06-19T20:03:00.0000000Z","extra":null},{"type":"webpage","title":"Excel File Extensions: XLSX, XLSM, XLS, XLTX, and XLTM - Lifewire","url":"https://www.lifewire.com/excel-file-extensions-and-their-uses-3124113","text":"\nPrior to Excel 2007, the default file extension was XLS. The main difference between the two is that XLSX is an XML-based open file format and XLS is a proprietary Microsoft format. But, the newer versions of Excel save and open XLS files for the sake of compatibility with earlier versions of the program. Determine if a file contains macros ...\n","pub_date":"2024-06-20T14:33:00.0000000Z","extra":null},{"type":"webpage","title":"XLSM File - What is an .xlsm file and how do I open it? - FileInfo.com","url":"https://fileinfo.com/extension/xlsm","text":"\nAn XLSM file is a macro-enabled spreadsheet created by Microsoft Excel, a widely-used spreadsheet program included in the Microsoft Office suite. It contains worksheets of cells arranged by rows and columns as well as embedded macros programmed in the Visual Basic for Applications (VBA) language. XLSM files are saved in the Open XML format ...\n","pub_date":"2024-06-22T02:46:00.0000000Z","extra":null},{"type":"webpage","title":"File Types in Excel: When to use XLSX, XLSM, XLSB, XLS etc.?","url":"https://professor-excel.com/xlsx-xlsm-xlsb-file-types/","text":"\nExcel provides 29 different file types when you save your Excel workbook. When you save an Excel workbook for the first time or use the \u201cSave As\u201d function, you are asked for a folder, file name as well as the file type. By default, Excel suggests the XLSX file type (unless your file has VBA macros). The window looks similar to the ...\n","pub_date":"2024-06-22T01:08:00.0000000Z","extra":null},{"type":"webpage","title":"What Is an XLSM File? - Lifewire","url":"https://www.lifewire.com/xlsm-file-2622538","text":"\nA file with the XLSM file extension is an Excel macro-enabled workbook file created in Excel 2007 or newer. These files are identical to Microsoft Excel Open XML Format Spreadsheet ( XLSX) files, with the only difference being that XLSM files will execute embedded macros that are programmed in the Visual Basic for Applications (VBA) language ...\n","pub_date":"2024-06-20T22:24:00.0000000Z","extra":null},{"type":"webpage","title":"XLSM File: How to open XLSM file (and what it is)","url":"https://file.org/extension/xlsm","text":"\nAn .XLSM file is a Microsoft Excel Open XML Format Spreadsheet file. Files that contain the .xlsm file extension are files that contain macro-enabled spreadsheet files that have been created with the Microsoft Excel spreadsheet application. XLSM worksheets contain cells that are arranged in columns and rows.\n","pub_date":"2024-06-21T09:50:00.0000000Z","extra":null},{"type":"webpage","title":"What is an XLSM file? - File Format Docs","url":"https://docs.fileformat.com/spreadsheet/xlsm/","text":"\nWhat is an XLSM file? Files with XLSM extension is a type of Spreadsheet files that support Macros. From application point of view, a Macro is set of instructions that are used for automating processes. A macro is used to record the steps that are performed repeatedly and facilitates performing the actions by running the macro again.\n","pub_date":"2024-06-21T08:26:00.0000000Z","extra":null},{"type":"webpage","title":"XLSM Converter | CloudConvert","url":"https://cloudconvert.com/xlsm-converter","text":"\nXLSM Converter. CloudConvert is an online document and spreadsheet converter. Amongst many others, we support PDF, XLSX, XLS, CSV, ODS. Thanks to our advanced conversion technology the quality of the output will be as good as if the file was saved through the latest Microsoft Office 2021 suite. convert. XLSM.\n","pub_date":"2024-06-21T07:17:00.0000000Z","extra":null},{"type":"webpage","title":"Saving xls to xlsx xlsm - Microsoft Support","url":"https://support.microsoft.com/en-us/office/saving-xls-to-xlsx-xlsm-d74fe848-d887-4e63-9638-8f752cd743a2","text":"\nSteps to convert .xls (macros workbooks) to .xlsm. Open the identified .xls workbook which contains macros. Navigate to File > Save As > Browse \u2013 which will open the below window. Make sure you are in the right path to save the workbook > Select Excel Macro-Enabled Workbook (*.xlsm) as shown below and select Save.\n","pub_date":"2024-06-22T18:24:00.0000000Z","extra":null},{"type":"webpage","title":"XLReporter V14 Go Beyond Excel - sytech.com","url":"https://www.sytech.com/docs/datasheet-xlreporter-professional.pdf","text":"\nGo Beyond Excel. Microsoft Excel\u00ae is indisputably one of the most widely used software applications in the world. XLReporter\u00ae takes the best of this product and delivers it to an industrial environment, without needing Excel! The result is a secure, efficient reporting solution with around-the-clock robustness.\n","pub_date":"2024-06-17T19:00:00.0000000Z","extra":null},{"type":"webpage","title":"how to unblock macro enabled excel files .xlsm?","url":"https://answers.microsoft.com/en-us/msoffice/forum/all/how-to-unblock-macro-enabled-excel-files-xlsm/da8b8a16-e2f6-4282-8302-5ca7fe23eee0","text":"\nMethod 1: Trust Center Settings. Open Microsoft Excel on the computer where you want to unblock the .xlsm file. Click on the \"File\" tab in the Excel ribbon. Choose \"Options\" at the bottom of the left sidebar. This opens the Excel Options window. In the Excel Options window, select \"Trust Center\" from the left sidebar.\n","pub_date":"2024-06-21T10:07:00.0000000Z","extra":null},{"type":"webpage","title":"Excel File Formats: XLSX, XLSM, XLS, XLTX, XLTM","url":"https://blog.fileformat.com/spreadsheet/excel-file-extensions-xlsx-xlsm-xls-xltx-xltm/","text":"\nA file with XLSX, XLSM, XLS, XLTX or XLTM extension is a Microsoft Excel file that uses specific standard file format. You can show or display file extension on Windows OS from Folder Options. MS Excel lets you save files in any of these file formats using the Save As option. These Excel file formats serve different purposes for working with Spreadsheet files as explained in this article.\n","pub_date":"2024-06-18T13:35:00.0000000Z","extra":null},{"type":"webpage","title":"View XLSM Files Online for Free - Aspose","url":"https://products.aspose.app/cells/viewer/xlsm","text":"\nOpen Excel online and access your files from any device, anywhere, without the need for editing capabilities. Our platform provides a reliable, user-friendly way to view your Excel files with crystal clear clarity. Share, view, and navigate through your worksheets seamlessly while ensuring the top-quality viewing experience.\n","pub_date":"2024-06-21T06:47:00.0000000Z","extra":null},{"type":"webpage","title":"XLSM Malware with MacroSheets - McAfee","url":"https://www.mcafee.com/blogs/other-blogs/mcafee-labs/xlsm-malware-with-macrosheets/","text":"\nExcel 4.0 and XLSM are both known to download other malware payloads like ZLoader, Trickbot, Qakbot, Ursnif, IcedID, etc. Field hits for XLSM macrosheet malware detection. The above figure shows the Number of samples weekly detected by the detected name \u201cDownloader-FCEI\u201d which specifically targets XLSM macrosheet based malware.\n","pub_date":"2024-06-21T07:51:00.0000000Z","extra":null},{"type":"webpage","title":"Difference Between Excel Files- XLSM Vs. XLSX","url":"https://www.reliablecounter.com/blog/difference-excel-files-xlsm-vs-xlsx/","text":"\nXLSM is a file extension in Excel that was introduced in 2007. This will allow you to create macro-enabled workbook file. It is similar to the default extension XLSX but has a slight difference. The distinctive feature of XLSM is that is able to save a file with embedded macros. These are programmed in the VBA language.\n","pub_date":"2024-06-18T23:19:00.0000000Z","extra":null},{"type":"webpage","title":"What is f_xltsm.dll from Fasoo.com? (id:2088701) - System Explorer","url":"https://systemexplorer.net/file-database/file/f_xltsm-dll/2088701","text":"\nOur database contains 35 different files for filename f_xltsm.dll but this page contains information about single file with specific attributes. If you want to see general information about the f_xltsm.dll then visit General Information Page .You can also check most distributed file variants with name f_xltsm.dll.This file belongs to product f_xltsm and was developed by company Fasoo.com.\n","pub_date":"2024-06-16T23:08:00.0000000Z","extra":null},{"type":"webpage","title":"Excel xlsm files not opening - Microsoft Community","url":"https://answers.microsoft.com/en-us/msoffice/forum/all/excel-xlsm-files-not-opening/b076782e-e535-4666-b955-3c5120c9ab72","text":"\nSolution 3: Reset File Associations. Try resetting the file associations this worked for many Excel users to fix xlsm file not opening issue. Follow the steps: Go to Control Panel > click Default Programs. Reset File Associations 1. Now click on Set your default programs > click Excel. Reset File Associations 2.\n","pub_date":"2024-06-21T17:18:00.0000000Z","extra":null},{"type":"webpage","title":"Differences between xls, xlsx, xlsm, xlsb and csv - WiseCleaner","url":"https://www.wisecleaner.com/think-tank/291-Differences-between-xls-xlsx-xlsm-xlsb-and-csv.html","text":"\nIn addition to some functional differences, the biggest difference between these types of tables is the file size. For the same table content, the \".xlsb\" format has the smallest file size, followed by the \".xlsx\" and \".xlsm\" formats, then the \".csv\", and finally the xls format. Generally speaking, for the content of the same table, the table ...\nVisible: 0% - 100%","pub_date":"2024-06-21T17:01:00.0000000Z","extra":null},{"type":"webpage","title":"Search results for query: 'xLTSM'","url":"","text":"# \u30100\u2020How to Open an XLSM File: A Step-by-Step Guide\u2020www.solveyourtech.com\u3011\nStep 2: Open the File. Locate the XLSM file on your computer and double-click it to open. Double-clicking the file should launch your preferred spreadsheet program, and the file should open. If it doesn\u2019t, you might need to right-click the file, select \u201cOpen with,\u201d and choose the appropriate program.\n# \u30101\u2020Reports, Forms and Dashboards for Industry | XLReporter\u2020www.sytech.com\u3011\nThe Design Studio is provided standalone or built into Microsoft Excel. Take full advantage of charts, formulas, formatting, pagination, headers/footers plus the XLReporter functions specifically designed for industry. Use your existing Excel skills and avoid the steep learning curve usually associated with new software.\n# \u30102\u2020XLReporter - SmartSights\u2020smartsights.com\u3011\nXLReporter isn\u2019t just a reporting tool. It provides clear insights for effective decision-making and action, empowering you with operational intelligence to elevate performance. It offers easy access to data from PLCs, historians, alarm history, and IoT devices, coupled with built-in analytics that transform raw data into actionable insights.\n# \u30103\u2020Excel File Extensions: XLSX, XLSM, XLS, XLTX, and XLTM - Lifewire\u2020www.lifewire.com\u3011\nPrior to Excel 2007, the default file extension was XLS. The main difference between the two is that XLSX is an XML-based open file format and XLS is a proprietary Microsoft format. But, the newer versions of Excel save and open XLS files for the sake of compatibility with earlier versions of the program. Determine if a file contains macros ...\n# \u30104\u2020XLSM File - What is an .xlsm file and how do I open it? - FileInfo.com\u2020fileinfo.com\u3011\nAn XLSM file is a macro-enabled spreadsheet created by Microsoft Excel, a widely-used spreadsheet program included in the Microsoft Office suite. It contains worksheets of cells arranged by rows and columns as well as embedded macros programmed in the Visual Basic for Applications (VBA) language. XLSM files are saved in the Open XML format ...\n# \u30105\u2020File Types in Excel: When to use XLSX, XLSM, XLSB, XLS etc.?\u2020professor-excel.com\u3011\nExcel provides 29 different file types when you save your Excel workbook. When you save an Excel workbook for the first time or use the \u201cSave As\u201d function, you are asked for a folder, file name as well as the file type. By default, Excel suggests the XLSX file type (unless your file has VBA macros). The window looks similar to the ...\n# \u30106\u2020What Is an XLSM File? - Lifewire\u2020www.lifewire.com\u3011\nA file with the XLSM file extension is an Excel macro-enabled workbook file created in Excel 2007 or newer. These files are identical to Microsoft Excel Open XML Format Spreadsheet ( XLSX) files, with the only difference being that XLSM files will execute embedded macros that are programmed in the Visual Basic for Applications (VBA) language ...\n# \u30107\u2020XLSM File: How to open XLSM file (and what it is)\u2020file.org\u3011\nAn .XLSM file is a Microsoft Excel Open XML Format Spreadsheet file. Files that contain the .xlsm file extension are files that contain macro-enabled spreadsheet files that have been created with the Microsoft Excel spreadsheet application. XLSM worksheets contain cells that are arranged in columns and rows.\n# \u30108\u2020What is an XLSM file? - File Format Docs\u2020docs.fileformat.com\u3011\nWhat is an XLSM file? Files with XLSM extension is a type of Spreadsheet files that support Macros. From application point of view, a Macro is set of instructions that are used for automating processes. A macro is used to record the steps that are performed repeatedly and facilitates performing the actions by running the macro again.\n# \u30109\u2020XLSM Converter | CloudConvert\u2020cloudconvert.com\u3011\nXLSM Converter. CloudConvert is an online document and spreadsheet converter. Amongst many others, we support PDF, XLSX, XLS, CSV, ODS. Thanks to our advanced conversion technology the quality of the output will be as good as if the file was saved through the latest Microsoft Office 2021 suite. convert. XLSM.\n# \u301010\u2020Saving xls to xlsx xlsm - Microsoft Support\u2020support.microsoft.com\u3011\nSteps to convert .xls (macros workbooks) to .xlsm. Open the identified .xls workbook which contains macros. Navigate to File > Save As > Browse \u2013 which will open the below window. Make sure you are in the right path to save the workbook > Select Excel Macro-Enabled Workbook (*.xlsm) as shown below and select Save.\n# \u301011\u2020XLReporter V14 Go Beyond Excel - sytech.com\u2020www.sytech.com\u3011\nGo Beyond Excel. Microsoft Excel\u00ae is indisputably one of the most widely used software applications in the world. XLReporter\u00ae takes the best of this product and delivers it to an industrial environment, without needing Excel! The result is a secure, efficient reporting solution with around-the-clock robustness.\n# \u301012\u2020how to unblock macro enabled excel files .xlsm?\u2020answers.microsoft.com\u3011\nMethod 1: Trust Center Settings. Open Microsoft Excel on the computer where you want to unblock the .xlsm file. Click on the \"File\" tab in the Excel ribbon. Choose \"Options\" at the bottom of the left sidebar. This opens the Excel Options window. In the Excel Options window, select \"Trust Center\" from the left sidebar.\n# \u301013\u2020Excel File Formats: XLSX, XLSM, XLS, XLTX, XLTM\u2020blog.fileformat.com\u3011\nA file with XLSX, XLSM, XLS, XLTX or XLTM extension is a Microsoft Excel file that uses specific standard file format. You can show or display file extension on Windows OS from Folder Options. MS Excel lets you save files in any of these file formats using the Save As option. These Excel file formats serve different purposes for working with Spreadsheet files as explained in this article.\n# \u301014\u2020View XLSM Files Online for Free - Aspose\u2020products.aspose.app\u3011\nOpen Excel online and access your files from any device, anywhere, without the need for editing capabilities. Our platform provides a reliable, user-friendly way to view your Excel files with crystal clear clarity. Share, view, and navigate through your worksheets seamlessly while ensuring the top-quality viewing experience.\n# \u301015\u2020XLSM Malware with MacroSheets - McAfee\u2020www.mcafee.com\u3011\nExcel 4.0 and XLSM are both known to download other malware payloads like ZLoader, Trickbot, Qakbot, Ursnif, IcedID, etc. Field hits for XLSM macrosheet malware detection. The above figure shows the Number of samples weekly detected by the detected name \u201cDownloader-FCEI\u201d which specifically targets XLSM macrosheet based malware.\n# \u301016\u2020Difference Between Excel Files- XLSM Vs. XLSX\u2020www.reliablecounter.com\u3011\nXLSM is a file extension in Excel that was introduced in 2007. This will allow you to create macro-enabled workbook file. It is similar to the default extension XLSX but has a slight difference. The distinctive feature of XLSM is that is able to save a file with embedded macros. These are programmed in the VBA language.\n# \u301017\u2020What is f_xltsm.dll from Fasoo.com? (id:2088701) - System Explorer\u2020systemexplorer.net\u3011\nOur database contains 35 different files for filename f_xltsm.dll but this page contains information about single file with specific attributes. If you want to see general information about the f_xltsm.dll then visit General Information Page .You can also check most distributed file variants with name f_xltsm.dll.This file belongs to product f_xltsm and was developed by company Fasoo.com.\n# \u301018\u2020Excel xlsm files not opening - Microsoft Community\u2020answers.microsoft.com\u3011\nSolution 3: Reset File Associations. Try resetting the file associations this worked for many Excel users to fix xlsm file not opening issue. Follow the steps: Go to Control Panel > click Default Programs. Reset File Associations 1. Now click on Set your default programs > click Excel. Reset File Associations 2.\n# \u301019\u2020Differences between xls, xlsx, xlsm, xlsb and csv - WiseCleaner\u2020www.wisecleaner.com\u3011\nIn addition to some functional differences, the biggest difference between these types of tables is the file size. For the same table content, the \".xlsb\" format has the smallest file size, followed by the \".xlsx\" and \".xlsm\" formats, then the \".csv\", and finally the xls format. Generally speaking, for the content of the same table, the table ...\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"search","args":["xLTSM"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"34e25596-3dce-4e90-997a-d7d077060661","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"34e25596-3dce-4e90-997a-d7d077060661","children":["318a53d6-da56-4493-bbf6-9e2f53dc96fd"]},"318a53d6-da56-4493-bbf6-9e2f53dc96fd":{"id":"318a53d6-da56-4493-bbf6-9e2f53dc96fd","message":{"id":"318a53d6-da56-4493-bbf6-9e2f53dc96fd","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125600.998348,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"xLSTM\")"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"ad42b002-ec93-4ad0-802a-db82a25bdad2","finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"browser","channel":null},"parent":"ad42b002-ec93-4ad0-802a-db82a25bdad2","children":["a8b052b8-4759-4648-b57d-44042efb318f"]},"a8b052b8-4759-4648-b57d-44042efb318f":{"id":"a8b052b8-4759-4648-b57d-44042efb318f","message":{"id":"a8b052b8-4759-4648-b57d-44042efb318f","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125600.998387,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020GitHub - NX-AI/xlstm: Official repository of the xLSTM.\u2020github.com\u3011\nOfficial repository of the xLSTM. Contribute to NX-AI/xlstm development by creating an account on GitHub.\n# \u30101\u2020xLSTM: Extended Long Short-Term Memory - arXiv.org\u2020arxiv.org\u3011\nxLSTM: Extended Long Short-Term Memory Maximilian Beck \u2217 1,2Korbinian P\u00f6ppel Markus Spanring Andreas Auer 1,2 Oleksandra Prudnikova Michael Kopp G\u00fcnter Klambauer 1 ,2Johannes Brandstetter 3Sepp Hochreiter \u2217Equal contribution 1ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria 2NXAI Lab, Linz, Austria, 3NXAI GmbH, Linz, Austria ...\n# \u30102\u2020[2405.04517] xLSTM: Extended Long Short-Term Memory - arXiv.org\u2020arxiv.org\u3011\nExponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:2405.04517 [cs.LG]\n# \u30103\u2020xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory\u2020www.unite.ai\u3011\nKey Features and Advantages of xLSTM. Ability to Revise Storage Decisions: Thanks to exponential gating, xLSTM can effectively revise stored values when encountering more relevant information, overcoming a significant limitation of traditional LSTMs.; Enhanced Storage Capacities: The matrix memory in mLSTM provides increased storage capacity, enabling xLSTM to handle rare tokens, long-range ...\n# \u30104\u2020Extended Long Short-Term Memory (xLSTM) - GitHub\u2020github.com\u3011\nThis repository contains the implementation of the Extended Long Short-Term Memory (xLSTM) architecture, as described in the paper xLSTM: Extended Long Short-Term Memory. xLSTM is an extension of the original LSTM architecture that aims to overcome some of its limitations while leveraging the latest techniques from modern large language models.\n# \u30105\u2020xLSTM: Extended Long Short-Term Memory | Papers With Code\u2020paperswithcode.com\u3011\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n# \u30106\u2020AI-Guru/xlstm-resources: Resources about xLSTM by Sepp Hochreiter - GitHub\u2020github.com\u3011\nResources about xLSTM by Sepp Hochreiter. Contribute to AI-Guru/xlstm-resources development by creating an account on GitHub.\n# \u30107\u2020[2405.04517] xLSTM: Extended Long Short-Term Memory\u2020export.arxiv.org\u3011\nExponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:2405.04517 [cs.LG]\n# \u30108\u2020xlstm \u00b7 PyPI\u2020pypi.org\u3011\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State ...\n# \u30109\u2020Paper page - xLSTM: Extended Long Short-Term Memory - Hugging Face\u2020huggingface.co\u3011\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n# \u301010\u2020[2406.04303] Vision-LSTM: xLSTM as Generic Vision Backbone - arXiv.org\u2020arxiv.org\u3011\nVision-LSTM: xLSTM as Generic Vision Backbone. Benedikt Alkin, Maximilian Beck, Korbinian P\u00f6ppel, Sepp Hochreiter, Johannes Brandstetter. Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing. Recently, the Long Short-Term Memory (LSTM) has been extended to a scalable ...\n# \u301011\u2020GitHub - myscience/x-lstm: Pytorch implementation of the xLSTM model by ...\u2020github.com\u3011\nThe xlstm module exposes both the sLSTM (scalar-LSTM) and the mLSTM (matrix-LSTM) modules. Both expect their input to have shape (batch_size, d_input) as they consume an input sequence sequentially. They output the model current (projected) hidden state h_t (which is considered the module output and has the same shape as the input, see Figure 9 ...\n# \u301012\u2020XLSTM To Take Down OpenAI's LLM - Analytics Vidhya\u2020www.analyticsvidhya.com\u3011\nIntroducing XLSTM: The Hidden Successor. Breaking news from the machine learning world reveals Professor Josef Hochreiter\u2019s new creation, XLSTM. Still concealed from public view, XLSTM is poised to carry LSTM\u2019s legacy forward and revolutionize the landscape of autoregressive language modeling. The Quest to Oust OpenAI from the Throne\n# \u301013\u2020xlstm \u00b7 GitHub Topics \u00b7 GitHub\u2020github.com\u3011\nPyxLSTM is a Python library that provides an efficient and extensible implementation of the Extended Long Short-Term Memory (xLSTM) architecture. xLSTM enhances the traditional LSTM by introducing exponential gating, memory mixing, and a matrix memory structure, enabling improved performance and scalability for sequence modeling tasks.\n# \u301014\u2020[Research] xLSTM: Extended Long Short-Term Memory\u2020www.reddit.com\u3011\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n# \u301015\u2020XLSTM: Extended Long Short-Term Memory | Hacker News\u2020news.ycombinator.com\u3011\nE.g. in the figure about scaling laws, replace num of params by compute time. Specifically, the sLSTM has still recurrence in it, i.e. you cannot fully parallelize the computation. So scaling up Transformer could still look better when you look at compute time. It seems neither the code nor the model params are released.\n# \u301016\u2020GitHub - kyegomez/xLSTM: Implementation of xLSTM in Pytorch from the ...\u2020github.com\u3011\nImplementation of xLSTM in Pytorch from the paper: \"xLSTM: Extended Long Short-Term Memory\" - kyegomez/xLSTM\n# \u301017\u2020NISCHALPI/xLSTM-and-mLSTM - GitHub\u2020github.com\u3011\nThe module contains two classes, sLSTMCell and mLSTMCell, which are defined in the xLSTM.py file. The sLSTMCell class mirrors the structure of a standard LSTM cell, akin to PyTorch's implementation. On the other hand, the mLSTMCell class is designed to operate as an mLSTM cell. Both classes offer a familiar interface resembling PyTorch's LSTM ...\n# \u301018\u2020x-lstm/README.md at main \u00b7 myscience/x-lstm \u00b7 GitHub\u2020github.com\u3011\nThe xlstm module exposes both the sLSTM (scalar-LSTM) and the mLSTM (matrix-LSTM) modules. Both expect their input to have shape (batch_size, d_input) as they consume an input sequence sequentially. They output the model current (projected) hidden state h_t (which is considered the module output and has the same shape as the input, see Figure 9 ...\nVisible: 0% - 100%","summary":null,"assets":[],"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"cloud_doc_urls":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"GitHub - NX-AI/xlstm: Official repository of the xLSTM.","url":"https://github.com/NX-AI/xlstm","text":"\nOfficial repository of the xLSTM. Contribute to NX-AI/xlstm development by creating an account on GitHub.\n","pub_date":"2024-06-20T21:42:00.0000000Z","extra":null},{"type":"webpage","title":"xLSTM: Extended Long Short-Term Memory - arXiv.org","url":"https://arxiv.org/pdf/2405.04517","text":"\nxLSTM: Extended Long Short-Term Memory Maximilian Beck \u2217 1,2Korbinian P\u00f6ppel Markus Spanring Andreas Auer 1,2 Oleksandra Prudnikova Michael Kopp G\u00fcnter Klambauer 1 ,2Johannes Brandstetter 3Sepp Hochreiter \u2217Equal contribution 1ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria 2NXAI Lab, Linz, Austria, 3NXAI GmbH, Linz, Austria ...\n","pub_date":"2024-06-21T16:55:00.0000000Z","extra":null},{"type":"webpage","title":"[2405.04517] xLSTM: Extended Long Short-Term Memory - arXiv.org","url":"https://arxiv.org/abs/2405.04517","text":"\nExponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:2405.04517 [cs.LG]\n","pub_date":"2024-06-21T12:24:00.0000000Z","extra":null},{"type":"webpage","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","text":"\nKey Features and Advantages of xLSTM. Ability to Revise Storage Decisions: Thanks to exponential gating, xLSTM can effectively revise stored values when encountering more relevant information, overcoming a significant limitation of traditional LSTMs.; Enhanced Storage Capacities: The matrix memory in mLSTM provides increased storage capacity, enabling xLSTM to handle rare tokens, long-range ...\n","pub_date":"2024-06-20T15:50:00.0000000Z","extra":null},{"type":"webpage","title":"Extended Long Short-Term Memory (xLSTM) - GitHub","url":"https://github.com/smvorwerk/xlstm-cuda","text":"\nThis repository contains the implementation of the Extended Long Short-Term Memory (xLSTM) architecture, as described in the paper xLSTM: Extended Long Short-Term Memory. xLSTM is an extension of the original LSTM architecture that aims to overcome some of its limitations while leveraging the latest techniques from modern large language models.\n","pub_date":"2024-06-21T13:47:00.0000000Z","extra":null},{"type":"webpage","title":"xLSTM: Extended Long Short-Term Memory | Papers With Code","url":"https://paperswithcode.com/paper/xlstm-extended-long-short-term-memory","text":"\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n","pub_date":"2024-06-20T10:33:00.0000000Z","extra":null},{"type":"webpage","title":"AI-Guru/xlstm-resources: Resources about xLSTM by Sepp Hochreiter - GitHub","url":"https://github.com/AI-Guru/xlstm-resources","text":"\nResources about xLSTM by Sepp Hochreiter. Contribute to AI-Guru/xlstm-resources development by creating an account on GitHub.\n","pub_date":"2024-06-21T14:13:00.0000000Z","extra":null},{"type":"webpage","title":"[2405.04517] xLSTM: Extended Long Short-Term Memory","url":"http://export.arxiv.org/abs/2405.04517","text":"\nExponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:2405.04517 [cs.LG]\n","pub_date":"2024-06-15T16:29:00.0000000Z","extra":null},{"type":"webpage","title":"xlstm \u00b7 PyPI","url":"https://pypi.org/project/xlstm/","text":"\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State ...\n","pub_date":"2024-06-19T10:20:00.0000000Z","extra":null},{"type":"webpage","title":"Paper page - xLSTM: Extended Long Short-Term Memory - Hugging Face","url":"https://huggingface.co/papers/2405.04517","text":"\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n","pub_date":"2024-06-19T14:39:00.0000000Z","extra":null},{"type":"webpage","title":"[2406.04303] Vision-LSTM: xLSTM as Generic Vision Backbone - arXiv.org","url":"https://arxiv.org/abs/2406.04303","text":"\nVision-LSTM: xLSTM as Generic Vision Backbone. Benedikt Alkin, Maximilian Beck, Korbinian P\u00f6ppel, Sepp Hochreiter, Johannes Brandstetter. Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing. Recently, the Long Short-Term Memory (LSTM) has been extended to a scalable ...\n","pub_date":"2024-06-21T14:00:00.0000000Z","extra":null},{"type":"webpage","title":"GitHub - myscience/x-lstm: Pytorch implementation of the xLSTM model by ...","url":"https://github.com/myscience/x-lstm","text":"\nThe xlstm module exposes both the sLSTM (scalar-LSTM) and the mLSTM (matrix-LSTM) modules. Both expect their input to have shape (batch_size, d_input) as they consume an input sequence sequentially. They output the model current (projected) hidden state h_t (which is considered the module output and has the same shape as the input, see Figure 9 ...\n","pub_date":"2024-06-20T12:06:00.0000000Z","extra":null},{"type":"webpage","title":"XLSTM To Take Down OpenAI's LLM - Analytics Vidhya","url":"https://www.analyticsvidhya.com/blog/2023/07/xlstm-to-take-down-openais-llm/","text":"\nIntroducing XLSTM: The Hidden Successor. Breaking news from the machine learning world reveals Professor Josef Hochreiter\u2019s new creation, XLSTM. Still concealed from public view, XLSTM is poised to carry LSTM\u2019s legacy forward and revolutionize the landscape of autoregressive language modeling. The Quest to Oust OpenAI from the Throne\n","pub_date":"2024-06-19T09:24:00.0000000Z","extra":null},{"type":"webpage","title":"xlstm \u00b7 GitHub Topics \u00b7 GitHub","url":"https://github.com/topics/xlstm","text":"\nPyxLSTM is a Python library that provides an efficient and extensible implementation of the Extended Long Short-Term Memory (xLSTM) architecture. xLSTM enhances the traditional LSTM by introducing exponential gating, memory mixing, and a matrix memory structure, enabling improved performance and scalability for sequence modeling tasks.\n","pub_date":"2024-06-17T07:39:00.0000000Z","extra":null},{"type":"webpage","title":"[Research] xLSTM: Extended Long Short-Term Memory","url":"https://www.reddit.com/r/MachineLearning/comments/1cmwljs/research_xlstm_extended_long_shortterm_memory/","text":"\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n","pub_date":"2024-06-04T21:38:00.0000000Z","extra":null},{"type":"webpage","title":"XLSTM: Extended Long Short-Term Memory | Hacker News","url":"https://news.ycombinator.com/item?id=40294650","text":"\nE.g. in the figure about scaling laws, replace num of params by compute time. Specifically, the sLSTM has still recurrence in it, i.e. you cannot fully parallelize the computation. So scaling up Transformer could still look better when you look at compute time. It seems neither the code nor the model params are released.\n","pub_date":"2024-06-20T09:51:00.0000000Z","extra":null},{"type":"webpage","title":"GitHub - kyegomez/xLSTM: Implementation of xLSTM in Pytorch from the ...","url":"https://github.com/kyegomez/xLSTM","text":"\nImplementation of xLSTM in Pytorch from the paper: \"xLSTM: Extended Long Short-Term Memory\" - kyegomez/xLSTM\n","pub_date":"2024-06-21T16:53:00.0000000Z","extra":null},{"type":"webpage","title":"NISCHALPI/xLSTM-and-mLSTM - GitHub","url":"https://github.com/NISCHALPI/xLSTM-and-mLSTM","text":"\nThe module contains two classes, sLSTMCell and mLSTMCell, which are defined in the xLSTM.py file. The sLSTMCell class mirrors the structure of a standard LSTM cell, akin to PyTorch's implementation. On the other hand, the mLSTMCell class is designed to operate as an mLSTM cell. Both classes offer a familiar interface resembling PyTorch's LSTM ...\n","pub_date":"2024-06-20T02:26:00.0000000Z","extra":null},{"type":"webpage","title":"x-lstm/README.md at main \u00b7 myscience/x-lstm \u00b7 GitHub","url":"https://github.com/myscience/x-lstm/blob/main/README.md","text":"\nThe xlstm module exposes both the sLSTM (scalar-LSTM) and the mLSTM (matrix-LSTM) modules. Both expect their input to have shape (batch_size, d_input) as they consume an input sequence sequentially. They output the model current (projected) hidden state h_t (which is considered the module output and has the same shape as the input, see Figure 9 ...\nVisible: 0% - 100%","pub_date":"2024-06-19T05:44:00.0000000Z","extra":null},{"type":"webpage","title":"Search results for query: 'xLSTM'","url":"","text":"# \u30100\u2020GitHub - NX-AI/xlstm: Official repository of the xLSTM.\u2020github.com\u3011\nOfficial repository of the xLSTM. Contribute to NX-AI/xlstm development by creating an account on GitHub.\n# \u30101\u2020xLSTM: Extended Long Short-Term Memory - arXiv.org\u2020arxiv.org\u3011\nxLSTM: Extended Long Short-Term Memory Maximilian Beck \u2217 1,2Korbinian P\u00f6ppel Markus Spanring Andreas Auer 1,2 Oleksandra Prudnikova Michael Kopp G\u00fcnter Klambauer 1 ,2Johannes Brandstetter 3Sepp Hochreiter \u2217Equal contribution 1ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria 2NXAI Lab, Linz, Austria, 3NXAI GmbH, Linz, Austria ...\n# \u30102\u2020[2405.04517] xLSTM: Extended Long Short-Term Memory - arXiv.org\u2020arxiv.org\u3011\nExponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:2405.04517 [cs.LG]\n# \u30103\u2020xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory\u2020www.unite.ai\u3011\nKey Features and Advantages of xLSTM. Ability to Revise Storage Decisions: Thanks to exponential gating, xLSTM can effectively revise stored values when encountering more relevant information, overcoming a significant limitation of traditional LSTMs.; Enhanced Storage Capacities: The matrix memory in mLSTM provides increased storage capacity, enabling xLSTM to handle rare tokens, long-range ...\n# \u30104\u2020Extended Long Short-Term Memory (xLSTM) - GitHub\u2020github.com\u3011\nThis repository contains the implementation of the Extended Long Short-Term Memory (xLSTM) architecture, as described in the paper xLSTM: Extended Long Short-Term Memory. xLSTM is an extension of the original LSTM architecture that aims to overcome some of its limitations while leveraging the latest techniques from modern large language models.\n# \u30105\u2020xLSTM: Extended Long Short-Term Memory | Papers With Code\u2020paperswithcode.com\u3011\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n# \u30106\u2020AI-Guru/xlstm-resources: Resources about xLSTM by Sepp Hochreiter - GitHub\u2020github.com\u3011\nResources about xLSTM by Sepp Hochreiter. Contribute to AI-Guru/xlstm-resources development by creating an account on GitHub.\n# \u30107\u2020[2405.04517] xLSTM: Extended Long Short-Term Memory\u2020export.arxiv.org\u3011\nExponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:2405.04517 [cs.LG]\n# \u30108\u2020xlstm \u00b7 PyPI\u2020pypi.org\u3011\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State ...\n# \u30109\u2020Paper page - xLSTM: Extended Long Short-Term Memory - Hugging Face\u2020huggingface.co\u3011\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n# \u301010\u2020[2406.04303] Vision-LSTM: xLSTM as Generic Vision Backbone - arXiv.org\u2020arxiv.org\u3011\nVision-LSTM: xLSTM as Generic Vision Backbone. Benedikt Alkin, Maximilian Beck, Korbinian P\u00f6ppel, Sepp Hochreiter, Johannes Brandstetter. Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing. Recently, the Long Short-Term Memory (LSTM) has been extended to a scalable ...\n# \u301011\u2020GitHub - myscience/x-lstm: Pytorch implementation of the xLSTM model by ...\u2020github.com\u3011\nThe xlstm module exposes both the sLSTM (scalar-LSTM) and the mLSTM (matrix-LSTM) modules. Both expect their input to have shape (batch_size, d_input) as they consume an input sequence sequentially. They output the model current (projected) hidden state h_t (which is considered the module output and has the same shape as the input, see Figure 9 ...\n# \u301012\u2020XLSTM To Take Down OpenAI's LLM - Analytics Vidhya\u2020www.analyticsvidhya.com\u3011\nIntroducing XLSTM: The Hidden Successor. Breaking news from the machine learning world reveals Professor Josef Hochreiter\u2019s new creation, XLSTM. Still concealed from public view, XLSTM is poised to carry LSTM\u2019s legacy forward and revolutionize the landscape of autoregressive language modeling. The Quest to Oust OpenAI from the Throne\n# \u301013\u2020xlstm \u00b7 GitHub Topics \u00b7 GitHub\u2020github.com\u3011\nPyxLSTM is a Python library that provides an efficient and extensible implementation of the Extended Long Short-Term Memory (xLSTM) architecture. xLSTM enhances the traditional LSTM by introducing exponential gating, memory mixing, and a matrix memory structure, enabling improved performance and scalability for sequence modeling tasks.\n# \u301014\u2020[Research] xLSTM: Extended Long Short-Term Memory\u2020www.reddit.com\u3011\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n# \u301015\u2020XLSTM: Extended Long Short-Term Memory | Hacker News\u2020news.ycombinator.com\u3011\nE.g. in the figure about scaling laws, replace num of params by compute time. Specifically, the sLSTM has still recurrence in it, i.e. you cannot fully parallelize the computation. So scaling up Transformer could still look better when you look at compute time. It seems neither the code nor the model params are released.\n# \u301016\u2020GitHub - kyegomez/xLSTM: Implementation of xLSTM in Pytorch from the ...\u2020github.com\u3011\nImplementation of xLSTM in Pytorch from the paper: \"xLSTM: Extended Long Short-Term Memory\" - kyegomez/xLSTM\n# \u301017\u2020NISCHALPI/xLSTM-and-mLSTM - GitHub\u2020github.com\u3011\nThe module contains two classes, sLSTMCell and mLSTMCell, which are defined in the xLSTM.py file. The sLSTMCell class mirrors the structure of a standard LSTM cell, akin to PyTorch's implementation. On the other hand, the mLSTMCell class is designed to operate as an mLSTM cell. Both classes offer a familiar interface resembling PyTorch's LSTM ...\n# \u301018\u2020x-lstm/README.md at main \u00b7 myscience/x-lstm \u00b7 GitHub\u2020github.com\u3011\nThe xlstm module exposes both the sLSTM (scalar-LSTM) and the mLSTM (matrix-LSTM) modules. Both expect their input to have shape (batch_size, d_input) as they consume an input sequence sequentially. They output the model current (projected) hidden state h_t (which is considered the module output and has the same shape as the input, see Figure 9 ...\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"search","args":["xLSTM"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"318a53d6-da56-4493-bbf6-9e2f53dc96fd","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"318a53d6-da56-4493-bbf6-9e2f53dc96fd","children":["b02d7504-d5a4-4893-ab9c-23097bd3bfa8"]},"b02d7504-d5a4-4893-ab9c-23097bd3bfa8":{"id":"b02d7504-d5a4-4893-ab9c-23097bd3bfa8","message":{"id":"b02d7504-d5a4-4893-ab9c-23097bd3bfa8","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125600.998427,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"mclick([0, 1, 3, 5, 7])"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"a8b052b8-4759-4648-b57d-44042efb318f","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"browser","channel":null},"parent":"a8b052b8-4759-4648-b57d-44042efb318f","children":["ca6f3525-e8df-477c-ac93-984e77725719"]},"ca6f3525-e8df-477c-ac93-984e77725719":{"id":"ca6f3525-e8df-477c-ac93-984e77725719","message":{"id":"ca6f3525-e8df-477c-ac93-984e77725719","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125600.998466,"update_time":null,"content":{"content_type":"tether_quote","url":"https://github.com/NX-AI/xlstm","domain":"github.com","text":"\n\u301052\u2020experiments\u3011\n\n\u301053\u2020res\u3011\n\n\u301053\u2020res\u3011\n\n\u301054\u2020test\u3011\n\n\u301054\u2020test\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301062\u2020setup.cfg\u3011\n\n\u301062\u2020setup.cfg\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * AGPL-3.0 license \n\n# xLSTM: Extended Long Short-Term Memory\n\n[Image 0: xLSTM Figure]\n\n> Paper: \u301063\u2020https://arxiv.org/abs/2405.04517\u2020arxiv.org\u3011\n\n## About\n\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State Space Models.\n\n## Minimal Installation\n\nCreate a conda environment from the file `environment_pt220cu121.yaml`. Install the model code only (i.e. the module `xlstm`) as package:\n\nInstally via pip:\n\n    pip install xlstm\n\nClone from github:\n\n    git clone https://github.com/NX-AI/xlstm.git\n    cd xlstm\n    pip install -e .\n\n## Requirements\n\nThis package is based on PyTorch and was tested for versions `>=1.8`. For the CUDA version of sLSTM, you need Compute Capability >= 8.0, see \u301064\u2020https://developer.nvidia.com/cuda-gpus\u2020developer.nvidia.com\u3011. For a well-tested environment, install the `environment_pt220cu121.yaml` as:\n\n    conda env create -n xlstm -f environment_pt220cu121.yaml\n    conda activate xlstm\n\n## Usage\n\nFor non language applications or for integrating in other architectures you can use the `xLSTMBlockStack` and for language modeling or other token-based applications you can use the `xLSTMLMModel`.\n\n### xLSTM Block Stack\n\nThe `xLSTMBLockStack` is meant for use as alternative backbone in existing projects. It is similar to a stack of Transformer blocks, but uses xLSTM blocks:\n\n    import torch\n\n    from xlstm import (\n        xLSTMBlockStack,\n        xLSTMBlockStackConfig,\n        mLSTMBlockConfig,\n        mLSTMLayerConfig,\n        sLSTMBlockConfig,\n        sLSTMLayerConfig,\n        FeedForwardConfig,\n    )\n\n    cfg = xLSTMBlockStackConfig(\n        mlstm_block=mLSTMBlockConfig(\n            mlstm=mLSTMLayerConfig(\n                conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n            )\n        ),\n        slstm_block=sLSTMBlockConfig(\n            slstm=sLSTMLayerConfig(\n                backend=\"cuda\",\n                num_heads=4,\n                conv1d_kernel_size=4,\n                bias_init=\"powerlaw_blockdependent\",\n            ),\n            feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n        ),\n        context_length=256,\n        num_blocks=7,\n        embedding_dim=128,\n        slstm_at=[1],\n\n    )\n\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\nIf you are working with yaml strings / files for configuration you can also use dacite to create the config dataclasses. This is the same as the snippet above:\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n\n    xlstm_cfg = \"\"\" \n    mlstm_block:\n      mlstm:\n        conv1d_kernel_size: 4\n        qkv_proj_blocksize: 4\n        num_heads: 4\n    slstm_block:\n      slstm:\n        backend: cuda\n        num_heads: 4\n        conv1d_kernel_size: 4\n        bias_init: powerlaw_blockdependent\n      feedforward:\n        proj_factor: 1.3\n        act_fn: gelu\n    context_length: 256\n    num_blocks: 7\n    embedding_dim: 128\n    slstm_at: [1]\n    \"\"\"\n    cfg = OmegaConf.create(xlstm_cfg)\n    cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\n### xLSTM Language Model\n\nThe `xLSTMLMModel` is a wrapper around the `xLSTMBlockStack` that adds the token embedding and lm head.\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n\n    xlst","title":"GitHub - NX-AI/xlstm: Official repository of the xLSTM.","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"GitHub - NX-AI/xlstm: Official repository of the xLSTM.","url":"https://github.com/NX-AI/xlstm","text":"\n\u301052\u2020experiments\u3011\n\n\u301053\u2020res\u3011\n\n\u301053\u2020res\u3011\n\n\u301054\u2020test\u3011\n\n\u301054\u2020test\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301062\u2020setup.cfg\u3011\n\n\u301062\u2020setup.cfg\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * AGPL-3.0 license \n\n# xLSTM: Extended Long Short-Term Memory\n\n[Image 0: xLSTM Figure]\n\n> Paper: \u301063\u2020https://arxiv.org/abs/2405.04517\u2020arxiv.org\u3011\n\n## About\n\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State Space Models.\n\n## Minimal Installation\n\nCreate a conda environment from the file `environment_pt220cu121.yaml`. Install the model code only (i.e. the module `xlstm`) as package:\n\nInstally via pip:\n\n    pip install xlstm\n\nClone from github:\n\n    git clone https://github.com/NX-AI/xlstm.git\n    cd xlstm\n    pip install -e .\n\n## Requirements\n\nThis package is based on PyTorch and was tested for versions `>=1.8`. For the CUDA version of sLSTM, you need Compute Capability >= 8.0, see \u301064\u2020https://developer.nvidia.com/cuda-gpus\u2020developer.nvidia.com\u3011. For a well-tested environment, install the `environment_pt220cu121.yaml` as:\n\n    conda env create -n xlstm -f environment_pt220cu121.yaml\n    conda activate xlstm\n\n## Usage\n\nFor non language applications or for integrating in other architectures you can use the `xLSTMBlockStack` and for language modeling or other token-based applications you can use the `xLSTMLMModel`.\n\n### xLSTM Block Stack\n\nThe `xLSTMBLockStack` is meant for use as alternative backbone in existing projects. It is similar to a stack of Transformer blocks, but uses xLSTM blocks:\n\n    import torch\n\n    from xlstm import (\n        xLSTMBlockStack,\n        xLSTMBlockStackConfig,\n        mLSTMBlockConfig,\n        mLSTMLayerConfig,\n        sLSTMBlockConfig,\n        sLSTMLayerConfig,\n        FeedForwardConfig,\n    )\n\n    cfg = xLSTMBlockStackConfig(\n        mlstm_block=mLSTMBlockConfig(\n            mlstm=mLSTMLayerConfig(\n                conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n            )\n        ),\n        slstm_block=sLSTMBlockConfig(\n            slstm=sLSTMLayerConfig(\n                backend=\"cuda\",\n                num_heads=4,\n                conv1d_kernel_size=4,\n                bias_init=\"powerlaw_blockdependent\",\n            ),\n            feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n        ),\n        context_length=256,\n        num_blocks=7,\n        embedding_dim=128,\n        slstm_at=[1],\n\n    )\n\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\nIf you are working with yaml strings / files for configuration you can also use dacite to create the config dataclasses. This is the same as the snippet above:\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n\n    xlstm_cfg = \"\"\" \n    mlstm_block:\n      mlstm:\n        conv1d_kernel_size: 4\n        qkv_proj_blocksize: 4\n        num_heads: 4\n    slstm_block:\n      slstm:\n        backend: cuda\n        num_heads: 4\n        conv1d_kernel_size: 4\n        bias_init: powerlaw_blockdependent\n      feedforward:\n        proj_factor: 1.3\n        act_fn: gelu\n    context_length: 256\n    num_blocks: 7\n    embedding_dim: 128\n    slstm_at: [1]\n    \"\"\"\n    cfg = OmegaConf.create(xlstm_cfg)\n    cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\n### xLSTM Language Model\n\nThe `xLSTMLMModel` is a wrapper around the `xLSTMBlockStack` that adds the token embedding and lm head.\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n\n    xlst","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 1, 3, 5, 7]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"b02d7504-d5a4-4893-ab9c-23097bd3bfa8","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"b02d7504-d5a4-4893-ab9c-23097bd3bfa8","children":["d3f37366-63c2-4681-8b5f-b8c59a26159c"]},"d3f37366-63c2-4681-8b5f-b8c59a26159c":{"id":"d3f37366-63c2-4681-8b5f-b8c59a26159c","message":{"id":"d3f37366-63c2-4681-8b5f-b8c59a26159c","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125600.998505,"update_time":null,"content":{"content_type":"tether_quote","url":"https://ar5iv.org/pdf/2405.04517","domain":"ar5iv.org","text":"\nURL: https://ar5iv.org/pdf/2405.04517\nConversion to HTML had a Fatal error and exited abruptly. This document may be truncated or damaged. \n\n\u30100\u2020\u25c4\u3011 [Image 0: ar5iv homepage] \u30101\u2020Feeling lucky?\u3011 \u30102\u2020Conversion report\u3011 \u30103\u2020Report an issue\u2020github.com\u3011 \u30104\u2020View original on arXiv\u2020arxiv.org\u3011\u30105\u2020\u25ba\u3011 \n\n\u30106\u2020Copyright\u2020arxiv.org\u3011 \u30107\u2020Privacy Policy\u2020arxiv.org\u3011 \n\nGenerated on Wed Jun 5 13:43:55 2024 by \u30108\u2020L a T e XML\u2020dlmf.nist.gov\u3011\nVisible: 0% - 100%","title":"[2405.04517] Untitled Document","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"[2405.04517] Untitled Document","url":"https://ar5iv.org/pdf/2405.04517","text":"\nURL: https://ar5iv.org/pdf/2405.04517\nConversion to HTML had a Fatal error and exited abruptly. This document may be truncated or damaged. \n\n\u30100\u2020\u25c4\u3011 [Image 0: ar5iv homepage] \u30101\u2020Feeling lucky?\u3011 \u30102\u2020Conversion report\u3011 \u30103\u2020Report an issue\u2020github.com\u3011 \u30104\u2020View original on arXiv\u2020arxiv.org\u3011\u30105\u2020\u25ba\u3011 \n\n\u30106\u2020Copyright\u2020arxiv.org\u3011 \u30107\u2020Privacy Policy\u2020arxiv.org\u3011 \n\nGenerated on Wed Jun 5 13:43:55 2024 by \u30108\u2020L a T e XML\u2020dlmf.nist.gov\u3011\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 1, 3, 5, 7]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"ca6f3525-e8df-477c-ac93-984e77725719","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"ca6f3525-e8df-477c-ac93-984e77725719","children":["f5e333bb-35c0-46dd-8884-0f1bb26409ec"]},"f5e333bb-35c0-46dd-8884-0f1bb26409ec":{"id":"f5e333bb-35c0-46dd-8884-0f1bb26409ec","message":{"id":"f5e333bb-35c0-46dd-8884-0f1bb26409ec","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125600.998544,"update_time":null,"content":{"content_type":"tether_quote","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","domain":"www.unite.ai","text":"\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise Storage Decisions: Thanks to exponential gating, xLSTM can effectively revise stored values when encountering more relevant information, overcoming a significant limitation of traditional LSTMs.\n  2. Enhanced Storage Capacities: The matrix memory in mLSTM provides increased storage capacity, enabling xLSTM to handle rare tokens, long-range dependencies, and complex data patterns mor","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","text":"\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise Storage Decisions: Thanks to exponential gating, xLSTM can effectively revise stored values when encountering more relevant information, overcoming a significant limitation of traditional LSTMs.\n  2. Enhanced Storage Capacities: The matrix memory in mLSTM provides increased storage capacity, enabling xLSTM to handle rare tokens, long-range dependencies, and complex data patterns mor","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 1, 3, 5, 7]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"d3f37366-63c2-4681-8b5f-b8c59a26159c","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"d3f37366-63c2-4681-8b5f-b8c59a26159c","children":["cdfb4eee-b1eb-4e7d-8498-620c3745f027"]},"cdfb4eee-b1eb-4e7d-8498-620c3745f027":{"id":"cdfb4eee-b1eb-4e7d-8498-620c3745f027","message":{"id":"cdfb4eee-b1eb-4e7d-8498-620c3745f027","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125600.998582,"update_time":null,"content":{"content_type":"tether_quote","url":"https://paperswithcode.com/paper/xlstm-extended-long-short-term-memory","domain":"paperswithcode.com","text":"\n\u00d7\n\nsmvorwerk/xlstm-cuda \n\n46 \n\n* * *\n\nThere is no official implementation \n\n* * *\n\nMultiple official implementations \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTask:*\n\n--------- Language Modelling\n\nNot in the list? Add a task.\n\nDataset:*\n\n---------\n\nModel name:*\n\nMetric name:*\n\n---------\n\nHigher is better (for the metric) \n\nMetric value:*\n\nUses extra training data \n\nData evaluated on \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE  \n\n##### Add a task\n\n\u00d7\n\nAttached tasks:\n\n  * LANGUAGE MODELLING\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new task. \n\nNew task name: \n\nTop-level area: \n\n--------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series\n\nParent task (if any): \n\n---------\n\nDescription: \n\nSubmit \n\n##### Remove a task\n\n\u00d7\n\n  * Language Modelling - \n\n##### Add a method\n\n\u00d7\n\nAttached methods:\n\n  * ABSOLUTE POSITION ENCODINGS\n\n  * ADAM\n\n  * BATCH NORMALIZATION\n\n  * BPE\n\n  * CONVOLUTION\n\n  * DENSE CONNECTIONS\n\n  * DROPOUT\n\n  * LABEL SMOOTHING\n\n  * LAYER NORMALIZATION\n\n  * LINEAR LAYER\n\n  * LSTM\n\n  * MLSTM\n\n  * MULTI-HEAD ATTENTION\n\n  * POSITION-WISE FEED-FORWARD LAYER\n\n  * RELU\n\n  * RESIDUAL BLOCK\n\n  * RESIDUAL CONNECTION\n\n  * SCALED DOT-PRODUCT ATTENTION\n\n  * SIGMOID ACTIVATION\n\n  * SOFTMAX\n\n  * TANH ACTIVATION\n\n  * TRANSFORMER\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new method. \n\nNew method name (e.g. ReLU): \n\nNew method full name (e.g. Rectified Linear Unit): \n\nPaper where method was first introduced: \n\n---------\n\nMethod category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.\n\n---------\n\nMarkdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.\n\nSubmit \n\n##### Remove a method\n\n\u00d7\n\n  * ABSOLUTE POSITION ENCODINGS - \n\n  * ADAM - \n\n  * BATCH NORMALIZATION - \n\n  * BPE - \n\n  * CONVOLUTION - \n\n  * DENSE CONNECTIONS - \n\n  * DROPOUT - \n\n  * LABEL SMOOTHING - \n\n  * LAYER NORMALIZATION - \n\n  * LINEAR LAYER - \n\n  * LSTM - \n\n  * MLSTM - \n\n  * MULTI-HEAD ATTENTION - \n\n  * POSITION-WISE FEED-FORWARD LAYER - \n\n  * RELU - \n\n  * RESIDUAL BLOCK - \n\n  * RESIDUAL CONNECTION - \n\n  * SCALED DOT-PRODUCT ATTENTION - \n\n  * SIGMOID ACTIVATION - \n\n  * SOFTMAX - \n\n  * TANH ACTIVATION - \n\n  * TRANSFORMER - \n\n##### Edit Datasets\n\n\u00d7\n\nAdd or remove datasets introduced in this paper: \n\nAdd or remove other datasets used in this paper: \n\nC4 LAMBADA mC4\n\nPaper introduces a new dataset? \n\nAdd a new dataset here \n\nSave \n\n#  xLSTM: Extended Long Short-Term Memory \n\n7 May 2024 \u00a0\u00b7\u00a0 \u301013\u2020Maximilian Beck\u3011,  \u301014\u2020Korbinian P\u00f6ppel\u3011,  \u301015\u2020Markus Spanring\u3011,  \u301016\u2020Andreas Auer\u3011,  \u301017\u2020Oleksandra Prudnikova\u3011,  \u301018\u2020Michael Kopp\u3011,  \u301019\u2020G\u00fcnter Klambauer\u3011,  \u301020\u2020Johannes Brandstetter\u3011,  \u301021\u2020Sepp Hochreiter\u3011 \u00b7  Edit social preview\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. \n\n\u301022\u2020 PDF \u2020arxiv.org\u3011 \u301023\u2020 Abstract \u2020arxiv.org\u3011 \n\n## Code \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \u301024\u2020 Mark official\u3011 \n\n* * *\n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 0] \n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 1] \n\n## Tasks \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \n\n* *","title":"xLSTM: Extended Long Short-Term Memory | Papers With Code","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"xLSTM: Extended Long Short-Term Memory | Papers With Code","url":"https://paperswithcode.com/paper/xlstm-extended-long-short-term-memory","text":"\n\u00d7\n\nsmvorwerk/xlstm-cuda \n\n46 \n\n* * *\n\nThere is no official implementation \n\n* * *\n\nMultiple official implementations \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTask:*\n\n--------- Language Modelling\n\nNot in the list? Add a task.\n\nDataset:*\n\n---------\n\nModel name:*\n\nMetric name:*\n\n---------\n\nHigher is better (for the metric) \n\nMetric value:*\n\nUses extra training data \n\nData evaluated on \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE  \n\n##### Add a task\n\n\u00d7\n\nAttached tasks:\n\n  * LANGUAGE MODELLING\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new task. \n\nNew task name: \n\nTop-level area: \n\n--------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series\n\nParent task (if any): \n\n---------\n\nDescription: \n\nSubmit \n\n##### Remove a task\n\n\u00d7\n\n  * Language Modelling - \n\n##### Add a method\n\n\u00d7\n\nAttached methods:\n\n  * ABSOLUTE POSITION ENCODINGS\n\n  * ADAM\n\n  * BATCH NORMALIZATION\n\n  * BPE\n\n  * CONVOLUTION\n\n  * DENSE CONNECTIONS\n\n  * DROPOUT\n\n  * LABEL SMOOTHING\n\n  * LAYER NORMALIZATION\n\n  * LINEAR LAYER\n\n  * LSTM\n\n  * MLSTM\n\n  * MULTI-HEAD ATTENTION\n\n  * POSITION-WISE FEED-FORWARD LAYER\n\n  * RELU\n\n  * RESIDUAL BLOCK\n\n  * RESIDUAL CONNECTION\n\n  * SCALED DOT-PRODUCT ATTENTION\n\n  * SIGMOID ACTIVATION\n\n  * SOFTMAX\n\n  * TANH ACTIVATION\n\n  * TRANSFORMER\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new method. \n\nNew method name (e.g. ReLU): \n\nNew method full name (e.g. Rectified Linear Unit): \n\nPaper where method was first introduced: \n\n---------\n\nMethod category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.\n\n---------\n\nMarkdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.\n\nSubmit \n\n##### Remove a method\n\n\u00d7\n\n  * ABSOLUTE POSITION ENCODINGS - \n\n  * ADAM - \n\n  * BATCH NORMALIZATION - \n\n  * BPE - \n\n  * CONVOLUTION - \n\n  * DENSE CONNECTIONS - \n\n  * DROPOUT - \n\n  * LABEL SMOOTHING - \n\n  * LAYER NORMALIZATION - \n\n  * LINEAR LAYER - \n\n  * LSTM - \n\n  * MLSTM - \n\n  * MULTI-HEAD ATTENTION - \n\n  * POSITION-WISE FEED-FORWARD LAYER - \n\n  * RELU - \n\n  * RESIDUAL BLOCK - \n\n  * RESIDUAL CONNECTION - \n\n  * SCALED DOT-PRODUCT ATTENTION - \n\n  * SIGMOID ACTIVATION - \n\n  * SOFTMAX - \n\n  * TANH ACTIVATION - \n\n  * TRANSFORMER - \n\n##### Edit Datasets\n\n\u00d7\n\nAdd or remove datasets introduced in this paper: \n\nAdd or remove other datasets used in this paper: \n\nC4 LAMBADA mC4\n\nPaper introduces a new dataset? \n\nAdd a new dataset here \n\nSave \n\n#  xLSTM: Extended Long Short-Term Memory \n\n7 May 2024 \u00a0\u00b7\u00a0 \u301013\u2020Maximilian Beck\u3011,  \u301014\u2020Korbinian P\u00f6ppel\u3011,  \u301015\u2020Markus Spanring\u3011,  \u301016\u2020Andreas Auer\u3011,  \u301017\u2020Oleksandra Prudnikova\u3011,  \u301018\u2020Michael Kopp\u3011,  \u301019\u2020G\u00fcnter Klambauer\u3011,  \u301020\u2020Johannes Brandstetter\u3011,  \u301021\u2020Sepp Hochreiter\u3011 \u00b7  Edit social preview\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. \n\n\u301022\u2020 PDF \u2020arxiv.org\u3011 \u301023\u2020 Abstract \u2020arxiv.org\u3011 \n\n## Code \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \u301024\u2020 Mark official\u3011 \n\n* * *\n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 0] \n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 1] \n\n## Tasks \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \n\n* *","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 1, 3, 5, 7]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"f5e333bb-35c0-46dd-8884-0f1bb26409ec","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"f5e333bb-35c0-46dd-8884-0f1bb26409ec","children":["e3013e62-1f08-43e4-9a98-1b4c781d7196"]},"e3013e62-1f08-43e4-9a98-1b4c781d7196":{"id":"e3013e62-1f08-43e4-9a98-1b4c781d7196","message":{"id":"e3013e62-1f08-43e4-9a98-1b4c781d7196","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125600.998619,"update_time":null,"content":{"content_type":"text","parts":["xLSTM, or Extended Long Short-Term Memory, is an advanced variant of the traditional LSTM architecture designed to address some of the limitations of LSTMs, particularly in the context of scaling and performance.\n\n### Key Features of xLSTM\n\n1. **Exponential Gating**: This mechanism allows for more flexible control over the information flow within the LSTM by using exponential activation functions for the input and forget gates. This helps in revising stored values when more relevant information is encountered.\n\n2. **Enhanced Storage Capacities**: Unlike traditional LSTMs, which use scalar cell states, xLSTM introduces matrix memory structures (mLSTM) that significantly increase storage capacity. This is particularly useful for handling complex data patterns, rare tokens, and long-range dependencies.\n\n3. **Parallelizability**: The new memory structure in xLSTM, especially in the mLSTM variant, allows for full parallelizability, overcoming the sequential processing limitation of traditional LSTMs. This makes xLSTM more scalable and efficient on modern hardware.\n\n4. **Memory Mixing and Normalization**: The sLSTM (scalar LSTM) variant incorporates memory mixing and normalization techniques to prevent numerical instabilities and enhance the extraction of complex patterns.\n\n### Applications and Performance\n\nxLSTM integrates these innovations into residual block architectures, forming xLSTM blocks that can be residually stacked to create powerful models tailored for specific tasks. These models have shown promising performance in language modeling and other sequence modeling tasks, often performing favorably compared to state-of-the-art Transformers and State Space Models.\n\nOverall, xLSTM represents a significant evolution of the LSTM architecture, leveraging modern techniques to enhance performance and scalability\u30109\u2020source\u3011\u301010\u2020source\u3011\u301011\u2020source\u3011\u301012\u2020source\u3011."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[{"start_ix":1839,"end_ix":1849,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"GitHub - NX-AI/xlstm: Official repository of the xLSTM.","url":"https://github.com/NX-AI/xlstm","text":"\n\u301052\u2020experiments\u3011\n\n\u301053\u2020res\u3011\n\n\u301053\u2020res\u3011\n\n\u301054\u2020test\u3011\n\n\u301054\u2020test\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301062\u2020setup.cfg\u3011\n\n\u301062\u2020setup.cfg\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * AGPL-3.0 license \n\n# xLSTM: Extended Long Short-Term Memory\n\n[Image 0: xLSTM Figure]\n\n> Paper: \u301063\u2020https://arxiv.org/abs/2405.04517\u2020arxiv.org\u3011\n\n## About\n\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State Space Models.\n\n## Minimal Installation\n\nCreate a conda environment from the file `environment_pt220cu121.yaml`. Install the model code only (i.e. the module `xlstm`) as package:\n\nInstally via pip:\n\n    pip install xlstm\n\nClone from github:\n\n    git clone https://github.com/NX-AI/xlstm.git\n    cd xlstm\n    pip install -e .\n\n## Requirements\n\nThis package is based on PyTorch and was tested for versions `>=1.8`. For the CUDA version of sLSTM, you need Compute Capability >= 8.0, see \u301064\u2020https://developer.nvidia.com/cuda-gpus\u2020developer.nvidia.com\u3011. For a well-tested environment, install the `environment_pt220cu121.yaml` as:\n\n    conda env create -n xlstm -f environment_pt220cu121.yaml\n    conda activate xlstm\n\n## Usage\n\nFor non language applications or for integrating in other architectures you can use the `xLSTMBlockStack` and for language modeling or other token-based applications you can use the `xLSTMLMModel`.\n\n### xLSTM Block Stack\n\nThe `xLSTMBLockStack` is meant for use as alternative backbone in existing projects. It is similar to a stack of Transformer blocks, but uses xLSTM blocks:\n\n    import torch\n\n    from xlstm import (\n        xLSTMBlockStack,\n        xLSTMBlockStackConfig,\n        mLSTMBlockConfig,\n        mLSTMLayerConfig,\n        sLSTMBlockConfig,\n        sLSTMLayerConfig,\n        FeedForwardConfig,\n    )\n\n    cfg = xLSTMBlockStackConfig(\n        mlstm_block=mLSTMBlockConfig(\n            mlstm=mLSTMLayerConfig(\n                conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n            )\n        ),\n        slstm_block=sLSTMBlockConfig(\n            slstm=sLSTMLayerConfig(\n                backend=\"cuda\",\n                num_heads=4,\n                conv1d_kernel_size=4,\n                bias_init=\"powerlaw_blockdependent\",\n            ),\n            feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n        ),\n        context_length=256,\n        num_blocks=7,\n        embedding_dim=128,\n        slstm_at=[1],\n\n    )\n\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\nIf you are working with yaml strings / files for configuration you can also use dacite to create the config dataclasses. This is the same as the snippet above:\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n\n    xlstm_cfg = \"\"\" \n    mlstm_block:\n      mlstm:\n        conv1d_kernel_size: 4\n        qkv_proj_blocksize: 4\n        num_heads: 4\n    slstm_block:\n      slstm:\n        backend: cuda\n        num_heads: 4\n        conv1d_kernel_size: 4\n        bias_init: powerlaw_blockdependent\n      feedforward:\n        proj_factor: 1.3\n        act_fn: gelu\n    context_length: 256\n    num_blocks: 7\n    embedding_dim: 128\n    slstm_at: [1]\n    \"\"\"\n    cfg = OmegaConf.create(xlstm_cfg)\n    cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\n### xLSTM Language Model\n\nThe `xLSTMLMModel` is a wrapper around the `xLSTMBlockStack` that adds the token embedding and lm head.\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n\n    xlst","pub_date":null,"extra":{"cited_message_idx":9,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":1849,"end_ix":1860,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2405.04517] Untitled Document","url":"https://ar5iv.org/pdf/2405.04517#:~:text=URL%3A%20https%3A%2F%2Far5iv,100","text":"\nURL: https://ar5iv.org/pdf/2405.04517\nConversion to HTML had a Fatal error and exited abruptly. This document may be truncated or damaged. \n\n\u30100\u2020\u25c4\u3011 [Image 0: ar5iv homepage] \u30101\u2020Feeling lucky?\u3011 \u30102\u2020Conversion report\u3011 \u30103\u2020Report an issue\u2020github.com\u3011 \u30104\u2020View original on arXiv\u2020arxiv.org\u3011\u30105\u2020\u25ba\u3011 \n\n\u30106\u2020Copyright\u2020arxiv.org\u3011 \u30107\u2020Privacy Policy\u2020arxiv.org\u3011 \n\nGenerated on Wed Jun 5 13:43:55 2024 by \u30108\u2020L a T e XML\u2020dlmf.nist.gov\u3011\nVisible: 0% - 100%","pub_date":null,"extra":{"cited_message_idx":10,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":1860,"end_ix":1871,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","text":"\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise Storage Decisions: Thanks to exponential gating, xLSTM can effectively revise stored values when encountering more relevant information, overcoming a significant limitation of traditional LSTMs.\n  2. Enhanced Storage Capacities: The matrix memory in mLSTM provides increased storage capacity, enabling xLSTM to handle rare tokens, long-range dependencies, and complex data patterns mor","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":1871,"end_ix":1882,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"xLSTM: Extended Long Short-Term Memory | Papers With Code","url":"https://paperswithcode.com/paper/xlstm-extended-long-short-term-memory","text":"\n\u00d7\n\nsmvorwerk/xlstm-cuda \n\n46 \n\n* * *\n\nThere is no official implementation \n\n* * *\n\nMultiple official implementations \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTask:*\n\n--------- Language Modelling\n\nNot in the list? Add a task.\n\nDataset:*\n\n---------\n\nModel name:*\n\nMetric name:*\n\n---------\n\nHigher is better (for the metric) \n\nMetric value:*\n\nUses extra training data \n\nData evaluated on \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE  \n\n##### Add a task\n\n\u00d7\n\nAttached tasks:\n\n  * LANGUAGE MODELLING\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new task. \n\nNew task name: \n\nTop-level area: \n\n--------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series\n\nParent task (if any): \n\n---------\n\nDescription: \n\nSubmit \n\n##### Remove a task\n\n\u00d7\n\n  * Language Modelling - \n\n##### Add a method\n\n\u00d7\n\nAttached methods:\n\n  * ABSOLUTE POSITION ENCODINGS\n\n  * ADAM\n\n  * BATCH NORMALIZATION\n\n  * BPE\n\n  * CONVOLUTION\n\n  * DENSE CONNECTIONS\n\n  * DROPOUT\n\n  * LABEL SMOOTHING\n\n  * LAYER NORMALIZATION\n\n  * LINEAR LAYER\n\n  * LSTM\n\n  * MLSTM\n\n  * MULTI-HEAD ATTENTION\n\n  * POSITION-WISE FEED-FORWARD LAYER\n\n  * RELU\n\n  * RESIDUAL BLOCK\n\n  * RESIDUAL CONNECTION\n\n  * SCALED DOT-PRODUCT ATTENTION\n\n  * SIGMOID ACTIVATION\n\n  * SOFTMAX\n\n  * TANH ACTIVATION\n\n  * TRANSFORMER\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new method. \n\nNew method name (e.g. ReLU): \n\nNew method full name (e.g. Rectified Linear Unit): \n\nPaper where method was first introduced: \n\n---------\n\nMethod category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.\n\n---------\n\nMarkdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.\n\nSubmit \n\n##### Remove a method\n\n\u00d7\n\n  * ABSOLUTE POSITION ENCODINGS - \n\n  * ADAM - \n\n  * BATCH NORMALIZATION - \n\n  * BPE - \n\n  * CONVOLUTION - \n\n  * DENSE CONNECTIONS - \n\n  * DROPOUT - \n\n  * LABEL SMOOTHING - \n\n  * LAYER NORMALIZATION - \n\n  * LINEAR LAYER - \n\n  * LSTM - \n\n  * MLSTM - \n\n  * MULTI-HEAD ATTENTION - \n\n  * POSITION-WISE FEED-FORWARD LAYER - \n\n  * RELU - \n\n  * RESIDUAL BLOCK - \n\n  * RESIDUAL CONNECTION - \n\n  * SCALED DOT-PRODUCT ATTENTION - \n\n  * SIGMOID ACTIVATION - \n\n  * SOFTMAX - \n\n  * TANH ACTIVATION - \n\n  * TRANSFORMER - \n\n##### Edit Datasets\n\n\u00d7\n\nAdd or remove datasets introduced in this paper: \n\nAdd or remove other datasets used in this paper: \n\nC4 LAMBADA mC4\n\nPaper introduces a new dataset? \n\nAdd a new dataset here \n\nSave \n\n#  xLSTM: Extended Long Short-Term Memory \n\n7 May 2024 \u00a0\u00b7\u00a0 \u301013\u2020Maximilian Beck\u3011,  \u301014\u2020Korbinian P\u00f6ppel\u3011,  \u301015\u2020Markus Spanring\u3011,  \u301016\u2020Andreas Auer\u3011,  \u301017\u2020Oleksandra Prudnikova\u3011,  \u301018\u2020Michael Kopp\u3011,  \u301019\u2020G\u00fcnter Klambauer\u3011,  \u301020\u2020Johannes Brandstetter\u3011,  \u301021\u2020Sepp Hochreiter\u3011 \u00b7  Edit social preview\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. \n\n\u301022\u2020 PDF \u2020arxiv.org\u3011 \u301023\u2020 Abstract \u2020arxiv.org\u3011 \n\n## Code \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \u301024\u2020 Mark official\u3011 \n\n* * *\n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 0] \n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 1] \n\n## Tasks \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \n\n* *","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}}],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","pad":"AAAAAAAAAAAAAAAAA","parent_id":"cdfb4eee-b1eb-4e7d-8498-620c3745f027","request_id":"89828f2fcd333d95-SIN","timestamp_":"absolute","content_references":[{"matched_text":"\u30109\u2020source\u3011","start_idx":1839,"end_idx":1849,"alt":null,"type":"webpage_extended","title":"GitHub - NX-AI/xlstm: Official repository of the xLSTM.","url":"https://github.com/NX-AI/xlstm","pub_date":null,"snippet":"\n\u301052\u2020experiments\u3011\n\n\u301053\u2020res\u3011\n\n\u301053\u2020res\u3011\n\n\u301054\u2020test\u3011\n\n\u301054\u2020test\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301062\u2020setup.cfg\u3011\n\n\u301062\u2020setup.cfg\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * AGPL-3.0 license \n\n# xLSTM: Extended Long Short-Term Memory\n\n[Image 0: xLSTM Figure]\n\n> Paper: \u301063\u2020https://arxiv.org/abs/2405.04517\u2020arxiv.org\u3011\n\n## About\n\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State Space Models.\n\n## Minimal Installation\n\nCreate a conda environment from the file `environment_pt220cu121.yaml`. Install the model code only (i.e. the module `xlstm`) as package:\n\nInstally via pip:\n\n    pip install xlstm\n\nClone from github:\n\n    git clone https://github.com/NX-AI/xlstm.git\n    cd xlstm\n    pip install -e .\n\n## Requirements\n\nThis package is based on PyTorch and was tested for versions `>=1.8`. For the CUDA version of sLSTM, you need Compute Capability >= 8.0, see \u301064\u2020https://developer.nvidia.com/cuda-gpus\u2020developer.nvidia.com\u3011. For a well-tested environment, install the `environment_pt220cu121.yaml` as:\n\n    conda env create -n xlstm -f environment_pt220cu121.yaml\n    conda activate xlstm\n\n## Usage\n\nFor non language applications or for integrating in other architectures you can use the `xLSTMBlockStack` and for language modeling or other token-based applications you can use the `xLSTMLMModel`.\n\n### xLSTM Block Stack\n\nThe `xLSTMBLockStack` is meant for use as alternative backbone in existing projects. It is similar to a stack of Transformer blocks, but uses xLSTM blocks:\n\n    import torch\n\n    from xlstm import (\n        xLSTMBlockStack,\n        xLSTMBlockStackConfig,\n        mLSTMBlockConfig,\n        mLSTMLayerConfig,\n        sLSTMBlockConfig,\n        sLSTMLayerConfig,\n        FeedForwardConfig,\n    )\n\n    cfg = xLSTMBlockStackConfig(\n        mlstm_block=mLSTMBlockConfig(\n            mlstm=mLSTMLayerConfig(\n                conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n            )\n        ),\n        slstm_block=sLSTMBlockConfig(\n            slstm=sLSTMLayerConfig(\n                backend=\"cuda\",\n                num_heads=4,\n                conv1d_kernel_size=4,\n                bias_init=\"powerlaw_blockdependent\",\n            ),\n            feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n        ),\n        context_length=256,\n        num_blocks=7,\n        embedding_dim=128,\n        slstm_at=[1],\n\n    )\n\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\nIf you are working with yaml strings / files for configuration you can also use dacite to create the config dataclasses. This is the same as the snippet above:\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n\n    xlstm_cfg = \"\"\" \n    mlstm_block:\n      mlstm:\n        conv1d_kernel_size: 4\n        qkv_proj_blocksize: 4\n        num_heads: 4\n    slstm_block:\n      slstm:\n        backend: cuda\n        num_heads: 4\n        conv1d_kernel_size: 4\n        bias_init: powerlaw_blockdependent\n      feedforward:\n        proj_factor: 1.3\n        act_fn: gelu\n    context_length: 256\n    num_blocks: 7\n    embedding_dim: 128\n    slstm_at: [1]\n    \"\"\"\n    cfg = OmegaConf.create(xlstm_cfg)\n    cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\n### xLSTM Language Model\n\nThe `xLSTMLMModel` is a wrapper around the `xLSTMBlockStack` that adds the token embedding and lm head.\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n\n    xlst","attribution":"github.com","icon_type":null},{"matched_text":"\u301010\u2020source\u3011","start_idx":1849,"end_idx":1860,"alt":null,"type":"webpage_extended","title":"[2405.04517] Untitled Document","url":"https://ar5iv.org/pdf/2405.04517#:~:text=URL%3A%20https%3A%2F%2Far5iv,100","pub_date":null,"snippet":"\nURL: https://ar5iv.org/pdf/2405.04517\nConversion to HTML had a Fatal error and exited abruptly. This document may be truncated or damaged. \n\n\u30100\u2020\u25c4\u3011 [Image 0: ar5iv homepage] \u30101\u2020Feeling lucky?\u3011 \u30102\u2020Conversion report\u3011 \u30103\u2020Report an issue\u2020github.com\u3011 \u30104\u2020View original on arXiv\u2020arxiv.org\u3011\u30105\u2020\u25ba\u3011 \n\n\u30106\u2020Copyright\u2020arxiv.org\u3011 \u30107\u2020Privacy Policy\u2020arxiv.org\u3011 \n\nGenerated on Wed Jun 5 13:43:55 2024 by \u30108\u2020L a T e XML\u2020dlmf.nist.gov\u3011\nVisible: 0% - 100%","attribution":"ar5iv.org","icon_type":null},{"matched_text":"\u301011\u2020source\u3011","start_idx":1860,"end_idx":1871,"alt":null,"type":"webpage_extended","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","pub_date":null,"snippet":"\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise Storage Decisions: Thanks to exponential gating, xLSTM can effectively revise stored values when encountering more relevant information, overcoming a significant limitation of traditional LSTMs.\n  2. Enhanced Storage Capacities: The matrix memory in mLSTM provides increased storage capacity, enabling xLSTM to handle rare tokens, long-range dependencies, and complex data patterns mor","attribution":"unite.ai","icon_type":null},{"matched_text":"\u301012\u2020source\u3011","start_idx":1871,"end_idx":1882,"alt":null,"type":"webpage_extended","title":"xLSTM: Extended Long Short-Term Memory | Papers With Code","url":"https://paperswithcode.com/paper/xlstm-extended-long-short-term-memory","pub_date":null,"snippet":"\n\u00d7\n\nsmvorwerk/xlstm-cuda \n\n46 \n\n* * *\n\nThere is no official implementation \n\n* * *\n\nMultiple official implementations \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTask:*\n\n--------- Language Modelling\n\nNot in the list? Add a task.\n\nDataset:*\n\n---------\n\nModel name:*\n\nMetric name:*\n\n---------\n\nHigher is better (for the metric) \n\nMetric value:*\n\nUses extra training data \n\nData evaluated on \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE  \n\n##### Add a task\n\n\u00d7\n\nAttached tasks:\n\n  * LANGUAGE MODELLING\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new task. \n\nNew task name: \n\nTop-level area: \n\n--------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series\n\nParent task (if any): \n\n---------\n\nDescription: \n\nSubmit \n\n##### Remove a task\n\n\u00d7\n\n  * Language Modelling - \n\n##### Add a method\n\n\u00d7\n\nAttached methods:\n\n  * ABSOLUTE POSITION ENCODINGS\n\n  * ADAM\n\n  * BATCH NORMALIZATION\n\n  * BPE\n\n  * CONVOLUTION\n\n  * DENSE CONNECTIONS\n\n  * DROPOUT\n\n  * LABEL SMOOTHING\n\n  * LAYER NORMALIZATION\n\n  * LINEAR LAYER\n\n  * LSTM\n\n  * MLSTM\n\n  * MULTI-HEAD ATTENTION\n\n  * POSITION-WISE FEED-FORWARD LAYER\n\n  * RELU\n\n  * RESIDUAL BLOCK\n\n  * RESIDUAL CONNECTION\n\n  * SCALED DOT-PRODUCT ATTENTION\n\n  * SIGMOID ACTIVATION\n\n  * SOFTMAX\n\n  * TANH ACTIVATION\n\n  * TRANSFORMER\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new method. \n\nNew method name (e.g. ReLU): \n\nNew method full name (e.g. Rectified Linear Unit): \n\nPaper where method was first introduced: \n\n---------\n\nMethod category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.\n\n---------\n\nMarkdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.\n\nSubmit \n\n##### Remove a method\n\n\u00d7\n\n  * ABSOLUTE POSITION ENCODINGS - \n\n  * ADAM - \n\n  * BATCH NORMALIZATION - \n\n  * BPE - \n\n  * CONVOLUTION - \n\n  * DENSE CONNECTIONS - \n\n  * DROPOUT - \n\n  * LABEL SMOOTHING - \n\n  * LAYER NORMALIZATION - \n\n  * LINEAR LAYER - \n\n  * LSTM - \n\n  * MLSTM - \n\n  * MULTI-HEAD ATTENTION - \n\n  * POSITION-WISE FEED-FORWARD LAYER - \n\n  * RELU - \n\n  * RESIDUAL BLOCK - \n\n  * RESIDUAL CONNECTION - \n\n  * SCALED DOT-PRODUCT ATTENTION - \n\n  * SIGMOID ACTIVATION - \n\n  * SOFTMAX - \n\n  * TANH ACTIVATION - \n\n  * TRANSFORMER - \n\n##### Edit Datasets\n\n\u00d7\n\nAdd or remove datasets introduced in this paper: \n\nAdd or remove other datasets used in this paper: \n\nC4 LAMBADA mC4\n\nPaper introduces a new dataset? \n\nAdd a new dataset here \n\nSave \n\n#  xLSTM: Extended Long Short-Term Memory \n\n7 May 2024 \u00a0\u00b7\u00a0 \u301013\u2020Maximilian Beck\u3011,  \u301014\u2020Korbinian P\u00f6ppel\u3011,  \u301015\u2020Markus Spanring\u3011,  \u301016\u2020Andreas Auer\u3011,  \u301017\u2020Oleksandra Prudnikova\u3011,  \u301018\u2020Michael Kopp\u3011,  \u301019\u2020G\u00fcnter Klambauer\u3011,  \u301020\u2020Johannes Brandstetter\u3011,  \u301021\u2020Sepp Hochreiter\u3011 \u00b7  Edit social preview\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. \n\n\u301022\u2020 PDF \u2020arxiv.org\u3011 \u301023\u2020 Abstract \u2020arxiv.org\u3011 \n\n## Code \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \u301024\u2020 Mark official\u3011 \n\n* * *\n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 0] \n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 1] \n\n## Tasks \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \n\n* *","attribution":"paperswithcode.com","icon_type":null}],"can_save":true},"recipient":"all","channel":null},"parent":"cdfb4eee-b1eb-4e7d-8498-620c3745f027","children":["aaa27491-b7b5-43f3-bd1a-b9fcb4f69635"]},"aaa27491-b7b5-43f3-bd1a-b9fcb4f69635":{"id":"aaa27491-b7b5-43f3-bd1a-b9fcb4f69635","message":{"id":"aaa27491-b7b5-43f3-bd1a-b9fcb4f69635","author":{"role":"user","name":null,"metadata":{}},"create_time":1719125924.886492,"update_time":null,"content":{"content_type":"text","parts":["what are some of the successful use cases of xLSTMs?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"898297645f7144a1-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"e3013e62-1f08-43e4-9a98-1b4c781d7196","children":["5dae29cf-4b4f-4869-8346-6db2e4562b1c"]},"5dae29cf-4b4f-4869-8346-6db2e4562b1c":{"id":"5dae29cf-4b4f-4869-8346-6db2e4562b1c","message":{"id":"5dae29cf-4b4f-4869-8346-6db2e4562b1c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125938.372728,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"successful use cases of xLSTMs\")"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"aaa27491-b7b5-43f3-bd1a-b9fcb4f69635","finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"browser","channel":null},"parent":"aaa27491-b7b5-43f3-bd1a-b9fcb4f69635","children":["ab6ba3ca-c2ba-44fc-bbab-27629a14acae"]},"ab6ba3ca-c2ba-44fc-bbab-27629a14acae":{"id":"ab6ba3ca-c2ba-44fc-bbab-27629a14acae","message":{"id":"ab6ba3ca-c2ba-44fc-bbab-27629a14acae","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125938.372793,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020File Types in Excel: When to use XLSX, XLSM, XLSB, XLS etc.?\u2020professor-excel.com\u3011\nFile types. Excel provides 29 different file types when you save your Excel workbook. When you save an Excel workbook for the first time or use the \u201cSave As\u201d function, you are asked for a folder, file name as well as the file type. By default, Excel suggests the XLSX file type (unless your file has VBA macros).\n# \u30101\u2020Excel Automation: Easily Automate Spreadsheet Tasks\u2020blog.golayer.io\u3011\nAdd Developer tab, including VBA and Macros features. 1. Click on the drop-down arrow and select \u201cMore Commands\u2026\u201d as shown below. Excel Automation: Easily Automate Spreadsheet Tasks - More Commands\u2026. 2. Select \u201cCustomize Ribbon\u201d from the left-side menu and then \u201cMain Tabs\u201d from the \u201cCustomize the Ribbon\u201d drop-down menu to ...\n# \u30102\u2020How To Use VBA Workbook SaveAs In Excel - MarketSplash\u2020marketsplash.com\u3011\nThe SaveAs method allows you to add a layer of protection to your workbooks. Example: ' Save the workbook with password protection. ActiveWorkbook. SaveAs Filename:=\"C:\\SecureWorkbook.xlsx\", Password:=\"secure123\". \ud83d\udccc. Upon opening the saved workbook, Excel will prompt the user for the specified password.\n# \u30103\u2020VBA Save Copy As - Examples (CSV, XLSM, PDF), How to Use? - ExcelMojo\u2020www.excelmojo.com\u3011\nStep 3: Use the ExportAsFixedFormat function and specify the type as PDF using the keyword \u201cxlTypePDF\u201d. Save this file in the location string defined in the previous step. Note: You can use VBA Save Copy As to save the file in PDF format. But this will corrupt the file, and you will be unable to open it. Code:\n# \u30104\u2020How to Use Macroenabled Files in Power Automate Flows - Process Street\u2020www.process.st\u3011\nFirstly, ensure that your macroenabled file is saved in a compatible format, such as .xlsm for Excel. Create a new flow in Power Automate. Choose the appropriate trigger for the flow, such as \u201cWhen a new file is added to a folder.\u201d. Add an action to the flow, such as \u201cConvert file\u201d or \u201cModify file.\u201d.\n# \u30105\u2020What Is an XLSM File? - Lifewire\u2020www.lifewire.com\u3011\nA file with the XLSM file extension is an Excel macro-enabled workbook file created in Excel 2007 or newer. These files are identical to Microsoft Excel Open XML Format Spreadsheet ( XLSX) files, with the only difference being that XLSM files will execute embedded macros that are programmed in the Visual Basic for Applications (VBA) language ...\n# \u30106\u2020XLSM to XLSX Converter - FreeConvert.com\u2020www.freeconvert.com\u3011\nEasy to Use. Simply upload your XLSM files and click the convert button. ... In most cases, you can fine-tune conversion parameters using \u201cAdvanced Settings\u201d (optional, look for the icon). Free & Secure. Our XLSM to XLSX Converter is free and works on any web browser. We guarantee file security and privacy. Files are protected with 256-bit SSL encryption and automatically delete after a ...\n# \u30107\u2020Excel crashes when opening a specific (complex) .xlsb file (and all ...\u2020techcommunity.microsoft.com\u3011\nYou can also use a third-party tool to repair your .xlsb file, such as Stellar Repair for Excel or Kernel for Excel Repair. These tools can scan your file and recover data from damaged worksheets. Have not used these tools myself, but are recommended by some, so without guarantee. I hope this helps you resolve your issue. NikolinoDE. I know I don't know anything (Socrates) 0 Likes . Reply ...\n# \u30108\u2020Solved: R Tool to Read from a password protected XLSM Macr... - Alteryx ...\u2020community.alteryx.com\u3011\nSolved: I was successful in using the R TOol Code below to read ffrom an XLSX file that was password protected WITHOUT the red parameter below.\n# \u30109\u2020Excel XLSB vs XLSX file format. The Pros and Cons of XLSB Files\u2020analystcave.com\u3011\nXLSX and XLSM files are in fact compressed archives with XML files inside. ... In some cases I heard of there being XLSB files that required 10% of the original file size \u2013 this is more visible with VERY large files; Opens/saves more quickly \u2013 loading binary data is faster than parsing text (XML) files \u2013 similarly as you would compare opening a book in Spanish and having to translate ...\n# \u301010\u2020Excel Macros disappear from xlsm file - Microsoft Community Hub\u2020techcommunity.microsoft.com\u3011\nBUT if I change the xlsm extension to zip of the file and I go to the xl directory then I can see the vbaProject.bin file which seems to be fine, so in the very background the macros are there. This happens total randomly with files. Probably the only link between the cases is that the files, from which the macros disappear, where used after the crash of the Excel (prior story) and after Excel ...\n# \u301011\u2020XLSB vs. XLSX File Formats: Pros and Cons - Spreadsheet Planet\u2020spreadsheetplanet.com\u3011\nThe primary difference is that XLSB is a binary file format, while XLSX is an XML-based file format. XLSB files use a binary file format, which means they store information as raw 1\u2019s and 0\u2019s. This makes them faster to read and write than other file formats like XLSX or XLSM. However, both file formats can exhibit an increase in file size ...\n# \u301012\u2020How to work with .XLSM spreadsheets - is it any wa... - Power Platform ...\u2020powerusers.microsoft.com\u3011\n08-03-2021 01:59 PM. I would recommend using an Office Script. They should be able to work with .XLSM files and you should be able to return the even non table data to Power Automate for use later in the Flow. It seems you tried using Office Scripts to do this a bit.\n# \u301013\u2020Solved: Update macro-enabled Excel file? - Power Platform Community\u2020powerusers.microsoft.com\u3011\nHello, That is exactly what I did, but updating the .xlsm file in such a way corrupts it. I have worked a temporary workaround where I update an Excel file on an O365 Team site, I synchronise the file with OneDrive for Business, and I get the data imported via macro in my macro-enabled file, but that's a real hassle and not what I would expect from automation of tasks \ud83d\ude42\n# \u301014\u2020Excel xlsm files not opening - Microsoft Community\u2020answers.microsoft.com\u3011\nSolution 3: Reset File Associations. Try resetting the file associations this worked for many Excel users to fix xlsm file not opening issue. Follow the steps: Go to Control Panel > click Default Programs. Reset File Associations 1. Now click on Set your default programs > click Excel. Reset File Associations 2.\n# \u301015\u2020Excel files with VBA code (XLSM) are suddenly blocked\u2020answers.microsoft.com\u3011\nAnd an \"Enable Content\" button. One recommendation I've seen is to go into the file's Properties, General tab, and select Unblock. Like for some other users, it's not offered. And from the Developer tab, click Macro Security. In the Trust Center, Macro Settings, the option \"Enable VBA macros (not recommended.....) is selected\".\n# \u301016\u2020Femap API - Create Body Load Cases.xlsm - GitHub\u2020github.com\u3011\nUse saved searches to filter your results more quickly. Name. Query. To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session ...\n# \u301017\u2020When should the xlsm or xlsb formats be used? - Stack Overflow\u2020stackoverflow.com\u3011\nSince Excel 2007, Microsoft has split the classical .xls format to several formats (in particular, .xlsx, .xlsm, .xlsb).I've got no problem to understand the use and purpose of .xlsx format but I am still wondering whether we should use a .xlsm or a .xlsb format when creating a file containing some VBA.. Of course, you can find some topics on the web, for instance:\n# \u301018\u2020java - how to read from xlsm encrypted file? - Stack Overflow\u2020stackoverflow.com\u3011\nI am trying to read data from an xlsm Excel file encrypted with password. So far non of the methods I found on Apache POI web site nor stack-overflow were successful here is what I tried so far, an...\n# \u301019\u2020python - how to update existing xlsm sheet with macros using pandas ...\u2020stackoverflow.com\u3011\nNow this values already existing in the xlsm sheet in column called SiteCode in sheet called Gov_Tracker I want to update some columns depending rows on this values, like I want to update in columns called Pending, Pending Status and blablabla. Now I want to modify this columns rows values like change the old value in Pending, Pending Status, blablabla equals to the new values TE, Waiting TE ...\nVisible: 0% - 100%","summary":null,"assets":[],"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"cloud_doc_urls":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"File Types in Excel: When to use XLSX, XLSM, XLSB, XLS etc.?","url":"https://professor-excel.com/xlsx-xlsm-xlsb-file-types/","text":"\nFile types. Excel provides 29 different file types when you save your Excel workbook. When you save an Excel workbook for the first time or use the \u201cSave As\u201d function, you are asked for a folder, file name as well as the file type. By default, Excel suggests the XLSX file type (unless your file has VBA macros).\n","pub_date":"2024-06-22T01:08:00.0000000Z","extra":null},{"type":"webpage","title":"Excel Automation: Easily Automate Spreadsheet Tasks","url":"https://blog.golayer.io/excel/excel-automation","text":"\nAdd Developer tab, including VBA and Macros features. 1. Click on the drop-down arrow and select \u201cMore Commands\u2026\u201d as shown below. Excel Automation: Easily Automate Spreadsheet Tasks - More Commands\u2026. 2. Select \u201cCustomize Ribbon\u201d from the left-side menu and then \u201cMain Tabs\u201d from the \u201cCustomize the Ribbon\u201d drop-down menu to ...\n","pub_date":"2024-06-19T03:44:00.0000000Z","extra":null},{"type":"webpage","title":"How To Use VBA Workbook SaveAs In Excel - MarketSplash","url":"https://marketsplash.com/vba-workbook-saveas/","text":"\nThe SaveAs method allows you to add a layer of protection to your workbooks. Example: ' Save the workbook with password protection. ActiveWorkbook. SaveAs Filename:=\"C:\\SecureWorkbook.xlsx\", Password:=\"secure123\". \ud83d\udccc. Upon opening the saved workbook, Excel will prompt the user for the specified password.\n","pub_date":"2024-06-22T06:39:00.0000000Z","extra":null},{"type":"webpage","title":"VBA Save Copy As - Examples (CSV, XLSM, PDF), How to Use? - ExcelMojo","url":"https://www.excelmojo.com/vba-save-copy-as/","text":"\nStep 3: Use the ExportAsFixedFormat function and specify the type as PDF using the keyword \u201cxlTypePDF\u201d. Save this file in the location string defined in the previous step. Note: You can use VBA Save Copy As to save the file in PDF format. But this will corrupt the file, and you will be unable to open it. Code:\n","pub_date":"2024-06-20T06:42:00.0000000Z","extra":null},{"type":"webpage","title":"How to Use Macroenabled Files in Power Automate Flows - Process Street","url":"https://www.process.st/how-to/use-macroenabled-files-in-power-automate-flows/","text":"\nFirstly, ensure that your macroenabled file is saved in a compatible format, such as .xlsm for Excel. Create a new flow in Power Automate. Choose the appropriate trigger for the flow, such as \u201cWhen a new file is added to a folder.\u201d. Add an action to the flow, such as \u201cConvert file\u201d or \u201cModify file.\u201d.\n","pub_date":"2024-06-21T09:26:00.0000000Z","extra":null},{"type":"webpage","title":"What Is an XLSM File? - Lifewire","url":"https://www.lifewire.com/xlsm-file-2622538","text":"\nA file with the XLSM file extension is an Excel macro-enabled workbook file created in Excel 2007 or newer. These files are identical to Microsoft Excel Open XML Format Spreadsheet ( XLSX) files, with the only difference being that XLSM files will execute embedded macros that are programmed in the Visual Basic for Applications (VBA) language ...\n","pub_date":"2024-06-20T22:24:00.0000000Z","extra":null},{"type":"webpage","title":"XLSM to XLSX Converter - FreeConvert.com","url":"https://www.freeconvert.com/xlsm-to-xlsx","text":"\nEasy to Use. Simply upload your XLSM files and click the convert button. ... In most cases, you can fine-tune conversion parameters using \u201cAdvanced Settings\u201d (optional, look for the icon). Free & Secure. Our XLSM to XLSX Converter is free and works on any web browser. We guarantee file security and privacy. Files are protected with 256-bit SSL encryption and automatically delete after a ...\n","pub_date":"2024-06-20T22:11:00.0000000Z","extra":null},{"type":"webpage","title":"Excel crashes when opening a specific (complex) .xlsb file (and all ...","url":"https://techcommunity.microsoft.com/t5/excel/excel-crashes-when-opening-a-specific-complex-xlsb-file-and-all/td-p/3776416","text":"\nYou can also use a third-party tool to repair your .xlsb file, such as Stellar Repair for Excel or Kernel for Excel Repair. These tools can scan your file and recover data from damaged worksheets. Have not used these tools myself, but are recommended by some, so without guarantee. I hope this helps you resolve your issue. NikolinoDE. I know I don't know anything (Socrates) 0 Likes . Reply ...\n","pub_date":"2024-06-21T00:07:00.0000000Z","extra":null},{"type":"webpage","title":"Solved: R Tool to Read from a password protected XLSM Macr... - Alteryx ...","url":"https://community.alteryx.com/t5/Alteryx-Designer-Desktop-Discussions/R-Tool-to-Read-from-a-password-protected-XLSM-Macro-Enabled-File/td-p/591108","text":"\nSolved: I was successful in using the R TOol Code below to read ffrom an XLSX file that was password protected WITHOUT the red parameter below.\n","pub_date":"2024-06-15T08:18:00.0000000Z","extra":null},{"type":"webpage","title":"Excel XLSB vs XLSX file format. The Pros and Cons of XLSB Files","url":"https://analystcave.com/excel-working-with-large-excel-files-the-xlsb-format/","text":"\nXLSX and XLSM files are in fact compressed archives with XML files inside. ... In some cases I heard of there being XLSB files that required 10% of the original file size \u2013 this is more visible with VERY large files; Opens/saves more quickly \u2013 loading binary data is faster than parsing text (XML) files \u2013 similarly as you would compare opening a book in Spanish and having to translate ...\n","pub_date":"2024-06-21T10:57:00.0000000Z","extra":null},{"type":"webpage","title":"Excel Macros disappear from xlsm file - Microsoft Community Hub","url":"https://techcommunity.microsoft.com/t5/excel/excel-macros-disappear-from-xlsm-file/td-p/4009168","text":"\nBUT if I change the xlsm extension to zip of the file and I go to the xl directory then I can see the vbaProject.bin file which seems to be fine, so in the very background the macros are there. This happens total randomly with files. Probably the only link between the cases is that the files, from which the macros disappear, where used after the crash of the Excel (prior story) and after Excel ...\n","pub_date":"2024-06-15T11:39:00.0000000Z","extra":null},{"type":"webpage","title":"XLSB vs. XLSX File Formats: Pros and Cons - Spreadsheet Planet","url":"https://spreadsheetplanet.com/xlsb-vs-xlsx-file-formats/","text":"\nThe primary difference is that XLSB is a binary file format, while XLSX is an XML-based file format. XLSB files use a binary file format, which means they store information as raw 1\u2019s and 0\u2019s. This makes them faster to read and write than other file formats like XLSX or XLSM. However, both file formats can exhibit an increase in file size ...\n","pub_date":"2024-06-20T20:23:00.0000000Z","extra":null},{"type":"webpage","title":"How to work with .XLSM spreadsheets - is it any wa... - Power Platform ...","url":"https://powerusers.microsoft.com/t5/General-Power-Automate/How-to-work-with-XLSM-spreadsheets-is-it-any-way-currently/td-p/1215543","text":"\n08-03-2021 01:59 PM. I would recommend using an Office Script. They should be able to work with .XLSM files and you should be able to return the even non table data to Power Automate for use later in the Flow. It seems you tried using Office Scripts to do this a bit.\n","pub_date":"2024-06-13T04:56:00.0000000Z","extra":null},{"type":"webpage","title":"Solved: Update macro-enabled Excel file? - Power Platform Community","url":"https://powerusers.microsoft.com/t5/Building-Flows/Update-macro-enabled-Excel-file/td-p/59080","text":"\nHello, That is exactly what I did, but updating the .xlsm file in such a way corrupts it. I have worked a temporary workaround where I update an Excel file on an O365 Team site, I synchronise the file with OneDrive for Business, and I get the data imported via macro in my macro-enabled file, but that's a real hassle and not what I would expect from automation of tasks \ud83d\ude42\n","pub_date":"2024-06-13T04:35:00.0000000Z","extra":null},{"type":"webpage","title":"Excel xlsm files not opening - Microsoft Community","url":"https://answers.microsoft.com/en-us/msoffice/forum/all/excel-xlsm-files-not-opening/b076782e-e535-4666-b955-3c5120c9ab72","text":"\nSolution 3: Reset File Associations. Try resetting the file associations this worked for many Excel users to fix xlsm file not opening issue. Follow the steps: Go to Control Panel > click Default Programs. Reset File Associations 1. Now click on Set your default programs > click Excel. Reset File Associations 2.\n","pub_date":"2024-06-21T17:18:00.0000000Z","extra":null},{"type":"webpage","title":"Excel files with VBA code (XLSM) are suddenly blocked","url":"https://answers.microsoft.com/en-us/msoffice/forum/all/excel-files-with-vba-code-xlsm-are-suddenly/e049aeef-61c8-444c-9161-6604cec9d4e9","text":"\nAnd an \"Enable Content\" button. One recommendation I've seen is to go into the file's Properties, General tab, and select Unblock. Like for some other users, it's not offered. And from the Developer tab, click Macro Security. In the Trust Center, Macro Settings, the option \"Enable VBA macros (not recommended.....) is selected\".\n","pub_date":"2024-06-19T13:42:00.0000000Z","extra":null},{"type":"webpage","title":"Femap API - Create Body Load Cases.xlsm - GitHub","url":"https://github.com/TestForEcho12/Femap-APIs/blob/master/Femap%20API%20-%20Create%20Body%20Load%20Cases.xlsm","text":"\nUse saved searches to filter your results more quickly. Name. Query. To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session ...\n","pub_date":"2024-06-22T11:19:00.0000000Z","extra":null},{"type":"webpage","title":"When should the xlsm or xlsb formats be used? - Stack Overflow","url":"https://stackoverflow.com/questions/7821632/when-should-the-xlsm-or-xlsb-formats-be-used","text":"\nSince Excel 2007, Microsoft has split the classical .xls format to several formats (in particular, .xlsx, .xlsm, .xlsb).I've got no problem to understand the use and purpose of .xlsx format but I am still wondering whether we should use a .xlsm or a .xlsb format when creating a file containing some VBA.. Of course, you can find some topics on the web, for instance:\n","pub_date":"2024-06-21T01:18:00.0000000Z","extra":null},{"type":"webpage","title":"java - how to read from xlsm encrypted file? - Stack Overflow","url":"https://stackoverflow.com/questions/47683873/how-to-read-from-xlsm-encrypted-file","text":"\nI am trying to read data from an xlsm Excel file encrypted with password. So far non of the methods I found on Apache POI web site nor stack-overflow were successful here is what I tried so far, an...\n","pub_date":"2024-06-15T23:01:00.0000000Z","extra":null},{"type":"webpage","title":"python - how to update existing xlsm sheet with macros using pandas ...","url":"https://stackoverflow.com/questions/56931921/how-to-update-existing-xlsm-sheet-with-macros-using-pandas-openpyxl-xlwings-wi","text":"\nNow this values already existing in the xlsm sheet in column called SiteCode in sheet called Gov_Tracker I want to update some columns depending rows on this values, like I want to update in columns called Pending, Pending Status and blablabla. Now I want to modify this columns rows values like change the old value in Pending, Pending Status, blablabla equals to the new values TE, Waiting TE ...\nVisible: 0% - 100%","pub_date":"2024-06-12T10:22:00.0000000Z","extra":null},{"type":"webpage","title":"Search results for query: 'successful use cases of xLSTMs'","url":"","text":"# \u30100\u2020File Types in Excel: When to use XLSX, XLSM, XLSB, XLS etc.?\u2020professor-excel.com\u3011\nFile types. Excel provides 29 different file types when you save your Excel workbook. When you save an Excel workbook for the first time or use the \u201cSave As\u201d function, you are asked for a folder, file name as well as the file type. By default, Excel suggests the XLSX file type (unless your file has VBA macros).\n# \u30101\u2020Excel Automation: Easily Automate Spreadsheet Tasks\u2020blog.golayer.io\u3011\nAdd Developer tab, including VBA and Macros features. 1. Click on the drop-down arrow and select \u201cMore Commands\u2026\u201d as shown below. Excel Automation: Easily Automate Spreadsheet Tasks - More Commands\u2026. 2. Select \u201cCustomize Ribbon\u201d from the left-side menu and then \u201cMain Tabs\u201d from the \u201cCustomize the Ribbon\u201d drop-down menu to ...\n# \u30102\u2020How To Use VBA Workbook SaveAs In Excel - MarketSplash\u2020marketsplash.com\u3011\nThe SaveAs method allows you to add a layer of protection to your workbooks. Example: ' Save the workbook with password protection. ActiveWorkbook. SaveAs Filename:=\"C:\\SecureWorkbook.xlsx\", Password:=\"secure123\". \ud83d\udccc. Upon opening the saved workbook, Excel will prompt the user for the specified password.\n# \u30103\u2020VBA Save Copy As - Examples (CSV, XLSM, PDF), How to Use? - ExcelMojo\u2020www.excelmojo.com\u3011\nStep 3: Use the ExportAsFixedFormat function and specify the type as PDF using the keyword \u201cxlTypePDF\u201d. Save this file in the location string defined in the previous step. Note: You can use VBA Save Copy As to save the file in PDF format. But this will corrupt the file, and you will be unable to open it. Code:\n# \u30104\u2020How to Use Macroenabled Files in Power Automate Flows - Process Street\u2020www.process.st\u3011\nFirstly, ensure that your macroenabled file is saved in a compatible format, such as .xlsm for Excel. Create a new flow in Power Automate. Choose the appropriate trigger for the flow, such as \u201cWhen a new file is added to a folder.\u201d. Add an action to the flow, such as \u201cConvert file\u201d or \u201cModify file.\u201d.\n# \u30105\u2020What Is an XLSM File? - Lifewire\u2020www.lifewire.com\u3011\nA file with the XLSM file extension is an Excel macro-enabled workbook file created in Excel 2007 or newer. These files are identical to Microsoft Excel Open XML Format Spreadsheet ( XLSX) files, with the only difference being that XLSM files will execute embedded macros that are programmed in the Visual Basic for Applications (VBA) language ...\n# \u30106\u2020XLSM to XLSX Converter - FreeConvert.com\u2020www.freeconvert.com\u3011\nEasy to Use. Simply upload your XLSM files and click the convert button. ... In most cases, you can fine-tune conversion parameters using \u201cAdvanced Settings\u201d (optional, look for the icon). Free & Secure. Our XLSM to XLSX Converter is free and works on any web browser. We guarantee file security and privacy. Files are protected with 256-bit SSL encryption and automatically delete after a ...\n# \u30107\u2020Excel crashes when opening a specific (complex) .xlsb file (and all ...\u2020techcommunity.microsoft.com\u3011\nYou can also use a third-party tool to repair your .xlsb file, such as Stellar Repair for Excel or Kernel for Excel Repair. These tools can scan your file and recover data from damaged worksheets. Have not used these tools myself, but are recommended by some, so without guarantee. I hope this helps you resolve your issue. NikolinoDE. I know I don't know anything (Socrates) 0 Likes . Reply ...\n# \u30108\u2020Solved: R Tool to Read from a password protected XLSM Macr... - Alteryx ...\u2020community.alteryx.com\u3011\nSolved: I was successful in using the R TOol Code below to read ffrom an XLSX file that was password protected WITHOUT the red parameter below.\n# \u30109\u2020Excel XLSB vs XLSX file format. The Pros and Cons of XLSB Files\u2020analystcave.com\u3011\nXLSX and XLSM files are in fact compressed archives with XML files inside. ... In some cases I heard of there being XLSB files that required 10% of the original file size \u2013 this is more visible with VERY large files; Opens/saves more quickly \u2013 loading binary data is faster than parsing text (XML) files \u2013 similarly as you would compare opening a book in Spanish and having to translate ...\n# \u301010\u2020Excel Macros disappear from xlsm file - Microsoft Community Hub\u2020techcommunity.microsoft.com\u3011\nBUT if I change the xlsm extension to zip of the file and I go to the xl directory then I can see the vbaProject.bin file which seems to be fine, so in the very background the macros are there. This happens total randomly with files. Probably the only link between the cases is that the files, from which the macros disappear, where used after the crash of the Excel (prior story) and after Excel ...\n# \u301011\u2020XLSB vs. XLSX File Formats: Pros and Cons - Spreadsheet Planet\u2020spreadsheetplanet.com\u3011\nThe primary difference is that XLSB is a binary file format, while XLSX is an XML-based file format. XLSB files use a binary file format, which means they store information as raw 1\u2019s and 0\u2019s. This makes them faster to read and write than other file formats like XLSX or XLSM. However, both file formats can exhibit an increase in file size ...\n# \u301012\u2020How to work with .XLSM spreadsheets - is it any wa... - Power Platform ...\u2020powerusers.microsoft.com\u3011\n08-03-2021 01:59 PM. I would recommend using an Office Script. They should be able to work with .XLSM files and you should be able to return the even non table data to Power Automate for use later in the Flow. It seems you tried using Office Scripts to do this a bit.\n# \u301013\u2020Solved: Update macro-enabled Excel file? - Power Platform Community\u2020powerusers.microsoft.com\u3011\nHello, That is exactly what I did, but updating the .xlsm file in such a way corrupts it. I have worked a temporary workaround where I update an Excel file on an O365 Team site, I synchronise the file with OneDrive for Business, and I get the data imported via macro in my macro-enabled file, but that's a real hassle and not what I would expect from automation of tasks \ud83d\ude42\n# \u301014\u2020Excel xlsm files not opening - Microsoft Community\u2020answers.microsoft.com\u3011\nSolution 3: Reset File Associations. Try resetting the file associations this worked for many Excel users to fix xlsm file not opening issue. Follow the steps: Go to Control Panel > click Default Programs. Reset File Associations 1. Now click on Set your default programs > click Excel. Reset File Associations 2.\n# \u301015\u2020Excel files with VBA code (XLSM) are suddenly blocked\u2020answers.microsoft.com\u3011\nAnd an \"Enable Content\" button. One recommendation I've seen is to go into the file's Properties, General tab, and select Unblock. Like for some other users, it's not offered. And from the Developer tab, click Macro Security. In the Trust Center, Macro Settings, the option \"Enable VBA macros (not recommended.....) is selected\".\n# \u301016\u2020Femap API - Create Body Load Cases.xlsm - GitHub\u2020github.com\u3011\nUse saved searches to filter your results more quickly. Name. Query. To see all available qualifiers, see our documentation. Cancel Create saved search Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session ...\n# \u301017\u2020When should the xlsm or xlsb formats be used? - Stack Overflow\u2020stackoverflow.com\u3011\nSince Excel 2007, Microsoft has split the classical .xls format to several formats (in particular, .xlsx, .xlsm, .xlsb).I've got no problem to understand the use and purpose of .xlsx format but I am still wondering whether we should use a .xlsm or a .xlsb format when creating a file containing some VBA.. Of course, you can find some topics on the web, for instance:\n# \u301018\u2020java - how to read from xlsm encrypted file? - Stack Overflow\u2020stackoverflow.com\u3011\nI am trying to read data from an xlsm Excel file encrypted with password. So far non of the methods I found on Apache POI web site nor stack-overflow were successful here is what I tried so far, an...\n# \u301019\u2020python - how to update existing xlsm sheet with macros using pandas ...\u2020stackoverflow.com\u3011\nNow this values already existing in the xlsm sheet in column called SiteCode in sheet called Gov_Tracker I want to update some columns depending rows on this values, like I want to update in columns called Pending, Pending Status and blablabla. Now I want to modify this columns rows values like change the old value in Pending, Pending Status, blablabla equals to the new values TE, Waiting TE ...\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"search","args":["successful use cases of xLSTMs"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"5dae29cf-4b4f-4869-8346-6db2e4562b1c","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"5dae29cf-4b4f-4869-8346-6db2e4562b1c","children":["5d1253da-0bc3-490b-a71b-1acbe199e59e"]},"5d1253da-0bc3-490b-a71b-1acbe199e59e":{"id":"5d1253da-0bc3-490b-a71b-1acbe199e59e","message":{"id":"5d1253da-0bc3-490b-a71b-1acbe199e59e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125938.372834,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"successful applications of xLSTMs\")"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"ab6ba3ca-c2ba-44fc-bbab-27629a14acae","finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"browser","channel":null},"parent":"ab6ba3ca-c2ba-44fc-bbab-27629a14acae","children":["64de275d-28b1-4286-8e9f-a2da80b7f449"]},"64de275d-28b1-4286-8e9f-a2da80b7f449":{"id":"64de275d-28b1-4286-8e9f-a2da80b7f449","message":{"id":"64de275d-28b1-4286-8e9f-a2da80b7f449","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125938.372871,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory\u2020www.unite.ai\u3011\nExtended LSTM (xLSTM) family. At the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM). sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing.\n# \u30101\u2020Successful Y Combinator Application Examples \u2013 ycombinator.\u2020ycombinator.dev\u3011\nNow, let\u2019s explore some examples of successful Y Combinator applications and what made them stand out. 1. Dropbox (2007) Dropbox\u2019s YC application was notable for its clarity in expressing the widespread issue of file storage and sharing across multiple devices. The team demonstrated a deep understanding of the problem and proposed a simple ...\n# \u30102\u2020The application that got Metlo into Y Combinator : r/ycombinator - Reddit\u2020www.reddit.com\u3011\nThe application that got Metlo into Y Combinator. We applied to Y Combinator in March 2021 and got accepted into the Summer 2021 batch. Thinking back to March 2021, I remember my co-founder, Akshay, and I, nervously scouring the internet for examples of recently successful YC applications. Companies like OpenPhone had generously published their ...\n# \u30103\u2020Anyone have a list of example successful YC applications?\u2020www.reddit.com\u3011\nThis is the unofficial subreddit for all things concerning the International Baccalaureate, an academic credential accorded to secondary students from around the world after two vigorous years of study, culminating in challenging exams.\n# \u30104\u2020Top 15 Most Successful Apps of 2022 | Best Apps by Category - SF AppWorks\u2020www.sfappworks.com\u3011\nTikTok is one of the most popular short-form video entertainment apps. It grew by 5x in 2020 mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with 62 million installs in January 2021. TikTok started as a lip-syncing app that transformed into a social media phenomenon.\n# \u30105\u2020The Data School - Successful Applications\u2020www.thedataschool.co.uk\u3011\nHere are their initial application and final visualisations that got them into DS40. If you are interested in joining you can apply for The Data School here. Asha Daniels Francisco Rei Lydia Wren Nadia Holloway Olivier Newman Otto Richardson Sanjeev Dosanjh Toby Horne. View. Mon 05 Jun 2023.\n# \u30106\u2020An Overview of the Fuzzy Data Envelopment Analysis Research ... - Springer\u2020link.springer.com\u3011\nSome successful applications of the FDEAs in efficiency analysis. For the DEA, an obvious and suitable application is in the field of efficiency analysis. Almost two decades ago, Girod and Triantis proposed the FDEA based on fuzzy mathematical programming and used it to the newspaper preprint insertion process. This application clearly showed ...\n# \u30107\u2020My Successful Harvard Application (Complete Common App - PrepScholar\u2020blog.prepscholar.com\u3011\nIn my complete analysis, I'll take you through my Common Application, Harvard supplemental application, personal statements and essays, extracurricular activities, teachers' letters of recommendation, counselor recommendation, complete high school transcript, and more. I'll also give you in-depth commentary on every part of my application.\n# \u30108\u2020Most Successful Applications Have These 9 Things in Common - Koombea\u2020www.koombea.com\u3011\n9) The App\u2019s Users Are Habitually Dependent on It. It\u2019s not enough for an app to provide a one-time solution to a user\u2019s needs; to reach the heights of success, an app needs to engage the user and continue to prove relevant. Regardless of the industry, the most successful applications not only meet, but exceed user expectations and keep ...\n# \u30109\u2020My Apps\u2020myapplications.microsoft.com\u3011\nMy Apps is a web-based portal for managing and launching applications in Microsoft. You can customize your experience, control access rights, and monitor costs with My Apps.\n# \u301010\u2020Successful Applications | Bureau of Justice Assistance\u2020bja.ojp.gov\u3011\nThe following table contains links to examples of project narratives associated with applications that have successfully received funding from the Bureau of Justice Assistance (BJA). See the Funding Awards page for details about all funding awarded by BJA and the Available Funding page for currently available opportunities. BJA Program.\n# \u301011\u2020Successful applications | Research and Innovation Services - UCL ...\u2020www.ucl.ac.uk\u3011\nAn application will only be marked as successful on receipt of formal notification from the funder, through either an Award Letter on funder letterhead or generated via the funder\u2019s portal. As a minimum, the funding notification should include: Terms and conditions on which the award is made. Where UCL is not the lead institution, a ...\n# \u301012\u2020Successful application to Columbia - Crimson Education US\u2020www.crimsoneducation.org\u3011\nOut of 800. AP: Out of 5. Honors & Awards. 5. Extracurriculars. 10. View Applicant Profile. Discover the academic achievements, extracurricular involvement and personal essay of a successful Columbia University applicant.\n# \u301013\u2020The Key to Successful Applications | Office of Graduate Education\u2020oge.mit.edu\u3011\nWhile there isn\u2019t a quick-and-easy recipe that dictates sentence-by-sentence what your personal statement should be, this post will 1) identify two elements that we think are critical to a successful application and 2) present some guidelines and exercises to get you past writer\u2019s block. Figure 1: The \u201ckey\u201d to success is to show that ...\n# \u301014\u2020ERIC - EJ836365 - Successful Applications of the Balanced Scorecard in ...\u2020eric.ed.gov\u3011\nSuccessful Applications of the Balanced Scorecard in Higher Education. Beard, Deborah F. Journal of Education for Business, v84 n5 p275-282 May-Jun 2009. Are there management tools that professionals use in business that academics have used successfully in higher education? The answer to that question is \"yes\", and the balanced scorecard (BSC ...\n# \u301015\u2020SUCCESSFUL APPLICATION definition and meaning | Collins English Dictionary\u2020www.collinsdictionary.com\u3011\nSUCCESSFUL APPLICATION definition | Meaning, pronunciation, translations and examples\n# \u301016\u2020Variable Speed Pumping: A Guide to Successful Applications; Executive ...\u2020www.osti.gov\u3011\nVariable speed pumping: A guide to successful applications - Executive summary Technical Report \u00b7 Sat May 01 00:00:00 EDT 2004 \u00b7 OSTI ID: 15007606\n# \u301017\u2020Top Successful MIT Applications | Squired\u2020www.squired.co\u3011\nSuccessful MIT Applications. These are full college applications of students that were accepted to MIT. Use them to see what it takes to get into MIT and other top schools and get inspiration for your own Common App essay, supplements, and short answers.\n# \u301018\u2020Register | Writing Successful Applications\u2020www.bicar.org\u3011\nWriting Successful Applications. Online via Zoom. Workshop The Art and Architecture of Language. Download Workshop Poster. 24 - 25 February 2024. Saturday and Sunday. INR 4,500. Participation. 11:00 AM - 3:30 PM IST. Worldwide Registration Open. About the Workshop.\n# \u301019\u2020SOLUTION: The United Way Campaign of Greater Toledo had fifteen ...\u2020www.algebra.com\u3011\nThe United Way Campaign of Greater Toledo had fifteen applications for funding this year. If eight of these applications can be funded, how many different lists of successful applications are there?-----Each set of 8 successful applications is a group.---Combinations are used to count groups.-----# of groups of size 8 is 15C8 = 6453\nVisible: 0% - 100%","summary":null,"assets":[],"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"cloud_doc_urls":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","text":"\nExtended LSTM (xLSTM) family. At the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM). sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing.\n","pub_date":"2024-06-20T15:50:00.0000000Z","extra":null},{"type":"webpage","title":"Successful Y Combinator Application Examples \u2013 ycombinator.","url":"https://ycombinator.dev/ycombinator-application-examples/","text":"\nNow, let\u2019s explore some examples of successful Y Combinator applications and what made them stand out. 1. Dropbox (2007) Dropbox\u2019s YC application was notable for its clarity in expressing the widespread issue of file storage and sharing across multiple devices. The team demonstrated a deep understanding of the problem and proposed a simple ...\n","pub_date":"2024-06-20T01:01:00.0000000Z","extra":null},{"type":"webpage","title":"The application that got Metlo into Y Combinator : r/ycombinator - Reddit","url":"https://www.reddit.com/r/ycombinator/comments/quptir/the_application_that_got_metlo_into_y_combinator/","text":"\nThe application that got Metlo into Y Combinator. We applied to Y Combinator in March 2021 and got accepted into the Summer 2021 batch. Thinking back to March 2021, I remember my co-founder, Akshay, and I, nervously scouring the internet for examples of recently successful YC applications. Companies like OpenPhone had generously published their ...\n","pub_date":"2024-06-06T03:21:00.0000000Z","extra":null},{"type":"webpage","title":"Anyone have a list of example successful YC applications?","url":"https://www.reddit.com/r/ycombinator/comments/x6p88p/anyone_have_a_list_of_example_successful_yc/","text":"\nThis is the unofficial subreddit for all things concerning the International Baccalaureate, an academic credential accorded to secondary students from around the world after two vigorous years of study, culminating in challenging exams.\n","pub_date":"2024-06-10T05:17:00.0000000Z","extra":null},{"type":"webpage","title":"Top 15 Most Successful Apps of 2022 | Best Apps by Category - SF AppWorks","url":"https://www.sfappworks.com/blogs/top-10-most-successful-apps-of-2020-and-what-you-can-learn-from-them","text":"\nTikTok is one of the most popular short-form video entertainment apps. It grew by 5x in 2020 mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with 62 million installs in January 2021. TikTok started as a lip-syncing app that transformed into a social media phenomenon.\n","pub_date":"2024-06-21T10:49:00.0000000Z","extra":null},{"type":"webpage","title":"The Data School - Successful Applications","url":"https://www.thedataschool.co.uk/successful/","text":"\nHere are their initial application and final visualisations that got them into DS40. If you are interested in joining you can apply for The Data School here. Asha Daniels Francisco Rei Lydia Wren Nadia Holloway Olivier Newman Otto Richardson Sanjeev Dosanjh Toby Horne. View. Mon 05 Jun 2023.\n","pub_date":"2024-06-20T01:55:00.0000000Z","extra":null},{"type":"webpage","title":"An Overview of the Fuzzy Data Envelopment Analysis Research ... - Springer","url":"https://link.springer.com/article/10.1007/s40815-020-00853-6","text":"\nSome successful applications of the FDEAs in efficiency analysis. For the DEA, an obvious and suitable application is in the field of efficiency analysis. Almost two decades ago, Girod and Triantis proposed the FDEA based on fuzzy mathematical programming and used it to the newspaper preprint insertion process. This application clearly showed ...\n","pub_date":"2024-06-18T10:02:00.0000000Z","extra":null},{"type":"webpage","title":"My Successful Harvard Application (Complete Common App - PrepScholar","url":"https://blog.prepscholar.com/successful-harvard-application-common-application-harvard-supplement","text":"\nIn my complete analysis, I'll take you through my Common Application, Harvard supplemental application, personal statements and essays, extracurricular activities, teachers' letters of recommendation, counselor recommendation, complete high school transcript, and more. I'll also give you in-depth commentary on every part of my application.\n","pub_date":"2024-06-22T02:14:00.0000000Z","extra":null},{"type":"webpage","title":"Most Successful Applications Have These 9 Things in Common - Koombea","url":"https://www.koombea.com/blog/most-successful-applications-have-these-9-things-in-common/","text":"\n9) The App\u2019s Users Are Habitually Dependent on It. It\u2019s not enough for an app to provide a one-time solution to a user\u2019s needs; to reach the heights of success, an app needs to engage the user and continue to prove relevant. Regardless of the industry, the most successful applications not only meet, but exceed user expectations and keep ...\n","pub_date":"2024-06-09T01:51:00.0000000Z","extra":null},{"type":"webpage","title":"My Apps","url":"https://myapplications.microsoft.com/","text":"\nMy Apps is a web-based portal for managing and launching applications in Microsoft. You can customize your experience, control access rights, and monitor costs with My Apps.\n","pub_date":"2024-06-21T21:09:00.0000000Z","extra":null},{"type":"webpage","title":"Successful Applications | Bureau of Justice Assistance","url":"https://bja.ojp.gov/funding/successful-applications","text":"\nThe following table contains links to examples of project narratives associated with applications that have successfully received funding from the Bureau of Justice Assistance (BJA). See the Funding Awards page for details about all funding awarded by BJA and the Available Funding page for currently available opportunities. BJA Program.\n","pub_date":"2024-06-21T23:35:00.0000000Z","extra":null},{"type":"webpage","title":"Successful applications | Research and Innovation Services - UCL ...","url":"https://www.ucl.ac.uk/research-innovation-services/award-services/applying-funding/successful-applications","text":"\nAn application will only be marked as successful on receipt of formal notification from the funder, through either an Award Letter on funder letterhead or generated via the funder\u2019s portal. As a minimum, the funding notification should include: Terms and conditions on which the award is made. Where UCL is not the lead institution, a ...\n","pub_date":"2024-06-20T08:43:00.0000000Z","extra":null},{"type":"webpage","title":"Successful application to Columbia - Crimson Education US","url":"https://www.crimsoneducation.org/us/resources/universities/columbia/successful-application/","text":"\nOut of 800. AP: Out of 5. Honors & Awards. 5. Extracurriculars. 10. View Applicant Profile. Discover the academic achievements, extracurricular involvement and personal essay of a successful Columbia University applicant.\n","pub_date":"2024-06-19T13:15:00.0000000Z","extra":null},{"type":"webpage","title":"The Key to Successful Applications | Office of Graduate Education","url":"https://oge.mit.edu/the-key-to-successful-applications/","text":"\nWhile there isn\u2019t a quick-and-easy recipe that dictates sentence-by-sentence what your personal statement should be, this post will 1) identify two elements that we think are critical to a successful application and 2) present some guidelines and exercises to get you past writer\u2019s block. Figure 1: The \u201ckey\u201d to success is to show that ...\n","pub_date":"2024-06-16T05:46:00.0000000Z","extra":null},{"type":"webpage","title":"ERIC - EJ836365 - Successful Applications of the Balanced Scorecard in ...","url":"https://eric.ed.gov/?id=EJ836365","text":"\nSuccessful Applications of the Balanced Scorecard in Higher Education. Beard, Deborah F. Journal of Education for Business, v84 n5 p275-282 May-Jun 2009. Are there management tools that professionals use in business that academics have used successfully in higher education? The answer to that question is \"yes\", and the balanced scorecard (BSC ...\n","pub_date":"2024-06-20T12:22:00.0000000Z","extra":null},{"type":"webpage","title":"SUCCESSFUL APPLICATION definition and meaning | Collins English Dictionary","url":"https://www.collinsdictionary.com/dictionary/english/successful-application","text":"\nSUCCESSFUL APPLICATION definition | Meaning, pronunciation, translations and examples\n","pub_date":"2024-06-19T01:46:00.0000000Z","extra":null},{"type":"webpage","title":"Variable Speed Pumping: A Guide to Successful Applications; Executive ...","url":"https://www.osti.gov/biblio/15007606","text":"\nVariable speed pumping: A guide to successful applications - Executive summary Technical Report \u00b7 Sat May 01 00:00:00 EDT 2004 \u00b7 OSTI ID: 15007606\n","pub_date":"2024-05-22T13:00:00.0000000Z","extra":null},{"type":"webpage","title":"Top Successful MIT Applications | Squired","url":"https://www.squired.co/full-college-applications/top-5-successful-mit-applications/","text":"\nSuccessful MIT Applications. These are full college applications of students that were accepted to MIT. Use them to see what it takes to get into MIT and other top schools and get inspiration for your own Common App essay, supplements, and short answers.\n","pub_date":"2024-06-18T21:43:00.0000000Z","extra":null},{"type":"webpage","title":"Register | Writing Successful Applications","url":"https://www.bicar.org/writing-successful-applications-register","text":"\nWriting Successful Applications. Online via Zoom. Workshop The Art and Architecture of Language. Download Workshop Poster. 24 - 25 February 2024. Saturday and Sunday. INR 4,500. Participation. 11:00 AM - 3:30 PM IST. Worldwide Registration Open. About the Workshop.\n","pub_date":"2024-06-18T08:54:00.0000000Z","extra":null},{"type":"webpage","title":"SOLUTION: The United Way Campaign of Greater Toledo had fifteen ...","url":"https://www.algebra.com/algebra/homework/Probability-and-statistics/Probability-and-statistics.faq.question.286818.html","text":"\nThe United Way Campaign of Greater Toledo had fifteen applications for funding this year. If eight of these applications can be funded, how many different lists of successful applications are there?-----Each set of 8 successful applications is a group.---Combinations are used to count groups.-----# of groups of size 8 is 15C8 = 6453\nVisible: 0% - 100%","pub_date":"2024-06-12T23:21:00.0000000Z","extra":null},{"type":"webpage","title":"Search results for query: 'successful applications of xLSTMs'","url":"","text":"# \u30100\u2020xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory\u2020www.unite.ai\u3011\nExtended LSTM (xLSTM) family. At the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM). sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing.\n# \u30101\u2020Successful Y Combinator Application Examples \u2013 ycombinator.\u2020ycombinator.dev\u3011\nNow, let\u2019s explore some examples of successful Y Combinator applications and what made them stand out. 1. Dropbox (2007) Dropbox\u2019s YC application was notable for its clarity in expressing the widespread issue of file storage and sharing across multiple devices. The team demonstrated a deep understanding of the problem and proposed a simple ...\n# \u30102\u2020The application that got Metlo into Y Combinator : r/ycombinator - Reddit\u2020www.reddit.com\u3011\nThe application that got Metlo into Y Combinator. We applied to Y Combinator in March 2021 and got accepted into the Summer 2021 batch. Thinking back to March 2021, I remember my co-founder, Akshay, and I, nervously scouring the internet for examples of recently successful YC applications. Companies like OpenPhone had generously published their ...\n# \u30103\u2020Anyone have a list of example successful YC applications?\u2020www.reddit.com\u3011\nThis is the unofficial subreddit for all things concerning the International Baccalaureate, an academic credential accorded to secondary students from around the world after two vigorous years of study, culminating in challenging exams.\n# \u30104\u2020Top 15 Most Successful Apps of 2022 | Best Apps by Category - SF AppWorks\u2020www.sfappworks.com\u3011\nTikTok is one of the most popular short-form video entertainment apps. It grew by 5x in 2020 mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with 62 million installs in January 2021. TikTok started as a lip-syncing app that transformed into a social media phenomenon.\n# \u30105\u2020The Data School - Successful Applications\u2020www.thedataschool.co.uk\u3011\nHere are their initial application and final visualisations that got them into DS40. If you are interested in joining you can apply for The Data School here. Asha Daniels Francisco Rei Lydia Wren Nadia Holloway Olivier Newman Otto Richardson Sanjeev Dosanjh Toby Horne. View. Mon 05 Jun 2023.\n# \u30106\u2020An Overview of the Fuzzy Data Envelopment Analysis Research ... - Springer\u2020link.springer.com\u3011\nSome successful applications of the FDEAs in efficiency analysis. For the DEA, an obvious and suitable application is in the field of efficiency analysis. Almost two decades ago, Girod and Triantis proposed the FDEA based on fuzzy mathematical programming and used it to the newspaper preprint insertion process. This application clearly showed ...\n# \u30107\u2020My Successful Harvard Application (Complete Common App - PrepScholar\u2020blog.prepscholar.com\u3011\nIn my complete analysis, I'll take you through my Common Application, Harvard supplemental application, personal statements and essays, extracurricular activities, teachers' letters of recommendation, counselor recommendation, complete high school transcript, and more. I'll also give you in-depth commentary on every part of my application.\n# \u30108\u2020Most Successful Applications Have These 9 Things in Common - Koombea\u2020www.koombea.com\u3011\n9) The App\u2019s Users Are Habitually Dependent on It. It\u2019s not enough for an app to provide a one-time solution to a user\u2019s needs; to reach the heights of success, an app needs to engage the user and continue to prove relevant. Regardless of the industry, the most successful applications not only meet, but exceed user expectations and keep ...\n# \u30109\u2020My Apps\u2020myapplications.microsoft.com\u3011\nMy Apps is a web-based portal for managing and launching applications in Microsoft. You can customize your experience, control access rights, and monitor costs with My Apps.\n# \u301010\u2020Successful Applications | Bureau of Justice Assistance\u2020bja.ojp.gov\u3011\nThe following table contains links to examples of project narratives associated with applications that have successfully received funding from the Bureau of Justice Assistance (BJA). See the Funding Awards page for details about all funding awarded by BJA and the Available Funding page for currently available opportunities. BJA Program.\n# \u301011\u2020Successful applications | Research and Innovation Services - UCL ...\u2020www.ucl.ac.uk\u3011\nAn application will only be marked as successful on receipt of formal notification from the funder, through either an Award Letter on funder letterhead or generated via the funder\u2019s portal. As a minimum, the funding notification should include: Terms and conditions on which the award is made. Where UCL is not the lead institution, a ...\n# \u301012\u2020Successful application to Columbia - Crimson Education US\u2020www.crimsoneducation.org\u3011\nOut of 800. AP: Out of 5. Honors & Awards. 5. Extracurriculars. 10. View Applicant Profile. Discover the academic achievements, extracurricular involvement and personal essay of a successful Columbia University applicant.\n# \u301013\u2020The Key to Successful Applications | Office of Graduate Education\u2020oge.mit.edu\u3011\nWhile there isn\u2019t a quick-and-easy recipe that dictates sentence-by-sentence what your personal statement should be, this post will 1) identify two elements that we think are critical to a successful application and 2) present some guidelines and exercises to get you past writer\u2019s block. Figure 1: The \u201ckey\u201d to success is to show that ...\n# \u301014\u2020ERIC - EJ836365 - Successful Applications of the Balanced Scorecard in ...\u2020eric.ed.gov\u3011\nSuccessful Applications of the Balanced Scorecard in Higher Education. Beard, Deborah F. Journal of Education for Business, v84 n5 p275-282 May-Jun 2009. Are there management tools that professionals use in business that academics have used successfully in higher education? The answer to that question is \"yes\", and the balanced scorecard (BSC ...\n# \u301015\u2020SUCCESSFUL APPLICATION definition and meaning | Collins English Dictionary\u2020www.collinsdictionary.com\u3011\nSUCCESSFUL APPLICATION definition | Meaning, pronunciation, translations and examples\n# \u301016\u2020Variable Speed Pumping: A Guide to Successful Applications; Executive ...\u2020www.osti.gov\u3011\nVariable speed pumping: A guide to successful applications - Executive summary Technical Report \u00b7 Sat May 01 00:00:00 EDT 2004 \u00b7 OSTI ID: 15007606\n# \u301017\u2020Top Successful MIT Applications | Squired\u2020www.squired.co\u3011\nSuccessful MIT Applications. These are full college applications of students that were accepted to MIT. Use them to see what it takes to get into MIT and other top schools and get inspiration for your own Common App essay, supplements, and short answers.\n# \u301018\u2020Register | Writing Successful Applications\u2020www.bicar.org\u3011\nWriting Successful Applications. Online via Zoom. Workshop The Art and Architecture of Language. Download Workshop Poster. 24 - 25 February 2024. Saturday and Sunday. INR 4,500. Participation. 11:00 AM - 3:30 PM IST. Worldwide Registration Open. About the Workshop.\n# \u301019\u2020SOLUTION: The United Way Campaign of Greater Toledo had fifteen ...\u2020www.algebra.com\u3011\nThe United Way Campaign of Greater Toledo had fifteen applications for funding this year. If eight of these applications can be funded, how many different lists of successful applications are there?-----Each set of 8 successful applications is a group.---Combinations are used to count groups.-----# of groups of size 8 is 15C8 = 6453\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"search","args":["successful applications of xLSTMs"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"5d1253da-0bc3-490b-a71b-1acbe199e59e","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"5d1253da-0bc3-490b-a71b-1acbe199e59e","children":["3bd82c3b-a03b-4363-a4b8-703ec0261286"]},"3bd82c3b-a03b-4363-a4b8-703ec0261286":{"id":"3bd82c3b-a03b-4363-a4b8-703ec0261286","message":{"id":"3bd82c3b-a03b-4363-a4b8-703ec0261286","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125938.372909,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"mclick([0, 4, 6, 8, 14])"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"64de275d-28b1-4286-8e9f-a2da80b7f449","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"browser","channel":null},"parent":"64de275d-28b1-4286-8e9f-a2da80b7f449","children":["adb9c7c4-952e-4a7d-a3be-a008ff794285"]},"adb9c7c4-952e-4a7d-a3be-a008ff794285":{"id":"adb9c7c4-952e-4a7d-a3be-a008ff794285","message":{"id":"adb9c7c4-952e-4a7d-a3be-a008ff794285","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125938.372945,"update_time":null,"content":{"content_type":"tether_quote","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","domain":"www.unite.ai","text":"\n## Understanding the Origins: The Limitations of LSTM \n\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise St","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","text":"\n## Understanding the Origins: The Limitations of LSTM \n\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise St","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 4, 6, 8, 14]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"3bd82c3b-a03b-4363-a4b8-703ec0261286","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"3bd82c3b-a03b-4363-a4b8-703ec0261286","children":["7669cc97-a467-4706-9880-f8063f7d0709"]},"7669cc97-a467-4706-9880-f8063f7d0709":{"id":"7669cc97-a467-4706-9880-f8063f7d0709","message":{"id":"7669cc97-a467-4706-9880-f8063f7d0709","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125938.372981,"update_time":null,"content":{"content_type":"tether_quote","url":"https://www.sfappworks.com/blogs/top-10-most-successful-apps-of-2020-and-what-you-can-learn-from-them","domain":"www.sfappworks.com","text":"\n[Image 9: airbnb app]\n\nSource:\u00a0\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\n\nWhat is Airbnb?\n\n\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\u00a0is a\u00a0popular booking app/website\u00a0that started in 2012. You can book hotels, apartments, condos, or even an entire house on Airbnb. Recently, you\u2019re also able to book experiences and restaurants as well.\n\nTop features: Profitability, international use\n\nBecause people rent out their own homes, they can choose how much to charge guests, making it a much\u00a0more affordable option than hotels. If you\u2019re traveling, chances are your expenses will be pricey for the duration of your trip, so being able to save on accommodation is what has made Airbnb such a success.\u00a0\n\nIt also allows people to\u00a0make extra income from their property\u00a0if they aren\u2019t home often and choose to rent it out.\n\nThe homes listed on the app are\u00a0much more appealing than a typical hotel room. In April of 2019, a story was released that guests can even\u00a0\u301039\u2020stay in a potato\u2020www.today.com\u3011\u00a0if they want. Well, it\u2019s not a real potato of course, but it sure does look like one!\n\nThe app can be downloaded if you have an iOS or Android phone.\n\nGet a Free Copy of 'What to Expect from A Clickable Prototype?'\n\n[Image 10: clickable prototype ebook cover image]\n\n## \u00a0\n\n## \u00a0\n\n## Top Apps - Health and Fitness\n\n### 9. Healthcare for Doctors \u2013 Epocrates\n\nWhat is Epocrates RX?\n\n\u301040\u2020Epocrates RX\u2020www.epocrates.com\u3011\u00a0is\u00a0a\u00a0clinical decision support tool (CDST)\u00a0that integrates a host of features into an\u00a0accessible healthcare application\u00a0for mobile use. The tool is full of useful features that\u00a0doctors can access on a subscription basis. They can use it on smartphones, tablets, or desktop since the login details make up the subscription.\n\nTop features: Offline capabilities, regular app updates\n\nEpocrates\u2019 developers focus on regularly updating the app to\u00a0reflect the latest research, discoveries, and medical innovations. It is available for offline use, so users don\u2019t need an internet connection when accessing the mobile app. It also allows for\u00a0connection with other doctors\u00a0through the app, powered by Athena Text, a texting service.\n\nKey features include:\n\n  * Clinical Guidelines\n  * Symptom Checker\n  * Medication Database\n  * Drug References\n  * Medicine Specific Calculator\n  * Disease Database\n\nThese features are available in the free app version. Premium subscription unlocks advanced features like Alternate Meds, Lab References and ICD-910, among others.\n\nEpocrates is\u00a0improving the provision of health services\u00a0on a global scale, and over\u00a01 million healthcare professionals use this CDST daily.\n\n### \u00a0\n\n### 10. Self-Care \u2013 Strides Habit Tracker\n\n[Image 11: habit app capture]\n\nSource:\u00a0\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\n\nWhat is Strides Habit Tracker?\n\n\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\u00a0is an essential app for the phone addict, compatible with all Apple devices, including Apple Watch. Basically, it can be used to\u00a0track all kinds of goals and daily habits\u00a0\u2013 you have to add each habit separately before the tracking, but it\u2019s simple to do. You set your goals depending on your definition of becoming your best self.\n\nTop features: Engaging UX\n\nOnce you\u00a0input your individual goals and better habits, you\u2019ll\u00a0see a daily checklist to help keep you on course. Enabling notifications means you can receive reminders of your goals in your status bar throughout the day.\u00a0\n\nYou can see daily and long term progress through in-app charts. And you can see if you need to make changes to achieve your goals.\u00a0\n\nStrides is still\u00a0free on all iOS devices. However, you can get Strides Plus, which features unlimited trackers, data protection, and web syncing, among other cool features. The user interface is\u00a0simple, insightful, and well-designed. The setup is easy, and your goal can be anything from brushing teeth daily to running a marathon.\n\nRelated:\u00a0\u301042\u2020The 3 Main Types of Application Development Methodologies\u3011\n\nCheck Out This Episode Of Our Technology Podcast\n\n* * *\n\nBest Mobile Apps - Games and Entertainment\n\n### 11. TikTok\n\n[Image 12: tiktok]\n\nsource:\u00a0\u301043\u2020https://www.tiktok.com/en\u2020www.tiktok.com\u3011\n\nWhat is TikTok?\n\n\u301043\u2020TikTok\u2020www.tiktok.com\u3011\u00a0is one of the\u00a0most popular short-form video entertainment apps. It grew by\u00a0\u301044\u20205x in 2020\u2020www.businessofapps.com\u3011\u00a0mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with\u00a062 million installs\u00a0in\u00a0\u301045\u2020January 2021\u2020zeenews.india.com\u3011.\n\nTikTok started as a lip-syncing app that transformed into a social media phenomenon. TikTok\u00a0simplified video creation\u00a0and created a platform where\u00a0users can easily share and view other users\u2019 content. And by making the content short format, it makes TikTok content bite size.\n\nTop Features: Localized content, personalized recommendations\n\nOne of the key features of TikTok is its\u00a0focus on localized content. It runs contests and challenges based on local trends, which spurs thousands of videos and viral content for the app.\u00a0\n\nAnother top feature is its\u00a0personalized recommendations\u00a0that it generates for its users based on their preferences. By serving them content that matches their preferences, it gives the users the latest trending videos so that the users have more ideas for content creation.\n\n### [Image 13: DOWNLOAD OUR FREE MOBILE APP DEVELOPMENT CHECKLIST]\n\n### \u00a0\n\n###","title":"Top 15 Most Successful Apps of 2022 | Best Apps by Category","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"Top 15 Most Successful Apps of 2022 | Best Apps by Category","url":"https://www.sfappworks.com/blogs/top-10-most-successful-apps-of-2020-and-what-you-can-learn-from-them","text":"\n[Image 9: airbnb app]\n\nSource:\u00a0\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\n\nWhat is Airbnb?\n\n\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\u00a0is a\u00a0popular booking app/website\u00a0that started in 2012. You can book hotels, apartments, condos, or even an entire house on Airbnb. Recently, you\u2019re also able to book experiences and restaurants as well.\n\nTop features: Profitability, international use\n\nBecause people rent out their own homes, they can choose how much to charge guests, making it a much\u00a0more affordable option than hotels. If you\u2019re traveling, chances are your expenses will be pricey for the duration of your trip, so being able to save on accommodation is what has made Airbnb such a success.\u00a0\n\nIt also allows people to\u00a0make extra income from their property\u00a0if they aren\u2019t home often and choose to rent it out.\n\nThe homes listed on the app are\u00a0much more appealing than a typical hotel room. In April of 2019, a story was released that guests can even\u00a0\u301039\u2020stay in a potato\u2020www.today.com\u3011\u00a0if they want. Well, it\u2019s not a real potato of course, but it sure does look like one!\n\nThe app can be downloaded if you have an iOS or Android phone.\n\nGet a Free Copy of 'What to Expect from A Clickable Prototype?'\n\n[Image 10: clickable prototype ebook cover image]\n\n## \u00a0\n\n## \u00a0\n\n## Top Apps - Health and Fitness\n\n### 9. Healthcare for Doctors \u2013 Epocrates\n\nWhat is Epocrates RX?\n\n\u301040\u2020Epocrates RX\u2020www.epocrates.com\u3011\u00a0is\u00a0a\u00a0clinical decision support tool (CDST)\u00a0that integrates a host of features into an\u00a0accessible healthcare application\u00a0for mobile use. The tool is full of useful features that\u00a0doctors can access on a subscription basis. They can use it on smartphones, tablets, or desktop since the login details make up the subscription.\n\nTop features: Offline capabilities, regular app updates\n\nEpocrates\u2019 developers focus on regularly updating the app to\u00a0reflect the latest research, discoveries, and medical innovations. It is available for offline use, so users don\u2019t need an internet connection when accessing the mobile app. It also allows for\u00a0connection with other doctors\u00a0through the app, powered by Athena Text, a texting service.\n\nKey features include:\n\n  * Clinical Guidelines\n  * Symptom Checker\n  * Medication Database\n  * Drug References\n  * Medicine Specific Calculator\n  * Disease Database\n\nThese features are available in the free app version. Premium subscription unlocks advanced features like Alternate Meds, Lab References and ICD-910, among others.\n\nEpocrates is\u00a0improving the provision of health services\u00a0on a global scale, and over\u00a01 million healthcare professionals use this CDST daily.\n\n### \u00a0\n\n### 10. Self-Care \u2013 Strides Habit Tracker\n\n[Image 11: habit app capture]\n\nSource:\u00a0\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\n\nWhat is Strides Habit Tracker?\n\n\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\u00a0is an essential app for the phone addict, compatible with all Apple devices, including Apple Watch. Basically, it can be used to\u00a0track all kinds of goals and daily habits\u00a0\u2013 you have to add each habit separately before the tracking, but it\u2019s simple to do. You set your goals depending on your definition of becoming your best self.\n\nTop features: Engaging UX\n\nOnce you\u00a0input your individual goals and better habits, you\u2019ll\u00a0see a daily checklist to help keep you on course. Enabling notifications means you can receive reminders of your goals in your status bar throughout the day.\u00a0\n\nYou can see daily and long term progress through in-app charts. And you can see if you need to make changes to achieve your goals.\u00a0\n\nStrides is still\u00a0free on all iOS devices. However, you can get Strides Plus, which features unlimited trackers, data protection, and web syncing, among other cool features. The user interface is\u00a0simple, insightful, and well-designed. The setup is easy, and your goal can be anything from brushing teeth daily to running a marathon.\n\nRelated:\u00a0\u301042\u2020The 3 Main Types of Application Development Methodologies\u3011\n\nCheck Out This Episode Of Our Technology Podcast\n\n* * *\n\nBest Mobile Apps - Games and Entertainment\n\n### 11. TikTok\n\n[Image 12: tiktok]\n\nsource:\u00a0\u301043\u2020https://www.tiktok.com/en\u2020www.tiktok.com\u3011\n\nWhat is TikTok?\n\n\u301043\u2020TikTok\u2020www.tiktok.com\u3011\u00a0is one of the\u00a0most popular short-form video entertainment apps. It grew by\u00a0\u301044\u20205x in 2020\u2020www.businessofapps.com\u3011\u00a0mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with\u00a062 million installs\u00a0in\u00a0\u301045\u2020January 2021\u2020zeenews.india.com\u3011.\n\nTikTok started as a lip-syncing app that transformed into a social media phenomenon. TikTok\u00a0simplified video creation\u00a0and created a platform where\u00a0users can easily share and view other users\u2019 content. And by making the content short format, it makes TikTok content bite size.\n\nTop Features: Localized content, personalized recommendations\n\nOne of the key features of TikTok is its\u00a0focus on localized content. It runs contests and challenges based on local trends, which spurs thousands of videos and viral content for the app.\u00a0\n\nAnother top feature is its\u00a0personalized recommendations\u00a0that it generates for its users based on their preferences. By serving them content that matches their preferences, it gives the users the latest trending videos so that the users have more ideas for content creation.\n\n### [Image 13: DOWNLOAD OUR FREE MOBILE APP DEVELOPMENT CHECKLIST]\n\n### \u00a0\n\n###","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 4, 6, 8, 14]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"adb9c7c4-952e-4a7d-a3be-a008ff794285","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"adb9c7c4-952e-4a7d-a3be-a008ff794285","children":["b5780cb9-8a3d-49e2-870a-5312f5f1a128"]},"b5780cb9-8a3d-49e2-870a-5312f5f1a128":{"id":"b5780cb9-8a3d-49e2-870a-5312f5f1a128","message":{"id":"b5780cb9-8a3d-49e2-870a-5312f5f1a128","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125938.373017,"update_time":null,"content":{"content_type":"tether_quote","url":"https://link.springer.com/article/10.1007/s40815-020-00853-6","domain":"link.springer.com","text":"\n  21. Azadeh, A., Seraj, O., Asadzadeh, S.M., Saberi, M.: An integrated fuzzy regression-data envelopment analysis algorithm for optimum oil consumption estimation with ambiguous data. Appl. Soft Comput. 12(8), 2614\u20132630 (2012)\n\n\u301051\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  22. Azadeh, A., Sheikhalishahi, M., Asadzadeh, S.M.: A flexible neural network-fuzzy data envelopment analysis approach for location optimization of solar plants with uncertainty and complexity. Renew. Energy 36(12), 3394\u20133401 (2011)\n\n\u301052\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  23. Azadi, M., Jafarian, M., Saen, R.F., Mirhedayatian, S.M.: A new fuzzy DEA model for evaluation of efficiency and effectiveness of suppliers in sustainable supply chain management context. Comput. Oper. Res. 54, 274\u2013285 (2015)\n\n\u301053\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301054\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301055\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  24. Baeza-Sampere, I., Coll-Serrano, V., M\u2019Zali, B., Mendez-Rodriguez, P.: A fuzzy data envelopment analysis model for evaluating the efficiency of socially responsible and conventional mutual funds. J. Risk 19(1), 77\u201390 (2016)\n\n\u301056\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  25. Baloch, A.U., Mohammadian, H.: Fuzzy failure modes and effects analysis by using fuzzy VIKOR and data envelopment analysis-based fuzzy AHP. Int. J. Adv. Appl. Sci. 3(8), 23\u201330 (2016)\n\n\u301057\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  26. Beheshtinia, M.A., Omidi, S.: A hybrid MCDM approach for performance evaluation in the banking industry. Kybernetes 46(8), 1386\u20131407 (2017)\n\n\u301058\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  27. Cakir, S.: Proposing integrated Shannon\u2019s entropy-inverse data envelopment analysis methods for resource allocation problem under a fuzzy environment. Eng. Optim. 49(10), 1733\u20131749 (2017)\n\n\u301059\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301060\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  28. Calik, A., Yapici Pehlivan, N., Kahraman, C.: An integrated fuzzy AHP/DEA approach for performance evaluation of territorial units in Turkey. Technol. Econ. Dev. Econ. 24(4), 1280\u20131302 (2018)\n\n\u301061\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  29. Celen, A.: Performance evaluation of Turkish electricity distribution market using a combined FAHP/TOPSIS method. Energy Educ. Sci. Technol. Part A Energy Sci. Res. 29(2), 1263\u20131276 (2012)\n\n\u301062\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301063\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  30. Chang, T.H., Kao, L.J., Ou, T.Y., Fu, H.P.: A hybrid method to measure the operational performance of fast food chain stores. Int. J. Inf. Technol. Decis. Mak. 17(4), 1269\u20131298 (2018)\n\n\u301064\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  31. Chao, S.L.: Integrating multi-stage data envelopment analysis and a fuzzy analytical hierarchical process to evaluate the efficiency of major global liner shipping companies. Maritime Policy Manage. 44(4), 496\u2013511 (2017)\n\n\u301065\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  32. Charnes, A., Cooper, W.W., Rhodes, E.: Measuring the efficiency of decision making units. Eur. J. Oper. Res. 2, 429\u2013444 (1978)\n\n\u301066\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301067\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301068\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  33. Che, Z.H., Wang, H.S., Chuang, C.L.: A fuzzy AHP and DEA approach for making bank loan decisions for small and medium enterprises in Taiwan. Expert Syst. Appl. 37(10), 7189\u20137199 (2010)\n\n\u301069\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  34. Chen, C.M.: Searching for intellectual turning points: progressive knowledge domain visualization. Proc. Natl. Acad. Sci. U.S.A. 101, 5303\u20135310 (2004)\n\n\u301070\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  35. Chen, C.M.: CiteSpace II: detecting and visualizing emerging trends and transient patterns in scientific literature. J. Assoc. Inf. Sci. Technol. 57(3), 359\u2013377 (2006)\n\n\u301071\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  36. Chen, L., Wang, Y.M.: Data envelopment analysis cross-efficiency model in fuzzy environments. J. Intell. Fuzzy Syst. 30(5), 2601\u20132609 (2016)\n\n\u301072\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301073","title":"An Overview of the Fuzzy Data Envelopment Analysis Research and Its Successful Applications | International Journal of Fuzzy Systems","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"An Overview of the Fuzzy Data Envelopment Analysis Research and Its Successful Applications | International Journal of Fuzzy Systems","url":"https://link.springer.com/article/10.1007/s40815-020-00853-6","text":"\n  21. Azadeh, A., Seraj, O., Asadzadeh, S.M., Saberi, M.: An integrated fuzzy regression-data envelopment analysis algorithm for optimum oil consumption estimation with ambiguous data. Appl. Soft Comput. 12(8), 2614\u20132630 (2012)\n\n\u301051\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  22. Azadeh, A., Sheikhalishahi, M., Asadzadeh, S.M.: A flexible neural network-fuzzy data envelopment analysis approach for location optimization of solar plants with uncertainty and complexity. Renew. Energy 36(12), 3394\u20133401 (2011)\n\n\u301052\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  23. Azadi, M., Jafarian, M., Saen, R.F., Mirhedayatian, S.M.: A new fuzzy DEA model for evaluation of efficiency and effectiveness of suppliers in sustainable supply chain management context. Comput. Oper. Res. 54, 274\u2013285 (2015)\n\n\u301053\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301054\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301055\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  24. Baeza-Sampere, I., Coll-Serrano, V., M\u2019Zali, B., Mendez-Rodriguez, P.: A fuzzy data envelopment analysis model for evaluating the efficiency of socially responsible and conventional mutual funds. J. Risk 19(1), 77\u201390 (2016)\n\n\u301056\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  25. Baloch, A.U., Mohammadian, H.: Fuzzy failure modes and effects analysis by using fuzzy VIKOR and data envelopment analysis-based fuzzy AHP. Int. J. Adv. Appl. Sci. 3(8), 23\u201330 (2016)\n\n\u301057\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  26. Beheshtinia, M.A., Omidi, S.: A hybrid MCDM approach for performance evaluation in the banking industry. Kybernetes 46(8), 1386\u20131407 (2017)\n\n\u301058\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  27. Cakir, S.: Proposing integrated Shannon\u2019s entropy-inverse data envelopment analysis methods for resource allocation problem under a fuzzy environment. Eng. Optim. 49(10), 1733\u20131749 (2017)\n\n\u301059\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301060\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  28. Calik, A., Yapici Pehlivan, N., Kahraman, C.: An integrated fuzzy AHP/DEA approach for performance evaluation of territorial units in Turkey. Technol. Econ. Dev. Econ. 24(4), 1280\u20131302 (2018)\n\n\u301061\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  29. Celen, A.: Performance evaluation of Turkish electricity distribution market using a combined FAHP/TOPSIS method. Energy Educ. Sci. Technol. Part A Energy Sci. Res. 29(2), 1263\u20131276 (2012)\n\n\u301062\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301063\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  30. Chang, T.H., Kao, L.J., Ou, T.Y., Fu, H.P.: A hybrid method to measure the operational performance of fast food chain stores. Int. J. Inf. Technol. Decis. Mak. 17(4), 1269\u20131298 (2018)\n\n\u301064\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  31. Chao, S.L.: Integrating multi-stage data envelopment analysis and a fuzzy analytical hierarchical process to evaluate the efficiency of major global liner shipping companies. Maritime Policy Manage. 44(4), 496\u2013511 (2017)\n\n\u301065\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  32. Charnes, A., Cooper, W.W., Rhodes, E.: Measuring the efficiency of decision making units. Eur. J. Oper. Res. 2, 429\u2013444 (1978)\n\n\u301066\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301067\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301068\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  33. Che, Z.H., Wang, H.S., Chuang, C.L.: A fuzzy AHP and DEA approach for making bank loan decisions for small and medium enterprises in Taiwan. Expert Syst. Appl. 37(10), 7189\u20137199 (2010)\n\n\u301069\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  34. Chen, C.M.: Searching for intellectual turning points: progressive knowledge domain visualization. Proc. Natl. Acad. Sci. U.S.A. 101, 5303\u20135310 (2004)\n\n\u301070\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  35. Chen, C.M.: CiteSpace II: detecting and visualizing emerging trends and transient patterns in scientific literature. J. Assoc. Inf. Sci. Technol. 57(3), 359\u2013377 (2006)\n\n\u301071\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  36. Chen, L., Wang, Y.M.: Data envelopment analysis cross-efficiency model in fuzzy environments. J. Intell. Fuzzy Syst. 30(5), 2601\u20132609 (2016)\n\n\u301072\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301073","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 4, 6, 8, 14]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"7669cc97-a467-4706-9880-f8063f7d0709","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"7669cc97-a467-4706-9880-f8063f7d0709","children":["bbcb97a9-6951-49e5-bab9-73cf0f4d0373"]},"bbcb97a9-6951-49e5-bab9-73cf0f4d0373":{"id":"bbcb97a9-6951-49e5-bab9-73cf0f4d0373","message":{"id":"bbcb97a9-6951-49e5-bab9-73cf0f4d0373","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125938.373052,"update_time":null,"content":{"content_type":"tether_quote","url":"https://www.koombea.com/blog/most-successful-applications-have-these-9-things-in-common/","domain":"www.koombea.com","text":"\nIn this post, we\u2019ll explore some of the most popular and trending mobile applications, the traits that the most successful apps of all time have in common, and how to make a successful app overall.\n\n## Top Successful App Launches of Our Generation \n\nWhat do the top trending apps have in common? They are popular and people want to use them. With the launch of several new mobile devices, any business considering a new application idea should take a close look at some of the most successful app companies and the products they\u2019ve launched that have solidified their position as leaders within their respective industries.\u00a0\n\nHere are \u301059\u2020examples\u2020www.ndtv.com\u3011 of some of the most successful mobile apps that have transformed the world as we know it over the past few years:\u00a0\n\n### Most Successful Social Media and Video Mobile Platforms \n\n  * YouTube \n  * Facebook \n  * Instagram \n  * Twitter \n  * Snapchat \n  * TikTok\n\n### \u00a0Most Popular Video Calling and Chat Applications \n\n  * Facebook Messenger \n  * WhatsApp \n  * Skype \n\n### Most Successful Mobile Games \n\n  * Subway Surfers \n  * Candy Crush Saga\u00a0\n  * Temple Run 2 \n\n### Most Successful Entertainment and Streaming Apps \n\n  * Netflix \n  * Pandora \n  * Spotify \n\n### Most Successful Two-Sided Marketplace Apps \n\n  * eBay \n  * Uber \n  * Amazon \n  * Airbnb\n  * \u301060\u2020Etsy\u3011\n\n## Tips to Build One of the Most Successful Apps In the Market \n\nWhether this is your first time conceptualizing a mobile app or have already launched a few applications already, you are undoubtedly thinking about all the ways you can make yours shine. While there is no one-size-fits-all approach as far as how to make an app successful, the most successful applications tend to share a few similar characteristics.\u00a0\n\nBelow, we explore 9 key factors of success for app design and development:\u00a0\n\n### 1) The App Is Unique \n\nFirst thing\u2019s first. If you want to build one of the most successful apps on the mobile app market, it has to be original. We\u2019re not saying you have to reinvent the proverbial wheel, but for your application to really take off, it needs to offer something in a way that has never been done before.\u00a0\n\nPerhaps a new concept for a gaming app or a better, more simplified way for users to consolidate their monthly expenditures. The idea itself need not defy the laws of technology, but it certainly needs to be unique enough that no other company has ever attempted to tackle the same problem in quite the same way.\u00a0\n\nThe concept itself should be catchy, creative, smart, and practical. It doesn\u2019t have to be overly complicated or filled with a dozen over-the-top functionalities; it just needs to offer a new way of approaching a particular action or fulfilling a need based on your company\u2019s specific industry.\u00a0\n\n### 2) The App Serves a Purpose Specific to Your Target Audience \n\nPerhaps one of the greatest mistakes a company can make when designing an app is failing to cater to its \u301061\u2020target audience\u3011. We already explored the ways in which an app should be unique, but aside from offering a new way of accomplishing a task or a different manner of tackling a problem, the application itself must be relevant to your intended audience, or else it will not succeed.\u00a0\n\nResearch your target audience and what they are searching for in terms of applications. Demographics play a vital role in how you should model your app, so pay close attention to what your customers are trying to accomplish and what their must-haves are.\u00a0\n\nIf your target audience is a younger demographic, you may want to consider incorporating a bit of a \u201cwow factor\u201d into your app design, such as animated graphics, more intricate features, and even pop culture references in your content. If your target demographic is more mature, try to keep your app simple and straightforward.\u00a0\n\n### 3) The App Is User-Friendly and Easy to Navigate \n\nRegardless of your target audience, the most successful applications are always built around User Experience (UX). User experience refers to the ease with which people can interact with your application\u2019s features, which directly influences how useful and desirable your app will ultimately be.\u00a0\u00a0\n\nUser-friendliness is a key aspect of UX and takes into account the time it takes to learn how to use and understand an app, how easy it is for a user to navigate the app\u2019s components, as well as the frequency with which issues occur, like screen freezes or full app crashes.\u00a0\n\nThe fewer the problems your customers may face, the better their User Experiences will be. In turn, this ensures that your customers are more likely to both continue using your app and even recommend it to their friends and family members.\u00a0\n\n### 4) The App Is Secure \n\nIn an age where people use the Internet to perform most of their transactions, it\u2019s vital to ensure an application abides by the highest standards of security so your users\u2019 personal data is not at risk for hacks. Incorporate multiple levels of \u301062\u2020security features\u2020www.digitalsamba.com\u3011 into your application, such as two-step authentication and biometrics to prevent unwanted hackers from accessing your customers\u2019 private information.\n\nAlso, make sure you abide by the proper protocols set forth by your industry. For example, if you are creating a MedTech app, make sure it is \u301063\u2020HIPAA compliant\u3011. If you are designing a FinTech app, it should be PCI compliant.\u00a0\n\n### 5) The App Is Responsive \n\nWith the release of multiple new smartphones, all of which have varying \u301064\u2020screen sizes\u3011 and resolutions, you can\u2019t risk having ","title":"Most Successful Applications Have These 9 Things in Common","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"Most Successful Applications Have These 9 Things in Common","url":"https://www.koombea.com/blog/most-successful-applications-have-these-9-things-in-common/","text":"\nIn this post, we\u2019ll explore some of the most popular and trending mobile applications, the traits that the most successful apps of all time have in common, and how to make a successful app overall.\n\n## Top Successful App Launches of Our Generation \n\nWhat do the top trending apps have in common? They are popular and people want to use them. With the launch of several new mobile devices, any business considering a new application idea should take a close look at some of the most successful app companies and the products they\u2019ve launched that have solidified their position as leaders within their respective industries.\u00a0\n\nHere are \u301059\u2020examples\u2020www.ndtv.com\u3011 of some of the most successful mobile apps that have transformed the world as we know it over the past few years:\u00a0\n\n### Most Successful Social Media and Video Mobile Platforms \n\n  * YouTube \n  * Facebook \n  * Instagram \n  * Twitter \n  * Snapchat \n  * TikTok\n\n### \u00a0Most Popular Video Calling and Chat Applications \n\n  * Facebook Messenger \n  * WhatsApp \n  * Skype \n\n### Most Successful Mobile Games \n\n  * Subway Surfers \n  * Candy Crush Saga\u00a0\n  * Temple Run 2 \n\n### Most Successful Entertainment and Streaming Apps \n\n  * Netflix \n  * Pandora \n  * Spotify \n\n### Most Successful Two-Sided Marketplace Apps \n\n  * eBay \n  * Uber \n  * Amazon \n  * Airbnb\n  * \u301060\u2020Etsy\u3011\n\n## Tips to Build One of the Most Successful Apps In the Market \n\nWhether this is your first time conceptualizing a mobile app or have already launched a few applications already, you are undoubtedly thinking about all the ways you can make yours shine. While there is no one-size-fits-all approach as far as how to make an app successful, the most successful applications tend to share a few similar characteristics.\u00a0\n\nBelow, we explore 9 key factors of success for app design and development:\u00a0\n\n### 1) The App Is Unique \n\nFirst thing\u2019s first. If you want to build one of the most successful apps on the mobile app market, it has to be original. We\u2019re not saying you have to reinvent the proverbial wheel, but for your application to really take off, it needs to offer something in a way that has never been done before.\u00a0\n\nPerhaps a new concept for a gaming app or a better, more simplified way for users to consolidate their monthly expenditures. The idea itself need not defy the laws of technology, but it certainly needs to be unique enough that no other company has ever attempted to tackle the same problem in quite the same way.\u00a0\n\nThe concept itself should be catchy, creative, smart, and practical. It doesn\u2019t have to be overly complicated or filled with a dozen over-the-top functionalities; it just needs to offer a new way of approaching a particular action or fulfilling a need based on your company\u2019s specific industry.\u00a0\n\n### 2) The App Serves a Purpose Specific to Your Target Audience \n\nPerhaps one of the greatest mistakes a company can make when designing an app is failing to cater to its \u301061\u2020target audience\u3011. We already explored the ways in which an app should be unique, but aside from offering a new way of accomplishing a task or a different manner of tackling a problem, the application itself must be relevant to your intended audience, or else it will not succeed.\u00a0\n\nResearch your target audience and what they are searching for in terms of applications. Demographics play a vital role in how you should model your app, so pay close attention to what your customers are trying to accomplish and what their must-haves are.\u00a0\n\nIf your target audience is a younger demographic, you may want to consider incorporating a bit of a \u201cwow factor\u201d into your app design, such as animated graphics, more intricate features, and even pop culture references in your content. If your target demographic is more mature, try to keep your app simple and straightforward.\u00a0\n\n### 3) The App Is User-Friendly and Easy to Navigate \n\nRegardless of your target audience, the most successful applications are always built around User Experience (UX). User experience refers to the ease with which people can interact with your application\u2019s features, which directly influences how useful and desirable your app will ultimately be.\u00a0\u00a0\n\nUser-friendliness is a key aspect of UX and takes into account the time it takes to learn how to use and understand an app, how easy it is for a user to navigate the app\u2019s components, as well as the frequency with which issues occur, like screen freezes or full app crashes.\u00a0\n\nThe fewer the problems your customers may face, the better their User Experiences will be. In turn, this ensures that your customers are more likely to both continue using your app and even recommend it to their friends and family members.\u00a0\n\n### 4) The App Is Secure \n\nIn an age where people use the Internet to perform most of their transactions, it\u2019s vital to ensure an application abides by the highest standards of security so your users\u2019 personal data is not at risk for hacks. Incorporate multiple levels of \u301062\u2020security features\u2020www.digitalsamba.com\u3011 into your application, such as two-step authentication and biometrics to prevent unwanted hackers from accessing your customers\u2019 private information.\n\nAlso, make sure you abide by the proper protocols set forth by your industry. For example, if you are creating a MedTech app, make sure it is \u301063\u2020HIPAA compliant\u3011. If you are designing a FinTech app, it should be PCI compliant.\u00a0\n\n### 5) The App Is Responsive \n\nWith the release of multiple new smartphones, all of which have varying \u301064\u2020screen sizes\u3011 and resolutions, you can\u2019t risk having ","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 4, 6, 8, 14]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"b5780cb9-8a3d-49e2-870a-5312f5f1a128","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"b5780cb9-8a3d-49e2-870a-5312f5f1a128","children":["c0c8d57d-d8a5-48dd-bef1-ac10aa4b9ebf"]},"c0c8d57d-d8a5-48dd-bef1-ac10aa4b9ebf":{"id":"c0c8d57d-d8a5-48dd-bef1-ac10aa4b9ebf","message":{"id":"c0c8d57d-d8a5-48dd-bef1-ac10aa4b9ebf","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719125938.373088,"update_time":null,"content":{"content_type":"tether_quote","url":"https://eric.ed.gov/?id=EJ836365","domain":"eric.ed.gov","text":"\nURL: https://eric.ed.gov/?id=EJ836365\n\n\n\u30100\u2020Notes\u3011\u30101\u2020FAQ\u3011\u30102\u2020Contact Us\u3011\n\n[Image 0: ERIC - Institute of Education Sciences]\n\nCollection\n\nThesaurus\n\n\u30103\u2020Advanced Search Tips\u3011 \n\n  Peer reviewed only Full text available on ERIC\n\nCollection\n\nThesaurus\n\n\u30104\u2020Browse Thesaurus\u3011 \n\n  Include Synonyms Include Dead terms\n\n[Image 1: Peer reviewed] Peer reviewed  \n\u30105\u2020Direct link\u3011  \n\nERIC Number: EJ836365 \n\nRecord Type: Journal \n\nPublication Date: 2009 \n\nPages: 8 \n\nAbstractor: As Provided \n\nISBN: N/A \n\nISSN: ISSN-0883-2323 \n\nEISSN: N/A \n\nSuccessful Applications of the Balanced Scorecard in Higher Education \n\nBeard, Deborah F.\n\nJournal of Education for Business, v84 n5 p275-282 May-Jun 2009 \n\nAre there management tools that professionals use in business that academics have used successfully in higher education? The answer to that question is \"yes\", and the balanced scorecard (BSC) is one such tool. The author reports on measures that administrators chose for the BSCs of 2 educational institutions whose successes have been recognized through the Malcolm Baldrige National Quality Award.\n\nDescriptors: \u30106\u2020Higher Education\u3011, \u30107\u2020Business Administration Education\u3011, \u30108\u2020College Administration\u3011, \u30109\u2020Administrative Principles\u3011, \u301010\u2020Evaluation Methods\u3011, \u301011\u2020Evaluation Criteria\u3011, \u301012\u2020Measurement\u3011, \u301013\u2020Institutional Evaluation\u3011, \u301014\u2020Awards\u3011, \u301015\u2020Strategic Planning\u3011\n\nHeldref Publications. 1319 Eighteenth Street, NW, Washington, DC 20036-1802. Tel: 800-365-9753; Tel: 202-296-6267; Fax: 202-293-6130; e-mail: subscribe@heldref.org; Web site: http://www.heldref.org \n\nPublication Type: Journal Articles; Reports - Descriptive \n\nEducation Level: Higher Education; Postsecondary Education \n\nAudience: N/A \n\nLanguage: English \n\nSponsor: N/A \n\nAuthoring Institution: N/A \n\nIdentifiers - Location: Colorado; Wisconsin \n\nGrant or Contract Numbers: N/A\n\n [Image 2: Facebook] [Image 3: Twitter]\n\n[Image 4: Department of Education][Image 5: Institute of Education Statistics]\n\n\u301016\u2020Privacy\u3011 | \u301017\u2020Copyright\u3011 | \u30102\u2020Contact Us\u3011 | \u301018\u2020Selection Policy\u3011 | \u301019\u2020API\u3011\n\n\u301020\u2020Journals\u3011 | \u301021\u2020Non-Journals\u3011 | \u301022\u2020Download\u3011 | \u301023\u2020Submit\u3011 | \u301024\u2020Multimedia\u3011 | \u301025\u2020Widget\u3011\nVisible: 0% - 100%","title":"ERIC - EJ836365 - Successful Applications of the Balanced Scorecard in Higher Education, Journal of Education for Business, 2009","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"ERIC - EJ836365 - Successful Applications of the Balanced Scorecard in Higher Education, Journal of Education for Business, 2009","url":"https://eric.ed.gov/?id=EJ836365","text":"\nURL: https://eric.ed.gov/?id=EJ836365\n\n\n\u30100\u2020Notes\u3011\u30101\u2020FAQ\u3011\u30102\u2020Contact Us\u3011\n\n[Image 0: ERIC - Institute of Education Sciences]\n\nCollection\n\nThesaurus\n\n\u30103\u2020Advanced Search Tips\u3011 \n\n  Peer reviewed only Full text available on ERIC\n\nCollection\n\nThesaurus\n\n\u30104\u2020Browse Thesaurus\u3011 \n\n  Include Synonyms Include Dead terms\n\n[Image 1: Peer reviewed] Peer reviewed  \n\u30105\u2020Direct link\u3011  \n\nERIC Number: EJ836365 \n\nRecord Type: Journal \n\nPublication Date: 2009 \n\nPages: 8 \n\nAbstractor: As Provided \n\nISBN: N/A \n\nISSN: ISSN-0883-2323 \n\nEISSN: N/A \n\nSuccessful Applications of the Balanced Scorecard in Higher Education \n\nBeard, Deborah F.\n\nJournal of Education for Business, v84 n5 p275-282 May-Jun 2009 \n\nAre there management tools that professionals use in business that academics have used successfully in higher education? The answer to that question is \"yes\", and the balanced scorecard (BSC) is one such tool. The author reports on measures that administrators chose for the BSCs of 2 educational institutions whose successes have been recognized through the Malcolm Baldrige National Quality Award.\n\nDescriptors: \u30106\u2020Higher Education\u3011, \u30107\u2020Business Administration Education\u3011, \u30108\u2020College Administration\u3011, \u30109\u2020Administrative Principles\u3011, \u301010\u2020Evaluation Methods\u3011, \u301011\u2020Evaluation Criteria\u3011, \u301012\u2020Measurement\u3011, \u301013\u2020Institutional Evaluation\u3011, \u301014\u2020Awards\u3011, \u301015\u2020Strategic Planning\u3011\n\nHeldref Publications. 1319 Eighteenth Street, NW, Washington, DC 20036-1802. Tel: 800-365-9753; Tel: 202-296-6267; Fax: 202-293-6130; e-mail: subscribe@heldref.org; Web site: http://www.heldref.org \n\nPublication Type: Journal Articles; Reports - Descriptive \n\nEducation Level: Higher Education; Postsecondary Education \n\nAudience: N/A \n\nLanguage: English \n\nSponsor: N/A \n\nAuthoring Institution: N/A \n\nIdentifiers - Location: Colorado; Wisconsin \n\nGrant or Contract Numbers: N/A\n\n [Image 2: Facebook] [Image 3: Twitter]\n\n[Image 4: Department of Education][Image 5: Institute of Education Statistics]\n\n\u301016\u2020Privacy\u3011 | \u301017\u2020Copyright\u3011 | \u30102\u2020Contact Us\u3011 | \u301018\u2020Selection Policy\u3011 | \u301019\u2020API\u3011\n\n\u301020\u2020Journals\u3011 | \u301021\u2020Non-Journals\u3011 | \u301022\u2020Download\u3011 | \u301023\u2020Submit\u3011 | \u301024\u2020Multimedia\u3011 | \u301025\u2020Widget\u3011\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[0, 4, 6, 8, 14]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"bbcb97a9-6951-49e5-bab9-73cf0f4d0373","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"bbcb97a9-6951-49e5-bab9-73cf0f4d0373","children":["22a8a12c-6a96-447e-aac5-cfeedbde8905"]},"22a8a12c-6a96-447e-aac5-cfeedbde8905":{"id":"22a8a12c-6a96-447e-aac5-cfeedbde8905","message":{"id":"22a8a12c-6a96-447e-aac5-cfeedbde8905","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719125938.373125,"update_time":null,"content":{"content_type":"text","parts":["xLSTM (Extended Long Short-Term Memory) has been successfully applied in several domains, leveraging its enhanced architecture to overcome limitations of traditional LSTMs. Here are some notable use cases:\n\n1. **Language Modeling**: xLSTMs have shown promising results in language modeling tasks. By incorporating exponential gating and advanced memory structures (sLSTM and mLSTM), xLSTMs can handle long-range dependencies and complex data patterns more effectively. This makes them competitive with state-of-the-art models like Transformers and State Space Models in various natural language processing tasks\u301020\u2020source\u3011\u301021\u2020source\u3011.\n\n2. **Healthcare Applications**: xLSTMs are used in clinical decision support tools, where their ability to process and update complex medical data dynamically is crucial. For instance, applications like Epocrates integrate xLSTMs to enhance features such as symptom checkers, medication databases, and disease reference tools, improving the accuracy and reliability of healthcare information\u301021\u2020source\u3011.\n\n3. **Financial Predictions and Analysis**: In the financial sector, xLSTMs are employed for tasks requiring high accuracy in time series predictions, such as stock market forecasting and risk management. Their improved storage and retrieval capabilities allow for more precise modeling of financial data, which often involves long-range dependencies and rare events\u301020\u2020source\u3011.\n\n4. **Supply Chain Management**: xLSTMs are used to enhance efficiency and effectiveness in supply chain management by evaluating supplier performance and optimizing logistics. Their ability to handle complex patterns and revise storage decisions makes them ideal for managing the dynamic and uncertain nature of supply chains\u301022\u2020source\u3011.\n\nThese successful applications highlight the versatility and advanced capabilities of xLSTMs in various fields, from healthcare and finance to supply chain management and beyond. The improvements in memory structure and gating mechanisms enable xLSTMs to outperform traditional LSTMs in tasks requiring dynamic updates and handling of complex data."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[{"start_ix":611,"end_ix":622,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","text":"\n## Understanding the Origins: The Limitations of LSTM \n\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise St","pub_date":null,"extra":{"cited_message_idx":20,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":622,"end_ix":633,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Top 15 Most Successful Apps of 2022 | Best Apps by Category","url":"https://www.sfappworks.com/blogs/top-10-most-successful-apps-of-2020-and-what-you-can-learn-from-them","text":"\n[Image 9: airbnb app]\n\nSource:\u00a0\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\n\nWhat is Airbnb?\n\n\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\u00a0is a\u00a0popular booking app/website\u00a0that started in 2012. You can book hotels, apartments, condos, or even an entire house on Airbnb. Recently, you\u2019re also able to book experiences and restaurants as well.\n\nTop features: Profitability, international use\n\nBecause people rent out their own homes, they can choose how much to charge guests, making it a much\u00a0more affordable option than hotels. If you\u2019re traveling, chances are your expenses will be pricey for the duration of your trip, so being able to save on accommodation is what has made Airbnb such a success.\u00a0\n\nIt also allows people to\u00a0make extra income from their property\u00a0if they aren\u2019t home often and choose to rent it out.\n\nThe homes listed on the app are\u00a0much more appealing than a typical hotel room. In April of 2019, a story was released that guests can even\u00a0\u301039\u2020stay in a potato\u2020www.today.com\u3011\u00a0if they want. Well, it\u2019s not a real potato of course, but it sure does look like one!\n\nThe app can be downloaded if you have an iOS or Android phone.\n\nGet a Free Copy of 'What to Expect from A Clickable Prototype?'\n\n[Image 10: clickable prototype ebook cover image]\n\n## \u00a0\n\n## \u00a0\n\n## Top Apps - Health and Fitness\n\n### 9. Healthcare for Doctors \u2013 Epocrates\n\nWhat is Epocrates RX?\n\n\u301040\u2020Epocrates RX\u2020www.epocrates.com\u3011\u00a0is\u00a0a\u00a0clinical decision support tool (CDST)\u00a0that integrates a host of features into an\u00a0accessible healthcare application\u00a0for mobile use. The tool is full of useful features that\u00a0doctors can access on a subscription basis. They can use it on smartphones, tablets, or desktop since the login details make up the subscription.\n\nTop features: Offline capabilities, regular app updates\n\nEpocrates\u2019 developers focus on regularly updating the app to\u00a0reflect the latest research, discoveries, and medical innovations. It is available for offline use, so users don\u2019t need an internet connection when accessing the mobile app. It also allows for\u00a0connection with other doctors\u00a0through the app, powered by Athena Text, a texting service.\n\nKey features include:\n\n  * Clinical Guidelines\n  * Symptom Checker\n  * Medication Database\n  * Drug References\n  * Medicine Specific Calculator\n  * Disease Database\n\nThese features are available in the free app version. Premium subscription unlocks advanced features like Alternate Meds, Lab References and ICD-910, among others.\n\nEpocrates is\u00a0improving the provision of health services\u00a0on a global scale, and over\u00a01 million healthcare professionals use this CDST daily.\n\n### \u00a0\n\n### 10. Self-Care \u2013 Strides Habit Tracker\n\n[Image 11: habit app capture]\n\nSource:\u00a0\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\n\nWhat is Strides Habit Tracker?\n\n\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\u00a0is an essential app for the phone addict, compatible with all Apple devices, including Apple Watch. Basically, it can be used to\u00a0track all kinds of goals and daily habits\u00a0\u2013 you have to add each habit separately before the tracking, but it\u2019s simple to do. You set your goals depending on your definition of becoming your best self.\n\nTop features: Engaging UX\n\nOnce you\u00a0input your individual goals and better habits, you\u2019ll\u00a0see a daily checklist to help keep you on course. Enabling notifications means you can receive reminders of your goals in your status bar throughout the day.\u00a0\n\nYou can see daily and long term progress through in-app charts. And you can see if you need to make changes to achieve your goals.\u00a0\n\nStrides is still\u00a0free on all iOS devices. However, you can get Strides Plus, which features unlimited trackers, data protection, and web syncing, among other cool features. The user interface is\u00a0simple, insightful, and well-designed. The setup is easy, and your goal can be anything from brushing teeth daily to running a marathon.\n\nRelated:\u00a0\u301042\u2020The 3 Main Types of Application Development Methodologies\u3011\n\nCheck Out This Episode Of Our Technology Podcast\n\n* * *\n\nBest Mobile Apps - Games and Entertainment\n\n### 11. TikTok\n\n[Image 12: tiktok]\n\nsource:\u00a0\u301043\u2020https://www.tiktok.com/en\u2020www.tiktok.com\u3011\n\nWhat is TikTok?\n\n\u301043\u2020TikTok\u2020www.tiktok.com\u3011\u00a0is one of the\u00a0most popular short-form video entertainment apps. It grew by\u00a0\u301044\u20205x in 2020\u2020www.businessofapps.com\u3011\u00a0mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with\u00a062 million installs\u00a0in\u00a0\u301045\u2020January 2021\u2020zeenews.india.com\u3011.\n\nTikTok started as a lip-syncing app that transformed into a social media phenomenon. TikTok\u00a0simplified video creation\u00a0and created a platform where\u00a0users can easily share and view other users\u2019 content. And by making the content short format, it makes TikTok content bite size.\n\nTop Features: Localized content, personalized recommendations\n\nOne of the key features of TikTok is its\u00a0focus on localized content. It runs contests and challenges based on local trends, which spurs thousands of videos and viral content for the app.\u00a0\n\nAnother top feature is its\u00a0personalized recommendations\u00a0that it generates for its users based on their preferences. By serving them content that matches their preferences, it gives the users the latest trending videos so that the users have more ideas for content creation.\n\n### [Image 13: DOWNLOAD OUR FREE MOBILE APP DEVELOPMENT CHECKLIST]\n\n### \u00a0\n\n###","pub_date":null,"extra":{"cited_message_idx":21,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":1027,"end_ix":1038,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Top 15 Most Successful Apps of 2022 | Best Apps by Category","url":"https://www.sfappworks.com/blogs/top-10-most-successful-apps-of-2020-and-what-you-can-learn-from-them","text":"\n[Image 9: airbnb app]\n\nSource:\u00a0\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\n\nWhat is Airbnb?\n\n\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\u00a0is a\u00a0popular booking app/website\u00a0that started in 2012. You can book hotels, apartments, condos, or even an entire house on Airbnb. Recently, you\u2019re also able to book experiences and restaurants as well.\n\nTop features: Profitability, international use\n\nBecause people rent out their own homes, they can choose how much to charge guests, making it a much\u00a0more affordable option than hotels. If you\u2019re traveling, chances are your expenses will be pricey for the duration of your trip, so being able to save on accommodation is what has made Airbnb such a success.\u00a0\n\nIt also allows people to\u00a0make extra income from their property\u00a0if they aren\u2019t home often and choose to rent it out.\n\nThe homes listed on the app are\u00a0much more appealing than a typical hotel room. In April of 2019, a story was released that guests can even\u00a0\u301039\u2020stay in a potato\u2020www.today.com\u3011\u00a0if they want. Well, it\u2019s not a real potato of course, but it sure does look like one!\n\nThe app can be downloaded if you have an iOS or Android phone.\n\nGet a Free Copy of 'What to Expect from A Clickable Prototype?'\n\n[Image 10: clickable prototype ebook cover image]\n\n## \u00a0\n\n## \u00a0\n\n## Top Apps - Health and Fitness\n\n### 9. Healthcare for Doctors \u2013 Epocrates\n\nWhat is Epocrates RX?\n\n\u301040\u2020Epocrates RX\u2020www.epocrates.com\u3011\u00a0is\u00a0a\u00a0clinical decision support tool (CDST)\u00a0that integrates a host of features into an\u00a0accessible healthcare application\u00a0for mobile use. The tool is full of useful features that\u00a0doctors can access on a subscription basis. They can use it on smartphones, tablets, or desktop since the login details make up the subscription.\n\nTop features: Offline capabilities, regular app updates\n\nEpocrates\u2019 developers focus on regularly updating the app to\u00a0reflect the latest research, discoveries, and medical innovations. It is available for offline use, so users don\u2019t need an internet connection when accessing the mobile app. It also allows for\u00a0connection with other doctors\u00a0through the app, powered by Athena Text, a texting service.\n\nKey features include:\n\n  * Clinical Guidelines\n  * Symptom Checker\n  * Medication Database\n  * Drug References\n  * Medicine Specific Calculator\n  * Disease Database\n\nThese features are available in the free app version. Premium subscription unlocks advanced features like Alternate Meds, Lab References and ICD-910, among others.\n\nEpocrates is\u00a0improving the provision of health services\u00a0on a global scale, and over\u00a01 million healthcare professionals use this CDST daily.\n\n### \u00a0\n\n### 10. Self-Care \u2013 Strides Habit Tracker\n\n[Image 11: habit app capture]\n\nSource:\u00a0\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\n\nWhat is Strides Habit Tracker?\n\n\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\u00a0is an essential app for the phone addict, compatible with all Apple devices, including Apple Watch. Basically, it can be used to\u00a0track all kinds of goals and daily habits\u00a0\u2013 you have to add each habit separately before the tracking, but it\u2019s simple to do. You set your goals depending on your definition of becoming your best self.\n\nTop features: Engaging UX\n\nOnce you\u00a0input your individual goals and better habits, you\u2019ll\u00a0see a daily checklist to help keep you on course. Enabling notifications means you can receive reminders of your goals in your status bar throughout the day.\u00a0\n\nYou can see daily and long term progress through in-app charts. And you can see if you need to make changes to achieve your goals.\u00a0\n\nStrides is still\u00a0free on all iOS devices. However, you can get Strides Plus, which features unlimited trackers, data protection, and web syncing, among other cool features. The user interface is\u00a0simple, insightful, and well-designed. The setup is easy, and your goal can be anything from brushing teeth daily to running a marathon.\n\nRelated:\u00a0\u301042\u2020The 3 Main Types of Application Development Methodologies\u3011\n\nCheck Out This Episode Of Our Technology Podcast\n\n* * *\n\nBest Mobile Apps - Games and Entertainment\n\n### 11. TikTok\n\n[Image 12: tiktok]\n\nsource:\u00a0\u301043\u2020https://www.tiktok.com/en\u2020www.tiktok.com\u3011\n\nWhat is TikTok?\n\n\u301043\u2020TikTok\u2020www.tiktok.com\u3011\u00a0is one of the\u00a0most popular short-form video entertainment apps. It grew by\u00a0\u301044\u20205x in 2020\u2020www.businessofapps.com\u3011\u00a0mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with\u00a062 million installs\u00a0in\u00a0\u301045\u2020January 2021\u2020zeenews.india.com\u3011.\n\nTikTok started as a lip-syncing app that transformed into a social media phenomenon. TikTok\u00a0simplified video creation\u00a0and created a platform where\u00a0users can easily share and view other users\u2019 content. And by making the content short format, it makes TikTok content bite size.\n\nTop Features: Localized content, personalized recommendations\n\nOne of the key features of TikTok is its\u00a0focus on localized content. It runs contests and challenges based on local trends, which spurs thousands of videos and viral content for the app.\u00a0\n\nAnother top feature is its\u00a0personalized recommendations\u00a0that it generates for its users based on their preferences. By serving them content that matches their preferences, it gives the users the latest trending videos so that the users have more ideas for content creation.\n\n### [Image 13: DOWNLOAD OUR FREE MOBILE APP DEVELOPMENT CHECKLIST]\n\n### \u00a0\n\n###","pub_date":null,"extra":{"cited_message_idx":21,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":1406,"end_ix":1417,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","text":"\n## Understanding the Origins: The Limitations of LSTM \n\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise St","pub_date":null,"extra":{"cited_message_idx":20,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":1745,"end_ix":1756,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"An Overview of the Fuzzy Data Envelopment Analysis Research and Its Successful Applications | International Journal of Fuzzy Systems","url":"https://link.springer.com/article/10.1007/s40815-020-00853-6","text":"\n  21. Azadeh, A., Seraj, O., Asadzadeh, S.M., Saberi, M.: An integrated fuzzy regression-data envelopment analysis algorithm for optimum oil consumption estimation with ambiguous data. Appl. Soft Comput. 12(8), 2614\u20132630 (2012)\n\n\u301051\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  22. Azadeh, A., Sheikhalishahi, M., Asadzadeh, S.M.: A flexible neural network-fuzzy data envelopment analysis approach for location optimization of solar plants with uncertainty and complexity. Renew. Energy 36(12), 3394\u20133401 (2011)\n\n\u301052\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  23. Azadi, M., Jafarian, M., Saen, R.F., Mirhedayatian, S.M.: A new fuzzy DEA model for evaluation of efficiency and effectiveness of suppliers in sustainable supply chain management context. Comput. Oper. Res. 54, 274\u2013285 (2015)\n\n\u301053\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301054\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301055\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  24. Baeza-Sampere, I., Coll-Serrano, V., M\u2019Zali, B., Mendez-Rodriguez, P.: A fuzzy data envelopment analysis model for evaluating the efficiency of socially responsible and conventional mutual funds. J. Risk 19(1), 77\u201390 (2016)\n\n\u301056\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  25. Baloch, A.U., Mohammadian, H.: Fuzzy failure modes and effects analysis by using fuzzy VIKOR and data envelopment analysis-based fuzzy AHP. Int. J. Adv. Appl. Sci. 3(8), 23\u201330 (2016)\n\n\u301057\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  26. Beheshtinia, M.A., Omidi, S.: A hybrid MCDM approach for performance evaluation in the banking industry. Kybernetes 46(8), 1386\u20131407 (2017)\n\n\u301058\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  27. Cakir, S.: Proposing integrated Shannon\u2019s entropy-inverse data envelopment analysis methods for resource allocation problem under a fuzzy environment. Eng. Optim. 49(10), 1733\u20131749 (2017)\n\n\u301059\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301060\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  28. Calik, A., Yapici Pehlivan, N., Kahraman, C.: An integrated fuzzy AHP/DEA approach for performance evaluation of territorial units in Turkey. Technol. Econ. Dev. Econ. 24(4), 1280\u20131302 (2018)\n\n\u301061\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  29. Celen, A.: Performance evaluation of Turkish electricity distribution market using a combined FAHP/TOPSIS method. Energy Educ. Sci. Technol. Part A Energy Sci. Res. 29(2), 1263\u20131276 (2012)\n\n\u301062\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301063\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  30. Chang, T.H., Kao, L.J., Ou, T.Y., Fu, H.P.: A hybrid method to measure the operational performance of fast food chain stores. Int. J. Inf. Technol. Decis. Mak. 17(4), 1269\u20131298 (2018)\n\n\u301064\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  31. Chao, S.L.: Integrating multi-stage data envelopment analysis and a fuzzy analytical hierarchical process to evaluate the efficiency of major global liner shipping companies. Maritime Policy Manage. 44(4), 496\u2013511 (2017)\n\n\u301065\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  32. Charnes, A., Cooper, W.W., Rhodes, E.: Measuring the efficiency of decision making units. Eur. J. Oper. Res. 2, 429\u2013444 (1978)\n\n\u301066\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301067\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301068\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  33. Che, Z.H., Wang, H.S., Chuang, C.L.: A fuzzy AHP and DEA approach for making bank loan decisions for small and medium enterprises in Taiwan. Expert Syst. Appl. 37(10), 7189\u20137199 (2010)\n\n\u301069\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  34. Chen, C.M.: Searching for intellectual turning points: progressive knowledge domain visualization. Proc. Natl. Acad. Sci. U.S.A. 101, 5303\u20135310 (2004)\n\n\u301070\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  35. Chen, C.M.: CiteSpace II: detecting and visualizing emerging trends and transient patterns in scientific literature. J. Assoc. Inf. Sci. Technol. 57(3), 359\u2013377 (2006)\n\n\u301071\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  36. Chen, L., Wang, Y.M.: Data envelopment analysis cross-efficiency model in fuzzy environments. J. Intell. Fuzzy Syst. 30(5), 2601\u20132609 (2016)\n\n\u301072\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301073","pub_date":null,"extra":{"cited_message_idx":22,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}}],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","pad":"AAAAAAAAAAAAAAAAAAAAAAAA","parent_id":"c0c8d57d-d8a5-48dd-bef1-ac10aa4b9ebf","request_id":"898297645f7144a1-SIN","timestamp_":"absolute","content_references":[{"matched_text":"\u301020\u2020source\u3011","start_idx":611,"end_idx":622,"alt":null,"type":"webpage_extended","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","pub_date":null,"snippet":"\n## Understanding the Origins: The Limitations of LSTM \n\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise St","attribution":"unite.ai","icon_type":null},{"matched_text":"\u301021\u2020source\u3011","start_idx":622,"end_idx":633,"alt":null,"type":"webpage_extended","title":"Top 15 Most Successful Apps of 2022 | Best Apps by Category","url":"https://www.sfappworks.com/blogs/top-10-most-successful-apps-of-2020-and-what-you-can-learn-from-them","pub_date":null,"snippet":"\n[Image 9: airbnb app]\n\nSource:\u00a0\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\n\nWhat is Airbnb?\n\n\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\u00a0is a\u00a0popular booking app/website\u00a0that started in 2012. You can book hotels, apartments, condos, or even an entire house on Airbnb. Recently, you\u2019re also able to book experiences and restaurants as well.\n\nTop features: Profitability, international use\n\nBecause people rent out their own homes, they can choose how much to charge guests, making it a much\u00a0more affordable option than hotels. If you\u2019re traveling, chances are your expenses will be pricey for the duration of your trip, so being able to save on accommodation is what has made Airbnb such a success.\u00a0\n\nIt also allows people to\u00a0make extra income from their property\u00a0if they aren\u2019t home often and choose to rent it out.\n\nThe homes listed on the app are\u00a0much more appealing than a typical hotel room. In April of 2019, a story was released that guests can even\u00a0\u301039\u2020stay in a potato\u2020www.today.com\u3011\u00a0if they want. Well, it\u2019s not a real potato of course, but it sure does look like one!\n\nThe app can be downloaded if you have an iOS or Android phone.\n\nGet a Free Copy of 'What to Expect from A Clickable Prototype?'\n\n[Image 10: clickable prototype ebook cover image]\n\n## \u00a0\n\n## \u00a0\n\n## Top Apps - Health and Fitness\n\n### 9. Healthcare for Doctors \u2013 Epocrates\n\nWhat is Epocrates RX?\n\n\u301040\u2020Epocrates RX\u2020www.epocrates.com\u3011\u00a0is\u00a0a\u00a0clinical decision support tool (CDST)\u00a0that integrates a host of features into an\u00a0accessible healthcare application\u00a0for mobile use. The tool is full of useful features that\u00a0doctors can access on a subscription basis. They can use it on smartphones, tablets, or desktop since the login details make up the subscription.\n\nTop features: Offline capabilities, regular app updates\n\nEpocrates\u2019 developers focus on regularly updating the app to\u00a0reflect the latest research, discoveries, and medical innovations. It is available for offline use, so users don\u2019t need an internet connection when accessing the mobile app. It also allows for\u00a0connection with other doctors\u00a0through the app, powered by Athena Text, a texting service.\n\nKey features include:\n\n  * Clinical Guidelines\n  * Symptom Checker\n  * Medication Database\n  * Drug References\n  * Medicine Specific Calculator\n  * Disease Database\n\nThese features are available in the free app version. Premium subscription unlocks advanced features like Alternate Meds, Lab References and ICD-910, among others.\n\nEpocrates is\u00a0improving the provision of health services\u00a0on a global scale, and over\u00a01 million healthcare professionals use this CDST daily.\n\n### \u00a0\n\n### 10. Self-Care \u2013 Strides Habit Tracker\n\n[Image 11: habit app capture]\n\nSource:\u00a0\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\n\nWhat is Strides Habit Tracker?\n\n\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\u00a0is an essential app for the phone addict, compatible with all Apple devices, including Apple Watch. Basically, it can be used to\u00a0track all kinds of goals and daily habits\u00a0\u2013 you have to add each habit separately before the tracking, but it\u2019s simple to do. You set your goals depending on your definition of becoming your best self.\n\nTop features: Engaging UX\n\nOnce you\u00a0input your individual goals and better habits, you\u2019ll\u00a0see a daily checklist to help keep you on course. Enabling notifications means you can receive reminders of your goals in your status bar throughout the day.\u00a0\n\nYou can see daily and long term progress through in-app charts. And you can see if you need to make changes to achieve your goals.\u00a0\n\nStrides is still\u00a0free on all iOS devices. However, you can get Strides Plus, which features unlimited trackers, data protection, and web syncing, among other cool features. The user interface is\u00a0simple, insightful, and well-designed. The setup is easy, and your goal can be anything from brushing teeth daily to running a marathon.\n\nRelated:\u00a0\u301042\u2020The 3 Main Types of Application Development Methodologies\u3011\n\nCheck Out This Episode Of Our Technology Podcast\n\n* * *\n\nBest Mobile Apps - Games and Entertainment\n\n### 11. TikTok\n\n[Image 12: tiktok]\n\nsource:\u00a0\u301043\u2020https://www.tiktok.com/en\u2020www.tiktok.com\u3011\n\nWhat is TikTok?\n\n\u301043\u2020TikTok\u2020www.tiktok.com\u3011\u00a0is one of the\u00a0most popular short-form video entertainment apps. It grew by\u00a0\u301044\u20205x in 2020\u2020www.businessofapps.com\u3011\u00a0mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with\u00a062 million installs\u00a0in\u00a0\u301045\u2020January 2021\u2020zeenews.india.com\u3011.\n\nTikTok started as a lip-syncing app that transformed into a social media phenomenon. TikTok\u00a0simplified video creation\u00a0and created a platform where\u00a0users can easily share and view other users\u2019 content. And by making the content short format, it makes TikTok content bite size.\n\nTop Features: Localized content, personalized recommendations\n\nOne of the key features of TikTok is its\u00a0focus on localized content. It runs contests and challenges based on local trends, which spurs thousands of videos and viral content for the app.\u00a0\n\nAnother top feature is its\u00a0personalized recommendations\u00a0that it generates for its users based on their preferences. By serving them content that matches their preferences, it gives the users the latest trending videos so that the users have more ideas for content creation.\n\n### [Image 13: DOWNLOAD OUR FREE MOBILE APP DEVELOPMENT CHECKLIST]\n\n### \u00a0\n\n###","attribution":"sfappworks.com","icon_type":null},{"matched_text":"\u301021\u2020source\u3011","start_idx":1027,"end_idx":1038,"alt":null,"type":"webpage_extended","title":"Top 15 Most Successful Apps of 2022 | Best Apps by Category","url":"https://www.sfappworks.com/blogs/top-10-most-successful-apps-of-2020-and-what-you-can-learn-from-them","pub_date":null,"snippet":"\n[Image 9: airbnb app]\n\nSource:\u00a0\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\n\nWhat is Airbnb?\n\n\u301038\u2020Airbnb\u2020www.airbnb.com\u3011\u00a0is a\u00a0popular booking app/website\u00a0that started in 2012. You can book hotels, apartments, condos, or even an entire house on Airbnb. Recently, you\u2019re also able to book experiences and restaurants as well.\n\nTop features: Profitability, international use\n\nBecause people rent out their own homes, they can choose how much to charge guests, making it a much\u00a0more affordable option than hotels. If you\u2019re traveling, chances are your expenses will be pricey for the duration of your trip, so being able to save on accommodation is what has made Airbnb such a success.\u00a0\n\nIt also allows people to\u00a0make extra income from their property\u00a0if they aren\u2019t home often and choose to rent it out.\n\nThe homes listed on the app are\u00a0much more appealing than a typical hotel room. In April of 2019, a story was released that guests can even\u00a0\u301039\u2020stay in a potato\u2020www.today.com\u3011\u00a0if they want. Well, it\u2019s not a real potato of course, but it sure does look like one!\n\nThe app can be downloaded if you have an iOS or Android phone.\n\nGet a Free Copy of 'What to Expect from A Clickable Prototype?'\n\n[Image 10: clickable prototype ebook cover image]\n\n## \u00a0\n\n## \u00a0\n\n## Top Apps - Health and Fitness\n\n### 9. Healthcare for Doctors \u2013 Epocrates\n\nWhat is Epocrates RX?\n\n\u301040\u2020Epocrates RX\u2020www.epocrates.com\u3011\u00a0is\u00a0a\u00a0clinical decision support tool (CDST)\u00a0that integrates a host of features into an\u00a0accessible healthcare application\u00a0for mobile use. The tool is full of useful features that\u00a0doctors can access on a subscription basis. They can use it on smartphones, tablets, or desktop since the login details make up the subscription.\n\nTop features: Offline capabilities, regular app updates\n\nEpocrates\u2019 developers focus on regularly updating the app to\u00a0reflect the latest research, discoveries, and medical innovations. It is available for offline use, so users don\u2019t need an internet connection when accessing the mobile app. It also allows for\u00a0connection with other doctors\u00a0through the app, powered by Athena Text, a texting service.\n\nKey features include:\n\n  * Clinical Guidelines\n  * Symptom Checker\n  * Medication Database\n  * Drug References\n  * Medicine Specific Calculator\n  * Disease Database\n\nThese features are available in the free app version. Premium subscription unlocks advanced features like Alternate Meds, Lab References and ICD-910, among others.\n\nEpocrates is\u00a0improving the provision of health services\u00a0on a global scale, and over\u00a01 million healthcare professionals use this CDST daily.\n\n### \u00a0\n\n### 10. Self-Care \u2013 Strides Habit Tracker\n\n[Image 11: habit app capture]\n\nSource:\u00a0\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\n\nWhat is Strides Habit Tracker?\n\n\u301041\u2020Strides Habit Tracker\u2020www.stridesapp.com\u3011\u00a0is an essential app for the phone addict, compatible with all Apple devices, including Apple Watch. Basically, it can be used to\u00a0track all kinds of goals and daily habits\u00a0\u2013 you have to add each habit separately before the tracking, but it\u2019s simple to do. You set your goals depending on your definition of becoming your best self.\n\nTop features: Engaging UX\n\nOnce you\u00a0input your individual goals and better habits, you\u2019ll\u00a0see a daily checklist to help keep you on course. Enabling notifications means you can receive reminders of your goals in your status bar throughout the day.\u00a0\n\nYou can see daily and long term progress through in-app charts. And you can see if you need to make changes to achieve your goals.\u00a0\n\nStrides is still\u00a0free on all iOS devices. However, you can get Strides Plus, which features unlimited trackers, data protection, and web syncing, among other cool features. The user interface is\u00a0simple, insightful, and well-designed. The setup is easy, and your goal can be anything from brushing teeth daily to running a marathon.\n\nRelated:\u00a0\u301042\u2020The 3 Main Types of Application Development Methodologies\u3011\n\nCheck Out This Episode Of Our Technology Podcast\n\n* * *\n\nBest Mobile Apps - Games and Entertainment\n\n### 11. TikTok\n\n[Image 12: tiktok]\n\nsource:\u00a0\u301043\u2020https://www.tiktok.com/en\u2020www.tiktok.com\u3011\n\nWhat is TikTok?\n\n\u301043\u2020TikTok\u2020www.tiktok.com\u3011\u00a0is one of the\u00a0most popular short-form video entertainment apps. It grew by\u00a0\u301044\u20205x in 2020\u2020www.businessofapps.com\u3011\u00a0mainly because of the COVID-19 lockdowns. By 2021, TikTok is the second most installed non-gaming app, with\u00a062 million installs\u00a0in\u00a0\u301045\u2020January 2021\u2020zeenews.india.com\u3011.\n\nTikTok started as a lip-syncing app that transformed into a social media phenomenon. TikTok\u00a0simplified video creation\u00a0and created a platform where\u00a0users can easily share and view other users\u2019 content. And by making the content short format, it makes TikTok content bite size.\n\nTop Features: Localized content, personalized recommendations\n\nOne of the key features of TikTok is its\u00a0focus on localized content. It runs contests and challenges based on local trends, which spurs thousands of videos and viral content for the app.\u00a0\n\nAnother top feature is its\u00a0personalized recommendations\u00a0that it generates for its users based on their preferences. By serving them content that matches their preferences, it gives the users the latest trending videos so that the users have more ideas for content creation.\n\n### [Image 13: DOWNLOAD OUR FREE MOBILE APP DEVELOPMENT CHECKLIST]\n\n### \u00a0\n\n###","attribution":"sfappworks.com","icon_type":null},{"matched_text":"\u301020\u2020source\u3011","start_idx":1406,"end_idx":1417,"alt":null,"type":"webpage_extended","title":"xLSTM : A Comprehensive Guide to Extended Long Short-Term Memory - Unite.AI","url":"https://www.unite.ai/xlstm-a-comprehensive-guide-to-extended-long-short-term-memory/","pub_date":null,"snippet":"\n## Understanding the Origins: The Limitations of LSTM \n\nBefore we dive into the world of xLSTM, it's essential to understand the limitations that traditional LSTM architectures have faced. These limitations have been the driving force behind the development of xLSTM and other alternative approaches.\n\n  1. Inability to Revise Storage Decisions: One of the primary limitations of LSTM is its struggle to revise stored values when a more similar vector is encountered. This can lead to suboptimal performance in tasks that require dynamic updates to stored information.\n  2. Limited Storage Capacities: LSTMs compress information into scalar cell states, which can limit their ability to effectively store and retrieve complex data patterns, particularly when dealing with rare tokens or long-range dependencies.\n  3. Lack of Parallelizability: The memory mixing mechanism in LSTMs, which involves hidden-hidden connections between time steps, enforces sequential processing, hindering the parallelization of computations and limiting scalability.\n\nThese limitations have paved the way for the emergence of Transformers and other architectures that have surpassed LSTMs in certain aspects, particularly when scaling to larger models.\n\n## The xLSTM Architecture\n\n[Image 4: Extended LSTM (xLSTM) family]\n\nExtended LSTM (xLSTM) family \n\nAt the core of xLSTM lies two main modifications to the traditional LSTM framework: exponential gating and novel memory structures. These enhancements introduce two new variants of LSTM, known as sLSTM (scalar LSTM) and mLSTM (matrix LSTM).\n\n  1. sLSTM: The Scalar LSTM with Exponential Gating and Memory Mixing \n     * Exponential Gating: sLSTM incorporates exponential activation functions for input and forget gates, enabling more flexible control over information flow.\n     * Normalization and Stabilization: To prevent numerical instabilities, sLSTM introduces a normalizer state that keeps track of the product of input gates and future forget gates.\n     * Memory Mixing: sLSTM supports multiple memory cells and allows for memory mixing via recurrent connections, enabling the extraction of complex patterns and state tracking capabilities.\n  2. mLSTM: The Matrix LSTM with Enhanced Storage Capacities \n     * Matrix Memory: Instead of a scalar memory cell, mLSTM utilizes a matrix memory, increasing its storage capacity and enabling more efficient retrieval of information.\n     * Covariance Update Rule: mLSTM employs a covariance update rule, inspired by Bidirectional Associative Memories (BAMs), to store and retrieve key-value pairs efficiently.\n     * Parallelizability: By abandoning memory mixing, mLSTM achieves full parallelizability, enabling efficient computations on modern hardware accelerators.\n\nThese two variants, sLSTM and mLSTM, can be integrated into residual block architectures, forming xLSTM blocks. By residually stacking these xLSTM blocks, researchers can construct powerful xLSTM architectures tailored for specific tasks and application domains.\n\n## The Math \n\n### Traditional LSTM:\n\nThe original LSTM architecture introduced the constant error carousel and gating mechanisms to overcome the vanishing gradient problem in recurrent neural networks.\n\n[Image 5: The repeating module in an LSTM]\n\nThe repeating module in an LSTM \u2013 \u301096\u2020Source\u2020colah.github.io\u3011\n\nThe LSTM memory cell updates are governed by the following equations:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nHidden State Update: ht = ot \u2299 tanh(ct)\n\nWhere:\n\n  * is the cell state vector at time t\n  * is the forget gate vector\n  * \u00a0is the input gate vector\n  * is the output gate vector\n  * \u00a0is the input modulated by the input gate\n  * \u2299\u00a0represents element-wise multiplication \n\nThe gates ft, it, and ot control what information gets stored, forgotten, and outputted from the cell state ct, mitigating the vanishing gradient issue.\n\n### xLSTM with Exponential Gating:\n\n[Image 6]\n\nThe xLSTM architecture introduces exponential gating to allow more flexible control over the information flow. For the scalar xLSTM (sLSTM) variant:\n\nCell State Update: ct = ft \u2299 ct-1 + it \u2299 zt \n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \n\nHidden State Update: ht = ot \u2299 (ct / nt)\n\nInput & Forget Gates: it = exp(W_i xt + R_i ht-1 + b_i) ft = \u03c3(W_f xt + R_f ht-1 + b_f) OR ft = exp(W_f xt + R_f ht-1 + b_f)\n\nThe exponential activation functions for the input (it) and forget (ft) gates, along with the normalizer state nt, enable more effective control over memory updates and revising stored information.\n\n### xLSTM with Matrix Memory:\n\nFor the matrix xLSTM (mLSTM) variant with enhanced storage capacity:\n\nCell State Update: Ct = ft \u2299 Ct-1 + it \u2299 (vt kt^T)\n\nNormalizer State Update: nt = ft \u2299 nt-1 + it \u2299 kt \n\nHidden State Update: ht = ot \u2299 (Ct qt / max(qt^T nt, 1))\n\nWhere:\n\n  * is the matrix cell state\n  * \u00a0and \u00a0are the value and key vectors\n  * \u00a0is the query vector used for retrieval \n\nThese key equations highlight how xLSTM extends the original LSTM formulation with exponential gating for more flexible memory control and matrix memory for enhanced storage capabilities. The combination of these innovations allows xLSTM to overcome limitations of traditional LSTMs.\n\n## Key Features and Advantages of xLSTM \n\n  1. Ability to Revise St","attribution":"unite.ai","icon_type":null},{"matched_text":"\u301022\u2020source\u3011","start_idx":1745,"end_idx":1756,"alt":null,"type":"webpage_extended","title":"An Overview of the Fuzzy Data Envelopment Analysis Research and Its Successful Applications | International Journal of Fuzzy Systems","url":"https://link.springer.com/article/10.1007/s40815-020-00853-6","pub_date":null,"snippet":"\n  21. Azadeh, A., Seraj, O., Asadzadeh, S.M., Saberi, M.: An integrated fuzzy regression-data envelopment analysis algorithm for optimum oil consumption estimation with ambiguous data. Appl. Soft Comput. 12(8), 2614\u20132630 (2012)\n\n\u301051\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  22. Azadeh, A., Sheikhalishahi, M., Asadzadeh, S.M.: A flexible neural network-fuzzy data envelopment analysis approach for location optimization of solar plants with uncertainty and complexity. Renew. Energy 36(12), 3394\u20133401 (2011)\n\n\u301052\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  23. Azadi, M., Jafarian, M., Saen, R.F., Mirhedayatian, S.M.: A new fuzzy DEA model for evaluation of efficiency and effectiveness of suppliers in sustainable supply chain management context. Comput. Oper. Res. 54, 274\u2013285 (2015)\n\n\u301053\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301054\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301055\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  24. Baeza-Sampere, I., Coll-Serrano, V., M\u2019Zali, B., Mendez-Rodriguez, P.: A fuzzy data envelopment analysis model for evaluating the efficiency of socially responsible and conventional mutual funds. J. Risk 19(1), 77\u201390 (2016)\n\n\u301056\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  25. Baloch, A.U., Mohammadian, H.: Fuzzy failure modes and effects analysis by using fuzzy VIKOR and data envelopment analysis-based fuzzy AHP. Int. J. Adv. Appl. Sci. 3(8), 23\u201330 (2016)\n\n\u301057\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  26. Beheshtinia, M.A., Omidi, S.: A hybrid MCDM approach for performance evaluation in the banking industry. Kybernetes 46(8), 1386\u20131407 (2017)\n\n\u301058\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  27. Cakir, S.: Proposing integrated Shannon\u2019s entropy-inverse data envelopment analysis methods for resource allocation problem under a fuzzy environment. Eng. Optim. 49(10), 1733\u20131749 (2017)\n\n\u301059\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301060\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  28. Calik, A., Yapici Pehlivan, N., Kahraman, C.: An integrated fuzzy AHP/DEA approach for performance evaluation of territorial units in Turkey. Technol. Econ. Dev. Econ. 24(4), 1280\u20131302 (2018)\n\n\u301061\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  29. Celen, A.: Performance evaluation of Turkish electricity distribution market using a combined FAHP/TOPSIS method. Energy Educ. Sci. Technol. Part A Energy Sci. Res. 29(2), 1263\u20131276 (2012)\n\n\u301062\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301063\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  30. Chang, T.H., Kao, L.J., Ou, T.Y., Fu, H.P.: A hybrid method to measure the operational performance of fast food chain stores. Int. J. Inf. Technol. Decis. Mak. 17(4), 1269\u20131298 (2018)\n\n\u301064\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  31. Chao, S.L.: Integrating multi-stage data envelopment analysis and a fuzzy analytical hierarchical process to evaluate the efficiency of major global liner shipping companies. Maritime Policy Manage. 44(4), 496\u2013511 (2017)\n\n\u301065\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  32. Charnes, A., Cooper, W.W., Rhodes, E.: Measuring the efficiency of decision making units. Eur. J. Oper. Res. 2, 429\u2013444 (1978)\n\n\u301066\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301067\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301068\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  33. Che, Z.H., Wang, H.S., Chuang, C.L.: A fuzzy AHP and DEA approach for making bank loan decisions for small and medium enterprises in Taiwan. Expert Syst. Appl. 37(10), 7189\u20137199 (2010)\n\n\u301069\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  34. Chen, C.M.: Searching for intellectual turning points: progressive knowledge domain visualization. Proc. Natl. Acad. Sci. U.S.A. 101, 5303\u20135310 (2004)\n\n\u301070\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  35. Chen, C.M.: CiteSpace II: detecting and visualizing emerging trends and transient patterns in scientific literature. J. Assoc. Inf. Sci. Technol. 57(3), 359\u2013377 (2006)\n\n\u301071\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  36. Chen, L., Wang, Y.M.: Data envelopment analysis cross-efficiency model in fuzzy environments. J. Intell. Fuzzy Syst. 30(5), 2601\u20132609 (2016)\n\n\u301072\u2020MATH\u2020www.emis.de\u3011\u00a0 \u301073","attribution":"link.springer.com","icon_type":null}],"can_save":true},"recipient":"all","channel":null},"parent":"c0c8d57d-d8a5-48dd-bef1-ac10aa4b9ebf","children":["aaa27cc0-30fd-4160-8c98-eee1aea023d4"]},"aaa27cc0-30fd-4160-8c98-eee1aea023d4":{"id":"aaa27cc0-30fd-4160-8c98-eee1aea023d4","message":{"id":"aaa27cc0-30fd-4160-8c98-eee1aea023d4","author":{"role":"user","name":null,"metadata":{}},"create_time":1719125998.176621,"update_time":null,"content":{"content_type":"text","parts":["Is the xLSTM architecture part of the Tensorflow or Pytorch libraries?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"8982992e8a6944a1-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"22a8a12c-6a96-447e-aac5-cfeedbde8905","children":["8bddce3a-fef0-4692-bd94-509b07929aa0"]},"8bddce3a-fef0-4692-bd94-509b07929aa0":{"id":"8bddce3a-fef0-4692-bd94-509b07929aa0","message":{"id":"8bddce3a-fef0-4692-bd94-509b07929aa0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719126011.088767,"update_time":null,"content":{"content_type":"text","parts":["The xLSTM architecture is not part of the official TensorFlow or PyTorch libraries. However, implementations of xLSTM can be found in repositories that are compatible with PyTorch. For instance, there is an xLSTM implementation available on GitHub that is designed to work with PyTorch, providing the necessary code and configurations for using xLSTM in your projects\u30109\u2020source\u3011\u301012\u2020source\u3011.\n\nTo use xLSTM in your projects, you typically need to clone the repository and follow the installation instructions provided. Here's a basic example of how you can set it up using PyTorch:\n\n1. **Clone the Repository:**\n   ```bash\n   git clone https://github.com/NX-AI/xlstm.git\n   cd xlstm\n   pip install -e .\n   ```\n\n2. **Install Required Dependencies:**\n   You may need to set up a conda environment with the required dependencies:\n   ```bash\n   conda env create -n xlstm -f environment_pt220cu121.yaml\n   conda activate xlstm\n   ```\n\n3. **Usage Example:**\n   ```python\n   import torch\n   from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig, mLSTMBlockConfig, mLSTMLayerConfig, sLSTMBlockConfig, sLSTMLayerConfig, FeedForwardConfig\n\n   cfg = xLSTMBlockStackConfig(\n       mlstm_block=mLSTMBlockConfig(\n           mlstm=mLSTMLayerConfig(\n               conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n           )\n       ),\n       slstm_block=sLSTMBlockConfig(\n           slstm=sLSTMLayerConfig(\n               backend=\"cuda\",\n               num_heads=4,\n               conv1d_kernel_size=4,\n               bias_init=\"powerlaw_blockdependent\",\n           ),\n           feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n       ),\n       context_length=256,\n       num_blocks=7,\n       embedding_dim=128,\n       slstm_at=[1],\n   )\n\n   xlstm_stack = xLSTMBlockStack(cfg)\n   x = torch.randn(4, 256, 128).to(\"cuda\")\n   xlstm_stack = xlstm_stack.to(\"cuda\")\n   y = xlstm_stack(x)\n   print(y.shape)\n   ```\n\nBy following these steps, you can integrate xLSTM into your PyTorch projects, leveraging its advanced capabilities for various applications."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[{"start_ix":367,"end_ix":377,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"GitHub - NX-AI/xlstm: Official repository of the xLSTM.","url":"https://github.com/NX-AI/xlstm","text":"\n\u301052\u2020experiments\u3011\n\n\u301053\u2020res\u3011\n\n\u301053\u2020res\u3011\n\n\u301054\u2020test\u3011\n\n\u301054\u2020test\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301062\u2020setup.cfg\u3011\n\n\u301062\u2020setup.cfg\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * AGPL-3.0 license \n\n# xLSTM: Extended Long Short-Term Memory\n\n[Image 0: xLSTM Figure]\n\n> Paper: \u301063\u2020https://arxiv.org/abs/2405.04517\u2020arxiv.org\u3011\n\n## About\n\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State Space Models.\n\n## Minimal Installation\n\nCreate a conda environment from the file `environment_pt220cu121.yaml`. Install the model code only (i.e. the module `xlstm`) as package:\n\nInstally via pip:\n\n    pip install xlstm\n\nClone from github:\n\n    git clone https://github.com/NX-AI/xlstm.git\n    cd xlstm\n    pip install -e .\n\n## Requirements\n\nThis package is based on PyTorch and was tested for versions `>=1.8`. For the CUDA version of sLSTM, you need Compute Capability >= 8.0, see \u301064\u2020https://developer.nvidia.com/cuda-gpus\u2020developer.nvidia.com\u3011. For a well-tested environment, install the `environment_pt220cu121.yaml` as:\n\n    conda env create -n xlstm -f environment_pt220cu121.yaml\n    conda activate xlstm\n\n## Usage\n\nFor non language applications or for integrating in other architectures you can use the `xLSTMBlockStack` and for language modeling or other token-based applications you can use the `xLSTMLMModel`.\n\n### xLSTM Block Stack\n\nThe `xLSTMBLockStack` is meant for use as alternative backbone in existing projects. It is similar to a stack of Transformer blocks, but uses xLSTM blocks:\n\n    import torch\n\n    from xlstm import (\n        xLSTMBlockStack,\n        xLSTMBlockStackConfig,\n        mLSTMBlockConfig,\n        mLSTMLayerConfig,\n        sLSTMBlockConfig,\n        sLSTMLayerConfig,\n        FeedForwardConfig,\n    )\n\n    cfg = xLSTMBlockStackConfig(\n        mlstm_block=mLSTMBlockConfig(\n            mlstm=mLSTMLayerConfig(\n                conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n            )\n        ),\n        slstm_block=sLSTMBlockConfig(\n            slstm=sLSTMLayerConfig(\n                backend=\"cuda\",\n                num_heads=4,\n                conv1d_kernel_size=4,\n                bias_init=\"powerlaw_blockdependent\",\n            ),\n            feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n        ),\n        context_length=256,\n        num_blocks=7,\n        embedding_dim=128,\n        slstm_at=[1],\n\n    )\n\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\nIf you are working with yaml strings / files for configuration you can also use dacite to create the config dataclasses. This is the same as the snippet above:\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n\n    xlstm_cfg = \"\"\" \n    mlstm_block:\n      mlstm:\n        conv1d_kernel_size: 4\n        qkv_proj_blocksize: 4\n        num_heads: 4\n    slstm_block:\n      slstm:\n        backend: cuda\n        num_heads: 4\n        conv1d_kernel_size: 4\n        bias_init: powerlaw_blockdependent\n      feedforward:\n        proj_factor: 1.3\n        act_fn: gelu\n    context_length: 256\n    num_blocks: 7\n    embedding_dim: 128\n    slstm_at: [1]\n    \"\"\"\n    cfg = OmegaConf.create(xlstm_cfg)\n    cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\n### xLSTM Language Model\n\nThe `xLSTMLMModel` is a wrapper around the `xLSTMBlockStack` that adds the token embedding and lm head.\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n\n    xlst","pub_date":null,"extra":{"cited_message_idx":9,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":377,"end_ix":388,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"xLSTM: Extended Long Short-Term Memory | Papers With Code","url":"https://paperswithcode.com/paper/xlstm-extended-long-short-term-memory","text":"\n\u00d7\n\nsmvorwerk/xlstm-cuda \n\n46 \n\n* * *\n\nThere is no official implementation \n\n* * *\n\nMultiple official implementations \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTask:*\n\n--------- Language Modelling\n\nNot in the list? Add a task.\n\nDataset:*\n\n---------\n\nModel name:*\n\nMetric name:*\n\n---------\n\nHigher is better (for the metric) \n\nMetric value:*\n\nUses extra training data \n\nData evaluated on \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE  \n\n##### Add a task\n\n\u00d7\n\nAttached tasks:\n\n  * LANGUAGE MODELLING\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new task. \n\nNew task name: \n\nTop-level area: \n\n--------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series\n\nParent task (if any): \n\n---------\n\nDescription: \n\nSubmit \n\n##### Remove a task\n\n\u00d7\n\n  * Language Modelling - \n\n##### Add a method\n\n\u00d7\n\nAttached methods:\n\n  * ABSOLUTE POSITION ENCODINGS\n\n  * ADAM\n\n  * BATCH NORMALIZATION\n\n  * BPE\n\n  * CONVOLUTION\n\n  * DENSE CONNECTIONS\n\n  * DROPOUT\n\n  * LABEL SMOOTHING\n\n  * LAYER NORMALIZATION\n\n  * LINEAR LAYER\n\n  * LSTM\n\n  * MLSTM\n\n  * MULTI-HEAD ATTENTION\n\n  * POSITION-WISE FEED-FORWARD LAYER\n\n  * RELU\n\n  * RESIDUAL BLOCK\n\n  * RESIDUAL CONNECTION\n\n  * SCALED DOT-PRODUCT ATTENTION\n\n  * SIGMOID ACTIVATION\n\n  * SOFTMAX\n\n  * TANH ACTIVATION\n\n  * TRANSFORMER\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new method. \n\nNew method name (e.g. ReLU): \n\nNew method full name (e.g. Rectified Linear Unit): \n\nPaper where method was first introduced: \n\n---------\n\nMethod category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.\n\n---------\n\nMarkdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.\n\nSubmit \n\n##### Remove a method\n\n\u00d7\n\n  * ABSOLUTE POSITION ENCODINGS - \n\n  * ADAM - \n\n  * BATCH NORMALIZATION - \n\n  * BPE - \n\n  * CONVOLUTION - \n\n  * DENSE CONNECTIONS - \n\n  * DROPOUT - \n\n  * LABEL SMOOTHING - \n\n  * LAYER NORMALIZATION - \n\n  * LINEAR LAYER - \n\n  * LSTM - \n\n  * MLSTM - \n\n  * MULTI-HEAD ATTENTION - \n\n  * POSITION-WISE FEED-FORWARD LAYER - \n\n  * RELU - \n\n  * RESIDUAL BLOCK - \n\n  * RESIDUAL CONNECTION - \n\n  * SCALED DOT-PRODUCT ATTENTION - \n\n  * SIGMOID ACTIVATION - \n\n  * SOFTMAX - \n\n  * TANH ACTIVATION - \n\n  * TRANSFORMER - \n\n##### Edit Datasets\n\n\u00d7\n\nAdd or remove datasets introduced in this paper: \n\nAdd or remove other datasets used in this paper: \n\nC4 LAMBADA mC4\n\nPaper introduces a new dataset? \n\nAdd a new dataset here \n\nSave \n\n#  xLSTM: Extended Long Short-Term Memory \n\n7 May 2024 \u00a0\u00b7\u00a0 \u301013\u2020Maximilian Beck\u3011,  \u301014\u2020Korbinian P\u00f6ppel\u3011,  \u301015\u2020Markus Spanring\u3011,  \u301016\u2020Andreas Auer\u3011,  \u301017\u2020Oleksandra Prudnikova\u3011,  \u301018\u2020Michael Kopp\u3011,  \u301019\u2020G\u00fcnter Klambauer\u3011,  \u301020\u2020Johannes Brandstetter\u3011,  \u301021\u2020Sepp Hochreiter\u3011 \u00b7  Edit social preview\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. \n\n\u301022\u2020 PDF \u2020arxiv.org\u3011 \u301023\u2020 Abstract \u2020arxiv.org\u3011 \n\n## Code \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \u301024\u2020 Mark official\u3011 \n\n* * *\n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 0] \n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 1] \n\n## Tasks \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \n\n* *","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}}],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"aaa27cc0-30fd-4160-8c98-eee1aea023d4","request_id":"8982992e8a6944a1-SIN","timestamp_":"absolute","content_references":[{"matched_text":"\u30109\u2020source\u3011","start_idx":367,"end_idx":377,"alt":null,"type":"webpage_extended","title":"GitHub - NX-AI/xlstm: Official repository of the xLSTM.","url":"https://github.com/NX-AI/xlstm","pub_date":null,"snippet":"\n\u301052\u2020experiments\u3011\n\n\u301053\u2020res\u3011\n\n\u301053\u2020res\u3011\n\n\u301054\u2020test\u3011\n\n\u301054\u2020test\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301055\u2020xlstm\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301056\u2020.gitignore\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301057\u2020AUTHORS\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301039\u2020LICENSE\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301058\u2020README.md\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301059\u2020__init__.py\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301060\u2020environment_pt220cu121.yaml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301061\u2020pyproject.toml\u3011\n\n\u301062\u2020setup.cfg\u3011\n\n\u301062\u2020setup.cfg\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * AGPL-3.0 license \n\n# xLSTM: Extended Long Short-Term Memory\n\n[Image 0: xLSTM Figure]\n\n> Paper: \u301063\u2020https://arxiv.org/abs/2405.04517\u2020arxiv.org\u3011\n\n## About\n\nxLSTM is a new Recurrent Neural Network architecture based on ideas of the original LSTM. Through Exponential Gating with appropriate normalization and stabilization techniques and a new Matrix Memory it overcomes the limitations of the original LSTM and shows promising performance on Language Modeling when compared to Transformers or State Space Models.\n\n## Minimal Installation\n\nCreate a conda environment from the file `environment_pt220cu121.yaml`. Install the model code only (i.e. the module `xlstm`) as package:\n\nInstally via pip:\n\n    pip install xlstm\n\nClone from github:\n\n    git clone https://github.com/NX-AI/xlstm.git\n    cd xlstm\n    pip install -e .\n\n## Requirements\n\nThis package is based on PyTorch and was tested for versions `>=1.8`. For the CUDA version of sLSTM, you need Compute Capability >= 8.0, see \u301064\u2020https://developer.nvidia.com/cuda-gpus\u2020developer.nvidia.com\u3011. For a well-tested environment, install the `environment_pt220cu121.yaml` as:\n\n    conda env create -n xlstm -f environment_pt220cu121.yaml\n    conda activate xlstm\n\n## Usage\n\nFor non language applications or for integrating in other architectures you can use the `xLSTMBlockStack` and for language modeling or other token-based applications you can use the `xLSTMLMModel`.\n\n### xLSTM Block Stack\n\nThe `xLSTMBLockStack` is meant for use as alternative backbone in existing projects. It is similar to a stack of Transformer blocks, but uses xLSTM blocks:\n\n    import torch\n\n    from xlstm import (\n        xLSTMBlockStack,\n        xLSTMBlockStackConfig,\n        mLSTMBlockConfig,\n        mLSTMLayerConfig,\n        sLSTMBlockConfig,\n        sLSTMLayerConfig,\n        FeedForwardConfig,\n    )\n\n    cfg = xLSTMBlockStackConfig(\n        mlstm_block=mLSTMBlockConfig(\n            mlstm=mLSTMLayerConfig(\n                conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n            )\n        ),\n        slstm_block=sLSTMBlockConfig(\n            slstm=sLSTMLayerConfig(\n                backend=\"cuda\",\n                num_heads=4,\n                conv1d_kernel_size=4,\n                bias_init=\"powerlaw_blockdependent\",\n            ),\n            feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n        ),\n        context_length=256,\n        num_blocks=7,\n        embedding_dim=128,\n        slstm_at=[1],\n\n    )\n\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\nIf you are working with yaml strings / files for configuration you can also use dacite to create the config dataclasses. This is the same as the snippet above:\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n\n    xlstm_cfg = \"\"\" \n    mlstm_block:\n      mlstm:\n        conv1d_kernel_size: 4\n        qkv_proj_blocksize: 4\n        num_heads: 4\n    slstm_block:\n      slstm:\n        backend: cuda\n        num_heads: 4\n        conv1d_kernel_size: 4\n        bias_init: powerlaw_blockdependent\n      feedforward:\n        proj_factor: 1.3\n        act_fn: gelu\n    context_length: 256\n    num_blocks: 7\n    embedding_dim: 128\n    slstm_at: [1]\n    \"\"\"\n    cfg = OmegaConf.create(xlstm_cfg)\n    cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n    xlstm_stack = xLSTMBlockStack(cfg)\n\n    x = torch.randn(4, 256, 128).to(\"cuda\")\n    xlstm_stack = xlstm_stack.to(\"cuda\")\n    y = xlstm_stack(x)\n    y.shape == x.shape\n\n### xLSTM Language Model\n\nThe `xLSTMLMModel` is a wrapper around the `xLSTMBlockStack` that adds the token embedding and lm head.\n\n    from omegaconf import OmegaConf\n    from dacite import from_dict\n    from dacite import Config as DaciteConfig\n    from xlstm import xLSTMLMModel, xLSTMLMModelConfig\n\n    xlst","attribution":"github.com","icon_type":null},{"matched_text":"\u301012\u2020source\u3011","start_idx":377,"end_idx":388,"alt":null,"type":"webpage_extended","title":"xLSTM: Extended Long Short-Term Memory | Papers With Code","url":"https://paperswithcode.com/paper/xlstm-extended-long-short-term-memory","pub_date":null,"snippet":"\n\u00d7\n\nsmvorwerk/xlstm-cuda \n\n46 \n\n* * *\n\nThere is no official implementation \n\n* * *\n\nMultiple official implementations \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTask:*\n\n--------- Language Modelling\n\nNot in the list? Add a task.\n\nDataset:*\n\n---------\n\nModel name:*\n\nMetric name:*\n\n---------\n\nHigher is better (for the metric) \n\nMetric value:*\n\nUses extra training data \n\nData evaluated on \n\nSubmit \n\n##### Add a new evaluation result row\n\n\u00d7\n\nTASK DATASET MODEL METRIC NAME METRIC VALUE GLOBAL RANK REMOVE  \n\n##### Add a task\n\n\u00d7\n\nAttached tasks:\n\n  * LANGUAGE MODELLING\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new task. \n\nNew task name: \n\nTop-level area: \n\n--------- Adversarial Audio Computer Code Computer Vision Graphs Knowledge Base Medical Methodology Miscellaneous Music Natural Language Processing Playing Games Reasoning Robots Speech Time Series\n\nParent task (if any): \n\n---------\n\nDescription: \n\nSubmit \n\n##### Remove a task\n\n\u00d7\n\n  * Language Modelling - \n\n##### Add a method\n\n\u00d7\n\nAttached methods:\n\n  * ABSOLUTE POSITION ENCODINGS\n\n  * ADAM\n\n  * BATCH NORMALIZATION\n\n  * BPE\n\n  * CONVOLUTION\n\n  * DENSE CONNECTIONS\n\n  * DROPOUT\n\n  * LABEL SMOOTHING\n\n  * LAYER NORMALIZATION\n\n  * LINEAR LAYER\n\n  * LSTM\n\n  * MLSTM\n\n  * MULTI-HEAD ATTENTION\n\n  * POSITION-WISE FEED-FORWARD LAYER\n\n  * RELU\n\n  * RESIDUAL BLOCK\n\n  * RESIDUAL CONNECTION\n\n  * SCALED DOT-PRODUCT ATTENTION\n\n  * SIGMOID ACTIVATION\n\n  * SOFTMAX\n\n  * TANH ACTIVATION\n\n  * TRANSFORMER\n\nAdd: \n\n---------\n\nNot in the list?  \nCreate a new method. \n\nNew method name (e.g. ReLU): \n\nNew method full name (e.g. Rectified Linear Unit): \n\nPaper where method was first introduced: \n\n---------\n\nMethod category (e.g. Activation Functions): If no match, add something for now then you can add a new category afterwards.\n\n---------\n\nMarkdown description (optional; $\\LaTeX$ enabled): You can edit this later, so feel free to start with something succinct.\n\nSubmit \n\n##### Remove a method\n\n\u00d7\n\n  * ABSOLUTE POSITION ENCODINGS - \n\n  * ADAM - \n\n  * BATCH NORMALIZATION - \n\n  * BPE - \n\n  * CONVOLUTION - \n\n  * DENSE CONNECTIONS - \n\n  * DROPOUT - \n\n  * LABEL SMOOTHING - \n\n  * LAYER NORMALIZATION - \n\n  * LINEAR LAYER - \n\n  * LSTM - \n\n  * MLSTM - \n\n  * MULTI-HEAD ATTENTION - \n\n  * POSITION-WISE FEED-FORWARD LAYER - \n\n  * RELU - \n\n  * RESIDUAL BLOCK - \n\n  * RESIDUAL CONNECTION - \n\n  * SCALED DOT-PRODUCT ATTENTION - \n\n  * SIGMOID ACTIVATION - \n\n  * SOFTMAX - \n\n  * TANH ACTIVATION - \n\n  * TRANSFORMER - \n\n##### Edit Datasets\n\n\u00d7\n\nAdd or remove datasets introduced in this paper: \n\nAdd or remove other datasets used in this paper: \n\nC4 LAMBADA mC4\n\nPaper introduces a new dataset? \n\nAdd a new dataset here \n\nSave \n\n#  xLSTM: Extended Long Short-Term Memory \n\n7 May 2024 \u00a0\u00b7\u00a0 \u301013\u2020Maximilian Beck\u3011,  \u301014\u2020Korbinian P\u00f6ppel\u3011,  \u301015\u2020Markus Spanring\u3011,  \u301016\u2020Andreas Auer\u3011,  \u301017\u2020Oleksandra Prudnikova\u3011,  \u301018\u2020Michael Kopp\u3011,  \u301019\u2020G\u00fcnter Klambauer\u3011,  \u301020\u2020Johannes Brandstetter\u3011,  \u301021\u2020Sepp Hochreiter\u3011 \u00b7  Edit social preview\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. \n\n\u301022\u2020 PDF \u2020arxiv.org\u3011 \u301023\u2020 Abstract \u2020arxiv.org\u3011 \n\n## Code \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \u301024\u2020 Mark official\u3011 \n\n* * *\n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 0] \n\n\u301025\u2020 smvorwerk/xlstm-cuda \u2020github.com\u3011 \n\n46 \n\n[Image 1] \n\n## Tasks \n\n  Edit \n\n\u301024\u2020 Add\u3011 \u301024\u2020 Remove\u3011 \n\n* *","attribution":"paperswithcode.com","icon_type":null}],"can_save":true},"recipient":"all","channel":null},"parent":"aaa27cc0-30fd-4160-8c98-eee1aea023d4","children":["aaa291b2-d17e-416a-8e53-ac94e0dc2206"]},"aaa291b2-d17e-416a-8e53-ac94e0dc2206":{"id":"aaa291b2-d17e-416a-8e53-ac94e0dc2206","message":{"id":"aaa291b2-d17e-416a-8e53-ac94e0dc2206","author":{"role":"user","name":null,"metadata":{}},"create_time":1719126493.438387,"update_time":null,"content":{"content_type":"text","parts":["Is there such a thing as an xLSTM with attention mechanism?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"8982a5450e123f4d-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"8bddce3a-fef0-4692-bd94-509b07929aa0","children":["cc0a3017-1612-43a2-8eab-7e7270e02e44"]},"cc0a3017-1612-43a2-8eab-7e7270e02e44":{"id":"cc0a3017-1612-43a2-8eab-7e7270e02e44","message":{"id":"cc0a3017-1612-43a2-8eab-7e7270e02e44","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719126499.505918,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"xLSTM with attention mechanism\")"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"aaa291b2-d17e-416a-8e53-ac94e0dc2206","request_id":"8982a5450e123f4d-SIN","timestamp_":"absolute","can_save":true},"recipient":"browser","channel":null},"parent":"aaa291b2-d17e-416a-8e53-ac94e0dc2206","children":["873b721c-c098-4fb3-a00b-892243013f3b"]},"873b721c-c098-4fb3-a00b-892243013f3b":{"id":"873b721c-c098-4fb3-a00b-892243013f3b","message":{"id":"873b721c-c098-4fb3-a00b-892243013f3b","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719126499.505969,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020Attention in Long Short-Term Memory Recurrent Neural Networks\u2020machinelearningmastery.com\u3011\nIn this post, you discovered the attention mechanism for sequence prediction problems with LSTM recurrent neural networks. Specifically, you learned: That the encoder-decoder architecture for recurrent neural networks uses a fixed-length internal representation that imposes a constraint that limits learning very long sequences.\n# \u30101\u2020attention-lstm \u00b7 GitHub Topics \u00b7 GitHub\u2020github.com\u3011\nnlp tensorflow attention-mechanism bidirectional-lstm attention-model image2latex im2latex attention-lstm webdemo im2markup Updated Apr 1, 2017; CSS; kaelzhang / DA-RNN-in-Tensorflow-2-and-PyTorch Star 30. Code Issues Pull requests A Tensorflow 2 (Keras) implementation of DA-RNN (A Dual-Stage Attention-Based Recurrent Neural Network for Time ...\n# \u30102\u2020Bidirectional LSTM with attention mechanism and ... - ScienceDirect\u2020www.sciencedirect.com\u3011\nThe combination of LSTM and attention mechanism can obtain better results. Especially, for sequence problems, attention mechanism has been used successfully in a variety of tasks including text classification, reading comprehension and so on. Yang et al. [42] proposed a hierarchical attention network for document classification. The model has ...\n# \u30103\u2020The Attention Mechanism from Scratch - Machine Learning Mastery\u2020machinelearningmastery.com\u3011\nThe Attention Mechanism from Scratch. By Stefania Cristina on January 6, 2023 in Attention 27. The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation. The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a ...\n# \u30104\u2020Advanced Stock Pattern Prediction using LSTM with the Attention ...\u2020drlee.io\u3011\nIn this guide, we explored the complex yet fascinating task of using LSTM networks with an attention mechanism for stock price prediction, specifically for Apple Inc. (AAPL). Key points include: LSTM\u2019s ability to capture long-term dependencies in time-series data. The added advantage of the attention mechanism in focusing on relevant data points.\n# \u30105\u2020Improving time series forecasting using LSTM and attention models ...\u2020link.springer.com\u3011\nIt reaches the best performance. The results demonstrated the predictive power of the attention mechanism in time series forecasting. Also, the two sets of experiments, Part 1 and Part 2, indicated that the proposed method of attention mechanism and LSTM outperformed state-of-the-art methods in time series forecasting including ARIMA, ETS, and MLP.\n# \u30106\u2020attention-based LSTM/Dense implemented by Keras - GitHub\u2020github.com\u3011\nLSTM_Attention. X = Input Sequence of length n. H = LSTM(X); Note that here the LSTM has return_sequences = True, so H is a sequence of vectors of length n. s is the hidden state of the LSTM (h and c) h is a weighted sum over H: \u52a0\u6743\u548c. h = sigma(j = 0 to n-1) alpha(j) * H(j)\n# \u30107\u2020Exploring the predictability of attention mechanism with LSTM: Evidence ...\u2020www.sciencedirect.com\u3011\n3.2. Theory of attention mechanism. The attention mechanism initially proposed in neuroscience and computational neuroscience in Desimone and Duncan (1995) and Itti et al. (1998) has gained widespread attention since its initiation. This popular mechanism comes from the fact that animals in nature only focus on specific parts of their visual view to provide adequate responses.\n# \u30108\u2020Attention-based LSTM predictive model for the attitude and position of ...\u2020www.sciencedirect.com\u3011\nThe attention mechanism is realized by reserving the intermediate output of the LSTM encoder for the input sequence, then training a model to learn these inputs selectively and associating the output sequence with them when the model outputs (Liu et al., 2019, Luo et al., 2021).\n# \u30109\u2020Bi-LS-AttM: A Bidirectional LSTM and Attention Mechanism Model for ...\u2020www.mdpi.com\u3011\nThe discipline of automatic image captioning represents an integration of two pivotal branches of artificial intelligence, namely computer vision (CV) and natural language processing (NLP). The principal functionality of this technology lies in transmuting the extracted visual features into semantic information of a higher order. The bidirectional long short-term memory (Bi-LSTM) has garnered ...\n# \u301010\u2020Intuitive Understanding of Attention Mechanism in Deep Learning\u2020towardsdatascience.com\u3011\nThe attention mechanism was born to resolve this problem. Let\u2019s break this down into finer details. ... Using LSTM layers in place of GRU and adding Bidirectional wrapper on the encoder will also help in improved performance. Deep Learning models are generally considered as black boxes, meaning that they do not have the ability to explain ...\n# \u301011\u2020Hands-On Guide to Bi-LSTM With Attention \u2013 AIM - Analytics India Magazine\u2020analyticsindiamag.com\u3011\nBefore the introduction of the attention mechanism the basic LSTM or RNN model was based on an encoder-decoder system. Where encoding is used to process the data for encoding it into a context vector and creates a good summary of the input data. Then this summary goes through the decoding part where in decoding the model understands the data ...\n# \u301012\u2020Improve the LSTM Trajectory Prediction Accuracy through an Attention ...\u2020ieeexplore.ieee.org\u3011\nTo solve this problem, we introduce a two-stage attention mechanism with the LSTM Encoder-Decoder model, which uses the spatial attention mechanism and the output attention mechanism to weight the input hidden layer features and the output prediction of the decoder. In this way, the accuracy of trajectory prediction is improved.\n# \u301013\u2020[2204.02623] Attention-based CNN-LSTM and XGBoost hybrid model for ...\u2020arxiv.org\u3011\nThe pre-training model is the Attention-based CNN-LSTM model based on sequence-to-sequence framework. The model first uses convolution to extract the deep features of the original stock data, and then uses the Long Short-Term Memory networks to mine the long-term time series features. Finally, the XGBoost model is adopted for fine-tuning.\n# \u301014\u2020Enhancing LSTM Models with Self-Attention and Stateful Training\u2020userweb.cs.txstate.edu\u3011\nAttention mechanisms are designed to resolve these problems. [1,11] Self-attention, also known as intra-attention, is an attention mechanism re-lating di erent positions of a sequence in order to model dependencies between di erent parts of the sequence. This di ers from general attention in that in-\n# \u301015\u2020Simple LSTM + Attention | Kaggle\u2020www.kaggle.com\u3011\nSomething went wrong and this page crashed! If the issue persists, it's likely a problem on our side. Unexpected token < in JSON at position 4. keyboard_arrow_up. content_copy. SyntaxError: Unexpected token < in JSON at position 4. Refresh. Explore and run machine learning code with Kaggle Notebooks | Using data from multiple data sources.\n# \u301016\u2020How Does Attention Work in Encoder-Decoder Recurrent Neural Networks\u2020machinelearningmastery.com\u3011\nAttention is a mechanism that was developed to improve the performance of the Encoder-Decoder RNN on machine translation. In this tutorial, you will discover the attention mechanism for the Encoder-Decoder model. After completing this tutorial, you will know: About the Encoder-Decoder model and attention mechanism for machine translation. How to implement the attention mechanism step-by-step\n# \u301017\u2020A simple overview of RNN, LSTM and Attention Mechanism\u2020medium.com\u3011\nA simple NN. An RNN feeds it\u2019s output to itself at next time-step, forming a loop, passing down much needed information. RNN feeding hidden state value to itself. To better understand the flow ...\n# \u301018\u2020Keras - Add attention mechanism to an LSTM model\u2020stackoverflow.com\u3011\nClosed 3 years ago. With the following code: LSTM(16, batch_input_shape=(None, num_samples, num_features), return_sequences=True, activation='tanh')) I'm trying to understand how can I add an attention mechanism before the first LSTM layer. I've found the following GitHub: keras-attention-mechanism by Philippe R\u00e9my but couldn't figure out how ...\n# \u301019\u2020lstm - Does attention make sense for Autoencoders? - Stack Overflow\u2020stackoverflow.com\u3011\nYes, but that seems to defeat the entire point of attention to begin with. Attention is about knowing which hidden states are relevant given the context. Adding a linear dimension will perform a static choice of importance. And given the recursive nature of an LSTM, the first hidden layer should be optimal for the recursion during decoding.\nVisible: 0% - 100%","summary":null,"assets":[],"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"cloud_doc_urls":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"Attention in Long Short-Term Memory Recurrent Neural Networks","url":"https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/","text":"\nIn this post, you discovered the attention mechanism for sequence prediction problems with LSTM recurrent neural networks. Specifically, you learned: That the encoder-decoder architecture for recurrent neural networks uses a fixed-length internal representation that imposes a constraint that limits learning very long sequences.\n","pub_date":"2024-06-22T11:07:00.0000000Z","extra":null},{"type":"webpage","title":"attention-lstm \u00b7 GitHub Topics \u00b7 GitHub","url":"https://github.com/topics/attention-lstm","text":"\nnlp tensorflow attention-mechanism bidirectional-lstm attention-model image2latex im2latex attention-lstm webdemo im2markup Updated Apr 1, 2017; CSS; kaelzhang / DA-RNN-in-Tensorflow-2-and-PyTorch Star 30. Code Issues Pull requests A Tensorflow 2 (Keras) implementation of DA-RNN (A Dual-Stage Attention-Based Recurrent Neural Network for Time ...\n","pub_date":"2024-06-19T14:26:00.0000000Z","extra":null},{"type":"webpage","title":"Bidirectional LSTM with attention mechanism and ... - ScienceDirect","url":"https://www.sciencedirect.com/science/article/pii/S0925231219301067","text":"\nThe combination of LSTM and attention mechanism can obtain better results. Especially, for sequence problems, attention mechanism has been used successfully in a variety of tasks including text classification, reading comprehension and so on. Yang et al. [42] proposed a hierarchical attention network for document classification. The model has ...\n","pub_date":"2024-06-18T16:34:00.0000000Z","extra":null},{"type":"webpage","title":"The Attention Mechanism from Scratch - Machine Learning Mastery","url":"https://machinelearningmastery.com/the-attention-mechanism-from-scratch/","text":"\nThe Attention Mechanism from Scratch. By Stefania Cristina on January 6, 2023 in Attention 27. The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation. The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a ...\n","pub_date":"2024-06-20T23:34:00.0000000Z","extra":null},{"type":"webpage","title":"Advanced Stock Pattern Prediction using LSTM with the Attention ...","url":"https://drlee.io/advanced-stock-pattern-prediction-using-lstm-with-the-attention-mechanism-in-tensorflow-a-step-by-143a2e8b0e95","text":"\nIn this guide, we explored the complex yet fascinating task of using LSTM networks with an attention mechanism for stock price prediction, specifically for Apple Inc. (AAPL). Key points include: LSTM\u2019s ability to capture long-term dependencies in time-series data. The added advantage of the attention mechanism in focusing on relevant data points.\n","pub_date":"2024-06-21T08:09:00.0000000Z","extra":null},{"type":"webpage","title":"Improving time series forecasting using LSTM and attention models ...","url":"https://link.springer.com/article/10.1007/s12652-020-02761-x","text":"\nIt reaches the best performance. The results demonstrated the predictive power of the attention mechanism in time series forecasting. Also, the two sets of experiments, Part 1 and Part 2, indicated that the proposed method of attention mechanism and LSTM outperformed state-of-the-art methods in time series forecasting including ARIMA, ETS, and MLP.\n","pub_date":"2024-06-20T06:32:00.0000000Z","extra":null},{"type":"webpage","title":"attention-based LSTM/Dense implemented by Keras - GitHub","url":"https://github.com/ningshixian/LSTM_Attention","text":"\nLSTM_Attention. X = Input Sequence of length n. H = LSTM(X); Note that here the LSTM has return_sequences = True, so H is a sequence of vectors of length n. s is the hidden state of the LSTM (h and c) h is a weighted sum over H: \u52a0\u6743\u548c. h = sigma(j = 0 to n-1) alpha(j) * H(j)\n","pub_date":"2024-06-18T02:47:00.0000000Z","extra":null},{"type":"webpage","title":"Exploring the predictability of attention mechanism with LSTM: Evidence ...","url":"https://www.sciencedirect.com/science/article/pii/S0275531923001460","text":"\n3.2. Theory of attention mechanism. The attention mechanism initially proposed in neuroscience and computational neuroscience in Desimone and Duncan (1995) and Itti et al. (1998) has gained widespread attention since its initiation. This popular mechanism comes from the fact that animals in nature only focus on specific parts of their visual view to provide adequate responses.\n","pub_date":"2024-06-06T23:22:00.0000000Z","extra":null},{"type":"webpage","title":"Attention-based LSTM predictive model for the attitude and position of ...","url":"https://www.sciencedirect.com/science/article/pii/S2467967423000880","text":"\nThe attention mechanism is realized by reserving the intermediate output of the LSTM encoder for the input sequence, then training a model to learn these inputs selectively and associating the output sequence with them when the model outputs (Liu et al., 2019, Luo et al., 2021).\n","pub_date":"2024-06-18T23:11:00.0000000Z","extra":null},{"type":"webpage","title":"Bi-LS-AttM: A Bidirectional LSTM and Attention Mechanism Model for ...","url":"https://www.mdpi.com/2076-3417/13/13/7916","text":"\nThe discipline of automatic image captioning represents an integration of two pivotal branches of artificial intelligence, namely computer vision (CV) and natural language processing (NLP). The principal functionality of this technology lies in transmuting the extracted visual features into semantic information of a higher order. The bidirectional long short-term memory (Bi-LSTM) has garnered ...\n","pub_date":"2024-06-20T16:14:00.0000000Z","extra":null},{"type":"webpage","title":"Intuitive Understanding of Attention Mechanism in Deep Learning","url":"https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f","text":"\nThe attention mechanism was born to resolve this problem. Let\u2019s break this down into finer details. ... Using LSTM layers in place of GRU and adding Bidirectional wrapper on the encoder will also help in improved performance. Deep Learning models are generally considered as black boxes, meaning that they do not have the ability to explain ...\n","pub_date":"2024-06-21T05:26:00.0000000Z","extra":null},{"type":"webpage","title":"Hands-On Guide to Bi-LSTM With Attention \u2013 AIM - Analytics India Magazine","url":"https://analyticsindiamag.com/hands-on-guide-to-bi-lstm-with-attention/","text":"\nBefore the introduction of the attention mechanism the basic LSTM or RNN model was based on an encoder-decoder system. Where encoding is used to process the data for encoding it into a context vector and creates a good summary of the input data. Then this summary goes through the decoding part where in decoding the model understands the data ...\n","pub_date":"2024-06-21T04:14:00.0000000Z","extra":null},{"type":"webpage","title":"Improve the LSTM Trajectory Prediction Accuracy through an Attention ...","url":"https://ieeexplore.ieee.org/document/9813863","text":"\nTo solve this problem, we introduce a two-stage attention mechanism with the LSTM Encoder-Decoder model, which uses the spatial attention mechanism and the output attention mechanism to weight the input hidden layer features and the output prediction of the decoder. In this way, the accuracy of trajectory prediction is improved.\n","pub_date":"2024-06-18T21:33:00.0000000Z","extra":null},{"type":"webpage","title":"[2204.02623] Attention-based CNN-LSTM and XGBoost hybrid model for ...","url":"https://arxiv.org/abs/2204.02623","text":"\nThe pre-training model is the Attention-based CNN-LSTM model based on sequence-to-sequence framework. The model first uses convolution to extract the deep features of the original stock data, and then uses the Long Short-Term Memory networks to mine the long-term time series features. Finally, the XGBoost model is adopted for fine-tuning.\n","pub_date":"2024-06-21T19:48:00.0000000Z","extra":null},{"type":"webpage","title":"Enhancing LSTM Models with Self-Attention and Stateful Training","url":"https://userweb.cs.txstate.edu/~amk181/IntelliSys_Enhancing_LSTM_Self-Attention.pdf","text":"\nAttention mechanisms are designed to resolve these problems. [1,11] Self-attention, also known as intra-attention, is an attention mechanism re-lating di erent positions of a sequence in order to model dependencies between di erent parts of the sequence. This di ers from general attention in that in-\n","pub_date":"2024-06-22T16:48:00.0000000Z","extra":null},{"type":"webpage","title":"Simple LSTM + Attention | Kaggle","url":"https://www.kaggle.com/code/ddw02141/simple-lstm-attention","text":"\nSomething went wrong and this page crashed! If the issue persists, it's likely a problem on our side. Unexpected token < in JSON at position 4. keyboard_arrow_up. content_copy. SyntaxError: Unexpected token < in JSON at position 4. Refresh. Explore and run machine learning code with Kaggle Notebooks | Using data from multiple data sources.\n","pub_date":"2024-06-20T20:47:00.0000000Z","extra":null},{"type":"webpage","title":"How Does Attention Work in Encoder-Decoder Recurrent Neural Networks","url":"https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/","text":"\nAttention is a mechanism that was developed to improve the performance of the Encoder-Decoder RNN on machine translation. In this tutorial, you will discover the attention mechanism for the Encoder-Decoder model. After completing this tutorial, you will know: About the Encoder-Decoder model and attention mechanism for machine translation. How to implement the attention mechanism step-by-step\n","pub_date":"2024-06-19T17:31:00.0000000Z","extra":null},{"type":"webpage","title":"A simple overview of RNN, LSTM and Attention Mechanism","url":"https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b","text":"\nA simple NN. An RNN feeds it\u2019s output to itself at next time-step, forming a loop, passing down much needed information. RNN feeding hidden state value to itself. To better understand the flow ...\n","pub_date":"2024-06-19T01:50:00.0000000Z","extra":null},{"type":"webpage","title":"Keras - Add attention mechanism to an LSTM model","url":"https://stackoverflow.com/questions/53151209/keras-add-attention-mechanism-to-an-lstm-model","text":"\nClosed 3 years ago. With the following code: LSTM(16, batch_input_shape=(None, num_samples, num_features), return_sequences=True, activation='tanh')) I'm trying to understand how can I add an attention mechanism before the first LSTM layer. I've found the following GitHub: keras-attention-mechanism by Philippe R\u00e9my but couldn't figure out how ...\n","pub_date":"2024-06-21T15:12:00.0000000Z","extra":null},{"type":"webpage","title":"lstm - Does attention make sense for Autoencoders? - Stack Overflow","url":"https://stackoverflow.com/questions/58145570/does-attention-make-sense-for-autoencoders","text":"\nYes, but that seems to defeat the entire point of attention to begin with. Attention is about knowing which hidden states are relevant given the context. Adding a linear dimension will perform a static choice of importance. And given the recursive nature of an LSTM, the first hidden layer should be optimal for the recursion during decoding.\nVisible: 0% - 100%","pub_date":"2024-06-20T23:02:00.0000000Z","extra":null},{"type":"webpage","title":"Search results for query: 'xLSTM with attention mechanism'","url":"","text":"# \u30100\u2020Attention in Long Short-Term Memory Recurrent Neural Networks\u2020machinelearningmastery.com\u3011\nIn this post, you discovered the attention mechanism for sequence prediction problems with LSTM recurrent neural networks. Specifically, you learned: That the encoder-decoder architecture for recurrent neural networks uses a fixed-length internal representation that imposes a constraint that limits learning very long sequences.\n# \u30101\u2020attention-lstm \u00b7 GitHub Topics \u00b7 GitHub\u2020github.com\u3011\nnlp tensorflow attention-mechanism bidirectional-lstm attention-model image2latex im2latex attention-lstm webdemo im2markup Updated Apr 1, 2017; CSS; kaelzhang / DA-RNN-in-Tensorflow-2-and-PyTorch Star 30. Code Issues Pull requests A Tensorflow 2 (Keras) implementation of DA-RNN (A Dual-Stage Attention-Based Recurrent Neural Network for Time ...\n# \u30102\u2020Bidirectional LSTM with attention mechanism and ... - ScienceDirect\u2020www.sciencedirect.com\u3011\nThe combination of LSTM and attention mechanism can obtain better results. Especially, for sequence problems, attention mechanism has been used successfully in a variety of tasks including text classification, reading comprehension and so on. Yang et al. [42] proposed a hierarchical attention network for document classification. The model has ...\n# \u30103\u2020The Attention Mechanism from Scratch - Machine Learning Mastery\u2020machinelearningmastery.com\u3011\nThe Attention Mechanism from Scratch. By Stefania Cristina on January 6, 2023 in Attention 27. The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation. The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a ...\n# \u30104\u2020Advanced Stock Pattern Prediction using LSTM with the Attention ...\u2020drlee.io\u3011\nIn this guide, we explored the complex yet fascinating task of using LSTM networks with an attention mechanism for stock price prediction, specifically for Apple Inc. (AAPL). Key points include: LSTM\u2019s ability to capture long-term dependencies in time-series data. The added advantage of the attention mechanism in focusing on relevant data points.\n# \u30105\u2020Improving time series forecasting using LSTM and attention models ...\u2020link.springer.com\u3011\nIt reaches the best performance. The results demonstrated the predictive power of the attention mechanism in time series forecasting. Also, the two sets of experiments, Part 1 and Part 2, indicated that the proposed method of attention mechanism and LSTM outperformed state-of-the-art methods in time series forecasting including ARIMA, ETS, and MLP.\n# \u30106\u2020attention-based LSTM/Dense implemented by Keras - GitHub\u2020github.com\u3011\nLSTM_Attention. X = Input Sequence of length n. H = LSTM(X); Note that here the LSTM has return_sequences = True, so H is a sequence of vectors of length n. s is the hidden state of the LSTM (h and c) h is a weighted sum over H: \u52a0\u6743\u548c. h = sigma(j = 0 to n-1) alpha(j) * H(j)\n# \u30107\u2020Exploring the predictability of attention mechanism with LSTM: Evidence ...\u2020www.sciencedirect.com\u3011\n3.2. Theory of attention mechanism. The attention mechanism initially proposed in neuroscience and computational neuroscience in Desimone and Duncan (1995) and Itti et al. (1998) has gained widespread attention since its initiation. This popular mechanism comes from the fact that animals in nature only focus on specific parts of their visual view to provide adequate responses.\n# \u30108\u2020Attention-based LSTM predictive model for the attitude and position of ...\u2020www.sciencedirect.com\u3011\nThe attention mechanism is realized by reserving the intermediate output of the LSTM encoder for the input sequence, then training a model to learn these inputs selectively and associating the output sequence with them when the model outputs (Liu et al., 2019, Luo et al., 2021).\n# \u30109\u2020Bi-LS-AttM: A Bidirectional LSTM and Attention Mechanism Model for ...\u2020www.mdpi.com\u3011\nThe discipline of automatic image captioning represents an integration of two pivotal branches of artificial intelligence, namely computer vision (CV) and natural language processing (NLP). The principal functionality of this technology lies in transmuting the extracted visual features into semantic information of a higher order. The bidirectional long short-term memory (Bi-LSTM) has garnered ...\n# \u301010\u2020Intuitive Understanding of Attention Mechanism in Deep Learning\u2020towardsdatascience.com\u3011\nThe attention mechanism was born to resolve this problem. Let\u2019s break this down into finer details. ... Using LSTM layers in place of GRU and adding Bidirectional wrapper on the encoder will also help in improved performance. Deep Learning models are generally considered as black boxes, meaning that they do not have the ability to explain ...\n# \u301011\u2020Hands-On Guide to Bi-LSTM With Attention \u2013 AIM - Analytics India Magazine\u2020analyticsindiamag.com\u3011\nBefore the introduction of the attention mechanism the basic LSTM or RNN model was based on an encoder-decoder system. Where encoding is used to process the data for encoding it into a context vector and creates a good summary of the input data. Then this summary goes through the decoding part where in decoding the model understands the data ...\n# \u301012\u2020Improve the LSTM Trajectory Prediction Accuracy through an Attention ...\u2020ieeexplore.ieee.org\u3011\nTo solve this problem, we introduce a two-stage attention mechanism with the LSTM Encoder-Decoder model, which uses the spatial attention mechanism and the output attention mechanism to weight the input hidden layer features and the output prediction of the decoder. In this way, the accuracy of trajectory prediction is improved.\n# \u301013\u2020[2204.02623] Attention-based CNN-LSTM and XGBoost hybrid model for ...\u2020arxiv.org\u3011\nThe pre-training model is the Attention-based CNN-LSTM model based on sequence-to-sequence framework. The model first uses convolution to extract the deep features of the original stock data, and then uses the Long Short-Term Memory networks to mine the long-term time series features. Finally, the XGBoost model is adopted for fine-tuning.\n# \u301014\u2020Enhancing LSTM Models with Self-Attention and Stateful Training\u2020userweb.cs.txstate.edu\u3011\nAttention mechanisms are designed to resolve these problems. [1,11] Self-attention, also known as intra-attention, is an attention mechanism re-lating di erent positions of a sequence in order to model dependencies between di erent parts of the sequence. This di ers from general attention in that in-\n# \u301015\u2020Simple LSTM + Attention | Kaggle\u2020www.kaggle.com\u3011\nSomething went wrong and this page crashed! If the issue persists, it's likely a problem on our side. Unexpected token < in JSON at position 4. keyboard_arrow_up. content_copy. SyntaxError: Unexpected token < in JSON at position 4. Refresh. Explore and run machine learning code with Kaggle Notebooks | Using data from multiple data sources.\n# \u301016\u2020How Does Attention Work in Encoder-Decoder Recurrent Neural Networks\u2020machinelearningmastery.com\u3011\nAttention is a mechanism that was developed to improve the performance of the Encoder-Decoder RNN on machine translation. In this tutorial, you will discover the attention mechanism for the Encoder-Decoder model. After completing this tutorial, you will know: About the Encoder-Decoder model and attention mechanism for machine translation. How to implement the attention mechanism step-by-step\n# \u301017\u2020A simple overview of RNN, LSTM and Attention Mechanism\u2020medium.com\u3011\nA simple NN. An RNN feeds it\u2019s output to itself at next time-step, forming a loop, passing down much needed information. RNN feeding hidden state value to itself. To better understand the flow ...\n# \u301018\u2020Keras - Add attention mechanism to an LSTM model\u2020stackoverflow.com\u3011\nClosed 3 years ago. With the following code: LSTM(16, batch_input_shape=(None, num_samples, num_features), return_sequences=True, activation='tanh')) I'm trying to understand how can I add an attention mechanism before the first LSTM layer. I've found the following GitHub: keras-attention-mechanism by Philippe R\u00e9my but couldn't figure out how ...\n# \u301019\u2020lstm - Does attention make sense for Autoencoders? - Stack Overflow\u2020stackoverflow.com\u3011\nYes, but that seems to defeat the entire point of attention to begin with. Attention is about knowing which hidden states are relevant given the context. Adding a linear dimension will perform a static choice of importance. And given the recursive nature of an LSTM, the first hidden layer should be optimal for the recursion during decoding.\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"search","args":["xLSTM with attention mechanism"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"cc0a3017-1612-43a2-8eab-7e7270e02e44","request_id":"8982a5450e123f4d-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"cc0a3017-1612-43a2-8eab-7e7270e02e44","children":["1a9cda43-870f-4d3f-b2b4-bd1124a95b34"]},"1a9cda43-870f-4d3f-b2b4-bd1124a95b34":{"id":"1a9cda43-870f-4d3f-b2b4-bd1124a95b34","message":{"id":"1a9cda43-870f-4d3f-b2b4-bd1124a95b34","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719126499.506008,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"mclick([2, 4, 5, 8, 10])"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"873b721c-c098-4fb3-a00b-892243013f3b","request_id":"8982a5450e123f4d-SIN","timestamp_":"absolute","can_save":true},"recipient":"browser","channel":null},"parent":"873b721c-c098-4fb3-a00b-892243013f3b","children":["1fb3af2a-f8cd-4752-9835-bbdc37914d2c"]},"1fb3af2a-f8cd-4752-9835-bbdc37914d2c":{"id":"1fb3af2a-f8cd-4752-9835-bbdc37914d2c","message":{"id":"1fb3af2a-f8cd-4752-9835-bbdc37914d2c","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1719126499.506047,"update_time":null,"content":{"content_type":"tether_quote","url":"https://link.springer.com/article/10.1007/s12652-020-02761-x","domain":"link.springer.com","text":"\n\u301060\u2020Article\u2020doi.org\u3011\u00a0 \u301061\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2018a) A new prediction model based on multi-block forecast engine in smart grid. J Ambient Intell Human Comput 9:1873\u20131888. \u301062\u2020https://doi.org/10.1007/s12652-017-0648-4\u2020doi.org\u3011\n\n\u301063\u2020Article\u3011\u00a0 \u301064\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2018b) Two stage forecast engine with feature selection technique and improved meta-heuristic algorithm for electricity load forecasting. Energy 161:130\u2013142. \u301065\u2020https://doi.org/10.1016/j.energy.2018.07.088\u2020doi.org\u3011\n\n\u301066\u2020Article\u2020doi.org\u3011\u00a0 \u301067\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2019) Application of a new hybrid forecast engine with feature selection algorithm in a power system. Int J Ambient Energy 40:494\u2013503. \u301068\u2020https://doi.org/10.1080/01430750.2017.1412350\u2020doi.org\u3011\n\n\u301069\u2020Article\u2020doi.org\u3011\u00a0 \u301070\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Graves A (2013) Generating sequences with recurrent neural networks. \u301071\u2020https://arxiv.org/\u2020arxiv.org\u3011. Accessed January 10, 2020\n\n  * Gundu V, Simon SP (2020) PSO\u2013LSTM for short term forecast of heterogeneous time series electricity price signals. J Ambient Intell Human Comput. \u301072\u2020https://doi.org/10.1007/s12652-020-02353-9\u2020doi.org\u3011\n\n\u301073\u2020Article\u3011\u00a0 \u301074\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9:1735\u20131780\n\n\u301075\u2020Article\u2020doi.org\u3011\u00a0 \u301076\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Hyndman R, Koehler AB, Ord JK, Snyder RD (2008) Forecasting with exponential smoothing: the state space approach. Springer, Berlin, Heidelberg. \u301077\u2020https://doi.org/10.1007/978-3-540-71918-2\u2020doi.org\u3011\n\n  * Khandelwal I, Adhikari R, Verma G (2015) Time series forecasting using hybrid ARIMA and ANN models based on DWT decomposition. Procedia Comput Sci 48:173\u2013179\n\n\u301078\u2020Article\u2020doi.org\u3011\u00a0 \u301079\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Khashei M, Bijari M (2011) A novel hybridization of artificial neural networks and ARIMA models for time series forecasting. Appl Soft Comput 11:2664\u20132675\n\n\u301080\u2020Article\u2020doi.org\u3011\u00a0 \u301081\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kim J, Moon N (2019) BiLSTM model based on multivariate time series data in multiple field for forecasting trading area. J Ambient Intell Human Comput. \u301082\u2020https://doi.org/10.1007/s12652-019-01398-9\u2020doi.org\u3011\n\n\u301083\u2020Article\u3011\u00a0 \u301084\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. arXiv preprint arXiv: 14126980\n\n  * Kulshrestha A, Krishnaswamy V, Sharma M (2020) Bayesian BILSTM approach for tourism demand forecasting. Ann Tourism Res 83:102925. \u301085\u2020https://doi.org/10.1016/j.annals.2020.102925\u2020doi.org\u3011\n\n\u301086\u2020Article\u2020doi.org\u3011\u00a0 \u301087\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kumaresan K, Ganeshkumar P (2020) Software reliability prediction model with realistic assumption using time series (S)ARIMA model. J Ambient Intell Human Comput. \u301088\u2020https://doi.org/10.1007/s12652-020-01912-4\u2020doi.org\u3011\n\n\u301089\u2020Article\u3011\u00a0 \u301090\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Law T, Shawe-Taylor J (2017) Practical Bayesian support vector regression for financial time series prediction and market condition change detection. Quant Financ 17:1403\u20131416\n\n\u301091\u2020Article\u2020doi.org\u3011\u00a0 \u301092\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301093\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Law R, Li G, Fong DKC, Han X (2019) Tourism demand forecasting: a deep learning approach. Ann Tourism Res 75:410\u2013423\n\n\u301094\u2020Article\u2020doi.org\u3011\u00a0 \u301095\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Li J, Tu Z, Yang B, Lyu MR, Zhang T (2018) Multi-head attention with disagreement regularization. Paper presented at the 2018 conference on empirical methods in natural language processing. Belgium, Brussels\n\n\u301096\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ma","title":"Improving time series forecasting using LSTM and attention models | Journal of Ambient Intelligence and Humanized Computing","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og"},"metadata_list":[{"type":"webpage","title":"Improving time series forecasting using LSTM and attention models | Journal of Ambient Intelligence and Humanized Computing","url":"https://link.springer.com/article/10.1007/s12652-020-02761-x","text":"\n\u301060\u2020Article\u2020doi.org\u3011\u00a0 \u301061\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2018a) A new prediction model based on multi-block forecast engine in smart grid. J Ambient Intell Human Comput 9:1873\u20131888. \u301062\u2020https://doi.org/10.1007/s12652-017-0648-4\u2020doi.org\u3011\n\n\u301063\u2020Article\u3011\u00a0 \u301064\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2018b) Two stage forecast engine with feature selection technique and improved meta-heuristic algorithm for electricity load forecasting. Energy 161:130\u2013142. \u301065\u2020https://doi.org/10.1016/j.energy.2018.07.088\u2020doi.org\u3011\n\n\u301066\u2020Article\u2020doi.org\u3011\u00a0 \u301067\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2019) Application of a new hybrid forecast engine with feature selection algorithm in a power system. Int J Ambient Energy 40:494\u2013503. \u301068\u2020https://doi.org/10.1080/01430750.2017.1412350\u2020doi.org\u3011\n\n\u301069\u2020Article\u2020doi.org\u3011\u00a0 \u301070\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Graves A (2013) Generating sequences with recurrent neural networks. \u301071\u2020https://arxiv.org/\u2020arxiv.org\u3011. Accessed January 10, 2020\n\n  * Gundu V, Simon SP (2020) PSO\u2013LSTM for short term forecast of heterogeneous time series electricity price signals. J Ambient Intell Human Comput. \u301072\u2020https://doi.org/10.1007/s12652-020-02353-9\u2020doi.org\u3011\n\n\u301073\u2020Article\u3011\u00a0 \u301074\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9:1735\u20131780\n\n\u301075\u2020Article\u2020doi.org\u3011\u00a0 \u301076\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Hyndman R, Koehler AB, Ord JK, Snyder RD (2008) Forecasting with exponential smoothing: the state space approach. Springer, Berlin, Heidelberg. \u301077\u2020https://doi.org/10.1007/978-3-540-71918-2\u2020doi.org\u3011\n\n  * Khandelwal I, Adhikari R, Verma G (2015) Time series forecasting using hybrid ARIMA and ANN models based on DWT decomposition. Procedia Comput Sci 48:173\u2013179\n\n\u301078\u2020Article\u2020doi.org\u3011\u00a0 \u301079\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Khashei M, Bijari M (2011) A novel hybridization of artificial neural networks and ARIMA models for time series forecasting. Appl Soft Comput 11:2664\u20132675\n\n\u301080\u2020Article\u2020doi.org\u3011\u00a0 \u301081\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kim J, Moon N (2019) BiLSTM model based on multivariate time series data in multiple field for forecasting trading area. J Ambient Intell Human Comput. \u301082\u2020https://doi.org/10.1007/s12652-019-01398-9\u2020doi.org\u3011\n\n\u301083\u2020Article\u3011\u00a0 \u301084\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. arXiv preprint arXiv: 14126980\n\n  * Kulshrestha A, Krishnaswamy V, Sharma M (2020) Bayesian BILSTM approach for tourism demand forecasting. Ann Tourism Res 83:102925. \u301085\u2020https://doi.org/10.1016/j.annals.2020.102925\u2020doi.org\u3011\n\n\u301086\u2020Article\u2020doi.org\u3011\u00a0 \u301087\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kumaresan K, Ganeshkumar P (2020) Software reliability prediction model with realistic assumption using time series (S)ARIMA model. J Ambient Intell Human Comput. \u301088\u2020https://doi.org/10.1007/s12652-020-01912-4\u2020doi.org\u3011\n\n\u301089\u2020Article\u3011\u00a0 \u301090\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Law T, Shawe-Taylor J (2017) Practical Bayesian support vector regression for financial time series prediction and market condition change detection. Quant Financ 17:1403\u20131416\n\n\u301091\u2020Article\u2020doi.org\u3011\u00a0 \u301092\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301093\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Law R, Li G, Fong DKC, Han X (2019) Tourism demand forecasting: a deep learning approach. Ann Tourism Res 75:410\u2013423\n\n\u301094\u2020Article\u2020doi.org\u3011\u00a0 \u301095\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Li J, Tu Z, Yang B, Lyu MR, Zhang T (2018) Multi-head attention with disagreement regularization. Paper presented at the 2018 conference on empirical methods in natural language processing. Belgium, Brussels\n\n\u301096\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ma","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[2, 4, 5, 8, 10]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"1a9cda43-870f-4d3f-b2b4-bd1124a95b34","request_id":"8982a5450e123f4d-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"1a9cda43-870f-4d3f-b2b4-bd1124a95b34","children":["cadab95b-f83f-4462-b2f5-0c0d2e5627de"]},"cadab95b-f83f-4462-b2f5-0c0d2e5627de":{"id":"cadab95b-f83f-4462-b2f5-0c0d2e5627de","message":{"id":"cadab95b-f83f-4462-b2f5-0c0d2e5627de","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1719126499.506083,"update_time":null,"content":{"content_type":"text","parts":["Yes, there are implementations of xLSTM that incorporate attention mechanisms. These hybrid models combine the strengths of xLSTM's advanced memory structures with the flexibility and focus provided by attention mechanisms. This integration enhances the model's ability to handle complex sequences and long-range dependencies effectively.\n\nFor instance, in time series forecasting and natural language processing, xLSTM models with attention mechanisms have shown improved performance by selectively focusing on relevant parts of the input data, thus improving accuracy and efficiency\u301032\u2020source\u3011\u301010\u2020source\u3011."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[{"start_ix":584,"end_ix":595,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Improving time series forecasting using LSTM and attention models | Journal of Ambient Intelligence and Humanized Computing","url":"https://link.springer.com/article/10.1007/s12652-020-02761-x","text":"\n\u301060\u2020Article\u2020doi.org\u3011\u00a0 \u301061\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2018a) A new prediction model based on multi-block forecast engine in smart grid. J Ambient Intell Human Comput 9:1873\u20131888. \u301062\u2020https://doi.org/10.1007/s12652-017-0648-4\u2020doi.org\u3011\n\n\u301063\u2020Article\u3011\u00a0 \u301064\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2018b) Two stage forecast engine with feature selection technique and improved meta-heuristic algorithm for electricity load forecasting. Energy 161:130\u2013142. \u301065\u2020https://doi.org/10.1016/j.energy.2018.07.088\u2020doi.org\u3011\n\n\u301066\u2020Article\u2020doi.org\u3011\u00a0 \u301067\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2019) Application of a new hybrid forecast engine with feature selection algorithm in a power system. Int J Ambient Energy 40:494\u2013503. \u301068\u2020https://doi.org/10.1080/01430750.2017.1412350\u2020doi.org\u3011\n\n\u301069\u2020Article\u2020doi.org\u3011\u00a0 \u301070\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Graves A (2013) Generating sequences with recurrent neural networks. \u301071\u2020https://arxiv.org/\u2020arxiv.org\u3011. Accessed January 10, 2020\n\n  * Gundu V, Simon SP (2020) PSO\u2013LSTM for short term forecast of heterogeneous time series electricity price signals. J Ambient Intell Human Comput. \u301072\u2020https://doi.org/10.1007/s12652-020-02353-9\u2020doi.org\u3011\n\n\u301073\u2020Article\u3011\u00a0 \u301074\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9:1735\u20131780\n\n\u301075\u2020Article\u2020doi.org\u3011\u00a0 \u301076\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Hyndman R, Koehler AB, Ord JK, Snyder RD (2008) Forecasting with exponential smoothing: the state space approach. Springer, Berlin, Heidelberg. \u301077\u2020https://doi.org/10.1007/978-3-540-71918-2\u2020doi.org\u3011\n\n  * Khandelwal I, Adhikari R, Verma G (2015) Time series forecasting using hybrid ARIMA and ANN models based on DWT decomposition. Procedia Comput Sci 48:173\u2013179\n\n\u301078\u2020Article\u2020doi.org\u3011\u00a0 \u301079\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Khashei M, Bijari M (2011) A novel hybridization of artificial neural networks and ARIMA models for time series forecasting. Appl Soft Comput 11:2664\u20132675\n\n\u301080\u2020Article\u2020doi.org\u3011\u00a0 \u301081\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kim J, Moon N (2019) BiLSTM model based on multivariate time series data in multiple field for forecasting trading area. J Ambient Intell Human Comput. \u301082\u2020https://doi.org/10.1007/s12652-019-01398-9\u2020doi.org\u3011\n\n\u301083\u2020Article\u3011\u00a0 \u301084\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. arXiv preprint arXiv: 14126980\n\n  * Kulshrestha A, Krishnaswamy V, Sharma M (2020) Bayesian BILSTM approach for tourism demand forecasting. Ann Tourism Res 83:102925. \u301085\u2020https://doi.org/10.1016/j.annals.2020.102925\u2020doi.org\u3011\n\n\u301086\u2020Article\u2020doi.org\u3011\u00a0 \u301087\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kumaresan K, Ganeshkumar P (2020) Software reliability prediction model with realistic assumption using time series (S)ARIMA model. J Ambient Intell Human Comput. \u301088\u2020https://doi.org/10.1007/s12652-020-01912-4\u2020doi.org\u3011\n\n\u301089\u2020Article\u3011\u00a0 \u301090\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Law T, Shawe-Taylor J (2017) Practical Bayesian support vector regression for financial time series prediction and market condition change detection. Quant Financ 17:1403\u20131416\n\n\u301091\u2020Article\u2020doi.org\u3011\u00a0 \u301092\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301093\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Law R, Li G, Fong DKC, Han X (2019) Tourism demand forecasting: a deep learning approach. Ann Tourism Res 75:410\u2013423\n\n\u301094\u2020Article\u2020doi.org\u3011\u00a0 \u301095\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Li J, Tu Z, Yang B, Lyu MR, Zhang T (2018) Multi-head attention with disagreement regularization. Paper presented at the 2018 conference on empirical methods in natural language processing. Belgium, Brussels\n\n\u301096\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ma","pub_date":null,"extra":{"cited_message_idx":32,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}},{"start_ix":595,"end_ix":606,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2405.04517] Untitled Document","url":"https://ar5iv.org/pdf/2405.04517#:~:text=URL%3A%20https%3A%2F%2Far5iv,100","text":"\nURL: https://ar5iv.org/pdf/2405.04517\nConversion to HTML had a Fatal error and exited abruptly. This document may be truncated or damaged. \n\n\u30100\u2020\u25c4\u3011 [Image 0: ar5iv homepage] \u30101\u2020Feeling lucky?\u3011 \u30102\u2020Conversion report\u3011 \u30103\u2020Report an issue\u2020github.com\u3011 \u30104\u2020View original on arXiv\u2020arxiv.org\u3011\u30105\u2020\u25ba\u3011 \n\n\u30106\u2020Copyright\u2020arxiv.org\u3011 \u30107\u2020Privacy Policy\u2020arxiv.org\u3011 \n\nGenerated on Wed Jun 5 13:43:55 2024 by \u30108\u2020L a T e XML\u2020dlmf.nist.gov\u3011\nVisible: 0% - 100%","pub_date":null,"extra":{"cited_message_idx":10,"search_result_idx":null,"evidence_text":"source","cloud_doc_url":null}}}],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","pad":"AAAAAAAAAAAAAAAAAAAAAAAAAAA","parent_id":"1fb3af2a-f8cd-4752-9835-bbdc37914d2c","request_id":"8982a5450e123f4d-SIN","timestamp_":"absolute","content_references":[{"matched_text":"\u301032\u2020source\u3011","start_idx":584,"end_idx":595,"alt":null,"type":"webpage_extended","title":"Improving time series forecasting using LSTM and attention models | Journal of Ambient Intelligence and Humanized Computing","url":"https://link.springer.com/article/10.1007/s12652-020-02761-x","pub_date":null,"snippet":"\n\u301060\u2020Article\u2020doi.org\u3011\u00a0 \u301061\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2018a) A new prediction model based on multi-block forecast engine in smart grid. J Ambient Intell Human Comput 9:1873\u20131888. \u301062\u2020https://doi.org/10.1007/s12652-017-0648-4\u2020doi.org\u3011\n\n\u301063\u2020Article\u3011\u00a0 \u301064\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2018b) Two stage forecast engine with feature selection technique and improved meta-heuristic algorithm for electricity load forecasting. Energy 161:130\u2013142. \u301065\u2020https://doi.org/10.1016/j.energy.2018.07.088\u2020doi.org\u3011\n\n\u301066\u2020Article\u2020doi.org\u3011\u00a0 \u301067\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ghadimi N, Akbarimajd A, Shayeghi H, Abedinia O (2019) Application of a new hybrid forecast engine with feature selection algorithm in a power system. Int J Ambient Energy 40:494\u2013503. \u301068\u2020https://doi.org/10.1080/01430750.2017.1412350\u2020doi.org\u3011\n\n\u301069\u2020Article\u2020doi.org\u3011\u00a0 \u301070\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Graves A (2013) Generating sequences with recurrent neural networks. \u301071\u2020https://arxiv.org/\u2020arxiv.org\u3011. Accessed January 10, 2020\n\n  * Gundu V, Simon SP (2020) PSO\u2013LSTM for short term forecast of heterogeneous time series electricity price signals. J Ambient Intell Human Comput. \u301072\u2020https://doi.org/10.1007/s12652-020-02353-9\u2020doi.org\u3011\n\n\u301073\u2020Article\u3011\u00a0 \u301074\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9:1735\u20131780\n\n\u301075\u2020Article\u2020doi.org\u3011\u00a0 \u301076\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Hyndman R, Koehler AB, Ord JK, Snyder RD (2008) Forecasting with exponential smoothing: the state space approach. Springer, Berlin, Heidelberg. \u301077\u2020https://doi.org/10.1007/978-3-540-71918-2\u2020doi.org\u3011\n\n  * Khandelwal I, Adhikari R, Verma G (2015) Time series forecasting using hybrid ARIMA and ANN models based on DWT decomposition. Procedia Comput Sci 48:173\u2013179\n\n\u301078\u2020Article\u2020doi.org\u3011\u00a0 \u301079\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Khashei M, Bijari M (2011) A novel hybridization of artificial neural networks and ARIMA models for time series forecasting. Appl Soft Comput 11:2664\u20132675\n\n\u301080\u2020Article\u2020doi.org\u3011\u00a0 \u301081\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kim J, Moon N (2019) BiLSTM model based on multivariate time series data in multiple field for forecasting trading area. J Ambient Intell Human Comput. \u301082\u2020https://doi.org/10.1007/s12652-019-01398-9\u2020doi.org\u3011\n\n\u301083\u2020Article\u3011\u00a0 \u301084\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. arXiv preprint arXiv: 14126980\n\n  * Kulshrestha A, Krishnaswamy V, Sharma M (2020) Bayesian BILSTM approach for tourism demand forecasting. Ann Tourism Res 83:102925. \u301085\u2020https://doi.org/10.1016/j.annals.2020.102925\u2020doi.org\u3011\n\n\u301086\u2020Article\u2020doi.org\u3011\u00a0 \u301087\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Kumaresan K, Ganeshkumar P (2020) Software reliability prediction model with realistic assumption using time series (S)ARIMA model. J Ambient Intell Human Comput. \u301088\u2020https://doi.org/10.1007/s12652-020-01912-4\u2020doi.org\u3011\n\n\u301089\u2020Article\u3011\u00a0 \u301090\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Law T, Shawe-Taylor J (2017) Practical Bayesian support vector regression for financial time series prediction and market condition change detection. Quant Financ 17:1403\u20131416\n\n\u301091\u2020Article\u2020doi.org\u3011\u00a0 \u301092\u2020MathSciNet\u2020www.ams.org\u3011\u00a0 \u301093\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Law R, Li G, Fong DKC, Han X (2019) Tourism demand forecasting: a deep learning approach. Ann Tourism Res 75:410\u2013423\n\n\u301094\u2020Article\u2020doi.org\u3011\u00a0 \u301095\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Li J, Tu Z, Yang B, Lyu MR, Zhang T (2018) Multi-head attention with disagreement regularization. Paper presented at the 2018 conference on empirical methods in natural language processing. Belgium, Brussels\n\n\u301096\u2020 Google Scholar\u2020scholar.google.com\u3011\u00a0\n\n  * Ma","attribution":"link.springer.com","icon_type":null},{"matched_text":"\u301010\u2020source\u3011","start_idx":595,"end_idx":606,"alt":null,"type":"webpage_extended","title":"[2405.04517] Untitled Document","url":"https://ar5iv.org/pdf/2405.04517#:~:text=URL%3A%20https%3A%2F%2Far5iv,100","pub_date":null,"snippet":"\nURL: https://ar5iv.org/pdf/2405.04517\nConversion to HTML had a Fatal error and exited abruptly. This document may be truncated or damaged. \n\n\u30100\u2020\u25c4\u3011 [Image 0: ar5iv homepage] \u30101\u2020Feeling lucky?\u3011 \u30102\u2020Conversion report\u3011 \u30103\u2020Report an issue\u2020github.com\u3011 \u30104\u2020View original on arXiv\u2020arxiv.org\u3011\u30105\u2020\u25ba\u3011 \n\n\u30106\u2020Copyright\u2020arxiv.org\u3011 \u30107\u2020Privacy Policy\u2020arxiv.org\u3011 \n\nGenerated on Wed Jun 5 13:43:55 2024 by \u30108\u2020L a T e XML\u2020dlmf.nist.gov\u3011\nVisible: 0% - 100%","attribution":"ar5iv.org","icon_type":null}],"can_save":true},"recipient":"all","channel":null},"parent":"1fb3af2a-f8cd-4752-9835-bbdc37914d2c","children":[]}}}