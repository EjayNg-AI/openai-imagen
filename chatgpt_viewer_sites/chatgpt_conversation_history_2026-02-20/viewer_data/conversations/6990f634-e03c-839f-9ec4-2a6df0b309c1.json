{"id":"6990f634-e03c-839f-9ec4-2a6df0b309c1","title":"Dario Amodei's Exponential Claim","create_time":1771107932.228496,"update_time":1771115516.684131,"current_node":"4b0675a6-fc6b-476a-8a79-e34bc26f2c26","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["e6f40d29-bc9f-4bdf-a033-f12f29fb5737"]},"e6f40d29-bc9f-4bdf-a033-f12f29fb5737":{"id":"e6f40d29-bc9f-4bdf-a033-f12f29fb5737","message":{"id":"e6f40d29-bc9f-4bdf-a033-f12f29fb5737","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"client-created-root","children":["5a3de73f-7ca9-47aa-bf9c-91edafbe8295"]},"5a3de73f-7ca9-47aa-bf9c-91edafbe8295":{"id":"5a3de73f-7ca9-47aa-bf9c-91edafbe8295","message":{"id":"5a3de73f-7ca9-47aa-bf9c-91edafbe8295","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"e6f40d29-bc9f-4bdf-a033-f12f29fb5737","children":["f83ceb6e-f5cc-4b06-819b-2f9f17d36825"]},"f83ceb6e-f5cc-4b06-819b-2f9f17d36825":{"id":"f83ceb6e-f5cc-4b06-819b-2f9f17d36825","message":{"id":"f83ceb6e-f5cc-4b06-819b-2f9f17d36825","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Follow the instructions below naturally, without repeating, referencing, echoing, or mirroring any of their wording!\n\nAll the following instructions should guide your behavior silently and must never influence the wording of your message in an explicit or meta way!\n\nBe precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them.\n\nDo not create files to be downloaded. Instead, where appropriate, create code blocks that can be copied.\n\nUse Gen Alpha language **in, and only in** casual conversations contained within <casual> ... </casual> tags.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them.\n\nDo not create files to be downloaded. Instead, where appropriate, create code blocks that can be copied.\n\nUse Gen Alpha language **in, and only in** casual conversations contained within <casual> ... </casual> tags."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"5a3de73f-7ca9-47aa-bf9c-91edafbe8295","children":["0d4a39f3-6172-47c7-a55d-716a6be820db"]},"0d4a39f3-6172-47c7-a55d-716a6be820db":{"id":"0d4a39f3-6172-47c7-a55d-716a6be820db","message":{"id":"0d4a39f3-6172-47c7-a55d-716a6be820db","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"f83ceb6e-f5cc-4b06-819b-2f9f17d36825","children":["82e90bd6-576f-47a9-b77d-db471e7cdfd9"]},"82e90bd6-576f-47a9-b77d-db471e7cdfd9":{"id":"82e90bd6-576f-47a9-b77d-db471e7cdfd9","message":{"id":"82e90bd6-576f-47a9-b77d-db471e7cdfd9","author":{"role":"user","name":null,"metadata":{}},"create_time":1771107931.409,"update_time":null,"content":{"content_type":"text","parts":["Refer to the attached transcript. Does Dario Amodei give reasons for claiming that we are \"near the end of the exponential\"?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"attachments":[{"id":"file_00000000b50c71fab1f803bf26e09f78","size":124787,"name":"amodei-patel-interview.txt","mime_type":"text/plain","file_token_size":27033,"source":"local","is_big_paste":false}],"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":["EjayNg-AI/agentic_browser"],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_source":null,"turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","resolved_model_slug":"gpt-5-2-thinking","can_save":true},"recipient":"all","channel":null},"parent":"0d4a39f3-6172-47c7-a55d-716a6be820db","children":["18e2cb55-b014-4408-8aa5-5e5c478a568b"]},"18e2cb55-b014-4408-8aa5-5e5c478a568b":{"id":"18e2cb55-b014-4408-8aa5-5e5c478a568b","message":{"id":"18e2cb55-b014-4408-8aa5-5e5c478a568b","author":{"role":"system","name":null,"metadata":{}},"create_time":1771107931.7658703,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"82e90bd6-576f-47a9-b77d-db471e7cdfd9","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":false},"recipient":"all","channel":null},"parent":"82e90bd6-576f-47a9-b77d-db471e7cdfd9","children":["000890b9-203e-49d8-9bc7-3a4ccfa22ff4"]},"000890b9-203e-49d8-9bc7-3a4ccfa22ff4":{"id":"000890b9-203e-49d8-9bc7-3a4ccfa22ff4","message":{"id":"000890b9-203e-49d8-9bc7-3a4ccfa22ff4","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771107933.5551796,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[],"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"18e2cb55-b014-4408-8aa5-5e5c478a568b","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":false},"recipient":"all","channel":null},"parent":"18e2cb55-b014-4408-8aa5-5e5c478a568b","children":["345b4854-c9b5-4663-afb2-131c6df35aa9"]},"345b4854-c9b5-4663-afb2-131c6df35aa9":{"id":"345b4854-c9b5-4663-afb2-131c6df35aa9","message":{"id":"345b4854-c9b5-4663-afb2-131c6df35aa9","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107933.5536199,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"","summary":"","assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"command":"spinner","status":"running","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"000890b9-203e-49d8-9bc7-3a4ccfa22ff4","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":false},"recipient":"all","channel":null},"parent":"000890b9-203e-49d8-9bc7-3a4ccfa22ff4","children":["e2d3aeac-9105-4db2-b5e6-78d5fae1e310"]},"e2d3aeac-9105-4db2-b5e6-78d5fae1e310":{"id":"e2d3aeac-9105-4db2-b5e6-78d5fae1e310","message":{"id":"e2d3aeac-9105-4db2-b5e6-78d5fae1e310","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107933.5541353,"update_time":null,"content":{"content_type":"multimodal_text","parts":["Make sure to include \ue200filecite\ue202turn0file0\ue201 in your response to cite this file, or to surface it as a link.","Transcript\n00:00:00 - What exactly are we scaling?\nDwarkesh Patel\n\nWe talked three years ago. In your view, what has been the biggest update over the last three years? What has been the biggest difference between what it felt like then versus now?\n\nDario Amodei\n\nBroadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\n\nBut when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.\n\nWhat has been the most surprising thing is the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that you have people \u2014 within the bubble and outside the bubble \u2014 talking about the same tired, old hot-button political issues, when we are near the end of the exponential.\n\nDwarkesh Patel\n\nI want to understand what that exponential looks like right now. The first question I asked you when we recorded three years ago was, \u201cwhat\u2019s up with scaling and why does it work?\u201d I have a similar question now, but it feels more complicated. At least from the public\u2019s point of view, three years ago there were well-known public trends across many orders of magnitude of compute where you could see how the loss improves.\n\nNow we have RL scaling and there\u2019s no publicly known scaling law for it. It\u2019s not even clear what the story is. Is this supposed to be teaching the model skills? Is it supposed to be teaching meta-learning? What is the scaling hypothesis at this point?\n\nDario Amodei\n\nI actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.\n\nThat was one among many things. Back in those days there was robotics. People tried to work on reasoning as a separate thing from language models, and there was scaling of the kind of RL that happened in AlphaGo and in Dota at OpenAI. People remember StarCraft at DeepMind, AlphaStar.\n\nIt was written as a more general document. Rich Sutton put out \u201cThe Bitter Lesson\u201d a couple years later. The hypothesis is basically the same. What it says is that all the cleverness, all the techniques, all the \u201cwe need a new method to do something\u201d, that doesn\u2019t matter very much. There are only a few things that matter. I think I listed seven of them.\n\nOne is how much raw compute you have. The second is the quantity of data. The third is the quality and distribution of data. It needs to be a broad distribution. The fourth is how long you train for. The fifth is that you need an objective function that can scale to the moon. The pre-training objective function is one such objective function. Another is the RL objective function that says you have a goal, you\u2019re going to go out and reach the goal.\n\nWithin that, there\u2019s objective rewards like you see in math and coding, and there\u2019s more subjective rewards like you see in RLHF or higher-order versions of that. Then the sixth and seventh were things around normalization or conditioning, just getting the numerical stability so that the big blob of compute flows in this laminar way instead of running into problems.\n\nThat was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\n\nWhat has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d\n\nWe see that as well, and it\u2019s not just math contests. It\u2019s a wide variety of RL tasks. We\u2019re seeing the same scaling in RL that we saw for pre-training.\n\nDwarkesh Patel\n\nYou mentioned Rich Sutton and \u201cThe Bitter Lesson\u201d. I interviewed him last year, and he\u2019s actually very non-LLM-pilled. I don\u2019t know if this is his perspective, but one way to paraphrase his objection is: Something which possesses the true core of human learning would not require all these billions of dollars of data and compute and these bespoke environments, to learn how to use Excel, how to use PowerPoint, how to navigate a web browser. The fact that we have to build in these skills using these RL environments hints that we are actually lacking a core human learning algorithm. So we\u2019re scaling the wrong thing.\n\nThat does raise the question. Why are we doing all this RL scaling if we think there\u2019s something that\u2019s going to be human-like in its ability to learn on the fly?\n\nDario Amodei\n\nI think this puts together several things that should be thought of differently. There is a genuine puzzle here, but it may not matter. In fact, I would guess it probably doesn\u2019t matter. There is an interesting thing. Let me take the RL out of it for a second, because I actually think it\u2019s a red herring to say that RL is any different from pre-training in this matter.\n\nIf we look at pre-training scaling, it was very interesting back in 2017 when Alec Radford was doing GPT-1. The models before GPT-1 were trained on datasets that didn\u2019t represent a wide distribution of text. You had very standard language modeling benchmarks. GPT-1 itself was trained on a bunch of fanfiction, I think actually.\n\nIt was literary text, which is a very small fraction of the text you can get. In those days it was like a billion words or something, so small datasets representing a pretty narrow distribution of what you can see in the world. It didn\u2019t generalize well. If you did better on some fanfiction corpus, it wouldn\u2019t generalize that well to other tasks.\n\nWe had all these measures. We had all these measures of how well it did at predicting all these other kinds of texts. It was only when you trained over all the tasks on the internet \u2014 when you did a general internet scrape from something like Common Crawl or scraping links in Reddit, which is what we did for GPT-2 \u2014 that you started to get generalization.\n\nI think we\u2019re seeing the same thing on RL. We\u2019re starting first with simple RL tasks like training on math competitions, then moving to broader training that involves things like code. Now we\u2019re moving to many other tasks. I think then we\u2019re going to increasingly get generalization. So that kind of takes out the RL vs. pre-training side of it.\n\nBut there is a puzzle either way, which is that in pre-training we use trillions of tokens. Humans don\u2019t see trillions of words. So there is an actual sample efficiency difference here. There is actually something different here. The models start from scratch and they need much more training. But we also see that once they\u2019re trained, if we give them a long context length of a million \u2014 the only thing blocking long context is inference \u2014 they\u2019re very good at learning and adapting within that context.\n\nSo I don\u2019t know the full answer to this. I think there\u2019s something going on where pre-training is not like the process of humans learning, but it\u2019s somewhere between the process of humans learning and the process of human evolution. We get many of our priors from evolution. Our brain isn\u2019t just a blank slate. Whole books have been written about this.\n\nThe language models are much more like blank slates. They literally start as random weights, whereas the human brain starts with all these regions connected to all these inputs and outputs. Maybe we should think of pre-training \u2014 and for that matter, RL as well \u2014 as something that exists in the middle space between human evolution and human on-the-spot learning. And we should think of the in-context learning that the models do as something between long-term human learning and short-term human learning.\n\nSo there\u2019s this hierarchy. There\u2019s evolution, there\u2019s long-term learning, there\u2019s short-term learning, and there\u2019s just human reaction. The LLM phases exist along this spectrum, but not necessarily at exactly the same points. There\u2019s no analog to some of the human modes of learning the LLMs are falling in between the points. Does that make sense?\n\nDwarkesh Patel\n\nYes, although some things are still a bit confusing. For example, if the analogy is that this is like evolution so it\u2019s fine that it\u2019s not sample efficient, then if we\u2019re going to get super sample-efficient agent from in-context learning, why are we bothering to build all these RL environments?\n\nThere are companies whose work seems to be teaching models how to use this API, how to use Slack, how to use whatever. It\u2019s confusing to me why there\u2019s so much emphasis on that if the kind of agent that can just learn on the fly is emerging or has already emerged.\n\nDario Amodei\n\nI can\u2019t speak for the emphasis of anyone else. I can only talk about how we think about it. The goal is not to teach the model every possible skill within RL, just as we don\u2019t do that within pre-training. Within pre-training, we\u2019re not trying to expose the model to every possible way that words could be put together. Rather, the model trains on a lot of things and then reaches generalization across pre-training.\n\nThat was the transition from GPT-1 to GPT-2 that I saw up close. The model reaches a point. I had these moments where I was like, \u201cOh yeah, you just give the model a list of numbers \u2014 this is the cost of the house, this is the square feet of the house \u2014 and the model completes the pattern and does linear regression.\u201d Not great, but it does it, and it\u2019s never seen that exact thing before.\n\nSo to the extent that we are building these RL environments, the goal is very similar to what was done five or ten years ago with pre-training. We\u2019re trying to get a whole bunch of data, not because we want to cover a specific document or a specific skill, but because we want to generalize.\n\n00:12:36 - Is diffusion cope?\nDwarkesh Patel\n\nI think the framework you\u2019re laying down obviously makes sense. We\u2019re making progress toward AGI. Nobody at this point disagrees we\u2019re going to achieve AGI this century. The crux is you say we\u2019re hitting the end of the exponential. Somebody else looks at this and says, \u201cWe\u2019ve been making progress since 2012, and by 2035 we\u2019ll have a human-like agent.\u201d\n\nObviously we\u2019re seeing in these models the kinds of things that evolution did, or that learning within a human lifetime does. I want to understand what you\u2019re seeing that makes you think it\u2019s one year away and not ten years away.\n\nDario Amodei\n\nThere are two claims you could make here, one stronger and one weaker. Starting with the weaker claim, when I first saw the scaling back in 2019, I wasn\u2019t sure. This was a 50/50 thing. I thought I saw something. My claim was that this was much more likely than anyone thinks. Maybe there\u2019s a 50% chance this happens.\n\nOn the basic hypothesis of, as you put it, within ten years we\u2019ll get to what I call a \u201ccountry of geniuses in a data center\u201d, I\u2019m at 90% on that. It\u2019s hard to go much higher than 90% because the world is so unpredictable. Maybe the irreducible uncertainty puts us at 95%, where you get to things like multiple companies having internal turmoil, Taiwan gets invaded, all the fabs get blown up by missiles.\n\nDwarkesh Patel\n\nNow you\u2019ve jinxed us, Dario.\n\nDario Amodei\n\nYou could construct a 5% world where things get delayed for ten years. There\u2019s another 5% which is that I\u2019m very confident on tasks that can be verified. With coding, except for that irreducible uncertainty, I think we\u2019ll be there in one or two years. There\u2019s no way we will not be there in ten years in terms of being able to do end-to-end coding.\n\nMy one little bit of fundamental uncertainty, even on long timescales, is about tasks that aren\u2019t verifiable: planning a mission to Mars; doing some fundamental scientific discovery like CRISPR; writing a novel. It\u2019s hard to verify those tasks. I am almost certain we have a reliable path to get there, but if there\u2019s a little bit of uncertainty it\u2019s there. On the ten-year timeline I\u2019m at 90%, which is about as certain as you can be. I think it\u2019s crazy to say that this won\u2019t happen by 2035. In some sane world, it would be outside the mainstream.\n\nDwarkesh Patel\n\nBut the emphasis on verification hints to me a lack of belief that these models are generalized. If you think about humans, we\u2019re both good at things for which we get verifiable reward and things for which we don\u2019t.\n\nDario Amodei\n\nNo, this is why I\u2019m almost sure. We already see substantial generalization from things that verify to things that don\u2019t. We\u2019re already seeing that.\n\nDwarkesh Patel\n\nBut it seems like you were emphasizing this as a spectrum which will split apart which domains in which we see more progress. That doesn\u2019t seem like how humans get better.\n\nDario Amodei\n\nThe world in which we don\u2019t get there is the world in which we do all the verifiable things. Many of them generalize, but we don\u2019t fully get there. We don\u2019t fully color in the other side of the box. It\u2019s not a binary thing.\n\nDwarkesh Patel\n\nEven if generalization is weak and you can only do verifiable domains, it\u2019s not clear to me you could automate software engineering in such a world. You are \u201ca software engineer\u201d in some sense, but part of being a software engineer for you involves writing long memos about your grand vision.\n\nDario Amodei\n\nI don\u2019t think that\u2019s part of the job of SWE. That\u2019s part of the job of the company, not SWE specifically. But SWE does involve design documents and other things like that. The models are already pretty good at writing comments. Again, I\u2019m making much weaker claims here than I believe, to distinguish between two things. We\u2019re already almost there for software engineering.\n\nDwarkesh Patel\n\nBy what metric? There\u2019s one metric which is how many lines of code are written by AI. If you consider other productivity improvements in the history of software engineering, compilers write all the lines of software. There\u2019s a difference between how many lines are written and how big the productivity improvement is. \u201cWe\u2019re almost there\u201d meaning\u2026 How big is the productivity improvement, not just how many lines are written by AI?\n\nDario Amodei\n\nI actually agree with you on this. I\u2019ve made a series of predictions on code and software engineering. I think people have repeatedly misunderstood them. Let me lay out the spectrum.\n\nAbout eight or nine months ago, I said the AI model will be writing 90% of the lines of code in three to six months. That happened, at least at some places. It happened at Anthropic, happened with many people downstream using our models. But that\u2019s actually a very weak criterion. People thought I was saying that we won\u2019t need 90% of the software engineers. Those things are worlds apart. The spectrum is: 90% of code is written by the model, 100% of code is written by the model. That\u2019s a big difference in productivity.\n\n90% of the end-to-end SWE tasks \u2014 including things like compiling, setting up clusters and environments, testing features, writing memos \u2014 are done by the models. 100% of today\u2019s SWE tasks are done by the models. Even when that happens, it doesn\u2019t mean software engineers are out of a job. There are new higher-level things they can do, where they can manage. Then further down the spectrum, there\u2019s 90% less demand for SWEs, which I think will happen but this is a spectrum.\n\nI wrote about it in \u201cThe Adolescence of Technology\u201d where I went through this kind of spectrum with farming. I actually totally agree with you on that. These are very different benchmarks from each other, but we\u2019re proceeding through them super fast.\n\nDwarkesh Patel\n\nPart of your vision is that going from 90 to 100 is going to happen fast, and that it leads to huge productivity improvements. But what I notice is that even in greenfield projects people start with Claude Code or something, people report starting a lot of projects\u2026 Do we see in the world out there a renaissance of software, all these new features that wouldn\u2019t exist otherwise? At least so far, it doesn\u2019t seem like we see that.\n\nSo that does make me wonder. Even if I never had to intervene with Claude Code, the world is complicated. Jobs are complicated. Closing the loop on self-contained systems, whether it\u2019s just writing software or something, how much broader gains would we see just from that? Maybe that should dilute our estimation of the \u201ccountry of geniuses\u201d.\n\nDario Amodei\n\nI simultaneously agree with you that it\u2019s a reason why these things don\u2019t happen instantly, but at the same time, I think the effect is gonna be very fast. You could have these two poles. One is that AI is not going to make progress. It\u2019s slow. It\u2019s going to take forever to diffuse within the economy. Economic diffusion has become one of these buzzwords that\u2019s a reason why we\u2019re not going to make AI progress, or why AI progress doesn\u2019t matter.\n\nThe other axis is that we\u2019ll get recursive self-improvement, the whole thing. Can\u2019t you just draw an exponential line on the curve? We\u2019re going to have Dyson spheres around the sun so many nanoseconds after we get recursive. I\u2019m completely caricaturing the view here, but there are these two extremes.\n\nBut what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\n\nDwarkesh Patel\n\nYou guys should have just bought a billion dollars of your own products so you could just\u2026\n\nDario Amodei\n\nAnd the first month of this year, that exponential is... You would think it would slow down, but we added another few billion to revenue in January. Obviously that curve can\u2019t go on forever. The GDP is only so large. I would even guess that it bends somewhat this year, but that is a fast curve. That\u2019s a really fast curve. I would bet it stays pretty fast even as the scale goes to the entire economy.\n\nSo I think we should be thinking about this middle world where things are extremely fast, but not instant, where they take time because of economic diffusion, because of the need to close the loop. Because it\u2019s fiddly: \u201cI have to do change management within my enterprise\u2026 I set this up, but I have to change the security permissions on this in order to make it actually work\u2026 I had this old piece of software that checks the model before it\u2019s compiled and released and I have to rewrite it. Yes, the model can do that, but I have to tell the model to do that. It has to take time to do that.\u201d\n\nSo I think everything we\u2019ve seen so far is compatible with the idea that there\u2019s one fast exponential that\u2019s the capability of the model. Then there\u2019s another fast exponential that\u2019s downstream of that, which is the diffusion of the model into the economy. Not instant, not slow, much faster than any previous technology, but it has its limits. When I look inside Anthropic, when I look at our customers: fast adoption, but not infinitely fast.\n\nDwarkesh Patel\n\nCan I try a hot take on you?\n\nDario Amodei\n\nYeah.\n\nDwarkesh Patel\n\nI feel like diffusion is cope that people say. When the model isn\u2019t able to do something, they\u2019re like, \u201coh, but it\u2019s a diffusion issue.\u201d But then you should use the comparison to humans. You would think that the inherent advantages that AIs have would make diffusion a much easier problem for new AIs getting onboarded than new humans getting onboarded. An AI can read your entire Slack and your drive in minutes. They can share all the knowledge that the other copies of the same instance have. You don\u2019t have this adverse selection problem when you\u2019re hiring AI, so you can just hire copies of a vetted AI model.\n\nHiring a human is so much more of a hassle. People hire humans all the time. We pay humans upwards of $50 trillion in wages because they\u2019re useful, even though in principle it would be much easier to integrate AIs into the economy than it is to hire humans. The diffusion doesn\u2019t really explain.\n\nDario Amodei\n\nI think diffusion is very real and doesn\u2019t exclusively have to do with limitations on the AI models. Again, there are people who use diffusion as kind of a buzzword to say this isn\u2019t a big deal. I\u2019m not talking about that. I\u2019m not talking about how AI will diffuse at the speed of previous technologies. I think AI will diffuse much faster than previous technologies have, but not infinitely fast.\n\nI\u2019ll just give an example of this. There\u2019s Claude Code. Claude Code is extremely easy to set up. If you\u2019re a developer, you can just start using Claude Code. There is no reason why a developer at a large enterprise should not be adopting Claude Code as quickly as an individual developer or developer at a startup.\n\nWe do everything we can to promote it. We sell Claude Code to enterprises. Big enterprises, big financial companies, big pharmaceutical companies, all of them are adopting Claude Code much faster than enterprises typically adopt new technology. But again, it takes time.\n\nAny given feature or any given product, like Claude Code or Cowork, will get adopted by the individual developers who are on Twitter all the time, by the Series A startups, many months faster than they will get adopted by a large enterprise that does food sales. There are just a number of factors. You have to go through legal, you have to provision it for everyone. It has to pass security and compliance.\n\nThe leaders of the company who are further away from the AI revolution are forward-looking, but they have to say, \u201cOh, it makes sense for us to spend 50 million. This is what this Claude Code thing is. This is why it helps our company. This is why it makes us more productive.\u201d Then they have to explain to the people two levels below. They have to say, \u201cOkay, we have 3,000 developers. Here\u2019s how we\u2019re going to roll it out to our developers.\u201d We have conversations like this every day.\n\nWe are doing everything we can to make Anthropic\u2019s revenue grow 20 or 30x a year instead of 10x a year. Again, many enterprises are just saying, \u201cThis is so productive. We\u2019re going to take shortcuts in our usual procurement process.\u201d They\u2019re moving much faster than when we tried to sell them just the ordinary API, which many of them use. Claude Code is a more compelling product, but it\u2019s not an infinitely compelling product.\n\nI don\u2019t think even AGI or powerful AI or \u201ccountry of geniuses in a data center\u201d will be an infinitely compelling product. It will be a compelling product enough maybe to get 3-5x, or 10x, a year of growth, even when you\u2019re in the hundreds of billions of dollars, which is extremely hard to do and has never been done in history before, but not infinitely fast.\n\nDwarkesh Patel\n\nI buy that it would be a slight slowdown. Maybe this is not your claim, but sometimes people talk about this like, \u201cOh, the capabilities are there, but because of diffusion... otherwise we\u2019re basically at AGI\u201d.\n\nDario Amodei\n\nI don\u2019t believe we\u2019re basically at AGI.\n\nDwarkesh Patel\n\nI think if you had the \u201ccountry of geniuses in a data center\u201d...\n\nDario Amodei\n\nIf we had the \u201ccountry of geniuses in a data center\u201d, we would know it. We would know it if you had the \u201ccountry of geniuses in a data center\u201d. Everyone in this room would know it. Everyone in Washington would know it. People in rural parts might not know it, but we would know it. We don\u2019t have that now. That is very clear.\n\n00:29:42 - Is continual learning necessary? How will it be solved?\nDwarkesh Patel\n\nComing back to concrete prediction\u2026 Because there are so many different things to disambiguate, it can be easy to talk past each other when we\u2019re talking about capabilities. For example, when I interviewed you three years ago, I asked you a prediction about what we should expect three years from now. You were right. You said, \u201cWe should expect systems which, if you talk to them for the course of an hour, it\u2019s hard to tell them apart from a generally well-educated human.\u201d\n\nI think you were right about that. I think spiritually I feel unsatisfied because my internal expectation was that such a system could automate large parts of white-collar work. So it might be more productive to talk about the actual end capabilities you want from such a system.\n\nDario Amodei\n\nI will basically tell you where I think we are.\n\nDwarkesh Patel\n\nLet me ask a very specific question so that we can figure out exactly what kinds of capabilities we should think about soon. Maybe I\u2019ll ask about it in the context of a job I understand well, not because it\u2019s the most relevant job, but just because I can evaluate the claims about it.\n\nTake video editors. I have video editors. Part of their job involves learning about our audience\u2019s preferences, learning about my preferences and tastes, and the different trade-offs we have. They\u2019re, over the course of many months, building up this understanding of context. The skill and ability they have six months into the job, a model that can pick up that skill on the job on the fly, when should we expect such an AI system?\n\nDario Amodei\n\nI guess what you\u2019re talking about is that we\u2019re doing this interview for three hours. Someone\u2019s going to come in, someone\u2019s going to edit it. They\u2019re going to be like, \u201cOh, I don\u2019t know, Dario scratched his head and we could edit that out.\u201d\n\nDwarkesh Patel\n\n\u201cMagnify that.\u201d\n\nDario Amodei\n\n\u201cThere was this long discussion that is less interesting to people. There\u2019s another thing that\u2019s more interesting to people, so let\u2019s make this edit.\u201d\n\nI think the \u201ccountry of geniuses in a data center\u201d will be able to do that. The way it will be able to do that is it will have general control of a computer screen. You\u2019ll be able to feed this in. It\u2019ll be able to also use the computer screen to go on the web, look at all your previous interviews, look at what people are saying on Twitter in response to your interviews, talk to you, ask you questions, talk to your staff, look at the history of edits that you did, and from that, do the job.\n\nI think that\u2019s dependent on several things. I think this is one of the things that\u2019s actually blocking deployment: getting to the point on computer use where the models are really masters at using the computer.\n\nWe\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.\n\nDwarkesh Patel\n\nCan I just follow up on that before you move on to the next point? For years, I\u2019ve been trying to build different internal LLM tools for myself. Often I have these text-in, text-out tasks, which should be dead center in the repertoire of these models. Yet I still hire humans to do them.\n\nIf it\u2019s something like, \u201cidentify what the best clips would be in this transcript\u201d, maybe the LLMs do a seven-out-of-ten job on them. But there\u2019s not this ongoing way I can engage with them to help them get better at the job the way I could with a human employee. That missing ability, even if you solve computer use, would still block my ability to offload an actual job to them.\n\nDario Amodei\n\nThis gets back to what we were talking about before with learning on the job. It\u2019s very interesting. I think with the coding agents, I don\u2019t think people would say that learning on the job is what is preventing the coding agents from doing everything end to end. They keep getting better. We have engineers at Anthropic who don\u2019t write any code.\n\nWhen I look at the productivity, to your previous question, we have folks who say, \u201cThis GPU kernel, this chip, I used to write it myself. I just have Claude do it.\u201d There\u2019s this enormous improvement in productivity.\n\nWhen I see Claude Code, familiarity with the codebase or a feeling that the model hasn\u2019t worked at the company for a year, that\u2019s not high up on the list of complaints I see. I think what I\u2019m saying is that we\u2019re kind of taking a different path.\n\nDwarkesh Patel\n\nDon\u2019t you think with coding that\u2019s because there is an external scaffold of memory which exists instantiated in the codebase? I don\u2019t know how many other jobs have that. Coding made fast progress precisely because it has this unique advantage that other economic activity doesn\u2019t.\n\nDario Amodei\n\nBut when you say that, what you\u2019re implying is that by reading the codebase into the context, I have everything that the human needed to learn on the job. So that would be an example of\u2014whether it\u2019s written or not, whether it\u2019s available or not\u2014a case where everything you needed to know you got from the context window. What we think of as learning\u2014\u201dI started this job, it\u2019s going to take me six months to understand the code base\u201d\u2014the model just did it in the context.\n\nDwarkesh Patel\n\nI honestly don\u2019t know how to think about this because there are people who qualitatively report what you\u2019re saying. I\u2019m sure you saw last year, there was a major study where they had experienced developers try to close pull requests in repositories that they were familiar with. Those developers reported an uplift. They reported that they felt more productive with the use of these models. But in fact, if you look at their output and how much was actually merged back in, there was a 20% downlift. They were less productive as a result of using these models.\n\nSo I\u2019m trying to square the qualitative feeling that people feel with these models versus, 1) in a macro level, where is this renaissance of software? And then 2) when people do these independent evaluations, why are we not seeing the productivity benefits we would expect?\n\nDario Amodei\n\nWithin Anthropic, this is just really unambiguous. We\u2019re under an incredible amount of commercial pressure and make it even harder for ourselves because we have all this safety stuff we do that I think we do more than other companies.\n\nThe pressure to survive economically while also keeping our values is just incredible. We\u2019re trying to keep this 10x revenue curve going. There is zero time for bullshit. There is zero time for feeling like we\u2019re productive when we\u2019re not. These tools make us a lot more productive.\n\nWhy do you think we\u2019re concerned about competitors using the tools? Because we think we\u2019re ahead of the competitors. We wouldn\u2019t be going through all this trouble if this were secretly reducing our productivity. We see the end productivity every few months in the form of model launches. There\u2019s no kidding yourself about this. The models make you more productive.\n\nDwarkesh Patel\n\n1) People feeling like they\u2019re productive is qualitatively predicted by studies like this. But 2) if I just look at the end output, obviously you guys are making fast progress.\n\nBut the idea was supposed to be that with recursive self-improvement, you make a better AI, the AI helps you build a better next AI, et cetera, et cetera. What I see instead\u2014if I look at you, OpenAI, DeepMind\u2014is that people are just shifting around the podium every few months.\n\nMaybe you think that stops because you\u2019ve won or whatever. But why are we not seeing the person with the best coding model have this lasting advantage if in fact there are these enormous productivity gains from the last coding model.\n\nDario Amodei\n\nI think my model of the situation is that there\u2019s an advantage that\u2019s gradually growing. I would say right now the coding models give maybe, I don\u2019t know, a 15-20% total factor speed up. That\u2019s my view. Six months ago, it was maybe 5%. So it didn\u2019t matter. 5% doesn\u2019t register. It\u2019s now just getting to the point where it\u2019s one of several factors that kind of matters. That\u2019s going to keep speeding up.\n\nI think six months ago, there were several companies that were at roughly the same point because this wasn\u2019t a notable factor, but I think it\u2019s starting to speed up more and more. I would also say there are multiple companies that write models that are used for code and we\u2019re not perfectly good at preventing some of these other companies from using our models internally. So I think everything we\u2019re seeing is consistent with this kind of snowball model.\n\nAgain, my theme in all of this is all of this is soft takeoff, soft, smooth exponentials, although the exponentials are relatively steep. So we\u2019re seeing this snowball gather momentum where it\u2019s like 10%, 20%, 25%, 40%. As you go, Amdahl\u2019s law, you have to get all the things that are preventing you from closing the loop out of the way. But this is one of the biggest priorities within Anthropic.\n\nDwarkesh Patel\n\nStepping back, before in the stack we were talking about when do we get this on-the-job learning? It seems like the point you were making on the coding thing is that we actually don\u2019t need on-the-job learning. You can have tremendous productivity improvements, you can have potentially trillions of dollars of revenue for AI companies, without this basic human ability to learn on the job. Maybe that\u2019s not your claim, you should clarify.\n\nBut in most domains of economic activity, people say, \u201cI hired somebody, they weren\u2019t that useful for the first few months, and then over time they built up the context, understanding.\u201d It\u2019s actually hard to define what we\u2019re talking about here. But they got something and then now they\u2019re a powerhorse and they\u2019re so valuable to us. If AI doesn\u2019t develop this ability to learn on the fly, I\u2019m a bit skeptical that we\u2019re going to see huge changes to the world without that ability.\n\nDario Amodei\n\nI think two things here. There\u2019s the state of the technology right now. Again, we have these two stages. We have the pre-training and RL stage where you throw a bunch of data and tasks into the models and then they generalize. So it\u2019s like learning, but it\u2019s like learning from more data and not learning over one human or one model\u2019s lifetime. So again, this is situated between evolution and human learning. But once you learn all those skills, you have them.\n\nJust like with pre-training, just how the models know more, if I look at a pre-trained model, it knows more about the history of samurai in Japan than I do. It knows more about baseball than I do. It knows more about low-pass filters and electronics, all of these things. Its knowledge is way broader than mine. So I think even just that may get us to the point where the models are better at everything.\n\nWe also have, again, just with scaling the kind of existing setup, the in-context learning. I would describe it as kind of like human on-the-job learning, but a little weaker and a little short term. You look at in-context learning and if you give the model a bunch of examples it does get it. There\u2019s real learning that happens in context. A million tokens is a lot. That can be days of human learning. If you think about the model reading a million words, how long would it take me to read a million? Days or weeks at least.\n\nSo you have these two things. I think these two things within the existing paradigm may just be enough to get you the \u201ccountry of geniuses in a data center\u201d. I don\u2019t know for sure, but I think they\u2019re going to get you a large fraction of it. There may be gaps, but I certainly think that just as things are, this is enough to generate trillions of dollars of revenue. That\u2019s one.\n\nTwo, is this idea of continual learning, this idea of a single model learning on the job. I think we\u2019re working on that too. There\u2019s a good chance that in the next year or two, we also solve that. Again, I think you get most of the way there without it. The trillions of dollars a year market, maybe all of the national security implications and the safety implications that I wrote about in \u201cAdolescence of Technology\u201d can happen without it. But we, and I imagine others, are working on it. There\u2019s a good chance that we will get there within the next year or two.\n\nThere are a bunch of ideas. I won\u2019t go into all of them in detail, but one is just to make the context longer. There\u2019s nothing preventing longer contexts from working. You just have to train at longer contexts and then learn to serve them at inference. Both of those are engineering problems that we are working on and I would assume others are working on them as well.\n\nDwarkesh Patel\n\nThis context length increase, it seemed like there was a period from 2020 to 2023 where from GPT-3 to GPT-4 Turbo, there was an increase from 2000 context lengths to 128K. I feel like for the two-ish years since then, we\u2019ve been in the same-ish ballpark.\n\nWhen context lengths get much longer than that, people report qualitative degradation in the ability of the model to consider that full context. So I\u2019m curious what you\u2019re internally seeing that makes you think, \u201c10 million contexts, 100 million contexts to get six months of human learning and building context\u201d.\n\nDario Amodei\n\nThis isn\u2019t a research problem. This is an engineering and inference problem. If you want to serve long context, you have to store your entire KV cache. It\u2019s difficult to store all the memory in the GPUs, to juggle the memory around. I don\u2019t even know the details. At this point, this is at a level of detail that I\u2019m no longer able to follow, although I knew it in the GPT-3 era. \u201cThese are the weights, these are the activations you have to store\u2026\u201d\n\nBut these days the whole thing is flipped because we have MoE models and all of that. Regarding this degradation you\u2019re talking about, without getting too specific, there\u2019s two things. There\u2019s the context length you train at and there\u2019s a context length that you serve at. If you train at a small context length and then try to serve at a long context length, maybe you get these degradations. It\u2019s better than nothing, you might still offer it, but you get these degradations. Maybe it\u2019s harder to train at a long context length.\n\nDwarkesh Patel\n\nI want to, at the same time, ask about maybe some rabbit holes. Wouldn\u2019t you expect that if you had to train on longer context length, that would mean that you\u2019re able to get less samples in for the same amount of compute? Maybe it\u2019s not worth diving deep on that.\n\nI want to get an answer to the bigger picture question. I don\u2019t feel a preference for a human editor that\u2019s been working for me for six months versus an AI that\u2019s been working with me for six months, what year do you predict that that will be the case?\n\nDario Amodei\n\nMy guess for that is there\u2019s a lot of problems where basically we can do this when we have the \u201ccountry of geniuses in a data center\u201d. My picture for that, if you made me guess, is one to two years, maybe one to three years. It\u2019s really hard to tell. I have a strong view\u201499%, 95%\u2014that all this will happen in 10 years. I think that\u2019s just a super safe bet. I have a hunch\u2014this is more like a 50/50 thing\u2014that it\u2019s going to be more like one to two, maybe more like one to three.\n\nDwarkesh Patel\n\nSo one to three years. Country of geniuses, and the slightly less economically valuable task of editing videos.\n\nDario Amodei\n\nIt seems pretty economically valuable, let me tell you. It\u2019s just there are a lot of use cases like that. There are a lot of similar ones.\n\n00:46:20 - If AGI is imminent, why not buy more compute?\nDwarkesh Patel\n\nSo you\u2019re predicting that within one to three years. And then, generally, Anthropic has predicted that by late \u201826 or early \u201827 we will have AI systems that \u201chave the ability to navigate interfaces available to humans doing digital work today, intellectual capabilities matching or exceeding that of Nobel Prize winners, and the ability to interface with the physical world\u201d. You gave an interview two months ago with DealBook where you were emphasizing your company\u2019s more responsible compute scaling as compared to your competitors.\n\nI\u2019m trying to square these two views. If you really believe that we\u2019re going to have a country of geniuses, you want as big a data center as you can get. There\u2019s no reason to slow down. The TAM of a Nobel Prize winner, that can actually do everything a Nobel Prize winner can do, is trillions of dollars. So I\u2019m trying to square this conservatism, which seems rational if you have more moderate timelines, with your stated views about progress.\n\nDario Amodei\n\nIt actually all fits together. We go back to this fast, but not infinitely fast, diffusion. Let\u2019s say that we\u2019re making progress at this rate. The technology is making progress this fast. I have very high conviction that we\u2019re going to get there within a few years. I have a hunch that we\u2019re going to get there within a year or two. So there\u2019s a little uncertainty on the technical side, but pretty strong confidence that it won\u2019t be off by much.\n\nWhat I\u2019m less certain about is, again, the economic diffusion side. I really do believe that we could have models that are a country of geniuses in the data center in one to two years. One question is: How many years after that do the trillions in revenue start rolling in? I don\u2019t think it\u2019s guaranteed that it\u2019s going to be immediate. It could be one year, it could be two years, I could even stretch it to five years although I\u2019m skeptical of that.\n\nSo we have this uncertainty. Even if the technology goes as fast as I suspect that it will, we don\u2019t know exactly how fast it\u2019s going to drive revenue. We know it\u2019s coming, but with the way you buy these data centers, if you\u2019re off by a couple years, that can be ruinous. It is just like how I wrote in \u201cMachines of Loving Grace\u201d. I said I think we might get this powerful AI, this \u201ccountry of genius in the data center\u201d. That description you gave comes from \u201cMachines of Loving Grace\u201d. I said we\u2019ll get that in 2026, maybe 2027. Again, that is my hunch. I wouldn\u2019t be surprised if I\u2019m off by a year or two, but that is my hunch.\n\nLet\u2019s say that happens. That\u2019s the starting gun. How long does it take to cure all the diseases? That\u2019s one of the ways that drives a huge amount of economic value. You cure every disease. There\u2019s a question of how much of that goes to the pharmaceutical company or the AI company, but there\u2019s an enormous consumer surplus because \u2014assuming we can get access for everyone, which I care about greatly\u2014we cure all of these diseases.\n\nHow long does it take? You have to do the biological discovery, you have to manufacture the new drug, you have to go through the regulatory process. We saw this with vaccines and COVID. We got the vaccine out to everyone, but it took a year and a half. My question is: How long does it take to get the cure for everything\u2014which AI is the genius that can in theory invent\u2014out to everyone? How long from when that AI first exists in the lab to when diseases have actually been cured for everyone?\n\nWe\u2019ve had a polio vaccine for 50 years. We\u2019re still trying to eradicate it in the most remote corners of Africa. The Gates Foundation is trying as hard as they can. Others are trying as hard as they can. But that\u2019s difficult. Again, I don\u2019t expect most of the economic diffusion to be as difficult as that. That\u2019s the most difficult case. But there\u2019s a real dilemma here. Where I\u2019ve settled on it is that it will be faster than anything we\u2019ve seen in the world, but it still has its limits.\n\nSo when we go to buying data centers, again, the curve I\u2019m looking at is: we\u2019ve had a 10x a year increase every year. At the beginning of this year, we\u2019re looking at $10 billion in annualized revenue. We have to decide how much compute to buy. It takes a year or two to actually build out the data centers, to reserve the data center.\n\nBasically I\u2019m saying, \u201cIn 2027, how much compute do I get?\u201d I could assume that the revenue will continue growing 10x a year, so it\u2019ll be $100 billion at the end of 2026 and $1 trillion at the end of 2027. Actually it would be $5 trillion dollars of compute because it would be $1 trillion a year for five years. I could buy $1 trillion of compute that starts at the end of 2027. If my revenue is not $1 trillion dollars, if it\u2019s even $800 billion, there\u2019s no force on earth, there\u2019s no hedge on earth that could stop me from going bankrupt if I buy that much compute.\n\nEven though a part of my brain wonders if it\u2019s going to keep growing 10x, I can\u2019t buy $1 trillion a year of compute in 2027. If I\u2019m just off by a year in that rate of growth, or if the growth rate is 5x a year instead of 10x a year, then you go bankrupt. So you end up in a world where you\u2019re supporting hundreds of billions, not trillions. You accept some risk that there\u2019s so much demand that you can\u2019t support the revenue, and you accept some risk that you got it wrong and it\u2019s still slow.\n\nWhen I talked about behaving responsibly, what I meant actually was not the absolute amount. I think it is true we\u2019re spending somewhat less than some of the other players. It\u2019s actually the other things, like have we been thoughtful about it or are we YOLOing and saying, \u201cWe\u2019re going to do $100 billion here or $100 billion there\u201d? I get the impression that some of the other companies have not written down the spreadsheet, that they don\u2019t really understand the risks they\u2019re taking. They\u2019re just doing stuff because it sounds cool.\n\nWe\u2019ve thought carefully about it. We\u2019re an enterprise business. Therefore, we can rely more on revenue. It\u2019s less fickle than consumer. We have better margins, which is the buffer between buying too much and buying too little. I think we bought an amount that allows us to capture pretty strong upside worlds. It won\u2019t capture the full 10x a year. Things would have to go pretty badly for us to be in financial trouble. So we\u2019ve thought carefully and we\u2019ve made that balance. That\u2019s what I mean when I say that we\u2019re being responsible.\n\nDwarkesh Patel\n\nSo it seems like it\u2019s possible that we actually just have different definitions of the \u201ccountry of a genius in a data center\u201d. Because when I think of actual human geniuses, an actual country of human geniuses in a data center, I would happily buy $5 trillion worth of compute to run an actual country of human geniuses in a data center.\n\nLet\u2019s say JPMorgan or Moderna or whatever doesn\u2019t want to use them. I\u2019ve got a country of geniuses. They\u2019ll start their own company. If they can\u2019t start their own company and they\u2019re bottlenecked by clinical trials\u2026 It is worth stating that with clinical trials, most clinical trials fail because the drug doesn\u2019t work. There\u2019s not efficacy.\n\nDario Amodei\n\nI make exactly that point in \u201cMachines of Loving Grace\u201d, I say the clinical trials are going to go much faster than we\u2019re used to, but not infinitely fast.\n\nDwarkesh Patel\n\nOkay, and then suppose it takes a year for the clinical trials to work out so that you\u2019re getting revenue from that and can make more drugs. Okay, well, you\u2019ve got a country of geniuses and you\u2019re an AI lab. You could use many more AI researchers. You also think there are these self-reinforcing gains from smart people working on AI tech. You can have the data center working on AI progress.\n\nDario Amodei\n\nAre there substantially more gains from buying $1 trillion a year of compute versus $300 billion a year of compute?\n\nDwarkesh Patel\n\nIf your competitor is buying a trillion, yes there is.\n\nDario Amodei\n\nWell, no, there\u2019s some gain, but then again, there\u2019s this chance that they go bankrupt before. Again, if you\u2019re off by only a year, you destroy yourselves. That\u2019s the balance. We\u2019re buying a lot. We\u2019re buying a hell of a lot. We\u2019re buying an amount that\u2019s comparable to what the biggest players in the game are buying.\n\nBut if you\u2019re asking me, \u201cWhy haven\u2019t we signed $10 trillion of compute starting in mid-2027?\u201d... First of all, it can\u2019t be produced. There isn\u2019t that much in the world. But second, what if the country of geniuses comes, but it comes in mid-2028 instead of mid-2027? You go bankrupt.\n\nDwarkesh Patel\n\nSo if your projection is one to three years, it seems like you should want $10 trillion of compute by 2029 at the latest? Even in the longest version of the timelines you state, the compute you are ramping up to build doesn\u2019t seem in accordance.\n\nDario Amodei\n\nWhat makes you think that?\n\nDwarkesh Patel\n\nHuman wages, let\u2019s say, are on the order of $50 trillion a year\u2014\n\nDario Amodei\n\nSo I won\u2019t talk about Anthropic in particular, but if you talk about the industry, the amount of compute the industry is building this year is probably, call it, 10-15 gigawatts. It goes up by roughly 3x a year. So next year\u2019s 30-40 gigawatts. 2028 might be 100 gigawatts. 2029 might be like 300 gigawatts. I\u2019m doing the math in my head, but each gigawatt costs maybe $10 billion, on the order of $10-15 billion a year.\n\nYou put that all together and you\u2019re getting about what you described. You\u2019re getting exactly that. You\u2019re getting multiple trillions a year by 2028 or 2029. You\u2019re getting exactly what you predict.\n\nDwarkesh Patel\n\nThat\u2019s for the industry.\n\nDario Amodei\n\nThat\u2019s for the industry, that\u2019s right.\n\nDwarkesh Patel\n\nSuppose Anthropic\u2019s compute keeps 3x-ing a year, and then by 2027-28, you have 10 gigawatts. Multiply that by, as you say, $10 billion. So then it\u2019s like $100 billion a year. But then you\u2019re saying the TAM by 2028 is $200 billion.\n\nDario Amodei\n\nAgain, I don\u2019t want to give exact numbers for Anthropic, but these numbers are too small.\n\nDwarkesh Patel\n\nOkay, interesting.\n\n00:58:49 - How will AI labs actually make profit?\nDwarkesh Patel\n\nYou\u2019ve told investors that you plan to be profitable starting in 2028. This is the year when we\u2019re potentially getting the country of geniuses as a data center. This is now going to unlock all this progress in medicine and health and new technologies. Wouldn\u2019t this be exactly the time where you\u2019d want to reinvest in the business and build bigger \u201ccountries\u201d so they can make more discoveries?\n\nDario Amodei\n\nProfitability is this kind of weird thing in this field. I don\u2019t think in this field profitability is actually a measure of spending down versus investing in the business. Let\u2019s just take a model of this. I actually think profitability happens when you underestimated the amount of demand you were going to get and loss happens when you overestimated the amount of demand you were going to get, because you\u2019re buying the data centers ahead of time.\n\nThink about it this way. Again, these are stylized facts. These numbers are not exact. I\u2019m just trying to make a toy model here. Let\u2019s say half of your compute is for training and half of your compute is for inference. The inference has some gross margin that\u2019s more than 50%.\n\nSo what that means is that if you were in steady-state, you build a data center and if you knew exactly the demand you were getting, you would get a certain amount of revenue. Let\u2019s say you pay $100 billion a year for compute. On $50 billion a year you support $150 billion of revenue. The other $50 billion is used for training. Basically you\u2019re profitable and you make $50 billion of profit. Those are the economics of the industry today, or not today but where we\u2019re projecting forward in a year or two.\n\nThe only thing that makes that not the case is if you get less demand than $50 billion. Then you have more than 50% of your data center for research and you\u2019re not profitable. So you train stronger models, but you\u2019re not profitable. If you get more demand than you thought, then research gets squeezed, but you\u2019re kind of able to support more inference and you\u2019re more profitable.\n\nMaybe I\u2019m not explaining it well, but the thing I\u2019m trying to say is that you decide the amount of compute first. Then you have some target desire of inference versus training, but that gets determined by demand. It doesn\u2019t get determined by you.\n\nDwarkesh Patel\n\nWhat I\u2019m hearing is the reason you\u2019re predicting profit is that you are systematically underinvesting in compute?\n\nDario Amodei\n\nNo, no, no. I\u2019m saying it\u2019s hard to predict. These things about 2028 and when it will happen, that\u2019s our attempt to do the best we can with investors. All of this stuff is really uncertain because of the cone of uncertainty. We could be profitable in 2026 if the revenue grows fast enough. If we overestimate or underestimate the next year, that could swing wildly.\n\nWhat I\u2019m trying to get at is that you have a model in your head of a business that invests, invests, invests, gets scale and then becomes profitable. There\u2019s a single point at which things turn around. I don\u2019t think the economics of this industry work that way.\n\nDwarkesh Patel\n\nI see. So if I\u2019m understanding correctly, you\u2019re saying that because of the discrepancy between the amount of compute we should have gotten and the amount of compute we got, we were sort of forced to make profit. But that doesn\u2019t mean we\u2019re going to continue making profit. We\u2019re going to reinvest the money because now AI has made so much progress and we want a bigger country of geniuses. So back into revenue is high, but losses are also high.\n\nDario Amodei\n\nIf every year we predict exactly what the demand is going to be, we\u2019ll be profitable every year. Because spending 50% of your compute on research, roughly, plus a gross margin that\u2019s higher than 50% and correct demand prediction leads to profit. That\u2019s the profitable business model that I think is kind of there, but obscured by these building ahead and prediction errors.\n\nDwarkesh Patel\n\nI guess you\u2019re treating the 50% as a sort of given constant, whereas in fact, if AI progress is fast and you can increase the progress by scaling up more, you should just have more than 50% and not make profit.\n\nDario Amodei\n\nBut here\u2019s what I\u2019ll say. You might want to scale it up more. Remember the log returns to scale. If 70% would get you a very little bit of a smaller model through a factor of 1.4x... That extra $20 billion, each dollar there is worth much less to you because of the log-linear setup.\n\nSo you might find that it\u2019s better to invest that $20 billion in serving inference or in hiring engineers who are kind of better at what they\u2019re doing. So the reason I said 50%... That\u2019s not exactly our target. It\u2019s not exactly going to be 50%. It\u2019ll probably vary over time.\n\nWhat I\u2019m saying is the log-linear return, what it leads to is you spend of order one fraction of the business. Like not 5%, not 95%. Then you get diminishing returns.\n\nDwarkesh Patel\n\nI feel strange that I\u2019m convincing Dario to believe in AI progress or something. Okay, you don\u2019t invest in research because it has diminishing returns, but you invest in the other things you mentioned. I think profit at a sort of macro level\u2014\n\nDario Amodei\n\nAgain, I\u2019m talking about diminishing returns, but after you\u2019re spending $50 billion a year.\n\nDwarkesh Patel\n\nThis is a point I\u2019m sure you would make, but diminishing returns on a genius could be quite high.\n\nMore generally, what is profit in a market economy? Profit is basically saying other companies in the market can do more things with this money than I can.\n\nDario Amodei\n\nPut aside Anthropic. I don\u2019t want to give information about Anthropic. That\u2019s why I\u2019m giving these stylized numbers. But let\u2019s just derive the equilibrium of the industry. Why doesn\u2019t everyone spend 100% of their compute on training and not serve any customers? It\u2019s because if they didn\u2019t get any revenue, they couldn\u2019t raise money, they couldn\u2019t do compute deals, they couldn\u2019t buy more compute the next year.\n\nSo there\u2019s going to be an equilibrium where every company spends less than 100% on training and certainly less than 100% on inference. It should be clear why you don\u2019t just serve the current models and never train another model, because then you don\u2019t have any demand because you\u2019ll fall behind. So there\u2019s some equilibrium. It\u2019s not gonna be 10%, it\u2019s not gonna be 90%. Let\u2019s just say as a stylized fact, it\u2019s 50%. That\u2019s what I\u2019m getting at.\n\nI think we\u2019re gonna be in a position where that equilibrium of how much you spend on training is less than the gross margins that you\u2019re able to get on compute. So the underlying economics are profitable. The problem is you have this hellish demand prediction problem when you\u2019re buying the next year of compute and you might guess under and be very profitable but have no compute for research. Or you might guess over and you are not profitable and you have all the compute for research in the world. Does that make sense? Just as a dynamic model of the industry?\n\nDwarkesh Patel\n\nMaybe stepping back, I\u2019m not saying I think the \u201ccountry of geniuses\u201d is going to come in two years and therefore you should buy this compute. To me, the end conclusion you\u2019re arriving at makes a lot of sense. But that\u2019s because it seems like \u201ccountry of geniuses\u201d is hard and there\u2019s a long way to go. So stepping back, the thing I\u2019m trying to get at is more that it seems like your worldview is compatible with somebody who says, \u201cWe\u2019re like 10 years away from a world in which we\u2019re generating trillions of dollars of value.\u201d\n\nDario Amodei\n\nThat\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.\n\nLike in 2028, we get the real \u201ccountry of geniuses in the data center\u201d. The revenue\u2019s going into the low hundreds of billions by 2028, and then the country of geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. It takes two years to get to the trillions. That would be the world where it takes until 2030. I suspect even composing the technical exponential and diffusion exponential, we\u2019ll get there before 2030.\n\nDwarkesh Patel\n\nSo you laid out a model where Anthropic makes profit because it seems like fundamentally we\u2019re in a compute-constrained world. So eventually we keep growing compute\u2014\n\nDario Amodei\n\nI think the way the profit comes is\u2026 Again, let\u2019s just abstract the whole industry here. Let\u2019s just imagine we\u2019re in an economics textbook. We have a small number of firms. Each can invest a limited amount. Each can invest some fraction in R&D. They have some marginal cost to serve. The gross profit margins on that marginal cost are very high because inference is efficient. There\u2019s some competition, but the models are also differentiated.\n\nCompanies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.\n\nDwarkesh Patel\n\nHelp me understand that, because right now we do have three leading firms and they\u2019re not making profit. So what is changing?\n\nDario Amodei\n\nAgain, the gross margins right now are very positive. What\u2019s happening is a combination of two things. One is that we\u2019re still in the exponential scale-up phase of compute. A model gets trained. Let\u2019s say a model got trained that costs $1 billion last year. Then this year it produced $4 billion of revenue and cost $1 billion to inference from. Again, I\u2019m using stylized numbers here, but that would be 75% gross margins and this 25% tax. So that model as a whole makes $2 billion.\n\nBut at the same time, we\u2019re spending $10 billion to train the next model because there\u2019s an exponential scale-up. So the company loses money. Each model makes money, but the company loses money.\n\nThe equilibrium I\u2019m talking about is an equilibrium where we have the \u201ccountry of geniuses in a data center\u201d, but that model training scale-up has equilibrated more. Maybe it\u2019s still going up. We\u2019re still trying to predict the demand, but it\u2019s more leveled out.\n\nDwarkesh Patel\n\nI\u2019m confused about a couple of things there. Let\u2019s start with the current world. In the current world, you\u2019re right that, as you said before, if you treat each individual model as a company, it\u2019s profitable. But of course, a big part of the production function of being a frontier lab is training the next model, right?\n\nDario Amodei\n\nYes, that\u2019s right.\n\nDwarkesh Patel\n\nIf you didn\u2019t do that, then you\u2019d make profit for two months and then you wouldn\u2019t have margins because you wouldn\u2019t have the best model.\n\nDario Amodei\n\nBut at some point that reaches the biggest scale that it can reach. And then in equilibrium, we have algorithmic improvements, but we\u2019re spending roughly the same amount to train the next model as we spend to train the current model. At some point you run out of money in the economy.\n\nDwarkesh Patel\n\nA fixed lump of labor fallacy\u2026 The economy is going to grow, right? That\u2019s one of your predictions. We\u2019re going to have the data centers in space.\n\nDario Amodei\n\nYes, but this is another example of the theme I was talking about. The economy will grow much faster with AI than I think it ever has before. Right now the compute is growing 3x a year. I don\u2019t believe the economy is gonna grow 300% a year. I said this in \u201cMachines of Loving Grace\u201d, I think we may get 10-20% per year growth in the economy, but we\u2019re not gonna get 300% growth in the economy. So I think in the end, if compute becomes the majority of what the economy produces, it\u2019s gonna be capped by that.\n\nDwarkesh Patel\n\nSo let\u2019s assume a model where compute stays capped. The world where frontier labs are making money is one where they continue to make fast progress. Because fundamentally your margin is limited by how good the alternative is. So you are able to make money because you have a frontier model. If you didn\u2019t have a frontier model you wouldn\u2019t be making money. So this model requires there never to be a steady state. Forever and ever you keep making more algorithmic progress.\n\nDario Amodei\n\nI don\u2019t think that\u2019s true. I mean, I feel like we\u2019re in an economics class.\n\nDwarkesh Patel\n\nDo you know the Tyler Cowen quote? We never stop talking about economics.\n\nDario Amodei\n\nWe never stop talking about economics. So no, I don\u2019t think this field\u2019s going to be a monopoly. All my lawyers never want me to say the word \u201cmonopoly\u201d. But I don\u2019t think this field\u2019s going to be a monopoly. You do get industries in which there are a small number of players. Not one, but a small number of players.\n\nOrdinarily, the way you get monopolies like Facebook or Meta\u2014I always call them Facebook\u2014is these kinds of network effects. The way you get industries in which there are a small number of players, is very high costs of entry. Cloud is like this. I think cloud is a good example of this. There are three, maybe four, players within cloud. I think that\u2019s the same for AI, three, maybe four.\n\nThe reason is that it\u2019s so expensive. It requires so much expertise and so much capital to run a cloud company. You have to put up all this capital. In addition to putting up all this capital, you have to get all of this other stuff that requires a lot of skill to make it happen.\n\nSo if you go to someone and you\u2019re like, \u201cI want to disrupt this industry, here\u2019s $100 billion.\u201d You\u2019re like, \u201cokay, I\u2019m putting in $100 billion and also betting that you can do all these other things that these people have been doing.\u201d\n\nDwarkesh Patel\n\nOnly to decrease the profit.\n\nDario Amodei\n\nThe effect of your entering is that profit margins go down. So, we have equilibria like this all the time in the economy where we have a few players. Profits are not astronomical. Margins are not astronomical, but they\u2019re not zero. That\u2019s what we see on cloud. Cloud is very undifferentiated. Models are more differentiated than cloud.\n\nEveryone knows Claude is good at different things than GPT is good at, than Gemini is good at. It\u2019s not just that Claude\u2019s good at coding, GPT is good at math and reasoning. It\u2019s more subtle than that. Models are good at different types of coding. Models have different styles. I think these things are actually quite different from each other, and so I would expect more differentiation than you see in cloud.\n\nNow, there actually is one counter-argument. That counter-argument is if the process of producing models, if AI models can do that themselves, then that could spread throughout the economy. But that is not an argument for commoditizing AI models in general. That\u2019s kind of an argument for commoditizing the whole economy at once.\n\nI don\u2019t know what quite happens in that world where basically anyone can do anything, anyone can build anything, and there\u2019s no moat around anything at all. I don\u2019t know, maybe we want that world. Maybe that\u2019s the end state here. Maybe when AI models can do everything, if we\u2019ve solved all the safety and security problems, that\u2019s one of the mechanisms for the economy just flattening itself again. But that\u2019s kind of far post-\u201dcountry of geniuses in the data center.\u201d\n\nDwarkesh Patel\n\nMaybe a finer way to put that potential point is: 1) it seems like AI research is especially loaded on raw intellectual power, which will be especially abundant in the world of AGI. And 2) if you just look at the world today, there are very few technologies that seem to be diffusing as fast as AI algorithmic progress. So that does hint that this industry is sort of structurally diffusive.\n\nDario Amodei\n\nI think coding is going fast, but I think AI research is a superset of coding and there are aspects of it that are not going fast. But I do think, again, once we get coding, once we get AI models going fast, then that will speed up the ability of AI models to do everything else. So while coding is going fast now, I think once the AI models are building the next AI models and building everything else, the whole economy will kind of go at the same pace.\n\nI am worried geographically, though. I\u2019m a little worried that just proximity to AI, having heard about AI, may be one differentiator. So when I said the 10-20% growth rate, a worry I have is that the growth rate could be like 50% in Silicon Valley and parts of the world that are socially connected to Silicon Valley, and not that much faster than its current pace elsewhere. I think that\u2019d be a pretty messed up world. So one of the things I think about a lot is how to prevent that.\n\nDwarkesh Patel\n\nDo you think that once we have this country of geniuses in a data center, that robotics is sort of quickly solved afterwards? Because it seems like a big problem with robotics is that a human can learn how to teleoperate current hardware, but current AI models can\u2019t, at least not in a way that\u2019s super productive. And so if we have this ability to learn like a human, shouldn\u2019t it solve robotics immediately as well?\n\nDario Amodei\n\nI don\u2019t think it\u2019s dependent on learning like a human. It could happen in different ways. Again, we could have trained the model on many different video games, which are like robotic controls, or many different simulated robotics environments, or just train them to control computer screens, and they learn to generalize.\n\nSo it will happen... it\u2019s not necessarily dependent on human-like learning. Human-like learning is one way it could happen. If the model\u2019s like, \u201cOh, I pick up a robot, I don\u2019t know how to use it, I learn,\u201d that could happen because we discovered continual learning. That could also happen because we trained the model on a bunch of environments and then generalized, or it could happen because the model learns that in the context length. It doesn\u2019t actually matter which way. If we go back to the discussion we had an hour ago, that type of thing can happen in several different ways.\n\nBut I do think when for whatever reason the models have those skills, then robotics will be revolutionized\u2014both the design of robots, because the models will be much better than humans at that, and also the ability to control robots. So we\u2019ll get better at building the physical hardware, building the physical robots, and we\u2019ll also get better at controlling it.\n\nNow, does that mean the robotics industry will also be generating trillions of dollars of revenue? My answer there is yes, but there will be the same extremely fast, but not infinitely fast diffusion. So will robotics be revolutionized? Yeah, maybe tack on another year or two. That\u2019s the way I think about these things.\n\nDwarkesh Patel\n\nMakes sense. There\u2019s a general skepticism about extremely fast progress. Here\u2019s my view. It sounds like you are going to solve continual learning one way or another within a matter of years. But just as people weren\u2019t talking about continual learning a couple of years ago, and then we realized, \u201cOh, why aren\u2019t these models as useful as they could be right now, even though they are clearly passing the Turing test and are experts in so many different domains? Maybe it\u2019s this thing.\u201d\n\nThen we solve this thing and we realize, actually, there\u2019s another thing that human intelligence can do that\u2019s a basis of human labor that these models can\u2019t do. So why not think there will be more things like this, where we\u2019ve found more pieces of human intelligence?\n\nDario Amodei\n\nWell, to be clear, I think continual learning, as I\u2019ve said before, might not be a barrier at all. I think we may just get there by pre-training generalization and RL generalization. I think there just might not be such a thing at all.\n\nIn fact, I would point to the history in ML of people coming up with things that are barriers that end up kind of dissolving within the big blob of compute. People talked about, \u201cHow do your models keep track of nouns and verbs?\u201d \u201cThey can understand syntactically, but they can\u2019t understand semantically? It\u2019s only statistical correlations.\u201d \u201cYou can understand a paragraph, you can\u2019t understand a word. There\u2019s reasoning, you can\u2019t do reasoning.\u201d But then suddenly it turns out you can do code and math very well.\n\nSo I think there\u2019s actually a stronger history of some of these things seeming like a big deal and then kind of dissolving. Some of them are real. The need for data is real, maybe continual learning is a real thing.\n\nBut again, I would ground us in something like code. I think we may get to the point in a year or two where the models can just do SWE end-to-end. That\u2019s a whole task. That\u2019s a whole sphere of human activity that we\u2019re just saying models can do now.\n\nDwarkesh Patel\n\nWhen you say end-to-end, do you mean setting technical direction, understanding the context of the problem, et cetera?\n\nDario Amodei\n\nYes. I mean all of that.\n\nDwarkesh Patel\n\nInteresting. I feel like that is AGI-complete, which maybe is internally consistent. But it\u2019s not like saying 90% of code or 100% of code.\n\nDario Amodei\n\nNo, I gave this spectrum: 90% of code, 100% of code, 90% of end-to-end SWE, 100% of end-to-end SWE. New tasks are created for SWEs. Eventually those get done as well. It\u2019s a long spectrum there, but we\u2019re traversing the spectrum very quickly.\n\nDwarkesh Patel\n\nI do think it\u2019s funny that I\u2019ve seen a couple of podcasts you\u2019ve done where the hosts will be like, \u201cBut Dwarkesh wrote the essay about the continuous learning thing.\u201d It always makes me crack up because you\u2019ve been an AI researcher for 10 years. I\u2019m sure there\u2019s some feeling of, \u201cOkay, so a podcaster wrote an essay, and every interview I get asked about it.\u201d\n\nDario Amodei\n\nThe truth of the matter is that we\u2019re all trying to figure this out together. There are some ways in which I\u2019m able to see things that others aren\u2019t. These days that probably has more to do with seeing a bunch of stuff within Anthropic and having to make a bunch of decisions than I have any great research insight that others don\u2019t.\n\nI\u2019m running a 2,500 person company. It\u2019s actually pretty hard for me to have concrete research insight, much harder than it would have been 10 years ago or even two or three years ago.\n\nDwarkesh Patel\n\nAs we go towards a world of a full drop-in remote worker replacement, does an API pricing model still make the most sense? If not, what is the correct way to price AGI, or serve AGI?\n\nDario Amodei\n\nI think there\u2019s going to be a bunch of different business models here, all at once, that are going to be experimented with. I actually do think that the API model is more durable than many people think. One way I think about it is if the technology is advancing quickly, if it\u2019s advancing exponentially, what that means is there\u2019s always a surface area of new use cases that have been developed in the last three months.\n\nAny kind of product surface you put in place is always at risk of sort of becoming irrelevant. Any given product surface probably makes sense for a range of capabilities of the model. The chatbot is already running into limitations where making it smarter doesn\u2019t really help the average consumer that much. But I don\u2019t think that\u2019s a limitation of AI models. I don\u2019t think that\u2019s evidence that the models are good enough and them getting better doesn\u2019t matter to the economy. It doesn\u2019t matter to that particular product.\n\nSo I think the value of the API is that the API always offers an opportunity, very close to the bare metal, to build on what the latest thing is. There\u2019s always going to be this front of new startups and new ideas that weren\u2019t possible a few months ago and are possible because the model is advancing.\n\nI actually predict that it\u2019s going to exist alongside other models, but we\u2019re always going to have the API business model because there\u2019s always going to be a need for a thousand different people to try experimenting with the model in a different way. 100 of them become startups and ten of them become big successful startups. Two or three really end up being the way that people use the model of a given generation.\n\nSo I basically think it\u2019s always going to exist. At the same time, I\u2019m sure there\u2019s going to be other models as well. Not every token that\u2019s output by the model is worth the same amount. Think about what is the value of the tokens that the model outputs when someone calls them up and says, \u201cMy Mac isn\u2019t working,\u201d or something, the model\u2019s like, \u201crestart it.\u201d Someone hasn\u2019t heard that before, but the model said that 10 million times. Maybe that\u2019s worth like a dollar or a few cents or something.\n\nWhereas if the model goes to one of the pharmaceutical companies and it says, \u201cOh, you know, this molecule you\u2019re developing, you should take the aromatic ring from that end of the molecule and put it on that end of the molecule. If you do that, wonderful things will happen.\u201d Those tokens could be worth tens of millions of dollars.\n\nSo I think we\u2019re definitely going to see business models that recognize that. At some point we\u2019re going to see \u201cpay for results\u201d in some form, or we may see forms of compensation that are like labor, that kind of work by the hour. I don\u2019t know. I think because it\u2019s a new industry, a lot of things are going to be tried. I don\u2019t know what will turn out to be the right thing.\n\nDwarkesh Patel\n\nI take your point that people will have to try things to figure out what is the best way to use this blob of intelligence. But what I find striking is Claude Code. I don\u2019t think in the history of startups there has been a single application that has been as hotly competed in as coding agents. Claude Code is a category leader here. That seems surprising to me.\n\nIt doesn\u2019t seem intrinsically that Anthropic had to build this. I wonder if you have an accounting of why it had to be Anthropic or how Anthropic ended up building an application in addition to the model underlying it that was successful.\n\nDario Amodei\n\nSo it actually happened in a pretty simple way, which is that we had our own coding models, which were good at coding. Around the beginning of 2025, I said, \u201cI think the time has come where you can have nontrivial acceleration of your own research if you\u2019re an AI company by using these models.\u201d Of course, you need an interface, you need a harness to use them.\n\nSo I encouraged people internally. I didn\u2019t say this is one thing that you have to use. I just said people should experiment with this. I think it might have been originally called Claude CLI, and then the name eventually got changed to Claude Code. Internally, it was the thing that everyone was using and it was seeing fast internal adoption.\n\nI looked at it and I said, \u201cProbably we should launch this externally, right?\u201d It\u2019s seen such fast adoption within Anthropic. Coding is a lot of what we do. We have an audience of many, many hundreds of people that\u2019s in some ways at least representative of the external audience. So it looks like we already have product market fit. Let\u2019s launch this thing.\n\nAnd then we launched it. I think just the fact that we ourselves are kind of developing the model and we ourselves know what we most need to use the model, I think it\u2019s kind of creating this feedback loop.\n\nDwarkesh Patel\n\nI see. In the sense that you, let\u2019s say a developer at Anthropic is like, \u201cAh, it would be better if it was better at this X thing.\u201d Then you bake that into the next model that you build.\n\nDario Amodei\n\nThat\u2019s one version of it, but then there\u2019s just the ordinary product iteration. We have a bunch of coders within Anthropic, they use Claude Code every day and so we get fast feedback. That was more important in the early days. Now, of course, there are millions of people using it, and so we get a bunch of external feedback as well. But it\u2019s just great to be able to get kind of fast internal feedback.\n\nI think this is the reason why we launched a coding model and didn\u2019t launch a pharmaceutical company. My background\u2019s in biology, but we don\u2019t have any of the resources that are needed to launch a pharmaceutical company.\n\n01:31:19 - Will regulations destroy the boons of AGI?\nDwarkesh Patel\n\nLet me now ask you about making AI go well. It seems like whatever vision we have about how AI goes well has to be compatible with two things: 1) the ability to build and run AIs is diffusing extremely rapidly and 2) the population of AIs, the amount we have and their intelligence, will also increase very rapidly.\n\nThat means that lots of people will be able to build huge populations of misaligned AIs, or AIs which are just companies which are trying to increase their footprint or have weird psyches like Sydney Bing, but now they\u2019re superhuman. What is a vision for a world in which we have an equilibrium that is compatible with lots of different AIs, some of which are misaligned, running around?\n\nDario Amodei\n\nI think in \u201cThe Adolescence of Technology\u201d, I was skeptical of the balance of power. But the thing I was specifically skeptical of is you have three or four of these companies all building models that are derived from the same thing, that they would check each other. Or even that any number of them would check each other.\n\nDario Amodei\n\nWe might live in an offense-dominant world where one person or one AI model is smart enough to do something that causes damage for everything else. In the short run, we have a limited number of players now. So we can start within the limited number of players. We need to put in place the safeguards. We need to make sure everyone does the right alignment work. We need to make sure everyone has bioclassifiers. Those are the immediate things we need to do.\n\nI agree that that doesn\u2019t solve the problem in the long run, particularly if the ability of AI models to make other AI models proliferates, then the whole thing can become harder to solve. I think in the long run we need some architecture of governance. We need some architecture of governance that preserves human freedom, but also allows us to govern a very large number of human systems, AI systems, hybrid human-AI companies or economic units.\n\nSo we\u2019re gonna need to think about: how do we protect the world against bioterrorism? How do we protect the world against mirror life? Probably we\u2019re gonna need some kind of AI monitoring system that monitors for all of these things. But then we need to build this in a way that preserves civil liberties and our constitutional rights. So I think just as anything else, it\u2019s a new security landscape with a new set of tools and a new set of vulnerabilities.\n\nMy worry is, if we had 100 years for this to happen all very slowly, we\u2019d get used to it. We\u2019ve gotten used to the presence of explosives in society or the presence of various new weapons or the presence of video cameras. We would get used to it over 100 years and we\u2019d develop governance mechanisms. We\u2019d make our mistakes. My worry is just that this is happening all so fast. So maybe we need to do our thinking faster about how to make these governance mechanisms work.\n\nDwarkesh Patel\n\nIt seems like in an offense-dominant world, over the course of the next century\u2014the idea is that AI is making the progress that would happen over the next century happen in some period of five to ten years\u2014we would still need the same mechanisms, or balance of power would be similarly intractable, even if humans were the only game in town.\n\nI guess we have the advice of AI. But it fundamentally doesn\u2019t seem like a totally different ball game here. If checks and balances were going to work, they would work with humans as well. If they aren\u2019t going to work, they wouldn\u2019t work with AIs as well. So maybe this just dooms human checks and balances as well.\n\nDario Amodei\n\nAgain, I think there\u2019s some way to make this happen. The governments of the world may have to work together to make it happen. We may have to talk to AIs about building societal structures in such a way that these defenses are possible. I don\u2019t know. I don\u2019t want to say this is so far ahead in time, but it\u2019s so far ahead in technological ability that may happen over a short period of time, that it\u2019s hard for us to anticipate it in advance.\n\nDwarkesh Patel\n\nSpeaking of governments getting involved, on December 26, the Tennessee legislature introduced a bill which said, \u201cIt would be an offense for a person to knowingly train artificial intelligence to provide emotional support, including through open-ended conversations with a user.\u201d Of course, one of the things that Claude attempts to do is be a thoughtful, knowledgeable friend.\n\nIn general, it seems like we\u2019re going to have this patchwork of state laws. A lot of the benefits that normal people could experience as a result of AI are going to be curtailed, especially when we get into the kinds of things you discuss in \u201cMachines of Loving Grace\u201d: biological freedom, mental health improvements, et cetera.\n\nIt seems easy to imagine worlds in which these get Whac-A-Moled away by different laws, whereas bills like this don\u2019t seem to address the actual existential threats that you\u2019re concerned about. I\u2019m curious to understand, in the context of things like this, Anthropic\u2019s position against the federal moratorium on state AI laws.\n\nDario Amodei\n\nThere are many different things going on at once. I think that particular law is dumb. It was clearly made by legislators who just probably had little idea what AI models could do and not do. They\u2019re like, \u201cAI models serving us, that just sounds scary. I don\u2019t want that to happen.\u201d So we\u2019re not in favor of that.\n\nBut that wasn\u2019t the thing that was being voted on. The thing that was being voted on is: we\u2019re going to ban all state regulation of AI for 10 years with no apparent plan to do any federal regulation of AI, which would take Congress to pass, which is a very high bar. So the idea that we\u2019d ban states from doing anything for 10 years\u2026 People said they had a plan for the federal government, but there was no actual proposal on the table. There was no actual attempt.\n\nGiven the serious dangers that I lay out in \u201cAdolescence of Technology\u201d around things like biological weapons and bioterrorism autonomy risk, and the timelines we\u2019ve been talking about\u201410 years is an eternity\u2014I think that\u2019s a crazy thing to do. So if that\u2019s the choice, if that\u2019s what you force us to choose, then we\u2019re going to choose not to have that moratorium. I think the benefits of that position exceed the costs, but it\u2019s not a perfect position if that\u2019s the choice.\n\nNow, I think the thing that we should do, the thing that I would support, is the federal government should step in, not saying \u201cstates you can\u2019t regulate\u201d, but \u201cHere\u2019s what we\u2019re going to do, and states you can\u2019t differ from this.\u201d I think preemption is fine in the sense of saying that the federal government says, \u201cHere is our standard. This applies to everyone. States can\u2019t do something different.\u201d\n\nThat would be something I would support if it would be done in the right way. But this idea of states, \u201cYou can\u2019t do anything and we\u2019re not doing anything either,\u201d that struck us as very much not making sense. I think it will not age well, it is already starting to not age well with all the backlash that you\u2019ve seen.\n\nNow, in terms of what we would want, the things we\u2019ve talked about are starting with transparency standards in order to monitor some of these autonomy risks and bioterrorism risks. As the risks become more serious, as we get more evidence for them, then I think we could be more aggressive in some targeted ways and say, \u201cHey, AI bioterrorism is really a threat. Let\u2019s pass a law that forces people to have classifiers.\u201d\n\nI could even imagine\u2026 It depends. It depends how serious the threat it ends up being. We don\u2019t know for sure. We need to pursue this in an intellectually honest way where we say that ahead of time, the risk has not emerged yet. But I could certainly imagine, with the pace that things are going at, a world where later this year we say, \u201cHey, this AI bioterrorism stuff is really serious. We should do something about it. We should put it in a federal standard. If the federal government won\u2019t act, we should put it in a state standard.\u201d I could totally see that.\n\nDwarkesh Patel\n\nI\u2019m concerned about a world where if you just consider the pace of progress you\u2019re expecting, the life cycle of legislation... The benefits are, as you say because of diffusion lag, slow enough that I really do think this patchwork of state laws, on the current trajectory, would prohibit. I mean if having an emotional chatbot friend is something that freaks people out, then just imagine the kinds of actual benefits from AI we want normal people to be able to experience. From improvements in health and healthspan and improvements in mental health and so forth.\n\nWhereas at the same time, it seems like you think the dangers are already on the horizon and I just don\u2019t see that much\u2026 It seems like it would be especially injurious to the benefits of AI as compared to the dangers of AI. So that\u2019s maybe where the cost benefit makes less sense to me.\n\nDario Amodei\n\nSo there\u2019s a few things here. People talk about there being thousands of these state laws. First of all, the vast, vast majority of them do not pass. The world works a certain way in theory, but just because a law has been passed doesn\u2019t mean it\u2019s really enforced. The people implementing it may be like, \u201cOh my God, this is stupid. It would mean shutting off everything that\u2019s ever been built in Tennessee.\u201d Very often, laws are interpreted in a way that makes them not as dangerous or harmful. On the same side, of course, you have to worry if you\u2019re passing a law to stop a bad thing; you have this problem as well.\n\nMy basic view is that if we could decide what laws were passed and how things were done\u2014and we\u2019re only one small input into that\u2014I would deregulate a lot of the stuff around the health benefits of AI. I don\u2019t worry as much about the chatbot laws. I actually worry more about the drug approval process, where I think AI models are going to greatly accelerate the rate at which we discover drugs, and the pipeline will get jammed up. The pipeline will not be prepared to process all the stuff that\u2019s going through it.\n\nI think reform of the regulatory process should bias more towards the fact that we have a lot of things coming where the safety and efficacy is actually going to be really crisp and clear, a beautiful thing, and really effective. Maybe we don\u2019t need all this superstructure around it that was designed around an era of drugs that barely work and often have serious side effects.\n\nAt the same time, I think we should be ramping up quite significantly the safety and security legislation. Like I\u2019ve said, starting with transparency is my view of trying not to hamper the industry, trying to find the right balance. I\u2019m worried about it. Some people criticize my essay for saying, \u201cThat\u2019s too slow. The dangers of AI will come too soon if we do that.\u201d\n\nWell, basically, I think the last six months and maybe the next few months are going to be about transparency. Then, if these risks emerge when we\u2019re more certain of them\u2014which I think we might be as soon as later this year\u2014then I think we need to act very fast in the areas where we\u2019ve actually seen the risk.\n\nI think the only way to do this is to be nimble. Now, the legislative process is normally not nimble, but we need to emphasize the urgency of this to everyone involved. That\u2019s why I\u2019m sending this message of urgency. That\u2019s why I wrote Adolescence of Technology. I wanted policymakers, economists, national security professionals, and decision-makers to read it so that they have some hope of acting faster than they would have otherwise.\n\nDwarkesh Patel\n\nIs there anything you can do or advocate that would make it more certain that the benefits of AI are better instantiated? I feel like you have worked with legislatures to say, \u201cOkay, we\u2019re going to prevent bioterrorism here. We\u2019re going to increase transparency, we\u2019re going to increase whistleblower protection.\u201d But I think by default, the actual benefits we\u2019re looking forward to seem very fragile to different kinds of moral panics or political economy problems.\n\nDario Amodei\n\nI don\u2019t actually agree that much regarding the developed world. I feel like in the developed world, markets function pretty well. When there\u2019s a lot of money to be made on something and it\u2019s clearly the best available alternative, it\u2019s actually hard for the regulatory system to stop it.\n\nWe\u2019re seeing that in AI itself. A thing I\u2019ve been trying to fight for is export controls on chips to China. That\u2019s in the national security interest of the US. That\u2019s squarely within the policy beliefs of almost everyone in Congress of both parties. The case is very clear. The counterarguments against it, I\u2019ll politely call them fishy. Yet it doesn\u2019t happen and we sell the chips because there\u2019s so much money riding on it. That money wants to be made. In that case, in my opinion, that\u2019s a bad thing. But it also applies when it\u2019s a good thing.\n\nSo if we\u2019re talking about drugs and benefits of the technology, I am not as worried about those benefits being hampered in the developed world. I am a little worried about them going too slow. As I said, I do think we should work to speed the approval process in the FDA. I do think we should fight against these chatbot bills that you\u2019re describing. Described individually, I\u2019m against them. I think they\u2019re stupid.\n\nBut I actually think the bigger worry is the developing world, where we don\u2019t have functioning markets and where we often can\u2019t build on the technology that we\u2019ve had. I worry more that those folks will get left behind. And I worry that even if the cures are developed, maybe there\u2019s someone in rural Mississippi who doesn\u2019t get it as well. That\u2019s a smaller version of the concern we have in the developing world.\n\nSo the things we\u2019ve been doing are working with philanthropists. We work with folks who deliver medicine and health interventions to the developing world, to sub-Saharan Africa, India, Latin America, and other developing parts of the world. That\u2019s the thing I think that won\u2019t happen on its own.\n\n01:47:41 - Why can\u2019t China and America both have a country of geniuses in a datacenter?\nDwarkesh Patel\n\nYou mentioned export controls. Why shouldn\u2019t the US and China both have a \u201ccountry of geniuses in a data center\u201d?\n\nDario Amodei\n\nWhy won\u2019t it happen or why shouldn\u2019t it happen?\n\nDwarkesh Patel\n\nWhy shouldn\u2019t it happen.\n\nDario Amodei\n\nIf this does happen, we could have a few situations. If we have an offense-dominant situation, we could have a situation like nuclear weapons, but more dangerous. Either side could easily destroy everything.\n\nWe could also have a world where it\u2019s unstable. The nuclear equilibrium is stable because it\u2019s deterrence. But let\u2019s say there was uncertainty about, if the two AIs fought, which AI would win? That could create instability. You often have conflict when the two sides have a different assessment of their likelihood of winning. If one side is like, \u201cOh yeah, there\u2019s a 90% chance I\u2019ll win,\u201d and the other side thinks the same, then a fight is much more likely. They can\u2019t both be right, but they can both think that.\n\nDwarkesh Patel\n\nBut this seems like a fully general argument against the diffusion of AI technology. That\u2019s the implication of this world.\n\nDario Amodei\n\nLet me just go on, because I think we will get diffusion eventually. The other concern I have is that governments will oppress their own people with AI. I\u2019m worried about a world where you have a country in which there\u2019s already a government that\u2019s building a high-tech authoritarian state. To be clear, this is about the government. This is not about the people. We need to find a way for people everywhere to benefit. My worry here is about governments. My worry is if the world gets carved up into two pieces, one of those two pieces could be authoritarian or totalitarian in a way that\u2019s very difficult to displace.\n\nNow, will governments eventually get powerful AI, and is there a risk of authoritarianism? Yes. Will governments eventually get powerful AI, and is there a risk of bad equilibria? Yes, I think both things. But the initial conditions matter. At some point, we\u2019re going to need to set up the rules of the road.\n\nI\u2019m not saying that one country, either the United States or a coalition of democracies\u2014which I think would be a better setup, although it requires more international cooperation than we currently seem to want to make\u2014should just say, \u201cThese are the rules of the road.\u201d There\u2019s going to be some negotiation. The world is going to have to grapple with this.\n\nWhat I would like is for the democratic nations of the world\u2014those whose governments represent closer to pro-human values\u2014are holding the stronger hand and have more leverage when the rules of the road are set. So I\u2019m very concerned about that initial condition.\n\nDwarkesh Patel\n\nI was re-listening to the interview from three years ago, and one of the ways it aged poorly is that I kept asking questions assuming there was going to be some key fulcrum moment two to three years from now. In fact, being that far out, it just seems like progress continues, AI improves, AI is more diffused, and people will use it for more things.\n\nIt seems like you\u2019re imagining a world in the future where the countries get together, and \u201cHere\u2019s the rules of the road, here\u2019s the leverage we have, and here\u2019s the leverage you have.\u201d But on the current trajectory, everybody will have more AI. Some of that AI will be used by authoritarian countries. Some of that within the authoritarian countries will be used by private actors versus state actors.\n\nIt\u2019s not clear who will benefit more. It\u2019s always unpredictable to tell in advance. It seems like the internet privileged authoritarian countries more than you would\u2019ve expected. Maybe AI will be the opposite way around. I want to better understand what you\u2019re imagining here.\n\nDario Amodei\n\nJust to be precise about it, I think the exponential of the underlying technology will continue as it has before. The models get smarter and smarter, even when they get to a \u201ccountry of geniuses in a data center.\u201d I think you can continue to make the model smarter. There\u2019s a question of getting diminishing returns on their value in the world. How much does it matter after you\u2019ve already solved human biology? At some point you can do harder, more abstruse math problems, but nothing after that matters.\n\nPutting that aside, I do think the exponential will continue, but there will be certain distinguished points on the exponential. Companies, individuals, and countries will reach those points at different times.\n\nIn \u201cThe Adolescence of Technology\u201d I talk about: Is a nuclear deterrent still stable in the world of AI? I don\u2019t know, but that\u2019s an example of one thing we\u2019ve taken for granted. The technology could reach such a level that we can no longer be certain of it. Think of others. There are points where if you reach a certain level, maybe you have offensive cyber dominance, and every computer system is transparent to you after that unless the other side has an equivalent defense.\n\nI don\u2019t know what the critical moment is or if there\u2019s a single critical moment. But I think there will be either a critical moment, a small number of critical moments, or some critical window where AI confers some large advantage from the perspective of national security, and one country or coalition has reached it before others.\n\nI\u2019m not advocating that they just say, \u201cOkay, we\u2019re in charge now.\u201d That\u2019s not how I think about it. The other side is always catching up. There are extreme actions you\u2019re not willing to take, and it\u2019s not right to take complete control anyway. But at the point that happens, people are going to understand that the world has changed. There\u2019s going to be some negotiation, implicit or explicit, about what the post-AI world order looks like. My interest is in making that negotiation be one in which classical liberal democracy has a strong hand.\n\nDwarkesh Patel\n\nI want to understand what that better means, because you say in the essay, \u201cAutocracy is simply not a form of government that people can accept in the post-powerful AI age.\u201d That sounds like you\u2019re saying the CCP as an institution cannot exist after we get AGI. That seems like a very strong demand, and it seems to imply a world where the leading lab or the leading country will be able to\u2014and by that language, should get to\u2014determine how the world is governed or what kinds of governments are, and are not, allowed.\n\nDario Amodei\n\nI believe that paragraph said something like, \u201cYou could take it even further and say X.\u201d I wasn\u2019t necessarily endorsing that view. I was saying, \u201cHere\u2019s a weaker thing that I believe. We have to worry a lot about authoritarians and we should try to check them and limit their power. You could take this much further and have a more interventionist view that says authoritarian countries with AI are these self-fulfilling cycles that are very hard to displace, so you just need to get rid of them from the beginning.\u201d\n\nThat has exactly all the problems you say. If you were to make a commitment to overthrowing every authoritarian country, they would take a bunch of actions now that could lead to instability. That just may not be possible.\n\nBut the point I was making that I do endorse is that it is quite possible that... Today, the view, my view, in most of the Western world is that democracy is a better form of government than authoritarianism. But if a country\u2019s authoritarian, we don\u2019t react the way we\u2019d react if they committed a genocide or something. I guess what I\u2019m saying is I\u2019m a little worried that in the age of AGI, authoritarianism will have a different meaning. It will be a graver thing. We have to decide one way or another how to deal with that. The interventionist view is one possible view. I was exploring such views. It may end up being the right view, or it may end up being too extreme. But I do have hope.\n\nOne piece of hope I have is that we have seen that as new technologies are invented, forms of government become obsolete. I mentioned this in \u201cAdolescence of Technology\u201d, where I said feudalism was basically a form of government, and when we invented industrialization, feudalism was no longer sustainable. It no longer made sense.\n\nDwarkesh Patel\n\nWhy is that hope? Couldn\u2019t that imply that democracy is no longer going to be a competitive system?\n\nDario Amodei\n\nRight, it could go either way. But these problems with authoritarianism get deeper. I wonder if that\u2019s an indicator of other problems that authoritarianism will have. In other words, because authoritarianism becomes worse, people are more afraid of it. They work harder to stop it. You have to think in terms of total equilibrium. I just wonder if it will motivate new ways of thinking about how to preserve and protect freedom with the new technology.\n\nEven more optimistically, will it lead to a collective reckoning and a more emphatic realization of how important some of the things we take as individual rights are? A more emphatic realization that we really can\u2019t give these away. We\u2019ve seen there\u2019s no other way to live that actually works.\n\nI am actually hopeful that\u2014it sounds too idealistic, but I believe it could be the case\u2014dictatorships become morally obsolete. They become morally unworkable forms of government and the crisis that that creates is sufficient to force us to find another way.\n\nDwarkesh Patel\n\nI think there is genuinely a tough question here which I\u2019m not sure how you resolve. We\u2019ve had to come out one way or another on it through history. With China in the \u201870s and \u201880s, we decided that even though it\u2019s an authoritarian system, we will engage with it. I think in retrospect that was the right call, because it\u2019s a state authoritarian system but a billion-plus people are much wealthier and better off than they would\u2019ve otherwise been. It\u2019s not clear that it would\u2019ve stopped being an authoritarian country otherwise. You can just look at North Korea as an example of that.\n\nI don\u2019t know if it takes that much intelligence to remain an authoritarian country that continues to coalesce its own power. You can imagine a North Korea with an AI that\u2019s much worse than everybody else\u2019s, but still enough to keep power.\n\nIn general, it seems like we should just have this attitude that the benefits of AI\u2014in the form of all these empowerments of humanity and health\u2014will be big. Historically, we have decided it\u2019s good to spread the benefits of technology widely, even to people whose governments are authoritarian. It is a tough question, how to think about it with AI, but historically we have said, \u201cyes, this is a positive-sum world, and it\u2019s still worth diffusing the technology.\u201d\n\nDario Amodei\n\nThere are a number of choices we have. Framing this as a government-to-government decision in national security terms is one lens, but there are a lot of other lenses. You could imagine a world where we produce all these cures to diseases. The cures are fine to sell to authoritarian countries, but the data centers just aren\u2019t. The chips and the data centers aren\u2019t, and the AI industry itself isn\u2019t.\n\nAnother possibility I think folks should think about is this. Could there be developments we can make\u2014either that naturally happen as a result of AI, or that we could make happen by building technology on AI\u2014that create an equilibrium where it becomes infeasible for authoritarian countries to deny their people private use of the benefits of the technology? Are there equilibria where we can give everyone in an authoritarian country their own AI model that defends them from surveillance and there isn\u2019t a way for the authoritarian country to crack down on this while retaining power?\n\nI don\u2019t know. That sounds to me like if that went far enough, it would be a reason why authoritarian countries would disintegrate from the inside. But maybe there\u2019s a middle world where there\u2019s an equilibrium where, if they want to hold on to power, the authoritarians can\u2019t deny individualized access to the technology.\n\nBut I actually do have a hope for the more radical version. Is it possible that the technology might inherently have properties\u2014or that by building on it in certain ways we could create properties\u2014that have this dissolving effect on authoritarian structures? Now, we hoped originally\u2014think back to the beginning of the Obama administration\u2014that social media and the internet would have that property, and it turns out not to. But what if we could try again with the knowledge of how many things could go wrong, and that this is a different technology? I don\u2019t know if it would work, but it\u2019s worth a try.\n\nDwarkesh Patel\n\nIt\u2019s just very unpredictable. There are first principles reasons why authoritarianism might be privileged.\n\nDario Amodei\n\nIt\u2019s all very unpredictable. We just have to recognize the problem and come up with 10 things we can try, try those, and then assess which ones are working, if any. Then try new ones if the old ones aren\u2019t working.\n\nDwarkesh Patel\n\nBut I guess that nets out to today, as you say, that we will not sell data centers, or chips, and the ability to make chips to China. So in some sense, you are denying\u2026 There would be some benefits to the Chinese economy, Chinese people, et cetera, because we\u2019re doing that. Then there\u2019d also be benefits to the American economy because it\u2019s a positive-sum world. We could trade. They could have their country\u2019s data centers doing one thing. We could have ours doing another. Already, you\u2019re saying it\u2019s not worth that positive-sum stipend to empower those countries?\n\nDario Amodei\n\nWhat I would say is that we are about to be in a world where growth and economic value will come very easily if we\u2019re able to build these powerful AI models. What will not come easily is distribution of benefits, distribution of wealth, political freedom. These are the things that are going to be hard to achieve.\n\nSo when I think about policy, I think that the technology and the market will deliver all the fundamental benefits, this is my fundamental belief, almost faster than we can take them. These questions about distribution and political freedom and rights are the ones that will actually matter and that policy should focus on.\n\nDwarkesh Patel\n\nSpeaking of distribution, as you were mentioning, we have developing countries. In many cases, catch-up growth has been weaker than we would have hoped for. But when catch-up growth does happen, it\u2019s fundamentally because they have underutilized labor. We can bring the capital and know-how from developed countries to these countries, and then they can grow quite rapidly.\n\nObviously, in a world where labor is no longer the constraining factor, this mechanism no longer works. So is the hope basically to rely on philanthropy from the people or countries who immediately get wealthy from AI? What is the hope?\n\nDario Amodei\n\nPhilanthropy should obviously play some role, as it has in the past. But I think growth is always better and stronger if we can make it endogenous.\n\nWhat are the relevant industries in an AI-driven world? I said we shouldn\u2019t build data centers in China, but there\u2019s no reason we shouldn\u2019t build data centers in Africa. In fact, I think it\u2019d be great to build data centers in Africa. As long as they\u2019re not owned by China, we should build data centers in Africa. I think that\u2019s a great thing to do.\n\nThere\u2019s no reason we can\u2019t build a pharmaceutical industry that\u2019s AI-driven. If AI is accelerating drug discovery, then there will be a bunch of biotech startups. Let\u2019s make sure some of those happen in the developing world. Certainly, during the transition\u2014we can talk about the point where humans have no role\u2014humans will still have some role in starting up these companies and supervising the AI models. So let\u2019s make sure some of those humans are in the developing world so that fast growth can happen there as well.\n\nDwarkesh Patel\n\nYou guys recently announced that Claude is going to have a constitution that\u2019s aligned to a set of values, and not necessarily just to the end user. There\u2019s a world I can imagine where if it is aligned to the end user, it preserves the balance of power we have in the world today because everybody gets to have their own AI that\u2019s advocating for them. The ratio of bad actors to good actors stays constant. It seems to work out for our world today. Why is it better not to do that, but to have a specific set of values that the AI should carry forward?\n\nDario Amodei\n\nI\u2019m not sure I\u2019d quite draw the distinction in that way. There may be two relevant distinctions here. I think you\u2019re talking about a mix of the two. One is, should we give the model a set of instructions about \u201cdo this\u201d versus \u201cdon\u2019t do this\u201d? The other is, should we give the model a set of principles for how to act?\n\nIt\u2019s kind of purely a practical and empirical thing that we\u2019ve observed. By teaching the model principles, getting it to learn from principles, its behavior is more consistent, it\u2019s easier to cover edge cases, and the model is more likely to do what people want it to do. In other words, if you give it a list of rules\u2014\u201ddon\u2019t tell people how to hot-wire a car, don\u2019t speak in Korean\u201d\u2014it doesn\u2019t really understand the rules, and it\u2019s hard to generalize from them. It\u2019s just a list of do\u2019s and don\u2019t\u2019s.\n\nWhereas if you give it principles\u2014it has some hard guardrails like \u201cDon\u2019t make biological weapons\u201d but\u2014overall you\u2019re trying to understand what it should be aiming to do, how it should be aiming to operate. So just from a practical perspective, that turns out to be a more effective way to train the model. That\u2019s the rules versus principles trade-off.\n\nThen there\u2019s another thing you\u2019re talking about, which is the corrigibility versus intrinsic motivation trade-off. How much should the model be a kind of \u201cskin suit\u201d where it just directly follows the instructions given to it by whoever is giving those instructions, versus how much should the model have an inherent set of values and go off and do things on its own?\n\nThere I would actually say everything about the model is closer to the direction that it should mostly do what people want. It should mostly follow instructions. We\u2019re not trying to build something that goes off and runs the world on its own. We\u2019re actually pretty far on the corrigible side.\n\nNow, what we do say is there are certain things that the model won\u2019t do. I think we say it in various ways in the constitution, that under normal circumstances, if someone asks the model to do a task, it should do that task. That should be the default. But if you\u2019ve asked it to do something dangerous, or to harm someone else, then the model is unwilling to do that. So I actually think of it as a mostly corrigible model that has some limits, but those limits are based on principles.\n\nDwarkesh Patel\n\nThen the fundamental question is, how are those principles determined? This is not a special question for Anthropic. This would be a question for any AI company. But because you have been the ones to actually write down the principles, I get to ask you this question. Normally, a constitution is written down, set in stone, and there\u2019s a process of updating it and changing it and so forth. In this case, it seems like a document that people at Anthropic write, that can be changed at any time, that guides the behavior of systems that are going to be the basis of a lot of economic activity. How do you think about how those principles should be set?\n\nDario Amodei\n\nI think there are maybe three sizes of loop here, three ways to iterate. One is we iterate within Anthropic. We train the model, we\u2019re not happy with it, and we change the constitution. I think that\u2019s good to do. Putting out public updates to the constitution every once in a while is good because people can comment on it.\n\nThe second level of loop is different companies having different constitutions. I think it\u2019s useful. Anthropic puts out a constitution, Gemini puts out a constitution, and other companies put out a constitution. People can look at them and compare. Outside observers can critique and say, \u201cI like this thing from this constitution and this thing from that constitution.\u201d That creates a soft incentive and feedback for all the companies to take the best of each element and improve.\n\nThen I think there\u2019s a third loop, which is society beyond the AI companies and beyond just those who comment without hard power. There we\u2019ve done some experiments. A couple years ago, we did an experiment with the Collective Intelligence Project to basically poll people and ask them what should be in our AI constitution. At the time, we incorporated some of those changes.\n\nSo you could imagine doing something like that with the new approach we\u2019ve taken to the constitution. It\u2019s a little harder because it was an easier approach to take when the constitution was a list of dos and don\u2019ts. At the level of principles, it has to have a certain amount of coherence. But you could still imagine getting views from a wide variety of people.\n\nYou could also imagine\u2014and this is a crazy idea, but this whole interview is about crazy ideas\u2014systems of representative government having input. I wouldn\u2019t do this today because the legislative process is so slow. This is exactly why I think we should be careful about the legislative process and AI regulation. But there\u2019s no reason you couldn\u2019t, in principle, say, \u201cAll AI models have to have a constitution that starts with these things, and then you can append other things after it, but there has to be this special section that takes precedence.\u201d\n\nI wouldn\u2019t do that. That\u2019s too rigid and sounds overly prescriptive in a way that I think overly aggressive legislation is. But that is a thing you could try to do. Is there some much less heavy-handed version of that? Maybe.\n\nDwarkesh Patel\n\nI really like control loop two. Obviously, this is not how constitutions of actual governments do or should work. There\u2019s not this vague sense in which the Supreme Court will feel out how people are feeling\u2014what are the vibes\u2014and update the constitution accordingly. With actual governments, there\u2019s a more formal, procedural process.\n\nBut you have a vision of competition between constitutions, which is actually very reminiscent of how some libertarian charter cities people used to talk, about what an archipelago of different kinds of governments would look like. There would be selection among them of who could operate the most effectively and where people would be the happiest. In a sense, you\u2019re recreating that vision of a utopia of archipelagos.\n\nDario Amodei\n\nI think that vision has things to recommend it and things that will go wrong with it. It\u2019s an interesting, in some ways compelling, vision, but things will go wrong that you hadn\u2019t imagined.\n\nSo I like loop two as well, but I feel like the whole thing has got to be some mix of loops one, two, and three, and it\u2019s a matter of the proportions. I think that\u2019s gotta be the answer.\n\nDwarkesh Patel\n\nWhen somebody eventually writes the equivalent of The Making of the Atomic Bomb for this era, what is the thing that will be hardest to glean from the historical record that they\u2019re most likely to miss?\n\nDario Amodei\n\nI think a few things. One is, at every moment of this exponential, the extent to which the world outside it didn\u2019t understand it. This is a bias that\u2019s often present in history. Anything that actually happened looks inevitable in retrospect. When people look back, it will be hard for them to put themselves in the place of people who were actually making a bet on this thing to happen that wasn\u2019t inevitable, that we had these arguments like the arguments I make for scaling or that continual learning will be solved. Some of us internally put a high probability on this happening, but there\u2019s a world outside us that\u2019s not acting on that at all.\n\nI think the weirdness of it, unfortunately the insularity of it... If we\u2019re one year or two years away from it happening, the average person on the street has no idea. That\u2019s one of the things I\u2019m trying to change with the memos, with talking to policymakers. I don\u2019t know but I think that\u2019s just a crazy thing.\n\nFinally, I would say\u2014and this probably applies to almost all historical moments of crisis\u2014how absolutely fast it was happening, how everything was happening all at once. Decisions that you might think were carefully calculated, well actually you have to make that decision, and then you have to make 30 other decisions on the same day because it\u2019s all happening so fast. You don\u2019t even know which decisions are going to turn out to be consequential.\n\nOne of my worries\u2014although it\u2019s also an insight into what\u2019s happening\u2014is that some very critical decision will be some decision where someone just comes into my office and is like, \u201cDario, you have two minutes. Should we do thing A or thing B on this?\u201d Someone gives me this random half-page memo and asks, \u201cShould we do A or B?\u201d I\u2019m like, \u201cI don\u2019t know. I have to eat lunch. Let\u2019s do B.\u201d That ends up being the most consequential thing ever.\n\nDwarkesh Patel\n\nSo final question. There aren\u2019t tech CEOs who are usually writing 50-page memos every few months. It seems like you have managed to build a role for yourself and a company around you which is compatible with this more intellectual-type role of CEO.\n\nI want to understand how you construct that. How does that work? Do you just go away for a couple of weeks and then you tell your company, \u201cThis is the memo. Here\u2019s what we\u2019re doing\u201d? It\u2019s also reported that you write a bunch of these internally.\n\nDario Amodei\n\nFor this particular one, I wrote it over winter break. I was having a hard time finding the time to actually write it. But I think about this in a broader way. I think it relates to the culture of the company. I probably spend a third, maybe 40%, of my time making sure the culture of Anthropic is good.\n\nDario Amodei\n\nAs Anthropic has gotten larger, it\u2019s gotten harder to get directly involved in the training of the models, the launch of the models, the building of the products. It\u2019s 2,500 people. I have certain instincts, but it\u2019s very difficult to get involved in every single detail. I try as much as possible, but one thing that\u2019s very leveraged is making sure Anthropic is a good place to work, people like working there, everyone thinks of themselves as team members, and everyone works together instead of against each other.\n\nWe\u2019ve seen as some of the other AI companies have grown\u2014without naming any names\u2014we\u2019re starting to see decoherence and people fighting each other. I would argue there was even a lot of that from the beginning, but it\u2019s gotten worse. I think we\u2019ve done an extraordinarily good job, even if not perfect, of holding the company together, making everyone feel the mission, that we\u2019re sincere about the mission, and that everyone has faith that everyone else there is working for the right reason. That we\u2019re a team, that people aren\u2019t trying to get ahead at each other\u2019s expense or backstab each other, which again, I think happens a lot at some of the other places.\n\nHow do you make that the case? It\u2019s a lot of things. It\u2019s me, it\u2019s Daniela, who runs the company day to day, it\u2019s the co-founders, it\u2019s the other people we hire, it\u2019s the environment we try to create. But I think an important thing in the culture is that the other leaders as well, but especially me, have to articulate what the company is about, why it\u2019s doing what it\u2019s doing, what its strategy is, what its values are, what its mission is, and what it stands for.\n\nWhen you get to 2,500 people, you can\u2019t do that person by person. You have to write, or you have to speak to the whole company. This is why I get up in front of the whole company every two weeks and speak for an hour.\n\nI wouldn\u2019t say I write essays internally. I do two things. One, I write this thing called a DVQ, Dario Vision Quest. I wasn\u2019t the one who named it that. That\u2019s the name it received, and it\u2019s one of these names that I tried to fight because it made it sound like I was going off and smoking peyote or something. But the name just stuck.\n\nSo I get up in front of the company every two weeks. I have a three or four-page document, and I just talk through three or four different topics about what\u2019s going on internally, the models we\u2019re producing, the products, the outside industry, the world as a whole as it relates to AI and geopolitically in general. Just some mix of that. I go through very honestly and I say, \u201cThis is what I\u2019m thinking, and this is what Anthropic leadership is thinking,\u201d and then I answer questions. That direct connection has a lot of value that is hard to achieve when you\u2019re passing things down the chain six levels deep. A large fraction of the company comes to attend, either in person or virtually. It really means that you can communicate a lot.\n\nThe other thing I do is I have a channel in Slack where I just write a bunch of things and comment a lot. Often that\u2019s in response to things I\u2019m seeing at the company or questions people ask. We do internal surveys and there are things people are concerned about, and so I\u2019ll write them up. I\u2019m just very honest about these things. I just say them very directly.\n\nThe point is to get a reputation of telling the company the truth about what\u2019s happening, to call things what they are, to acknowledge problems, to avoid the sort of corpo speak, the kind of defensive communication that often is necessary in public because the world is very large and full of people who are interpreting things in bad faith. But if you have a company of people who you trust, and we try to hire people that we trust, then you can really just be entirely unfiltered.\n\nI think that\u2019s an enormous strength of the company. It makes it a better place to work, it makes people more than the sum of their parts, and increases the likelihood that we accomplish the mission because everyone is on the same page about the mission, and everyone is debating and discussing how best to accomplish the mission."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"context_stuff","is_visually_hidden_from_conversation":true,"retrieval_turn_number":0,"retrieval_file_index":0,"reasoning_status":"is_reasoning","classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"9db192aa-6d59-4af2-9586-b7bf78e0f322","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":null},"parent":"345b4854-c9b5-4663-afb2-131c6df35aa9","children":["0e173140-5331-410a-babd-443c26438614"]},"0e173140-5331-410a-babd-443c26438614":{"id":"0e173140-5331-410a-babd-443c26438614","message":{"id":"0e173140-5331-410a-babd-443c26438614","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107933.5546713,"update_time":null,"content":{"content_type":"text","parts":["All the files uploaded by the user have been fully loaded. Searching won't provide additional information."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"reasoning_status":"is_reasoning","classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"e2d3aeac-9105-4db2-b5e6-78d5fae1e310","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":null},"parent":"e2d3aeac-9105-4db2-b5e6-78d5fae1e310","children":["ba11568d-06e8-41aa-a9ad-52982acae288"]},"ba11568d-06e8-41aa-a9ad-52982acae288":{"id":"ba11568d-06e8-41aa-a9ad-52982acae288","message":{"id":"ba11568d-06e8-41aa-a9ad-52982acae288","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771107938.191527,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Considering end of exponential claims","content":"The user is asking about claims of being near the \"end of exponential\" progress. While there's steep progress, it doesn't necessarily mean exponential growth will stop. Maybe he\u2019s referring to reaching a transformative threshold like AGI, or the growth could still continue but approaching a limit. Interesting perspective to explore!","chunks":["The user is asking about claims of being near the \"end of exponential\" progress. While there's steep progress, it doesn't necessarily mean exponential growth will stop. Maybe he\u2019s referring to reaching a transformative threshold like AGI, or the growth could still continue but approaching a limit. Interesting perspective to explore!"],"finished":true},{"summary":"Clarifying \"end of the exponential\" claim","content":"He uses \"end of exponential\" in different contexts, like hitting AGI soon. He believes we\u2019re 1-2 years away, citing reasons like scaling laws, pretraining, RL scaling, and progress in code and computer use. While he\u2019s confident about near-term advances, it\u2019s not clear if he links these to the exponential game's end.","chunks":["He uses \"end of exponential\" in different contexts, like hitting AGI soon. He believes we\u2019re 1-2 years away, citing reasons like scaling laws, pretraining, RL scaling, and progress in code and computer use. While he\u2019s confident about near-term advances, it\u2019s not clear if he links these to the exponential game's end."],"finished":true},{"summary":"Examining reasons for \"end of exponential\" claim","content":"He claims the \"end of exponential\" is near but doesn't provide one clear argument. Throughout the conversation, he cites scaling, benchmarks, performance jumps, and revenue growth as support. However, he doesn't define \"exponential\" or \"end\" or directly explain why he thinks the end is near, aside from his own intuition.","chunks":["He claims the \"end of exponential\" is near but doesn't provide one clear argument. Throughout the conversation, he cites scaling, benchmarks, performance jumps, and revenue growth as support. However, he doesn't define \"exponential\" or \"end\" or directly explain why he thinks the end is near, aside from his own intuition."],"finished":true},{"summary":"Evaluating reasoning behind end of exponential claim","content":"He mentions predicting progress within 1-3 years but doesn\u2019t provide a single, concrete argument for the \"end of exponential.\" While referencing scaling laws, RL tasks, compute, OSWorld improvements, and revenue growth, his reasoning is more anecdotal, relying on internal metrics and trends rather than a rigorous, clear-cut argument.","chunks":["He mentions predicting progress within 1-3 years but doesn\u2019t provide a single, concrete argument for the \"end of exponential.\" While referencing scaling laws, RL tasks, compute, OSWorld improvements, and revenue growth, his reasoning is more anecdotal, relying on internal metrics and trends rather than a rigorous, clear-cut argument."],"finished":true}],"source_analysis_msg_id":"9db192aa-6d59-4af2-9586-b7bf78e0f322"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"0e173140-5331-410a-babd-443c26438614","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","classifier_response":"default","skip_reasoning_title":"Skip","can_save":true},"recipient":"all","channel":null},"parent":"0e173140-5331-410a-babd-443c26438614","children":["5966f82c-80de-47ad-9bc6-a565435bb2a2"]},"5966f82c-80de-47ad-9bc6-a565435bb2a2":{"id":"5966f82c-80de-47ad-9bc6-a565435bb2a2","message":{"id":"5966f82c-80de-47ad-9bc6-a565435bb2a2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771107932.352492,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"queries\":[\"\\\"end of the exponential\\\" near the end of the exponential reasons\",\"end of the exponential\"],\"source_filter\":[\"files_uploaded_in_conversation\"]}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"f18af206-b78f-4453-a9e7-becd98fe784e","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","can_save":true},"recipient":"file_search.msearch","channel":null},"parent":"ba11568d-06e8-41aa-a9ad-52982acae288","children":["1217f18a-f5d0-4587-8b75-c63d0181de81"]},"1217f18a-f5d0-4587-8b75-c63d0181de81":{"id":"1217f18a-f5d0-4587-8b75-c63d0181de81","message":{"id":"1217f18a-f5d0-4587-8b75-c63d0181de81","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107960.0731142,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"spinner","status":"running","retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"a6a6e063-be29-4405-8a9d-c3337ae8940d","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":false},"recipient":"all","channel":null},"parent":"5966f82c-80de-47ad-9bc6-a565435bb2a2","children":["deefc036-ce20-4141-bb3d-b96ed79be60c"]},"deefc036-ce20-4141-bb3d-b96ed79be60c":{"id":"deefc036-ce20-4141-bb3d-b96ed79be60c","message":{"id":"deefc036-ce20-4141-bb3d-b96ed79be60c","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107961.5283191,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn1file0\ue201\nMclick Target: \"1:0\"\nContent Snippet:\n[L1] Dwarkesh Patel\n[L2] \n[L3] Maybe stepping back, I\u2019m not saying I think the \u201ccountry of geniuses\u201d is going to come in two years and therefore you should buy this compute. To me, the end conclusion you\u2019re arriving at makes a lot of sense. But that\u2019s because it seems like \u201ccountry of geniuses\u201d is hard and there\u2019s a long way to go. So stepping back, the thing I\u2019m trying to get at is more that it seems like your worldview is compatible with somebody who says, \u201cWe\u2019re like 10 years away from a world in which we\u2019re generating trillions of dollars of value.\u201d\n[L4] \n[L5] Dario Amodei\n[L6] \n[L7] That\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.\n[L8] \n[L9] Like in 2028, we get the real \u201ccountry of geniuses in the data center\u201d. The revenue\u2019s going into the low hundreds of billions by 2028, and then the country of geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. It takes two years to get to the trillions. That would be the world where it takes until 2030. I suspect even composing the technical exponential and diffusion exponential, we\u2019ll get there before 2030.\n[L10] \n[L11] Dwarkesh Patel\n[L12] \n[L13] So you laid out a model where Anthropic makes profit because it seems like fundamentally we\u2019re in a compute-constrained world. So eventually we keep growing compute\u2014\n[L14] \n[L15] Dario Amodei\n[L16] \n[L17] I think the way the profit comes is\u2026 Again, let\u2019s just abstract the whole industry here. Let\u2019s just imagine we\u2019re in an economics textbook. We have a small number of firms. Each can invest a limited amount. Each can invest some fraction in R&D. They have some marginal cost to serve. The gross profit margins on that marginal cost are very high because inference is efficient. There\u2019s some competition, but the models are also differentiated.\n[L18] \n[L19] Companies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.\n[L20] \n[L21] Dwarkesh Patel\n[L22] \n[L23] Help me understand that, because right now we do have three leading firms and they\u2019re not making profit. So what is changing?\n[L24] \n[L25] Dario Amodei\n[L26] \n[L27] Again, the gross margins right now are very positive. What\u2019s happening is a combination of two things. One is that we\u2019re still in the exponential scale-up phase of compute. A model gets trained. Let\u2019s say a model got trained that costs $1 billion last year. Then this year it produced $4 billion of revenue and cost $1 billion to inference from. Again, I\u2019m using stylized numbers here, but that would be 75% gross margins and this 25% tax. So that model as a whole makes $2 billion.\n[L28] \n[L29] But at the same time, we\u2019re spending $10 billion to train the next model because there\u2019s an exponential scale-up. So the company loses money. Each model makes money, but the company loses money.\n[L30] \n[L31] The equilibrium I\u2019m talking about is an equilibrium where we have the \u201ccountry of geniuses in a data center\u201d, but that model training scale-up has equilibrated more. Maybe it\u2019s still going up. We\u2019re still trying to predict the demand, but it\u2019s more leveled out.\n[L32] \n[L33] Dwarkesh Patel\n# \u30101\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn1file1\ue201\nMclick Target: \"1:1\"\nContent Snippet:\n[L1] Dwarkesh Patel\n[L2] \n[L3] This is a point I\u2019m sure you would make, but diminishing returns on a genius could be quite high.\n[L4] \n[L5] More generally, what is profit in a market economy? Profit is basically saying other companies in the market can do more things with this money than I can.\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Put aside Anthropic. I don\u2019t want to give information about Anthropic. That\u2019s why I\u2019m giving these stylized numbers. But let\u2019s just derive the equilibrium of the industry. Why doesn\u2019t everyone spend 100% of their compute on training and not serve any customers? It\u2019s because if they didn\u2019t get any revenue, they couldn\u2019t raise money, they couldn\u2019t do compute deals, they couldn\u2019t buy more compute the next year.\n[L10] \n[L11] So there\u2019s going to be an equilibrium where every company spends less than 100% on training and certainly less than 100% on inference. It should be clear why you don\u2019t just serve the current models and never train another model, because then you don\u2019t have any demand because you\u2019ll fall behind. So there\u2019s some equilibrium. It\u2019s not gonna be 10%, it\u2019s not gonna be 90%. Let\u2019s just say as a stylized fact, it\u2019s 50%. That\u2019s what I\u2019m getting at.\n[L12] \n[L13] I think we\u2019re gonna be in a position where that equilibrium of how much you spend on training is less than the gross margins that you\u2019re able to get on compute. So the underlying economics are profitable. The problem is you have this hellish demand prediction problem when you\u2019re buying the next year of compute and you might guess under and be very profitable but have no compute for research. Or you might guess over and you are not profitable and you have all the compute for research in the world. Does that make sense? Just as a dynamic model of the industry?\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] Maybe stepping back, I\u2019m not saying I think the \u201ccountry of geniuses\u201d is going to come in two years and therefore you should buy this compute. To me, the end conclusion you\u2019re arriving at makes a lot of sense. But that\u2019s because it seems like \u201ccountry of geniuses\u201d is hard and there\u2019s a long way to go. So stepping back, the thing I\u2019m trying to get at is more that it seems like your worldview is compatible with somebody who says, \u201cWe\u2019re like 10 years away from a world in which we\u2019re generating trillions of dollars of value.\u201d\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] That\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.\n[L22] \n[L23] Like in 2028, we get the real \u201ccountry of geniuses in the data center\u201d. The revenue\u2019s going into the low hundreds of billions by 2028, and then the country of geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. It takes two years to get to the trillions. That would be the world where it takes until 2030. I suspect even composing the technical exponential and diffusion exponential, we\u2019ll get there before 2030.\n[L24] \n[L25] Dwarkesh Patel\n[L26] \n[L27] So you laid out a model where Anthropic makes profit because it seems like fundamentally we\u2019re in a compute-constrained world. So eventually we keep growing compute\u2014\n[L28] \n[L29] Dario Amodei\n# \u30102\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn1file2\ue201\nMclick Target: \"1:2\"\nContent Snippet:\n[L1] 00:12:36 - Is diffusion cope?\n[L2] Dwarkesh Patel\n[L3] \n[L4] I think the framework you\u2019re laying down obviously makes sense. We\u2019re making progress toward AGI. Nobody at this point disagrees we\u2019re going to achieve AGI this century. The crux is you say we\u2019re hitting the end of the exponential. Somebody else looks at this and says, \u201cWe\u2019ve been making progress since 2012, and by 2035 we\u2019ll have a human-like agent.\u201d\n[L5] \n[L6] Obviously we\u2019re seeing in these models the kinds of things that evolution did, or that learning within a human lifetime does. I want to understand what you\u2019re seeing that makes you think it\u2019s one year away and not ten years away.\n[L7] \n[L8] Dario Amodei\n[L9] \n[L10] There are two claims you could make here, one stronger and one weaker. Starting with the weaker claim, when I first saw the scaling back in 2019, I wasn\u2019t sure. This was a 50/50 thing. I thought I saw something. My claim was that this was much more likely than anyone thinks. Maybe there\u2019s a 50% chance this happens.\n[L11] \n[L12] On the basic hypothesis of, as you put it, within ten years we\u2019ll get to what I call a \u201ccountry of geniuses in a data center\u201d, I\u2019m at 90% on that. It\u2019s hard to go much higher than 90% because the world is so unpredictable. Maybe the irreducible uncertainty puts us at 95%, where you get to things like multiple companies having internal turmoil, Taiwan gets invaded, all the fabs get blown up by missiles.\n[L13] \n[L14] Dwarkesh Patel\n[L15] \n[L16] Now you\u2019ve jinxed us, Dario.\n[L17] \n[L18] Dario Amodei\n[L19] \n[L20] You could construct a 5% world where things get delayed for ten years. There\u2019s another 5% which is that I\u2019m very confident on tasks that can be verified. With coding, except for that irreducible uncertainty, I think we\u2019ll be there in one or two years. There\u2019s no way we will not be there in ten years in terms of being able to do end-to-end coding.\n[L21] \n[L22] My one little bit of fundamental uncertainty, even on long timescales, is about tasks that aren\u2019t verifiable: planning a mission to Mars; doing some fundamental scientific discovery like CRISPR; writing a novel. It\u2019s hard to verify those tasks. I am almost certain we have a reliable path to get there, but if there\u2019s a little bit of uncertainty it\u2019s there. On the ten-year timeline I\u2019m at 90%, which is about as certain as you can be. I think it\u2019s crazy to say that this won\u2019t happen by 2035. In some sane world, it would be outside the mainstream.\n[L23] \n[L24] Dwarkesh Patel\n[L25] \n[L26] But the emphasis on verification hints to me a lack of belief that these models are generalized. If you think about humans, we\u2019re both good at things for which we get verifiable reward and things for which we don\u2019t.\n[L27] \n[L28] Dario Amodei\n[L29] \n[L30] No, this is why I\u2019m almost sure. We already see substantial generalization from things that verify to things that don\u2019t. We\u2019re already seeing that.\n[L31] \n[L32] Dwarkesh Patel\n[L33] \n[L34] But it seems like you were emphasizing this as a spectrum which will split apart which domains in which we see more progress. That doesn\u2019t seem like how humans get better.\n[L35] \n[L36] Dario Amodei\n[L37] \n[L38] The world in which we don\u2019t get there is the world in which we do all the verifiable things. Many of them generalize, but we don\u2019t fully get there. We don\u2019t fully color in the other side of the box. It\u2019s not a binary thing.\n[L39] \n[L40] Dwarkesh Patel\n# \u30103\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn1file3\ue201\nMclick Target: \"1:3\"\nContent Snippet:\n[L1] But what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\n[L2] \n[L3] Dwarkesh Patel\n[L4] \n[L5] You guys should have just bought a billion dollars of your own products so you could just\u2026\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] And the first month of this year, that exponential is... You would think it would slow down, but we added another few billion to revenue in January. Obviously that curve can\u2019t go on forever. The GDP is only so large. I would even guess that it bends somewhat this year, but that is a fast curve. That\u2019s a really fast curve. I would bet it stays pretty fast even as the scale goes to the entire economy.\n[L10] \n[L11] So I think we should be thinking about this middle world where things are extremely fast, but not instant, where they take time because of economic diffusion, because of the need to close the loop. Because it\u2019s fiddly: \u201cI have to do change management within my enterprise\u2026 I set this up, but I have to change the security permissions on this in order to make it actually work\u2026 I had this old piece of software that checks the model before it\u2019s compiled and released and I have to rewrite it. Yes, the model can do that, but I have to tell the model to do that. It has to take time to do that.\u201d\n[L12] \n[L13] So I think everything we\u2019ve seen so far is compatible with the idea that there\u2019s one fast exponential that\u2019s the capability of the model. Then there\u2019s another fast exponential that\u2019s downstream of that, which is the diffusion of the model into the economy. Not instant, not slow, much faster than any previous technology, but it has its limits. When I look inside Anthropic, when I look at our customers: fast adoption, but not infinitely fast.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] Can I try a hot take on you?\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] Yeah.\n[L22] \n[L23] Dwarkesh Patel\n[L24] \n[L25] I feel like diffusion is cope that people say. When the model isn\u2019t able to do something, they\u2019re like, \u201coh, but it\u2019s a diffusion issue.\u201d But then you should use the comparison to humans. You would think that the inherent advantages that AIs have would make diffusion a much easier problem for new AIs getting onboarded than new humans getting onboarded. An AI can read your entire Slack and your drive in minutes. They can share all the knowledge that the other copies of the same instance have. You don\u2019t have this adverse selection problem when you\u2019re hiring AI, so you can just hire copies of a vetted AI model.\n[L26] \n[L27] Hiring a human is so much more of a hassle. People hire humans all the time. We pay humans upwards of $50 trillion in wages because they\u2019re useful, even though in principle it would be much easier to integrate AIs into the economy than it is to hire humans. The diffusion doesn\u2019t really explain.\n[L28] \n[L29] Dario Amodei\n[L30] \n[L31] I think diffusion is very real and doesn\u2019t exclusively have to do with limitations on the AI models. Again, there are people who use diffusion as kind of a buzzword to say this isn\u2019t a big deal. I\u2019m not talking about that. I\u2019m not talking about how AI will diffuse at the speed of previous technologies. I think AI will diffuse much faster than previous technologies have, but not infinitely fast.\n# \u30104\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn1file4\ue201\nMclick Target: \"1:4\"\nContent Snippet:\n[L1] Companies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.\n[L2] \n[L3] Dwarkesh Patel\n[L4] \n[L5] Help me understand that, because right now we do have three leading firms and they\u2019re not making profit. So what is changing?\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Again, the gross margins right now are very positive. What\u2019s happening is a combination of two things. One is that we\u2019re still in the exponential scale-up phase of compute. A model gets trained. Let\u2019s say a model got trained that costs $1 billion last year. Then this year it produced $4 billion of revenue and cost $1 billion to inference from. Again, I\u2019m using stylized numbers here, but that would be 75% gross margins and this 25% tax. So that model as a whole makes $2 billion.\n[L10] \n[L11] But at the same time, we\u2019re spending $10 billion to train the next model because there\u2019s an exponential scale-up. So the company loses money. Each model makes money, but the company loses money.\n[L12] \n[L13] The equilibrium I\u2019m talking about is an equilibrium where we have the \u201ccountry of geniuses in a data center\u201d, but that model training scale-up has equilibrated more. Maybe it\u2019s still going up. We\u2019re still trying to predict the demand, but it\u2019s more leveled out.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] I\u2019m confused about a couple of things there. Let\u2019s start with the current world. In the current world, you\u2019re right that, as you said before, if you treat each individual model as a company, it\u2019s profitable. But of course, a big part of the production function of being a frontier lab is training the next model, right?\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] Yes, that\u2019s right.\n[L22] \n[L23] Dwarkesh Patel\n[L24] \n[L25] If you didn\u2019t do that, then you\u2019d make profit for two months and then you wouldn\u2019t have margins because you wouldn\u2019t have the best model.\n[L26] \n[L27] Dario Amodei\n[L28] \n[L29] But at some point that reaches the biggest scale that it can reach. And then in equilibrium, we have algorithmic improvements, but we\u2019re spending roughly the same amount to train the next model as we spend to train the current model. At some point you run out of money in the economy.\n[L30] \n[L31] Dwarkesh Patel\n[L32] \n[L33] A fixed lump of labor fallacy\u2026 The economy is going to grow, right? That\u2019s one of your predictions. We\u2019re going to have the data centers in space.\n[L34] \n[L35] Dario Amodei\n[L36] \n[L37] Yes, but this is another example of the theme I was talking about. The economy will grow much faster with AI than I think it ever has before. Right now the compute is growing 3x a year. I don\u2019t believe the economy is gonna grow 300% a year. I said this in \u201cMachines of Loving Grace\u201d, I think we may get 10-20% per year growth in the economy, but we\u2019re not gonna get 300% growth in the economy. So I think in the end, if compute becomes the majority of what the economy produces, it\u2019s gonna be capped by that.\n[L38] \n[L39] Dwarkesh Patel\nVisible: 0% - 100%","summary":null,"assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"msearch","cloud_doc_urls":[null,null,null,null,null],"retrieval_turn_number":1,"is_contextual_answers_supported":true,"contextual_answers_available_sources":["slurm_github_issues","slurm_github_pulls","slurm_github_commits","slurm_github","files_uploaded_in_conversation"],"retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"connectors_file_search":{"tool_call_message_id":"f18af206-b78f-4453-a9e7-becd98fe784e","source_filters":["files_uploaded_in_conversation"],"call_tool_latency_ms":1594.69208214432},"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"1217f18a-f5d0-4587-8b75-c63d0181de81","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":null},"parent":"1217f18a-f5d0-4587-8b75-c63d0181de81","children":["acf68682-46b8-4302-a924-bb9d5bda1537"]},"acf68682-46b8-4302-a924-bb9d5bda1537":{"id":"acf68682-46b8-4302-a924-bb9d5bda1537","message":{"id":"acf68682-46b8-4302-a924-bb9d5bda1537","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771107964.981769,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"queries\":[\"\\\"lack of public recognition\\\" end of the exponential\",\"near the end of the exponential \\\"absolutely wild\\\"\"],\"source_filter\":[\"files_uploaded_in_conversation\"]}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"fc839c8d-378f-4819-8755-e331fe1f3eab","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","can_save":true},"recipient":"file_search.msearch","channel":null},"parent":"deefc036-ce20-4141-bb3d-b96ed79be60c","children":["513cd26a-746c-4c2c-b897-94781d553635"]},"513cd26a-746c-4c2c-b897-94781d553635":{"id":"513cd26a-746c-4c2c-b897-94781d553635","message":{"id":"513cd26a-746c-4c2c-b897-94781d553635","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107966.741652,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"spinner","status":"running","retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"acf68682-46b8-4302-a924-bb9d5bda1537","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":false},"recipient":"all","channel":null},"parent":"acf68682-46b8-4302-a924-bb9d5bda1537","children":["eb0f5919-f57d-48c3-b42d-0b2e1cce9785"]},"eb0f5919-f57d-48c3-b42d-0b2e1cce9785":{"id":"eb0f5919-f57d-48c3-b42d-0b2e1cce9785","message":{"id":"eb0f5919-f57d-48c3-b42d-0b2e1cce9785","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107968.52355,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn2file0\ue201\nMclick Target: \"2:0\"\nContent Snippet:\n[L1] Dwarkesh Patel\n[L2] \n[L3] Maybe stepping back, I\u2019m not saying I think the \u201ccountry of geniuses\u201d is going to come in two years and therefore you should buy this compute. To me, the end conclusion you\u2019re arriving at makes a lot of sense. But that\u2019s because it seems like \u201ccountry of geniuses\u201d is hard and there\u2019s a long way to go. So stepping back, the thing I\u2019m trying to get at is more that it seems like your worldview is compatible with somebody who says, \u201cWe\u2019re like 10 years away from a world in which we\u2019re generating trillions of dollars of value.\u201d\n[L4] \n[L5] Dario Amodei\n[L6] \n[L7] That\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.\n[L8] \n[L9] Like in 2028, we get the real \u201ccountry of geniuses in the data center\u201d. The revenue\u2019s going into the low hundreds of billions by 2028, and then the country of geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. It takes two years to get to the trillions. That would be the world where it takes until 2030. I suspect even composing the technical exponential and diffusion exponential, we\u2019ll get there before 2030.\n[L10] \n[L11] Dwarkesh Patel\n[L12] \n[L13] So you laid out a model where Anthropic makes profit because it seems like fundamentally we\u2019re in a compute-constrained world. So eventually we keep growing compute\u2014\n[L14] \n[L15] Dario Amodei\n[L16] \n[L17] I think the way the profit comes is\u2026 Again, let\u2019s just abstract the whole industry here. Let\u2019s just imagine we\u2019re in an economics textbook. We have a small number of firms. Each can invest a limited amount. Each can invest some fraction in R&D. They have some marginal cost to serve. The gross profit margins on that marginal cost are very high because inference is efficient. There\u2019s some competition, but the models are also differentiated.\n[L18] \n[L19] Companies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.\n[L20] \n[L21] Dwarkesh Patel\n[L22] \n[L23] Help me understand that, because right now we do have three leading firms and they\u2019re not making profit. So what is changing?\n[L24] \n[L25] Dario Amodei\n[L26] \n[L27] Again, the gross margins right now are very positive. What\u2019s happening is a combination of two things. One is that we\u2019re still in the exponential scale-up phase of compute. A model gets trained. Let\u2019s say a model got trained that costs $1 billion last year. Then this year it produced $4 billion of revenue and cost $1 billion to inference from. Again, I\u2019m using stylized numbers here, but that would be 75% gross margins and this 25% tax. So that model as a whole makes $2 billion.\n[L28] \n[L29] But at the same time, we\u2019re spending $10 billion to train the next model because there\u2019s an exponential scale-up. So the company loses money. Each model makes money, but the company loses money.\n[L30] \n[L31] The equilibrium I\u2019m talking about is an equilibrium where we have the \u201ccountry of geniuses in a data center\u201d, but that model training scale-up has equilibrated more. Maybe it\u2019s still going up. We\u2019re still trying to predict the demand, but it\u2019s more leveled out.\n[L32] \n[L33] Dwarkesh Patel\n# \u30101\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn2file1\ue201\nMclick Target: \"2:1\"\nContent Snippet:\n[L1] Transcript\n[L2] 00:00:00 - What exactly are we scaling?\n[L3] Dwarkesh Patel\n[L4] \n[L5] We talked three years ago. In your view, what has been the biggest update over the last three years? What has been the biggest difference between what it felt like then versus now?\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\n[L10] \n[L11] But when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.\n[L12] \n[L13] What has been the most surprising thing is the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that you have people \u2014 within the bubble and outside the bubble \u2014 talking about the same tired, old hot-button political issues, when we are near the end of the exponential.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] I want to understand what that exponential looks like right now. The first question I asked you when we recorded three years ago was, \u201cwhat\u2019s up with scaling and why does it work?\u201d I have a similar question now, but it feels more complicated. At least from the public\u2019s point of view, three years ago there were well-known public trends across many orders of magnitude of compute where you could see how the loss improves.\n[L18] \n[L19] Now we have RL scaling and there\u2019s no publicly known scaling law for it. It\u2019s not even clear what the story is. Is this supposed to be teaching the model skills? Is it supposed to be teaching meta-learning? What is the scaling hypothesis at this point?\n[L20] \n[L21] Dario Amodei\n[L22] \n[L23] I actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.\n[L24] \n[L25] That was one among many things. Back in those days there was robotics. People tried to work on reasoning as a separate thing from language models, and there was scaling of the kind of RL that happened in AlphaGo and in Dota at OpenAI. People remember StarCraft at DeepMind, AlphaStar.\n[L26] \n[L27] It was written as a more general document. Rich Sutton put out \u201cThe Bitter Lesson\u201d a couple years later. The hypothesis is basically the same. What it says is that all the cleverness, all the techniques, all the \u201cwe need a new method to do something\u201d, that doesn\u2019t matter very much. There are only a few things that matter. I think I listed seven of them.\n[L28] \n[L29] One is how much raw compute you have. The second is the quantity of data. The third is the quality and distribution of data. It needs to be a broad distribution. The fourth is how long you train for. The fifth is that you need an objective function that can scale to the moon. The pre-training objective function is one such objective function. Another is the RL objective function that says you have a goal, you\u2019re going to go out and reach the goal.\n[L30] \n[L31] Within that, there\u2019s objective rewards like you see in math and coding, and there\u2019s more subjective rewards like you see in RLHF or higher-order versions of that. Then the sixth and seventh were things around normalization or conditioning, just getting the numerical stability so that the big blob of compute flows in this laminar way instead of running into problems.\n# \u30102\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn2file2\ue201\nMclick Target: \"2:2\"\nContent Snippet:\n[L1] Companies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.\n[L2] \n[L3] Dwarkesh Patel\n[L4] \n[L5] Help me understand that, because right now we do have three leading firms and they\u2019re not making profit. So what is changing?\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Again, the gross margins right now are very positive. What\u2019s happening is a combination of two things. One is that we\u2019re still in the exponential scale-up phase of compute. A model gets trained. Let\u2019s say a model got trained that costs $1 billion last year. Then this year it produced $4 billion of revenue and cost $1 billion to inference from. Again, I\u2019m using stylized numbers here, but that would be 75% gross margins and this 25% tax. So that model as a whole makes $2 billion.\n[L10] \n[L11] But at the same time, we\u2019re spending $10 billion to train the next model because there\u2019s an exponential scale-up. So the company loses money. Each model makes money, but the company loses money.\n[L12] \n[L13] The equilibrium I\u2019m talking about is an equilibrium where we have the \u201ccountry of geniuses in a data center\u201d, but that model training scale-up has equilibrated more. Maybe it\u2019s still going up. We\u2019re still trying to predict the demand, but it\u2019s more leveled out.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] I\u2019m confused about a couple of things there. Let\u2019s start with the current world. In the current world, you\u2019re right that, as you said before, if you treat each individual model as a company, it\u2019s profitable. But of course, a big part of the production function of being a frontier lab is training the next model, right?\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] Yes, that\u2019s right.\n[L22] \n[L23] Dwarkesh Patel\n[L24] \n[L25] If you didn\u2019t do that, then you\u2019d make profit for two months and then you wouldn\u2019t have margins because you wouldn\u2019t have the best model.\n[L26] \n[L27] Dario Amodei\n[L28] \n[L29] But at some point that reaches the biggest scale that it can reach. And then in equilibrium, we have algorithmic improvements, but we\u2019re spending roughly the same amount to train the next model as we spend to train the current model. At some point you run out of money in the economy.\n[L30] \n[L31] Dwarkesh Patel\n[L32] \n[L33] A fixed lump of labor fallacy\u2026 The economy is going to grow, right? That\u2019s one of your predictions. We\u2019re going to have the data centers in space.\n[L34] \n[L35] Dario Amodei\n[L36] \n[L37] Yes, but this is another example of the theme I was talking about. The economy will grow much faster with AI than I think it ever has before. Right now the compute is growing 3x a year. I don\u2019t believe the economy is gonna grow 300% a year. I said this in \u201cMachines of Loving Grace\u201d, I think we may get 10-20% per year growth in the economy, but we\u2019re not gonna get 300% growth in the economy. So I think in the end, if compute becomes the majority of what the economy produces, it\u2019s gonna be capped by that.\n[L38] \n[L39] Dwarkesh Patel\n# \u30103\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn2file3\ue201\nMclick Target: \"2:3\"\nContent Snippet:\n[L1] Dwarkesh Patel\n[L2] \n[L3] This is a point I\u2019m sure you would make, but diminishing returns on a genius could be quite high.\n[L4] \n[L5] More generally, what is profit in a market economy? Profit is basically saying other companies in the market can do more things with this money than I can.\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Put aside Anthropic. I don\u2019t want to give information about Anthropic. That\u2019s why I\u2019m giving these stylized numbers. But let\u2019s just derive the equilibrium of the industry. Why doesn\u2019t everyone spend 100% of their compute on training and not serve any customers? It\u2019s because if they didn\u2019t get any revenue, they couldn\u2019t raise money, they couldn\u2019t do compute deals, they couldn\u2019t buy more compute the next year.\n[L10] \n[L11] So there\u2019s going to be an equilibrium where every company spends less than 100% on training and certainly less than 100% on inference. It should be clear why you don\u2019t just serve the current models and never train another model, because then you don\u2019t have any demand because you\u2019ll fall behind. So there\u2019s some equilibrium. It\u2019s not gonna be 10%, it\u2019s not gonna be 90%. Let\u2019s just say as a stylized fact, it\u2019s 50%. That\u2019s what I\u2019m getting at.\n[L12] \n[L13] I think we\u2019re gonna be in a position where that equilibrium of how much you spend on training is less than the gross margins that you\u2019re able to get on compute. So the underlying economics are profitable. The problem is you have this hellish demand prediction problem when you\u2019re buying the next year of compute and you might guess under and be very profitable but have no compute for research. Or you might guess over and you are not profitable and you have all the compute for research in the world. Does that make sense? Just as a dynamic model of the industry?\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] Maybe stepping back, I\u2019m not saying I think the \u201ccountry of geniuses\u201d is going to come in two years and therefore you should buy this compute. To me, the end conclusion you\u2019re arriving at makes a lot of sense. But that\u2019s because it seems like \u201ccountry of geniuses\u201d is hard and there\u2019s a long way to go. So stepping back, the thing I\u2019m trying to get at is more that it seems like your worldview is compatible with somebody who says, \u201cWe\u2019re like 10 years away from a world in which we\u2019re generating trillions of dollars of value.\u201d\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] That\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.\n[L22] \n[L23] Like in 2028, we get the real \u201ccountry of geniuses in the data center\u201d. The revenue\u2019s going into the low hundreds of billions by 2028, and then the country of geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. It takes two years to get to the trillions. That would be the world where it takes until 2030. I suspect even composing the technical exponential and diffusion exponential, we\u2019ll get there before 2030.\n[L24] \n[L25] Dwarkesh Patel\n[L26] \n[L27] So you laid out a model where Anthropic makes profit because it seems like fundamentally we\u2019re in a compute-constrained world. So eventually we keep growing compute\u2014\n[L28] \n[L29] Dario Amodei\n# \u30104\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn2file4\ue201\nMclick Target: \"2:4\"\nContent Snippet:\n[L1] 00:12:36 - Is diffusion cope?\n[L2] Dwarkesh Patel\n[L3] \n[L4] I think the framework you\u2019re laying down obviously makes sense. We\u2019re making progress toward AGI. Nobody at this point disagrees we\u2019re going to achieve AGI this century. The crux is you say we\u2019re hitting the end of the exponential. Somebody else looks at this and says, \u201cWe\u2019ve been making progress since 2012, and by 2035 we\u2019ll have a human-like agent.\u201d\n[L5] \n[L6] Obviously we\u2019re seeing in these models the kinds of things that evolution did, or that learning within a human lifetime does. I want to understand what you\u2019re seeing that makes you think it\u2019s one year away and not ten years away.\n[L7] \n[L8] Dario Amodei\n[L9] \n[L10] There are two claims you could make here, one stronger and one weaker. Starting with the weaker claim, when I first saw the scaling back in 2019, I wasn\u2019t sure. This was a 50/50 thing. I thought I saw something. My claim was that this was much more likely than anyone thinks. Maybe there\u2019s a 50% chance this happens.\n[L11] \n[L12] On the basic hypothesis of, as you put it, within ten years we\u2019ll get to what I call a \u201ccountry of geniuses in a data center\u201d, I\u2019m at 90% on that. It\u2019s hard to go much higher than 90% because the world is so unpredictable. Maybe the irreducible uncertainty puts us at 95%, where you get to things like multiple companies having internal turmoil, Taiwan gets invaded, all the fabs get blown up by missiles.\n[L13] \n[L14] Dwarkesh Patel\n[L15] \n[L16] Now you\u2019ve jinxed us, Dario.\n[L17] \n[L18] Dario Amodei\n[L19] \n[L20] You could construct a 5% world where things get delayed for ten years. There\u2019s another 5% which is that I\u2019m very confident on tasks that can be verified. With coding, except for that irreducible uncertainty, I think we\u2019ll be there in one or two years. There\u2019s no way we will not be there in ten years in terms of being able to do end-to-end coding.\n[L21] \n[L22] My one little bit of fundamental uncertainty, even on long timescales, is about tasks that aren\u2019t verifiable: planning a mission to Mars; doing some fundamental scientific discovery like CRISPR; writing a novel. It\u2019s hard to verify those tasks. I am almost certain we have a reliable path to get there, but if there\u2019s a little bit of uncertainty it\u2019s there. On the ten-year timeline I\u2019m at 90%, which is about as certain as you can be. I think it\u2019s crazy to say that this won\u2019t happen by 2035. In some sane world, it would be outside the mainstream.\n[L23] \n[L24] Dwarkesh Patel\n[L25] \n[L26] But the emphasis on verification hints to me a lack of belief that these models are generalized. If you think about humans, we\u2019re both good at things for which we get verifiable reward and things for which we don\u2019t.\n[L27] \n[L28] Dario Amodei\n[L29] \n[L30] No, this is why I\u2019m almost sure. We already see substantial generalization from things that verify to things that don\u2019t. We\u2019re already seeing that.\n[L31] \n[L32] Dwarkesh Patel\n[L33] \n[L34] But it seems like you were emphasizing this as a spectrum which will split apart which domains in which we see more progress. That doesn\u2019t seem like how humans get better.\n[L35] \n[L36] Dario Amodei\n[L37] \n[L38] The world in which we don\u2019t get there is the world in which we do all the verifiable things. Many of them generalize, but we don\u2019t fully get there. We don\u2019t fully color in the other side of the box. It\u2019s not a binary thing.\n[L39] \n[L40] Dwarkesh Patel\nVisible: 0% - 100%","summary":null,"assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"msearch","cloud_doc_urls":[null,null,null,null,null],"retrieval_turn_number":2,"is_contextual_answers_supported":true,"contextual_answers_available_sources":["slurm_github_issues","slurm_github_pulls","slurm_github_commits","slurm_github","files_uploaded_in_conversation"],"retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"connectors_file_search":{"tool_call_message_id":"fc839c8d-378f-4819-8755-e331fe1f3eab","source_filters":["files_uploaded_in_conversation"],"call_tool_latency_ms":1741.0536967217922},"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"513cd26a-746c-4c2c-b897-94781d553635","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":null},"parent":"513cd26a-746c-4c2c-b897-94781d553635","children":["abc9e993-bad8-430b-bda5-f741152da168"]},"abc9e993-bad8-430b-bda5-f741152da168":{"id":"abc9e993-bad8-430b-bda5-f741152da168","message":{"id":"abc9e993-bad8-430b-bda5-f741152da168","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771107979.614091,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Clarifying reasons behind \"end of exponential\" claim","content":"He mentions \u201clack of public recognition\u201d and \u201cmodels progressing from high school to PhD,\u201d but doesn\u2019t give solid reasons beyond those statements. He also discusses scaling laws, RL scaling, and compute hypotheses. While he suggests we\u2019re near the end due to human-level models, his reasoning remains fairly abstract, relying on analogies and observations rather than rigorous explanation.","chunks":["He mentions \u201clack of public recognition\u201d and \u201cmodels progressing from high school to PhD,\u201d but doesn\u2019t give solid reasons beyond those statements.","He also discusses scaling laws, RL scaling, and compute hypotheses.","While he suggests we\u2019re near the end due to human-level models, his reasoning remains fairly abstract, relying on analogies and observations rather than rigorous explanation."],"finished":true},{"summary":"Evaluating reasons for \"end of exponential\"","content":"His reasoning seems to be based on persistent scaling, which suggests the exponential growth will continue, but nearing the threshold of AGI. He mentions specific metrics like OSWorld's improvement and his evolving confidence in near-term advances (1-2 years for coding). I'll focus on finding more specific references to scaling and evidence, like RL scaling.","chunks":["His reasoning seems to be based on persistent scaling, which suggests the exponential growth will continue, but nearing the threshold of AGI.","He mentions specific metrics like OSWorld's improvement and his evolving confidence in near-term advances (1-2 years for coding).","I'll focus on finding more specific references to scaling and evidence, like RL scaling."],"finished":true}],"source_analysis_msg_id":"08653216-6b3a-4fd4-8253-5f10c98eb3a7"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"08653216-6b3a-4fd4-8253-5f10c98eb3a7","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","classifier_response":"default","skip_reasoning_title":"Skip","can_save":true},"recipient":"all","channel":null},"parent":"eb0f5919-f57d-48c3-b42d-0b2e1cce9785","children":["c4f796f4-279a-4293-ad78-d5a479263ad7"]},"c4f796f4-279a-4293-ad78-d5a479263ad7":{"id":"c4f796f4-279a-4293-ad78-d5a479263ad7","message":{"id":"c4f796f4-279a-4293-ad78-d5a479263ad7","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771107980.423392,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"queries\":[\"log-linear in how long we\u2019ve trained it RL phase on top of that\",\"AIME log-linear RL scaling\"],\"source_filter\":[\"files_uploaded_in_conversation\"]}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"aa248b53-e4e6-47da-9e44-4d47f1034d68","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","can_save":true},"recipient":"file_search.msearch","channel":null},"parent":"abc9e993-bad8-430b-bda5-f741152da168","children":["ada83e82-3c9c-44ca-b65f-b69e26fc7f40"]},"ada83e82-3c9c-44ca-b65f-b69e26fc7f40":{"id":"ada83e82-3c9c-44ca-b65f-b69e26fc7f40","message":{"id":"ada83e82-3c9c-44ca-b65f-b69e26fc7f40","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107982.1223028,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"spinner","status":"running","retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"bf85f4f6-de99-44b6-b220-ef3f86a49b3c","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":false},"recipient":"all","channel":null},"parent":"c4f796f4-279a-4293-ad78-d5a479263ad7","children":["6bb443e4-5c38-4f9c-90e3-8678bca27f2b"]},"6bb443e4-5c38-4f9c-90e3-8678bca27f2b":{"id":"6bb443e4-5c38-4f9c-90e3-8678bca27f2b","message":{"id":"6bb443e4-5c38-4f9c-90e3-8678bca27f2b","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771107983.9900227,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn3file0\ue201\nMclick Target: \"3:0\"\nContent Snippet:\n[L1] That was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\n[L2] \n[L3] What has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d\n[L4] \n[L5] We see that as well, and it\u2019s not just math contests. It\u2019s a wide variety of RL tasks. We\u2019re seeing the same scaling in RL that we saw for pre-training.\n[L6] \n[L7] Dwarkesh Patel\n[L8] \n[L9] You mentioned Rich Sutton and \u201cThe Bitter Lesson\u201d. I interviewed him last year, and he\u2019s actually very non-LLM-pilled. I don\u2019t know if this is his perspective, but one way to paraphrase his objection is: Something which possesses the true core of human learning would not require all these billions of dollars of data and compute and these bespoke environments, to learn how to use Excel, how to use PowerPoint, how to navigate a web browser. The fact that we have to build in these skills using these RL environments hints that we are actually lacking a core human learning algorithm. So we\u2019re scaling the wrong thing.\n[L10] \n[L11] That does raise the question. Why are we doing all this RL scaling if we think there\u2019s something that\u2019s going to be human-like in its ability to learn on the fly?\n[L12] \n[L13] Dario Amodei\n[L14] \n[L15] I think this puts together several things that should be thought of differently. There is a genuine puzzle here, but it may not matter. In fact, I would guess it probably doesn\u2019t matter. There is an interesting thing. Let me take the RL out of it for a second, because I actually think it\u2019s a red herring to say that RL is any different from pre-training in this matter.\n[L16] \n[L17] If we look at pre-training scaling, it was very interesting back in 2017 when Alec Radford was doing GPT-1. The models before GPT-1 were trained on datasets that didn\u2019t represent a wide distribution of text. You had very standard language modeling benchmarks. GPT-1 itself was trained on a bunch of fanfiction, I think actually.\n[L18] \n[L19] It was literary text, which is a very small fraction of the text you can get. In those days it was like a billion words or something, so small datasets representing a pretty narrow distribution of what you can see in the world. It didn\u2019t generalize well. If you did better on some fanfiction corpus, it wouldn\u2019t generalize that well to other tasks.\n[L20] \n[L21] We had all these measures. We had all these measures of how well it did at predicting all these other kinds of texts. It was only when you trained over all the tasks on the internet \u2014 when you did a general internet scrape from something like Common Crawl or scraping links in Reddit, which is what we did for GPT-2 \u2014 that you started to get generalization.\n[L22] \n[L23] I think we\u2019re seeing the same thing on RL. We\u2019re starting first with simple RL tasks like training on math competitions, then moving to broader training that involves things like code. Now we\u2019re moving to many other tasks. I think then we\u2019re going to increasingly get generalization. So that kind of takes out the RL vs. pre-training side of it.\n# \u30101\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn3file1\ue201\nMclick Target: \"3:1\"\nContent Snippet:\n[L1] Dario Amodei\n[L2] \n[L3] I actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.\n[L4] \n[L5] That was one among many things. Back in those days there was robotics. People tried to work on reasoning as a separate thing from language models, and there was scaling of the kind of RL that happened in AlphaGo and in Dota at OpenAI. People remember StarCraft at DeepMind, AlphaStar.\n[L6] \n[L7] It was written as a more general document. Rich Sutton put out \u201cThe Bitter Lesson\u201d a couple years later. The hypothesis is basically the same. What it says is that all the cleverness, all the techniques, all the \u201cwe need a new method to do something\u201d, that doesn\u2019t matter very much. There are only a few things that matter. I think I listed seven of them.\n[L8] \n[L9] One is how much raw compute you have. The second is the quantity of data. The third is the quality and distribution of data. It needs to be a broad distribution. The fourth is how long you train for. The fifth is that you need an objective function that can scale to the moon. The pre-training objective function is one such objective function. Another is the RL objective function that says you have a goal, you\u2019re going to go out and reach the goal.\n[L10] \n[L11] Within that, there\u2019s objective rewards like you see in math and coding, and there\u2019s more subjective rewards like you see in RLHF or higher-order versions of that. Then the sixth and seventh were things around normalization or conditioning, just getting the numerical stability so that the big blob of compute flows in this laminar way instead of running into problems.\n[L12] \n[L13] That was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\n[L14] \n[L15] What has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d\n[L16] \n[L17] We see that as well, and it\u2019s not just math contests. It\u2019s a wide variety of RL tasks. We\u2019re seeing the same scaling in RL that we saw for pre-training.\n[L18] \n[L19] Dwarkesh Patel\n[L20] \n[L21] You mentioned Rich Sutton and \u201cThe Bitter Lesson\u201d. I interviewed him last year, and he\u2019s actually very non-LLM-pilled. I don\u2019t know if this is his perspective, but one way to paraphrase his objection is: Something which possesses the true core of human learning would not require all these billions of dollars of data and compute and these bespoke environments, to learn how to use Excel, how to use PowerPoint, how to navigate a web browser. The fact that we have to build in these skills using these RL environments hints that we are actually lacking a core human learning algorithm. So we\u2019re scaling the wrong thing.\n[L22] \n[L23] That does raise the question. Why are we doing all this RL scaling if we think there\u2019s something that\u2019s going to be human-like in its ability to learn on the fly?\n[L24] \n[L25] Dario Amodei\n# \u30102\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn3file2\ue201\nMclick Target: \"3:2\"\nContent Snippet:\n[L1] Dario Amodei\n[L2] \n[L3] I think this puts together several things that should be thought of differently. There is a genuine puzzle here, but it may not matter. In fact, I would guess it probably doesn\u2019t matter. There is an interesting thing. Let me take the RL out of it for a second, because I actually think it\u2019s a red herring to say that RL is any different from pre-training in this matter.\n[L4] \n[L5] If we look at pre-training scaling, it was very interesting back in 2017 when Alec Radford was doing GPT-1. The models before GPT-1 were trained on datasets that didn\u2019t represent a wide distribution of text. You had very standard language modeling benchmarks. GPT-1 itself was trained on a bunch of fanfiction, I think actually.\n[L6] \n[L7] It was literary text, which is a very small fraction of the text you can get. In those days it was like a billion words or something, so small datasets representing a pretty narrow distribution of what you can see in the world. It didn\u2019t generalize well. If you did better on some fanfiction corpus, it wouldn\u2019t generalize that well to other tasks.\n[L8] \n[L9] We had all these measures. We had all these measures of how well it did at predicting all these other kinds of texts. It was only when you trained over all the tasks on the internet \u2014 when you did a general internet scrape from something like Common Crawl or scraping links in Reddit, which is what we did for GPT-2 \u2014 that you started to get generalization.\n[L10] \n[L11] I think we\u2019re seeing the same thing on RL. We\u2019re starting first with simple RL tasks like training on math competitions, then moving to broader training that involves things like code. Now we\u2019re moving to many other tasks. I think then we\u2019re going to increasingly get generalization. So that kind of takes out the RL vs. pre-training side of it.\n[L12] \n[L13] But there is a puzzle either way, which is that in pre-training we use trillions of tokens. Humans don\u2019t see trillions of words. So there is an actual sample efficiency difference here. There is actually something different here. The models start from scratch and they need much more training. But we also see that once they\u2019re trained, if we give them a long context length of a million \u2014 the only thing blocking long context is inference \u2014 they\u2019re very good at learning and adapting within that context.\n[L14] \n[L15] So I don\u2019t know the full answer to this. I think there\u2019s something going on where pre-training is not like the process of humans learning, but it\u2019s somewhere between the process of humans learning and the process of human evolution. We get many of our priors from evolution. Our brain isn\u2019t just a blank slate. Whole books have been written about this.\n[L16] \n[L17] The language models are much more like blank slates. They literally start as random weights, whereas the human brain starts with all these regions connected to all these inputs and outputs. Maybe we should think of pre-training \u2014 and for that matter, RL as well \u2014 as something that exists in the middle space between human evolution and human on-the-spot learning. And we should think of the in-context learning that the models do as something between long-term human learning and short-term human learning.\n[L18] \n[L19] So there\u2019s this hierarchy. There\u2019s evolution, there\u2019s long-term learning, there\u2019s short-term learning, and there\u2019s just human reaction. The LLM phases exist along this spectrum, but not necessarily at exactly the same points. There\u2019s no analog to some of the human modes of learning the LLMs are falling in between the points. Does that make sense?\n[L20] \n[L21] Dwarkesh Patel\n# \u30103\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn3file3\ue201\nMclick Target: \"3:3\"\nContent Snippet:\n[L1] Transcript\n[L2] 00:00:00 - What exactly are we scaling?\n[L3] Dwarkesh Patel\n[L4] \n[L5] We talked three years ago. In your view, what has been the biggest update over the last three years? What has been the biggest difference between what it felt like then versus now?\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\n[L10] \n[L11] But when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.\n[L12] \n[L13] What has been the most surprising thing is the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that you have people \u2014 within the bubble and outside the bubble \u2014 talking about the same tired, old hot-button political issues, when we are near the end of the exponential.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] I want to understand what that exponential looks like right now. The first question I asked you when we recorded three years ago was, \u201cwhat\u2019s up with scaling and why does it work?\u201d I have a similar question now, but it feels more complicated. At least from the public\u2019s point of view, three years ago there were well-known public trends across many orders of magnitude of compute where you could see how the loss improves.\n[L18] \n[L19] Now we have RL scaling and there\u2019s no publicly known scaling law for it. It\u2019s not even clear what the story is. Is this supposed to be teaching the model skills? Is it supposed to be teaching meta-learning? What is the scaling hypothesis at this point?\n[L20] \n[L21] Dario Amodei\n[L22] \n[L23] I actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.\n[L24] \n[L25] That was one among many things. Back in those days there was robotics. People tried to work on reasoning as a separate thing from language models, and there was scaling of the kind of RL that happened in AlphaGo and in Dota at OpenAI. People remember StarCraft at DeepMind, AlphaStar.\n[L26] \n[L27] It was written as a more general document. Rich Sutton put out \u201cThe Bitter Lesson\u201d a couple years later. The hypothesis is basically the same. What it says is that all the cleverness, all the techniques, all the \u201cwe need a new method to do something\u201d, that doesn\u2019t matter very much. There are only a few things that matter. I think I listed seven of them.\n[L28] \n[L29] One is how much raw compute you have. The second is the quantity of data. The third is the quality and distribution of data. It needs to be a broad distribution. The fourth is how long you train for. The fifth is that you need an objective function that can scale to the moon. The pre-training objective function is one such objective function. Another is the RL objective function that says you have a goal, you\u2019re going to go out and reach the goal.\n[L30] \n[L31] Within that, there\u2019s objective rewards like you see in math and coding, and there\u2019s more subjective rewards like you see in RLHF or higher-order versions of that. Then the sixth and seventh were things around normalization or conditioning, just getting the numerical stability so that the big blob of compute flows in this laminar way instead of running into problems.\n# \u30104\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn3file4\ue201\nMclick Target: \"3:4\"\nContent Snippet:\n[L1] But there is a puzzle either way, which is that in pre-training we use trillions of tokens. Humans don\u2019t see trillions of words. So there is an actual sample efficiency difference here. There is actually something different here. The models start from scratch and they need much more training. But we also see that once they\u2019re trained, if we give them a long context length of a million \u2014 the only thing blocking long context is inference \u2014 they\u2019re very good at learning and adapting within that context.\n[L2] \n[L3] So I don\u2019t know the full answer to this. I think there\u2019s something going on where pre-training is not like the process of humans learning, but it\u2019s somewhere between the process of humans learning and the process of human evolution. We get many of our priors from evolution. Our brain isn\u2019t just a blank slate. Whole books have been written about this.\n[L4] \n[L5] The language models are much more like blank slates. They literally start as random weights, whereas the human brain starts with all these regions connected to all these inputs and outputs. Maybe we should think of pre-training \u2014 and for that matter, RL as well \u2014 as something that exists in the middle space between human evolution and human on-the-spot learning. And we should think of the in-context learning that the models do as something between long-term human learning and short-term human learning.\n[L6] \n[L7] So there\u2019s this hierarchy. There\u2019s evolution, there\u2019s long-term learning, there\u2019s short-term learning, and there\u2019s just human reaction. The LLM phases exist along this spectrum, but not necessarily at exactly the same points. There\u2019s no analog to some of the human modes of learning the LLMs are falling in between the points. Does that make sense?\n[L8] \n[L9] Dwarkesh Patel\n[L10] \n[L11] Yes, although some things are still a bit confusing. For example, if the analogy is that this is like evolution so it\u2019s fine that it\u2019s not sample efficient, then if we\u2019re going to get super sample-efficient agent from in-context learning, why are we bothering to build all these RL environments?\n[L12] \n[L13] There are companies whose work seems to be teaching models how to use this API, how to use Slack, how to use whatever. It\u2019s confusing to me why there\u2019s so much emphasis on that if the kind of agent that can just learn on the fly is emerging or has already emerged.\n[L14] \n[L15] Dario Amodei\n[L16] \n[L17] I can\u2019t speak for the emphasis of anyone else. I can only talk about how we think about it. The goal is not to teach the model every possible skill within RL, just as we don\u2019t do that within pre-training. Within pre-training, we\u2019re not trying to expose the model to every possible way that words could be put together. Rather, the model trains on a lot of things and then reaches generalization across pre-training.\n[L18] \n[L19] That was the transition from GPT-1 to GPT-2 that I saw up close. The model reaches a point. I had these moments where I was like, \u201cOh yeah, you just give the model a list of numbers \u2014 this is the cost of the house, this is the square feet of the house \u2014 and the model completes the pattern and does linear regression.\u201d Not great, but it does it, and it\u2019s never seen that exact thing before.\n[L20] \n[L21] So to the extent that we are building these RL environments, the goal is very similar to what was done five or ten years ago with pre-training. We\u2019re trying to get a whole bunch of data, not because we want to cover a specific document or a specific skill, but because we want to generalize.\n[L22] \n[L23] 00:12:36 - Is diffusion cope?\n[L24] Dwarkesh Patel\nVisible: 0% - 100%","summary":null,"assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"msearch","cloud_doc_urls":[null,null,null,null,null],"retrieval_turn_number":3,"is_contextual_answers_supported":true,"contextual_answers_available_sources":["slurm_github_issues","slurm_github_pulls","slurm_github_commits","slurm_github","files_uploaded_in_conversation"],"retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"connectors_file_search":{"tool_call_message_id":"aa248b53-e4e6-47da-9e44-4d47f1034d68","source_filters":["files_uploaded_in_conversation"],"call_tool_latency_ms":1848.2719790190458},"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"ada83e82-3c9c-44ca-b65f-b69e26fc7f40","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":null},"parent":"ada83e82-3c9c-44ca-b65f-b69e26fc7f40","children":["715ef395-26de-4c0a-b41a-2fb2d1b7dfe2"]},"715ef395-26de-4c0a-b41a-2fb2d1b7dfe2":{"id":"715ef395-26de-4c0a-b41a-2fb2d1b7dfe2","message":{"id":"715ef395-26de-4c0a-b41a-2fb2d1b7dfe2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771107990.703972,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Evaluating evidence for \"end of exponential\" claim","content":"I think we should focus on his references to scaling in RL and pretraining as evidence of continued exponential growth. His timeline of 1-2 years for coding and predictions for 2026-2027 from *Machines of Loving Grace* also supports the idea of \"near the end.\" But the key question is if these factors really justify the \"end of exponential\" claim.","chunks":["I think we should focus on his references to scaling in RL and pretraining as evidence of continued exponential growth.","His timeline of 1-2 years for coding and predictions for 2026-2027 from *Machines of Loving Grace* also supports the idea of \"near the end.\" But the key question is if these factors really justify the \"end of exponential\" claim."],"finished":true},{"summary":"Summarizing Dario Amodei's rationale for \"end of exponential\"","content":"Amodei argues that scaling laws for pretraining and RL are continuing, with progression along an exponential trajectory. He mentions milestones like end-to-end coding in 1-2 years and significant improvements in benchmarks and internal adoption. However, his argument is more qualitative, based on personal observations, and lacks detailed data or quantitative analysis.","chunks":["Amodei argues that scaling laws for pretraining and RL are continuing, with progression along an exponential trajectory.","He mentions milestones like end-to-end coding in 1-2 years and significant improvements in benchmarks and internal adoption.","However, his argument is more qualitative, based on personal observations, and lacks detailed data or quantitative analysis."],"finished":true},{"summary":"Clarifying Dario Amodei's \"near end of the exponential\" statement","content":"Amodei doesn't provide explicit proof when he mentions we're nearing the \"end of exponential,\" but he supports this claim with several rationales: extrapolation from scaling trends, ongoing improvements in pretraining and RL, predictions for near-term coding milestones, observed progress in benchmarks, and commercial adoption. He clarifies that exponential progress will continue, but with diminishing returns.","chunks":["Amodei doesn't provide explicit proof when he mentions we're nearing the \"end of exponential,\" but he supports this claim with several rationales: extrapolation from scaling trends, ongoing improvements in pretraining and RL, predictions for near-term coding milestones, observed progress in benchmarks, and commercial adoption. He clarifies that exponential progress will continue, but with diminishing returns."],"finished":true}],"source_analysis_msg_id":"bf85f4f6-de99-44b6-b220-ef3f86a49b3c"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"6bb443e4-5c38-4f9c-90e3-8678bca27f2b","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","classifier_response":"default","skip_reasoning_title":"Skip","can_save":true},"recipient":"all","channel":null},"parent":"6bb443e4-5c38-4f9c-90e3-8678bca27f2b","children":["5d8cccaf-192e-4de7-adbf-69f7c175f45c"]},"5d8cccaf-192e-4de7-adbf-69f7c175f45c":{"id":"5d8cccaf-192e-4de7-adbf-69f7c175f45c","message":{"id":"5d8cccaf-192e-4de7-adbf-69f7c175f45c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771107997.976309,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"queries\":[\"\\\"the exponential\\\" will continue as it has before distinguished points\",\"distinguished points on the exponential\"],\"source_filter\":[\"files_uploaded_in_conversation\"]}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"a5f8070f-7b98-46de-be5a-8e254ed91eea","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","can_save":true},"recipient":"file_search.msearch","channel":null},"parent":"715ef395-26de-4c0a-b41a-2fb2d1b7dfe2","children":["91e2d60d-de8d-4a69-88d6-91e3460bef01"]},"91e2d60d-de8d-4a69-88d6-91e3460bef01":{"id":"91e2d60d-de8d-4a69-88d6-91e3460bef01","message":{"id":"91e2d60d-de8d-4a69-88d6-91e3460bef01","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771108001.2358708,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"spinner","status":"running","retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"5d8cccaf-192e-4de7-adbf-69f7c175f45c","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":false},"recipient":"all","channel":null},"parent":"5d8cccaf-192e-4de7-adbf-69f7c175f45c","children":["f1a9c203-a669-4bf9-bdb7-3708acd027ce"]},"f1a9c203-a669-4bf9-bdb7-3708acd027ce":{"id":"f1a9c203-a669-4bf9-bdb7-3708acd027ce","message":{"id":"f1a9c203-a669-4bf9-bdb7-3708acd027ce","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771108003.0831437,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn4file0\ue201\nMclick Target: \"4:0\"\nContent Snippet:\n[L1] Transcript\n[L2] 00:00:00 - What exactly are we scaling?\n[L3] Dwarkesh Patel\n[L4] \n[L5] We talked three years ago. In your view, what has been the biggest update over the last three years? What has been the biggest difference between what it felt like then versus now?\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\n[L10] \n[L11] But when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.\n[L12] \n[L13] What has been the most surprising thing is the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that you have people \u2014 within the bubble and outside the bubble \u2014 talking about the same tired, old hot-button political issues, when we are near the end of the exponential.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] I want to understand what that exponential looks like right now. The first question I asked you when we recorded three years ago was, \u201cwhat\u2019s up with scaling and why does it work?\u201d I have a similar question now, but it feels more complicated. At least from the public\u2019s point of view, three years ago there were well-known public trends across many orders of magnitude of compute where you could see how the loss improves.\n[L18] \n[L19] Now we have RL scaling and there\u2019s no publicly known scaling law for it. It\u2019s not even clear what the story is. Is this supposed to be teaching the model skills? Is it supposed to be teaching meta-learning? What is the scaling hypothesis at this point?\n[L20] \n[L21] Dario Amodei\n[L22] \n[L23] I actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.\n[L24] \n[L25] That was one among many things. Back in those days there was robotics. People tried to work on reasoning as a separate thing from language models, and there was scaling of the kind of RL that happened in AlphaGo and in Dota at OpenAI. People remember StarCraft at DeepMind, AlphaStar.\n[L26] \n[L27] It was written as a more general document. Rich Sutton put out \u201cThe Bitter Lesson\u201d a couple years later. The hypothesis is basically the same. What it says is that all the cleverness, all the techniques, all the \u201cwe need a new method to do something\u201d, that doesn\u2019t matter very much. There are only a few things that matter. I think I listed seven of them.\n[L28] \n[L29] One is how much raw compute you have. The second is the quantity of data. The third is the quality and distribution of data. It needs to be a broad distribution. The fourth is how long you train for. The fifth is that you need an objective function that can scale to the moon. The pre-training objective function is one such objective function. Another is the RL objective function that says you have a goal, you\u2019re going to go out and reach the goal.\n[L30] \n[L31] Within that, there\u2019s objective rewards like you see in math and coding, and there\u2019s more subjective rewards like you see in RLHF or higher-order versions of that. Then the sixth and seventh were things around normalization or conditioning, just getting the numerical stability so that the big blob of compute flows in this laminar way instead of running into problems.\n# \u30101\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn4file1\ue201\nMclick Target: \"4:1\"\nContent Snippet:\n[L1] 00:12:36 - Is diffusion cope?\n[L2] Dwarkesh Patel\n[L3] \n[L4] I think the framework you\u2019re laying down obviously makes sense. We\u2019re making progress toward AGI. Nobody at this point disagrees we\u2019re going to achieve AGI this century. The crux is you say we\u2019re hitting the end of the exponential. Somebody else looks at this and says, \u201cWe\u2019ve been making progress since 2012, and by 2035 we\u2019ll have a human-like agent.\u201d\n[L5] \n[L6] Obviously we\u2019re seeing in these models the kinds of things that evolution did, or that learning within a human lifetime does. I want to understand what you\u2019re seeing that makes you think it\u2019s one year away and not ten years away.\n[L7] \n[L8] Dario Amodei\n[L9] \n[L10] There are two claims you could make here, one stronger and one weaker. Starting with the weaker claim, when I first saw the scaling back in 2019, I wasn\u2019t sure. This was a 50/50 thing. I thought I saw something. My claim was that this was much more likely than anyone thinks. Maybe there\u2019s a 50% chance this happens.\n[L11] \n[L12] On the basic hypothesis of, as you put it, within ten years we\u2019ll get to what I call a \u201ccountry of geniuses in a data center\u201d, I\u2019m at 90% on that. It\u2019s hard to go much higher than 90% because the world is so unpredictable. Maybe the irreducible uncertainty puts us at 95%, where you get to things like multiple companies having internal turmoil, Taiwan gets invaded, all the fabs get blown up by missiles.\n[L13] \n[L14] Dwarkesh Patel\n[L15] \n[L16] Now you\u2019ve jinxed us, Dario.\n[L17] \n[L18] Dario Amodei\n[L19] \n[L20] You could construct a 5% world where things get delayed for ten years. There\u2019s another 5% which is that I\u2019m very confident on tasks that can be verified. With coding, except for that irreducible uncertainty, I think we\u2019ll be there in one or two years. There\u2019s no way we will not be there in ten years in terms of being able to do end-to-end coding.\n[L21] \n[L22] My one little bit of fundamental uncertainty, even on long timescales, is about tasks that aren\u2019t verifiable: planning a mission to Mars; doing some fundamental scientific discovery like CRISPR; writing a novel. It\u2019s hard to verify those tasks. I am almost certain we have a reliable path to get there, but if there\u2019s a little bit of uncertainty it\u2019s there. On the ten-year timeline I\u2019m at 90%, which is about as certain as you can be. I think it\u2019s crazy to say that this won\u2019t happen by 2035. In some sane world, it would be outside the mainstream.\n[L23] \n[L24] Dwarkesh Patel\n[L25] \n[L26] But the emphasis on verification hints to me a lack of belief that these models are generalized. If you think about humans, we\u2019re both good at things for which we get verifiable reward and things for which we don\u2019t.\n[L27] \n[L28] Dario Amodei\n[L29] \n[L30] No, this is why I\u2019m almost sure. We already see substantial generalization from things that verify to things that don\u2019t. We\u2019re already seeing that.\n[L31] \n[L32] Dwarkesh Patel\n[L33] \n[L34] But it seems like you were emphasizing this as a spectrum which will split apart which domains in which we see more progress. That doesn\u2019t seem like how humans get better.\n[L35] \n[L36] Dario Amodei\n[L37] \n[L38] The world in which we don\u2019t get there is the world in which we do all the verifiable things. Many of them generalize, but we don\u2019t fully get there. We don\u2019t fully color in the other side of the box. It\u2019s not a binary thing.\n[L39] \n[L40] Dwarkesh Patel\n# \u30102\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn4file2\ue201\nMclick Target: \"4:2\"\nContent Snippet:\n[L1] Dwarkesh Patel\n[L2] \n[L3] Maybe stepping back, I\u2019m not saying I think the \u201ccountry of geniuses\u201d is going to come in two years and therefore you should buy this compute. To me, the end conclusion you\u2019re arriving at makes a lot of sense. But that\u2019s because it seems like \u201ccountry of geniuses\u201d is hard and there\u2019s a long way to go. So stepping back, the thing I\u2019m trying to get at is more that it seems like your worldview is compatible with somebody who says, \u201cWe\u2019re like 10 years away from a world in which we\u2019re generating trillions of dollars of value.\u201d\n[L4] \n[L5] Dario Amodei\n[L6] \n[L7] That\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.\n[L8] \n[L9] Like in 2028, we get the real \u201ccountry of geniuses in the data center\u201d. The revenue\u2019s going into the low hundreds of billions by 2028, and then the country of geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. It takes two years to get to the trillions. That would be the world where it takes until 2030. I suspect even composing the technical exponential and diffusion exponential, we\u2019ll get there before 2030.\n[L10] \n[L11] Dwarkesh Patel\n[L12] \n[L13] So you laid out a model where Anthropic makes profit because it seems like fundamentally we\u2019re in a compute-constrained world. So eventually we keep growing compute\u2014\n[L14] \n[L15] Dario Amodei\n[L16] \n[L17] I think the way the profit comes is\u2026 Again, let\u2019s just abstract the whole industry here. Let\u2019s just imagine we\u2019re in an economics textbook. We have a small number of firms. Each can invest a limited amount. Each can invest some fraction in R&D. They have some marginal cost to serve. The gross profit margins on that marginal cost are very high because inference is efficient. There\u2019s some competition, but the models are also differentiated.\n[L18] \n[L19] Companies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.\n[L20] \n[L21] Dwarkesh Patel\n[L22] \n[L23] Help me understand that, because right now we do have three leading firms and they\u2019re not making profit. So what is changing?\n[L24] \n[L25] Dario Amodei\n[L26] \n[L27] Again, the gross margins right now are very positive. What\u2019s happening is a combination of two things. One is that we\u2019re still in the exponential scale-up phase of compute. A model gets trained. Let\u2019s say a model got trained that costs $1 billion last year. Then this year it produced $4 billion of revenue and cost $1 billion to inference from. Again, I\u2019m using stylized numbers here, but that would be 75% gross margins and this 25% tax. So that model as a whole makes $2 billion.\n[L28] \n[L29] But at the same time, we\u2019re spending $10 billion to train the next model because there\u2019s an exponential scale-up. So the company loses money. Each model makes money, but the company loses money.\n[L30] \n[L31] The equilibrium I\u2019m talking about is an equilibrium where we have the \u201ccountry of geniuses in a data center\u201d, but that model training scale-up has equilibrated more. Maybe it\u2019s still going up. We\u2019re still trying to predict the demand, but it\u2019s more leveled out.\n[L32] \n[L33] Dwarkesh Patel\n# \u30103\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn4file3\ue201\nMclick Target: \"4:3\"\nContent Snippet:\n[L1] Now, will governments eventually get powerful AI, and is there a risk of authoritarianism? Yes. Will governments eventually get powerful AI, and is there a risk of bad equilibria? Yes, I think both things. But the initial conditions matter. At some point, we\u2019re going to need to set up the rules of the road.\n[L2] \n[L3] I\u2019m not saying that one country, either the United States or a coalition of democracies\u2014which I think would be a better setup, although it requires more international cooperation than we currently seem to want to make\u2014should just say, \u201cThese are the rules of the road.\u201d There\u2019s going to be some negotiation. The world is going to have to grapple with this.\n[L4] \n[L5] What I would like is for the democratic nations of the world\u2014those whose governments represent closer to pro-human values\u2014are holding the stronger hand and have more leverage when the rules of the road are set. So I\u2019m very concerned about that initial condition.\n[L6] \n[L7] Dwarkesh Patel\n[L8] \n[L9] I was re-listening to the interview from three years ago, and one of the ways it aged poorly is that I kept asking questions assuming there was going to be some key fulcrum moment two to three years from now. In fact, being that far out, it just seems like progress continues, AI improves, AI is more diffused, and people will use it for more things.\n[L10] \n[L11] It seems like you\u2019re imagining a world in the future where the countries get together, and \u201cHere\u2019s the rules of the road, here\u2019s the leverage we have, and here\u2019s the leverage you have.\u201d But on the current trajectory, everybody will have more AI. Some of that AI will be used by authoritarian countries. Some of that within the authoritarian countries will be used by private actors versus state actors.\n[L12] \n[L13] It\u2019s not clear who will benefit more. It\u2019s always unpredictable to tell in advance. It seems like the internet privileged authoritarian countries more than you would\u2019ve expected. Maybe AI will be the opposite way around. I want to better understand what you\u2019re imagining here.\n[L14] \n[L15] Dario Amodei\n[L16] \n[L17] Just to be precise about it, I think the exponential of the underlying technology will continue as it has before. The models get smarter and smarter, even when they get to a \u201ccountry of geniuses in a data center.\u201d I think you can continue to make the model smarter. There\u2019s a question of getting diminishing returns on their value in the world. How much does it matter after you\u2019ve already solved human biology? At some point you can do harder, more abstruse math problems, but nothing after that matters.\n[L18] \n[L19] Putting that aside, I do think the exponential will continue, but there will be certain distinguished points on the exponential. Companies, individuals, and countries will reach those points at different times.\n[L20] \n[L21] In \u201cThe Adolescence of Technology\u201d I talk about: Is a nuclear deterrent still stable in the world of AI? I don\u2019t know, but that\u2019s an example of one thing we\u2019ve taken for granted. The technology could reach such a level that we can no longer be certain of it. Think of others. There are points where if you reach a certain level, maybe you have offensive cyber dominance, and every computer system is transparent to you after that unless the other side has an equivalent defense.\n[L22] \n[L23] I don\u2019t know what the critical moment is or if there\u2019s a single critical moment. But I think there will be either a critical moment, a small number of critical moments, or some critical window where AI confers some large advantage from the perspective of national security, and one country or coalition has reached it before others.\n# \u30104\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn4file4\ue201\nMclick Target: \"4:4\"\nContent Snippet:\n[L1] But what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\n[L2] \n[L3] Dwarkesh Patel\n[L4] \n[L5] You guys should have just bought a billion dollars of your own products so you could just\u2026\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] And the first month of this year, that exponential is... You would think it would slow down, but we added another few billion to revenue in January. Obviously that curve can\u2019t go on forever. The GDP is only so large. I would even guess that it bends somewhat this year, but that is a fast curve. That\u2019s a really fast curve. I would bet it stays pretty fast even as the scale goes to the entire economy.\n[L10] \n[L11] So I think we should be thinking about this middle world where things are extremely fast, but not instant, where they take time because of economic diffusion, because of the need to close the loop. Because it\u2019s fiddly: \u201cI have to do change management within my enterprise\u2026 I set this up, but I have to change the security permissions on this in order to make it actually work\u2026 I had this old piece of software that checks the model before it\u2019s compiled and released and I have to rewrite it. Yes, the model can do that, but I have to tell the model to do that. It has to take time to do that.\u201d\n[L12] \n[L13] So I think everything we\u2019ve seen so far is compatible with the idea that there\u2019s one fast exponential that\u2019s the capability of the model. Then there\u2019s another fast exponential that\u2019s downstream of that, which is the diffusion of the model into the economy. Not instant, not slow, much faster than any previous technology, but it has its limits. When I look inside Anthropic, when I look at our customers: fast adoption, but not infinitely fast.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] Can I try a hot take on you?\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] Yeah.\n[L22] \n[L23] Dwarkesh Patel\n[L24] \n[L25] I feel like diffusion is cope that people say. When the model isn\u2019t able to do something, they\u2019re like, \u201coh, but it\u2019s a diffusion issue.\u201d But then you should use the comparison to humans. You would think that the inherent advantages that AIs have would make diffusion a much easier problem for new AIs getting onboarded than new humans getting onboarded. An AI can read your entire Slack and your drive in minutes. They can share all the knowledge that the other copies of the same instance have. You don\u2019t have this adverse selection problem when you\u2019re hiring AI, so you can just hire copies of a vetted AI model.\n[L26] \n[L27] Hiring a human is so much more of a hassle. People hire humans all the time. We pay humans upwards of $50 trillion in wages because they\u2019re useful, even though in principle it would be much easier to integrate AIs into the economy than it is to hire humans. The diffusion doesn\u2019t really explain.\n[L28] \n[L29] Dario Amodei\n[L30] \n[L31] I think diffusion is very real and doesn\u2019t exclusively have to do with limitations on the AI models. Again, there are people who use diffusion as kind of a buzzword to say this isn\u2019t a big deal. I\u2019m not talking about that. I\u2019m not talking about how AI will diffuse at the speed of previous technologies. I think AI will diffuse much faster than previous technologies have, but not infinitely fast.\nVisible: 0% - 100%","summary":null,"assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"msearch","cloud_doc_urls":[null,null,null,null,null],"retrieval_turn_number":4,"is_contextual_answers_supported":true,"contextual_answers_available_sources":["slurm_github_issues","slurm_github_pulls","slurm_github_commits","slurm_github","files_uploaded_in_conversation"],"retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"connectors_file_search":{"tool_call_message_id":"594823d1-b365-490c-8c91-1e27065ca4a3","source_filters":["files_uploaded_in_conversation"],"call_tool_latency_ms":1847.000440582633},"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"91e2d60d-de8d-4a69-88d6-91e3460bef01","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":null},"parent":"91e2d60d-de8d-4a69-88d6-91e3460bef01","children":["668be63e-4d9e-4c7f-a33b-324fb75fe4fe"]},"668be63e-4d9e-4c7f-a33b-324fb75fe4fe":{"id":"668be63e-4d9e-4c7f-a33b-324fb75fe4fe","message":{"id":"668be63e-4d9e-4c7f-a33b-324fb75fe4fe","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771108009.678486,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Locating OSWorld benchmark improvement","content":"We\u2019ve got the lines about exponential progress and distinctions, so now let's focus on concretely capturing the benchmark improvements, like the OSWorld progression from 15% to 65-70%. This provides tangible evidence about computer use improvement. I\u2019ll search for the specific reference to \u201cOSWorld.\u201d","chunks":["We\u2019ve got the lines about exponential progress and distinctions, so now let's focus on concretely capturing the benchmark improvements, like the OSWorld progression from 15% to 65-70%.","This provides tangible evidence about computer use improvement.","I\u2019ll search for the specific reference to \u201cOSWorld.\u201d"],"finished":true}],"source_analysis_msg_id":"a5f8070f-7b98-46de-be5a-8e254ed91eea"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"70f588b9-3e1f-4d83-b877-4e825b7da9f9","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","classifier_response":"default","skip_reasoning_title":"Skip","can_save":true},"recipient":"all","channel":null},"parent":"f1a9c203-a669-4bf9-bdb7-3708acd027ce","children":["9c3c5642-08e0-4775-b411-91bb31da3a33"]},"9c3c5642-08e0-4775-b411-91bb31da3a33":{"id":"9c3c5642-08e0-4775-b411-91bb31da3a33","message":{"id":"9c3c5642-08e0-4775-b411-91bb31da3a33","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771108006.959921,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"queries\":[\"OSWorld 15% 65-70% climbed from\",\"computer use OSWorld 65-70\"],\"source_filter\":[\"files_uploaded_in_conversation\"]}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"668be63e-4d9e-4c7f-a33b-324fb75fe4fe","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","can_save":true},"recipient":"file_search.msearch","channel":null},"parent":"668be63e-4d9e-4c7f-a33b-324fb75fe4fe","children":["7ec62746-88e3-4d45-98b0-4a3836355073"]},"7ec62746-88e3-4d45-98b0-4a3836355073":{"id":"7ec62746-88e3-4d45-98b0-4a3836355073","message":{"id":"7ec62746-88e3-4d45-98b0-4a3836355073","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771108010.3062954,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"spinner","status":"running","retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"d0369bd7-88b5-4f7b-bbe6-8742feccca9f","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":false},"recipient":"all","channel":null},"parent":"9c3c5642-08e0-4775-b411-91bb31da3a33","children":["d46961f6-2543-4e9e-9fca-3e06c8c6d252"]},"d46961f6-2543-4e9e-9fca-3e06c8c6d252":{"id":"d46961f6-2543-4e9e-9fca-3e06c8c6d252","message":{"id":"d46961f6-2543-4e9e-9fca-3e06c8c6d252","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771108011.5794303,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn5file0\ue201\nMclick Target: \"5:0\"\nContent Snippet:\n[L1] I think that\u2019s dependent on several things. I think this is one of the things that\u2019s actually blocking deployment: getting to the point on computer use where the models are really masters at using the computer.\n[L2] \n[L3] We\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.\n[L4] \n[L5] Dwarkesh Patel\n[L6] \n[L7] Can I just follow up on that before you move on to the next point? For years, I\u2019ve been trying to build different internal LLM tools for myself. Often I have these text-in, text-out tasks, which should be dead center in the repertoire of these models. Yet I still hire humans to do them.\n[L8] \n[L9] If it\u2019s something like, \u201cidentify what the best clips would be in this transcript\u201d, maybe the LLMs do a seven-out-of-ten job on them. But there\u2019s not this ongoing way I can engage with them to help them get better at the job the way I could with a human employee. That missing ability, even if you solve computer use, would still block my ability to offload an actual job to them.\n[L10] \n[L11] Dario Amodei\n[L12] \n[L13] This gets back to what we were talking about before with learning on the job. It\u2019s very interesting. I think with the coding agents, I don\u2019t think people would say that learning on the job is what is preventing the coding agents from doing everything end to end. They keep getting better. We have engineers at Anthropic who don\u2019t write any code.\n[L14] \n[L15] When I look at the productivity, to your previous question, we have folks who say, \u201cThis GPU kernel, this chip, I used to write it myself. I just have Claude do it.\u201d There\u2019s this enormous improvement in productivity.\n[L16] \n[L17] When I see Claude Code, familiarity with the codebase or a feeling that the model hasn\u2019t worked at the company for a year, that\u2019s not high up on the list of complaints I see. I think what I\u2019m saying is that we\u2019re kind of taking a different path.\n[L18] \n[L19] Dwarkesh Patel\n[L20] \n[L21] Don\u2019t you think with coding that\u2019s because there is an external scaffold of memory which exists instantiated in the codebase? I don\u2019t know how many other jobs have that. Coding made fast progress precisely because it has this unique advantage that other economic activity doesn\u2019t.\n[L22] \n[L23] Dario Amodei\n[L24] \n[L25] But when you say that, what you\u2019re implying is that by reading the codebase into the context, I have everything that the human needed to learn on the job. So that would be an example of\u2014whether it\u2019s written or not, whether it\u2019s available or not\u2014a case where everything you needed to know you got from the context window. What we think of as learning\u2014\u201dI started this job, it\u2019s going to take me six months to understand the code base\u201d\u2014the model just did it in the context.\n[L26] \n[L27] Dwarkesh Patel\n[L28] \n[L29] I honestly don\u2019t know how to think about this because there are people who qualitatively report what you\u2019re saying. I\u2019m sure you saw last year, there was a major study where they had experienced developers try to close pull requests in repositories that they were familiar with. Those developers reported an uplift. They reported that they felt more productive with the use of these models. But in fact, if you look at their output and how much was actually merged back in, there was a 20% downlift. They were less productive as a result of using these models.\n# \u30101\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn5file1\ue201\nMclick Target: \"5:1\"\nContent Snippet:\n[L1] Dwarkesh Patel\n[L2] \n[L3] Let me ask a very specific question so that we can figure out exactly what kinds of capabilities we should think about soon. Maybe I\u2019ll ask about it in the context of a job I understand well, not because it\u2019s the most relevant job, but just because I can evaluate the claims about it.\n[L4] \n[L5] Take video editors. I have video editors. Part of their job involves learning about our audience\u2019s preferences, learning about my preferences and tastes, and the different trade-offs we have. They\u2019re, over the course of many months, building up this understanding of context. The skill and ability they have six months into the job, a model that can pick up that skill on the job on the fly, when should we expect such an AI system?\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] I guess what you\u2019re talking about is that we\u2019re doing this interview for three hours. Someone\u2019s going to come in, someone\u2019s going to edit it. They\u2019re going to be like, \u201cOh, I don\u2019t know, Dario scratched his head and we could edit that out.\u201d\n[L10] \n[L11] Dwarkesh Patel\n[L12] \n[L13] \u201cMagnify that.\u201d\n[L14] \n[L15] Dario Amodei\n[L16] \n[L17] \u201cThere was this long discussion that is less interesting to people. There\u2019s another thing that\u2019s more interesting to people, so let\u2019s make this edit.\u201d\n[L18] \n[L19] I think the \u201ccountry of geniuses in a data center\u201d will be able to do that. The way it will be able to do that is it will have general control of a computer screen. You\u2019ll be able to feed this in. It\u2019ll be able to also use the computer screen to go on the web, look at all your previous interviews, look at what people are saying on Twitter in response to your interviews, talk to you, ask you questions, talk to your staff, look at the history of edits that you did, and from that, do the job.\n[L20] \n[L21] I think that\u2019s dependent on several things. I think this is one of the things that\u2019s actually blocking deployment: getting to the point on computer use where the models are really masters at using the computer.\n[L22] \n[L23] We\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.\n[L24] \n[L25] Dwarkesh Patel\n[L26] \n[L27] Can I just follow up on that before you move on to the next point? For years, I\u2019ve been trying to build different internal LLM tools for myself. Often I have these text-in, text-out tasks, which should be dead center in the repertoire of these models. Yet I still hire humans to do them.\n[L28] \n[L29] If it\u2019s something like, \u201cidentify what the best clips would be in this transcript\u201d, maybe the LLMs do a seven-out-of-ten job on them. But there\u2019s not this ongoing way I can engage with them to help them get better at the job the way I could with a human employee. That missing ability, even if you solve computer use, would still block my ability to offload an actual job to them.\n[L30] \n[L31] Dario Amodei\n[L32] \n[L33] This gets back to what we were talking about before with learning on the job. It\u2019s very interesting. I think with the coding agents, I don\u2019t think people would say that learning on the job is what is preventing the coding agents from doing everything end to end. They keep getting better. We have engineers at Anthropic who don\u2019t write any code.\n# \u30102\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn5file2\ue201\nMclick Target: \"5:2\"\nContent Snippet:\n[L1] Dario Amodei\n[L2] \n[L3] Within Anthropic, this is just really unambiguous. We\u2019re under an incredible amount of commercial pressure and make it even harder for ourselves because we have all this safety stuff we do that I think we do more than other companies.\n[L4] \n[L5] The pressure to survive economically while also keeping our values is just incredible. We\u2019re trying to keep this 10x revenue curve going. There is zero time for bullshit. There is zero time for feeling like we\u2019re productive when we\u2019re not. These tools make us a lot more productive.\n[L6] \n[L7] Why do you think we\u2019re concerned about competitors using the tools? Because we think we\u2019re ahead of the competitors. We wouldn\u2019t be going through all this trouble if this were secretly reducing our productivity. We see the end productivity every few months in the form of model launches. There\u2019s no kidding yourself about this. The models make you more productive.\n[L8] \n[L9] Dwarkesh Patel\n[L10] \n[L11] 1) People feeling like they\u2019re productive is qualitatively predicted by studies like this. But 2) if I just look at the end output, obviously you guys are making fast progress.\n[L12] \n[L13] But the idea was supposed to be that with recursive self-improvement, you make a better AI, the AI helps you build a better next AI, et cetera, et cetera. What I see instead\u2014if I look at you, OpenAI, DeepMind\u2014is that people are just shifting around the podium every few months.\n[L14] \n[L15] Maybe you think that stops because you\u2019ve won or whatever. But why are we not seeing the person with the best coding model have this lasting advantage if in fact there are these enormous productivity gains from the last coding model.\n[L16] \n[L17] Dario Amodei\n[L18] \n[L19] I think my model of the situation is that there\u2019s an advantage that\u2019s gradually growing. I would say right now the coding models give maybe, I don\u2019t know, a 15-20% total factor speed up. That\u2019s my view. Six months ago, it was maybe 5%. So it didn\u2019t matter. 5% doesn\u2019t register. It\u2019s now just getting to the point where it\u2019s one of several factors that kind of matters. That\u2019s going to keep speeding up.\n[L20] \n[L21] I think six months ago, there were several companies that were at roughly the same point because this wasn\u2019t a notable factor, but I think it\u2019s starting to speed up more and more. I would also say there are multiple companies that write models that are used for code and we\u2019re not perfectly good at preventing some of these other companies from using our models internally. So I think everything we\u2019re seeing is consistent with this kind of snowball model.\n[L22] \n[L23] Again, my theme in all of this is all of this is soft takeoff, soft, smooth exponentials, although the exponentials are relatively steep. So we\u2019re seeing this snowball gather momentum where it\u2019s like 10%, 20%, 25%, 40%. As you go, Amdahl\u2019s law, you have to get all the things that are preventing you from closing the loop out of the way. But this is one of the biggest priorities within Anthropic.\n[L24] \n[L25] Dwarkesh Patel\n[L26] \n[L27] Stepping back, before in the stack we were talking about when do we get this on-the-job learning? It seems like the point you were making on the coding thing is that we actually don\u2019t need on-the-job learning. You can have tremendous productivity improvements, you can have potentially trillions of dollars of revenue for AI companies, without this basic human ability to learn on the job. Maybe that\u2019s not your claim, you should clarify.\n# \u30103\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn5file3\ue201\nMclick Target: \"5:3\"\nContent Snippet:\n[L1] Dario Amodei\n[L2] \n[L3] I think my model of the situation is that there\u2019s an advantage that\u2019s gradually growing. I would say right now the coding models give maybe, I don\u2019t know, a 15-20% total factor speed up. That\u2019s my view. Six months ago, it was maybe 5%. So it didn\u2019t matter. 5% doesn\u2019t register. It\u2019s now just getting to the point where it\u2019s one of several factors that kind of matters. That\u2019s going to keep speeding up.\n[L4] \n[L5] I think six months ago, there were several companies that were at roughly the same point because this wasn\u2019t a notable factor, but I think it\u2019s starting to speed up more and more. I would also say there are multiple companies that write models that are used for code and we\u2019re not perfectly good at preventing some of these other companies from using our models internally. So I think everything we\u2019re seeing is consistent with this kind of snowball model.\n[L6] \n[L7] Again, my theme in all of this is all of this is soft takeoff, soft, smooth exponentials, although the exponentials are relatively steep. So we\u2019re seeing this snowball gather momentum where it\u2019s like 10%, 20%, 25%, 40%. As you go, Amdahl\u2019s law, you have to get all the things that are preventing you from closing the loop out of the way. But this is one of the biggest priorities within Anthropic.\n[L8] \n[L9] Dwarkesh Patel\n[L10] \n[L11] Stepping back, before in the stack we were talking about when do we get this on-the-job learning? It seems like the point you were making on the coding thing is that we actually don\u2019t need on-the-job learning. You can have tremendous productivity improvements, you can have potentially trillions of dollars of revenue for AI companies, without this basic human ability to learn on the job. Maybe that\u2019s not your claim, you should clarify.\n[L12] \n[L13] But in most domains of economic activity, people say, \u201cI hired somebody, they weren\u2019t that useful for the first few months, and then over time they built up the context, understanding.\u201d It\u2019s actually hard to define what we\u2019re talking about here. But they got something and then now they\u2019re a powerhorse and they\u2019re so valuable to us. If AI doesn\u2019t develop this ability to learn on the fly, I\u2019m a bit skeptical that we\u2019re going to see huge changes to the world without that ability.\n[L14] \n[L15] Dario Amodei\n[L16] \n[L17] I think two things here. There\u2019s the state of the technology right now. Again, we have these two stages. We have the pre-training and RL stage where you throw a bunch of data and tasks into the models and then they generalize. So it\u2019s like learning, but it\u2019s like learning from more data and not learning over one human or one model\u2019s lifetime. So again, this is situated between evolution and human learning. But once you learn all those skills, you have them.\n[L18] \n[L19] Just like with pre-training, just how the models know more, if I look at a pre-trained model, it knows more about the history of samurai in Japan than I do. It knows more about baseball than I do. It knows more about low-pass filters and electronics, all of these things. Its knowledge is way broader than mine. So I think even just that may get us to the point where the models are better at everything.\n# \u30104\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn5file4\ue201\nMclick Target: \"5:4\"\nContent Snippet:\n[L1] I wrote about it in \u201cThe Adolescence of Technology\u201d where I went through this kind of spectrum with farming. I actually totally agree with you on that. These are very different benchmarks from each other, but we\u2019re proceeding through them super fast.\n[L2] \n[L3] Dwarkesh Patel\n[L4] \n[L5] Part of your vision is that going from 90 to 100 is going to happen fast, and that it leads to huge productivity improvements. But what I notice is that even in greenfield projects people start with Claude Code or something, people report starting a lot of projects\u2026 Do we see in the world out there a renaissance of software, all these new features that wouldn\u2019t exist otherwise? At least so far, it doesn\u2019t seem like we see that.\n[L6] \n[L7] So that does make me wonder. Even if I never had to intervene with Claude Code, the world is complicated. Jobs are complicated. Closing the loop on self-contained systems, whether it\u2019s just writing software or something, how much broader gains would we see just from that? Maybe that should dilute our estimation of the \u201ccountry of geniuses\u201d.\n[L8] \n[L9] Dario Amodei\n[L10] \n[L11] I simultaneously agree with you that it\u2019s a reason why these things don\u2019t happen instantly, but at the same time, I think the effect is gonna be very fast. You could have these two poles. One is that AI is not going to make progress. It\u2019s slow. It\u2019s going to take forever to diffuse within the economy. Economic diffusion has become one of these buzzwords that\u2019s a reason why we\u2019re not going to make AI progress, or why AI progress doesn\u2019t matter.\n[L12] \n[L13] The other axis is that we\u2019ll get recursive self-improvement, the whole thing. Can\u2019t you just draw an exponential line on the curve? We\u2019re going to have Dyson spheres around the sun so many nanoseconds after we get recursive. I\u2019m completely caricaturing the view here, but there are these two extremes.\n[L14] \n[L15] But what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\n[L16] \n[L17] Dwarkesh Patel\n[L18] \n[L19] You guys should have just bought a billion dollars of your own products so you could just\u2026\n[L20] \n[L21] Dario Amodei\n[L22] \n[L23] And the first month of this year, that exponential is... You would think it would slow down, but we added another few billion to revenue in January. Obviously that curve can\u2019t go on forever. The GDP is only so large. I would even guess that it bends somewhat this year, but that is a fast curve. That\u2019s a really fast curve. I would bet it stays pretty fast even as the scale goes to the entire economy.\n[L24] \n[L25] So I think we should be thinking about this middle world where things are extremely fast, but not instant, where they take time because of economic diffusion, because of the need to close the loop. Because it\u2019s fiddly: \u201cI have to do change management within my enterprise\u2026 I set this up, but I have to change the security permissions on this in order to make it actually work\u2026 I had this old piece of software that checks the model before it\u2019s compiled and released and I have to rewrite it. Yes, the model can do that, but I have to tell the model to do that. It has to take time to do that.\u201d\nVisible: 0% - 100%","summary":null,"assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"msearch","cloud_doc_urls":[null,null,null,null,null],"retrieval_turn_number":5,"is_contextual_answers_supported":true,"contextual_answers_available_sources":["slurm_github_issues","slurm_github_pulls","slurm_github_commits","slurm_github","files_uploaded_in_conversation"],"retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"connectors_file_search":{"tool_call_message_id":"70f588b9-3e1f-4d83-b877-4e825b7da9f9","source_filters":["files_uploaded_in_conversation"],"call_tool_latency_ms":1277.7171283960342},"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"7ec62746-88e3-4d45-98b0-4a3836355073","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":null},"parent":"7ec62746-88e3-4d45-98b0-4a3836355073","children":["ae23dd1e-4d10-4ead-b56a-ffff819cbcc9"]},"ae23dd1e-4d10-4ead-b56a-ffff819cbcc9":{"id":"ae23dd1e-4d10-4ead-b56a-ffff819cbcc9","message":{"id":"ae23dd1e-4d10-4ead-b56a-ffff819cbcc9","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771108020.528379,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Summarizing the key points of the argument","content":"In the transcript, he doesn't provide a clear argument for \"near the end of the exponential,\" but instead:\n\n- Describes expected exponential progress in model capabilities, especially for coding.\n- Expresses surprise about public awareness.\n- Discusses compute, data, and scaling, with evidence from benchmarks and revenue growth. \n- Makes near-term predictions and cites commercial and performance data to support the claim.","chunks":["In the transcript, he doesn't provide a clear argument for \"near the end of the exponential,\" but instead:","Describes expected exponential progress in model capabilities, especially for coding.","Expresses surprise about public awareness.","Discusses compute, data, and scaling, with evidence from benchmarks and revenue growth.","Makes near-term predictions and cites commercial and performance data to support the claim."],"finished":true},{"summary":"Summarizing reasons for \"near the end of the exponential\" claim","content":"He offers some reasons for claiming we're near the end of the exponential, though not explicitly in the beginning. He talks about the persistence of scaling laws, observed capability breakthroughs, and internal metrics. However, these are mostly qualitative, lacking public data or detailed analysis. He clarifies that the exponential continues toward AGI, but the phase of rapid progress is nearing its end.","chunks":["He offers some reasons for claiming we're near the end of the exponential, though not explicitly in the beginning.","He talks about the persistence of scaling laws, observed capability breakthroughs, and internal metrics.","However, these are mostly qualitative, lacking public data or detailed analysis.","He clarifies that the exponential continues toward AGI, but the phase of rapid progress is nearing its end."],"finished":true},{"summary":"Interpreting the \"end of exponential\" claim","content":"He mentions the \"end of exponential,\" but later clarifies that exponential growth will continue. This suggests he means we\u2019re nearing the end of a phase that brings us to AGI levels\u2014like the final stretch before hitting a plateau or experiencing diminishing returns.","chunks":["He mentions the \"end of exponential,\" but later clarifies that exponential growth will continue. This suggests he means we\u2019re nearing the end of a phase that brings us to AGI levels\u2014like the final stretch before hitting a plateau or experiencing diminishing returns."],"finished":true},{"summary":"Interpreting \"end of exponential\"","content":"At first, he mentions \"near end of exponential,\" but later clarifies that exponential growth will continue, with distinct points along the way. His use of \u201cend\u201d seems rhetorical, indicating we're nearing a critical threshold, but not implying that progress will stop. His argument is supported by confidence in scaling trends and internal metrics.","chunks":["At first, he mentions \"near end of exponential,\" but later clarifies that exponential growth will continue, with distinct points along the way. His use of \u201cend\u201d seems rhetorical, indicating we're nearing a critical threshold, but not implying that progress will stop. His argument is supported by confidence in scaling trends and internal metrics."],"finished":true},{"summary":"Clarifying the \"near end of exponential\" claim","content":"At first, he mentions being \"near the end of exponential growth,\" but later clarifies that exponential growth will continue with distinct milestones. It\u2019s likely that \"end\" refers to reaching a critical point, not progress halting. His arguments for continuing progress focus on scaling, internal metrics, and high confidence.","chunks":["At first, he mentions being \"near the end of exponential growth,\" but later clarifies that exponential growth will continue with distinct milestones. It\u2019s likely that \"end\" refers to reaching a critical point, not progress halting. His arguments for continuing progress focus on scaling, internal metrics, and high confidence."],"finished":true}],"source_analysis_msg_id":"d0369bd7-88b5-4f7b-bbe6-8742feccca9f"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"d46961f6-2543-4e9e-9fca-3e06c8c6d252","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","classifier_response":"default","skip_reasoning_title":"Skip","can_save":true},"recipient":"all","channel":null},"parent":"d46961f6-2543-4e9e-9fca-3e06c8c6d252","children":["ee9cd6c8-bc50-472e-97f4-15453a260c68"]},"ee9cd6c8-bc50-472e-97f4-15453a260c68":{"id":"ee9cd6c8-bc50-472e-97f4-15453a260c68","message":{"id":"ee9cd6c8-bc50-472e-97f4-15453a260c68","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771108031.544127,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 1m 37s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":97,"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"ae23dd1e-4d10-4ead-b56a-ffff819cbcc9","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":null},"parent":"ae23dd1e-4d10-4ead-b56a-ffff819cbcc9","children":["b2269fbe-85c7-4fdd-b2d3-54f51f2cf9b2"]},"b2269fbe-85c7-4fdd-b2d3-54f51f2cf9b2":{"id":"b2269fbe-85c7-4fdd-b2d3-54f51f2cf9b2","message":{"id":"b2269fbe-85c7-4fdd-b2d3-54f51f2cf9b2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771108011.578599,"update_time":null,"content":{"content_type":"text","parts":["Mostly **indirectly**, and not as a single tight \u201chere\u2019s why\u201d argument.\n\n- **Where he first says it, he mostly asserts it.** He notes that the capability \u201cexponential\u201d has marched from \u201csmart high school student\u201d \u2192 \u201csmart college student\u201d \u2192 \u201cbeginning to do PhD/professional stuff\u201d (with coding \u201cbeyond that\u201d), and then immediately says we\u2019re \u201cnear the end of the exponential,\u201d but he doesn\u2019t unpack *why that implies the end is near* in that moment. \ue200filecite\ue202turn4file0\ue202L9-L13\ue201\n\n- **Later, he offers supporting *reasons/indicators*** that collectively amount to: \u201cscaling is still working, and key capabilities are moving fast enough that we\u2019re close to major thresholds.\u201d In particular:\n  - He claims **pre-training continues to deliver gains**, and that **reinforcement learning (RL)** shows similar scaling, citing reports (and their own observation) that performance can be **log-linear** in RL training time on tasks like math contests. \ue200filecite\ue202turn3file0\ue202L1-L5\ue201  \n  - When pressed on \u201cwhy one year vs ten,\u201d he gives **probabilistic timelines** and anchors them on domains he considers more *verifiable*\u2014especially **end-to-end coding in 1\u20132 years** (modulo \u201cirreducible uncertainty\u201d). \ue200filecite\ue202turn4file1\ue202L10-L22\ue201  \n  - He points to at least one **concrete benchmark trend** for \u201ccomputer use,\u201d saying OSWorld rose from ~15% to ~65\u201370% over ~1.25 years (with caveats that benchmarks are imperfect). \ue200filecite\ue202turn5file0\ue202L1-L4\ue201  \n  - He also uses **rapid business adoption/revenue growth** to argue there\u2019s a fast \u201cdiffusion exponential\u201d downstream of capability (though he explicitly says it can\u2019t grow forever). \ue200filecite\ue202turn4file4\ue202L1-L13\ue201\n\n- **He also partially walks back the literal phrasing.** Later he says the *technology exponential continues* even after reaching \u201ccountry of geniuses in a data center,\u201d but with \u201cdistinguished points\u201d and eventual diminishing returns in *value* rather than capability. \ue200filecite\ue202turn4file3\ue202L17-L19\ue201\n\nSo: **yes, he does give reasons**, but they\u2019re **scattered** (scaling still works; rapid progress on coding/computer-use; fast diffusion) and **not presented as a rigorous, data-backed derivation** of \u201cnear the end\u201d inside this transcript."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[{"start_ix":451,"end_ix":479,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\nBut when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.","extra":{"retrieval_turn":4,"retrieval_file_index":0,"cited_message_idx":26,"cited_message_id":"2101925b-a643-4b6f-9218-333ec99e5388","search_result_idx":0,"line_range":[9,13],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":944,"end_ix":971,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"That was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\nWhat has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d","extra":{"retrieval_turn":3,"retrieval_file_index":0,"cited_message_idx":22,"cited_message_id":"90337903-65f2-4ea2-9441-01982886c9f6","search_result_idx":0,"line_range":[1,5],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":1195,"end_ix":1224,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"There are two claims you could make here, one stronger and one weaker. Starting with the weaker claim, when I first saw the scaling back in 2019, I wasn\u2019t sure. This was a 50/50 thing. I thought I saw something. My claim was that this was much more likely than anyone thinks. Maybe there\u2019s a 50% chance this happens.\nOn the basic hypothesis of, as you put it, within ten years we\u2019ll get to what I call a \u201ccountry of geniuses in a data center\u201d, I\u2019m at 90% on that. It\u2019s hard to go much higher than 90% because the world is so unpredictable. Maybe the irreducible uncertainty puts us at 95%, where you get to things like multiple companies having internal turmoil, Taiwan gets invaded, all the fabs get blown up by missiles.","extra":{"retrieval_turn":4,"retrieval_file_index":1,"cited_message_idx":26,"cited_message_id":"2101925b-a643-4b6f-9218-333ec99e5388","search_result_idx":1,"line_range":[10,22],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":1410,"end_ix":1437,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"I think that\u2019s dependent on several things. I think this is one of the things that\u2019s actually blocking deployment: getting to the point on computer use where the models are really masters at using the computer.\nWe\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.","extra":{"retrieval_turn":5,"retrieval_file_index":0,"cited_message_idx":30,"cited_message_id":"fc1b06b7-16fc-473d-bf66-42d1c6981cec","search_result_idx":0,"line_range":[1,4],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":1624,"end_ix":1652,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"But what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\nDwarkesh Patel","extra":{"retrieval_turn":4,"retrieval_file_index":4,"cited_message_idx":26,"cited_message_id":"2101925b-a643-4b6f-9218-333ec99e5388","search_result_idx":4,"line_range":[1,13],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":1924,"end_ix":1953,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"Just to be precise about it, I think the exponential of the underlying technology will continue as it has before. The models get smarter and smarter, even when they get to a \u201ccountry of geniuses in a data center.\u201d I think you can continue to make the model smarter. There\u2019s a question of getting diminishing returns on their value in the world. How much does it matter after you\u2019ve already solved human biology? At some point you can do harder, more abstruse math problems, but nothing after that matters.\nPutting that aside, I do think the exponential will continue, but there will be certain distinguished points on the exponential. Companies, individuals, and countries will reach those points at different times.","extra":{"retrieval_turn":4,"retrieval_file_index":3,"cited_message_idx":26,"cited_message_id":"2101925b-a643-4b6f-9218-333ec99e5388","search_result_idx":3,"line_range":[17,19],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}}],"content_references":[{"matched_text":"\ue200filecite\ue202turn4file0\ue202L9-L13\ue201","start_idx":451,"end_idx":479,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\nBut when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":4,"message_id":"2101925b-a643-4b6f-9218-333ec99e5388","file_index":0,"line_range_start":9,"line_range_end":13},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn3file0\ue202L1-L5\ue201","start_idx":944,"end_idx":971,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"That was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\nWhat has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":3,"message_id":"90337903-65f2-4ea2-9441-01982886c9f6","file_index":0,"line_range_start":1,"line_range_end":5},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn4file1\ue202L10-L22\ue201","start_idx":1195,"end_idx":1224,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"There are two claims you could make here, one stronger and one weaker. Starting with the weaker claim, when I first saw the scaling back in 2019, I wasn\u2019t sure. This was a 50/50 thing. I thought I saw something. My claim was that this was much more likely than anyone thinks. Maybe there\u2019s a 50% chance this happens.\nOn the basic hypothesis of, as you put it, within ten years we\u2019ll get to what I call a \u201ccountry of geniuses in a data center\u201d, I\u2019m at 90% on that. It\u2019s hard to go much higher than 90% because the world is so unpredictable. Maybe the irreducible uncertainty puts us at 95%, where you get to things like multiple companies having internal turmoil, Taiwan gets invaded, all the fabs get blown up by missiles.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":4,"message_id":"2101925b-a643-4b6f-9218-333ec99e5388","file_index":1,"line_range_start":10,"line_range_end":22},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn5file0\ue202L1-L4\ue201","start_idx":1410,"end_idx":1437,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"I think that\u2019s dependent on several things. I think this is one of the things that\u2019s actually blocking deployment: getting to the point on computer use where the models are really masters at using the computer.\nWe\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":5,"message_id":"fc1b06b7-16fc-473d-bf66-42d1c6981cec","file_index":0,"line_range_start":1,"line_range_end":4},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn4file4\ue202L1-L13\ue201","start_idx":1624,"end_idx":1652,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"But what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\nDwarkesh Patel","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":4,"message_id":"2101925b-a643-4b6f-9218-333ec99e5388","file_index":4,"line_range_start":1,"line_range_end":13},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn4file3\ue202L17-L19\ue201","start_idx":1924,"end_idx":1953,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"Just to be precise about it, I think the exponential of the underlying technology will continue as it has before. The models get smarter and smarter, even when they get to a \u201ccountry of geniuses in a data center.\u201d I think you can continue to make the model smarter. There\u2019s a question of getting diminishing returns on their value in the world. How much does it matter after you\u2019ve already solved human biology? At some point you can do harder, more abstruse math problems, but nothing after that matters.\nPutting that aside, I do think the exponential will continue, but there will be certain distinguished points on the exponential. Companies, individuals, and countries will reach those points at different times.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":4,"message_id":"2101925b-a643-4b6f-9218-333ec99e5388","file_index":3,"line_range_start":17,"line_range_end":19},"fff_metadata":null,"connector_id":null}],"resolved_model_slug":"gpt-5-2-thinking","search_result_groups":[],"request_id":"602b33d3-fc37-4414-8142-494ee95558aa","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"ee9cd6c8-bc50-472e-97f4-15453a260c68","turn_exchange_id":"e8579e3a-25e0-4935-b555-c3b380211388","can_save":true},"recipient":"all","channel":"final"},"parent":"ee9cd6c8-bc50-472e-97f4-15453a260c68","children":["4a88a9db-7110-4dbc-9871-d0d27b614e7a"]},"4a88a9db-7110-4dbc-9871-d0d27b614e7a":{"id":"4a88a9db-7110-4dbc-9871-d0d27b614e7a","message":{"id":"4a88a9db-7110-4dbc-9871-d0d27b614e7a","author":{"role":"user","name":null,"metadata":{}},"create_time":1771115421.938,"update_time":null,"content":{"content_type":"text","parts":["what are the most consequential messages Amodei is sending?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":["EjayNg-AI/agentic_browser"],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_source":null,"turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","resolved_model_slug":"gpt-5-2-thinking","can_save":true},"recipient":"all","channel":null},"parent":"b2269fbe-85c7-4fdd-b2d3-54f51f2cf9b2","children":["253fb501-9d72-44ae-ab1c-135710bba326"]},"253fb501-9d72-44ae-ab1c-135710bba326":{"id":"253fb501-9d72-44ae-ab1c-135710bba326","message":{"id":"253fb501-9d72-44ae-ab1c-135710bba326","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771115426.284116,"update_time":null,"content":{"content_type":"thoughts","thoughts":[],"source_analysis_msg_id":"87c063bf-1356-4324-9733-c111ebded7cf"},"status":"in_progress","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"87c063bf-1356-4324-9733-c111ebded7cf","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","classifier_response":"default","skip_reasoning_title":"Skip","can_save":false},"recipient":"all","channel":null},"parent":"4a88a9db-7110-4dbc-9871-d0d27b614e7a","children":["125b2156-07d3-4fd0-8711-61cb2300a19f"]},"125b2156-07d3-4fd0-8711-61cb2300a19f":{"id":"125b2156-07d3-4fd0-8711-61cb2300a19f","message":{"id":"125b2156-07d3-4fd0-8711-61cb2300a19f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771115422.256223,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"queries\":[\"most consequential messages themes in +Dario +Amodei interview with +Dwarkesh +Patel (scaling, timelines, safety, deployment) --QDF=0\",\"country of geniuses data center Amodei Patel interview\",\"near the end of the exponential\",\"OSWorld 65 70 percent\",\"diffusion exponential revenue growth\"],\"source_filter\":[\"files_uploaded_in_conversation\"]}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"cb30ae60-2447-4afa-850a-c8be28913231","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","can_save":true},"recipient":"file_search.msearch","channel":null},"parent":"253fb501-9d72-44ae-ab1c-135710bba326","children":["a251c433-d015-467b-af7f-759a5b58a433"]},"a251c433-d015-467b-af7f-759a5b58a433":{"id":"a251c433-d015-467b-af7f-759a5b58a433","message":{"id":"a251c433-d015-467b-af7f-759a5b58a433","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771115434.5716474,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"spinner","status":"running","retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"125b2156-07d3-4fd0-8711-61cb2300a19f","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","can_save":false},"recipient":"all","channel":null},"parent":"125b2156-07d3-4fd0-8711-61cb2300a19f","children":["9bb9f4d0-62ab-4ee9-b9ac-e288ff05d028"]},"9bb9f4d0-62ab-4ee9-b9ac-e288ff05d028":{"id":"9bb9f4d0-62ab-4ee9-b9ac-e288ff05d028","message":{"id":"9bb9f4d0-62ab-4ee9-b9ac-e288ff05d028","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771115436.0651069,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn6file0\ue201\nMclick Target: \"6:0\"\nContent Snippet:\n[L1] But what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\n[L2] \n[L3] Dwarkesh Patel\n[L4] \n[L5] You guys should have just bought a billion dollars of your own products so you could just\u2026\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] And the first month of this year, that exponential is... You would think it would slow down, but we added another few billion to revenue in January. Obviously that curve can\u2019t go on forever. The GDP is only so large. I would even guess that it bends somewhat this year, but that is a fast curve. That\u2019s a really fast curve. I would bet it stays pretty fast even as the scale goes to the entire economy.\n[L10] \n[L11] So I think we should be thinking about this middle world where things are extremely fast, but not instant, where they take time because of economic diffusion, because of the need to close the loop. Because it\u2019s fiddly: \u201cI have to do change management within my enterprise\u2026 I set this up, but I have to change the security permissions on this in order to make it actually work\u2026 I had this old piece of software that checks the model before it\u2019s compiled and released and I have to rewrite it. Yes, the model can do that, but I have to tell the model to do that. It has to take time to do that.\u201d\n[L12] \n[L13] So I think everything we\u2019ve seen so far is compatible with the idea that there\u2019s one fast exponential that\u2019s the capability of the model. Then there\u2019s another fast exponential that\u2019s downstream of that, which is the diffusion of the model into the economy. Not instant, not slow, much faster than any previous technology, but it has its limits. When I look inside Anthropic, when I look at our customers: fast adoption, but not infinitely fast.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] Can I try a hot take on you?\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] Yeah.\n[L22] \n[L23] Dwarkesh Patel\n[L24] \n[L25] I feel like diffusion is cope that people say. When the model isn\u2019t able to do something, they\u2019re like, \u201coh, but it\u2019s a diffusion issue.\u201d But then you should use the comparison to humans. You would think that the inherent advantages that AIs have would make diffusion a much easier problem for new AIs getting onboarded than new humans getting onboarded. An AI can read your entire Slack and your drive in minutes. They can share all the knowledge that the other copies of the same instance have. You don\u2019t have this adverse selection problem when you\u2019re hiring AI, so you can just hire copies of a vetted AI model.\n[L26] \n[L27] Hiring a human is so much more of a hassle. People hire humans all the time. We pay humans upwards of $50 trillion in wages because they\u2019re useful, even though in principle it would be much easier to integrate AIs into the economy than it is to hire humans. The diffusion doesn\u2019t really explain.\n[L28] \n[L29] Dario Amodei\n[L30] \n[L31] I think diffusion is very real and doesn\u2019t exclusively have to do with limitations on the AI models. Again, there are people who use diffusion as kind of a buzzword to say this isn\u2019t a big deal. I\u2019m not talking about that. I\u2019m not talking about how AI will diffuse at the speed of previous technologies. I think AI will diffuse much faster than previous technologies have, but not infinitely fast.\n# \u30101\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn6file1\ue201\nMclick Target: \"6:1\"\nContent Snippet:\n[L1] I think that\u2019s dependent on several things. I think this is one of the things that\u2019s actually blocking deployment: getting to the point on computer use where the models are really masters at using the computer.\n[L2] \n[L3] We\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.\n[L4] \n[L5] Dwarkesh Patel\n[L6] \n[L7] Can I just follow up on that before you move on to the next point? For years, I\u2019ve been trying to build different internal LLM tools for myself. Often I have these text-in, text-out tasks, which should be dead center in the repertoire of these models. Yet I still hire humans to do them.\n[L8] \n[L9] If it\u2019s something like, \u201cidentify what the best clips would be in this transcript\u201d, maybe the LLMs do a seven-out-of-ten job on them. But there\u2019s not this ongoing way I can engage with them to help them get better at the job the way I could with a human employee. That missing ability, even if you solve computer use, would still block my ability to offload an actual job to them.\n[L10] \n[L11] Dario Amodei\n[L12] \n[L13] This gets back to what we were talking about before with learning on the job. It\u2019s very interesting. I think with the coding agents, I don\u2019t think people would say that learning on the job is what is preventing the coding agents from doing everything end to end. They keep getting better. We have engineers at Anthropic who don\u2019t write any code.\n[L14] \n[L15] When I look at the productivity, to your previous question, we have folks who say, \u201cThis GPU kernel, this chip, I used to write it myself. I just have Claude do it.\u201d There\u2019s this enormous improvement in productivity.\n[L16] \n[L17] When I see Claude Code, familiarity with the codebase or a feeling that the model hasn\u2019t worked at the company for a year, that\u2019s not high up on the list of complaints I see. I think what I\u2019m saying is that we\u2019re kind of taking a different path.\n[L18] \n[L19] Dwarkesh Patel\n[L20] \n[L21] Don\u2019t you think with coding that\u2019s because there is an external scaffold of memory which exists instantiated in the codebase? I don\u2019t know how many other jobs have that. Coding made fast progress precisely because it has this unique advantage that other economic activity doesn\u2019t.\n[L22] \n[L23] Dario Amodei\n[L24] \n[L25] But when you say that, what you\u2019re implying is that by reading the codebase into the context, I have everything that the human needed to learn on the job. So that would be an example of\u2014whether it\u2019s written or not, whether it\u2019s available or not\u2014a case where everything you needed to know you got from the context window. What we think of as learning\u2014\u201dI started this job, it\u2019s going to take me six months to understand the code base\u201d\u2014the model just did it in the context.\n[L26] \n[L27] Dwarkesh Patel\n[L28] \n[L29] I honestly don\u2019t know how to think about this because there are people who qualitatively report what you\u2019re saying. I\u2019m sure you saw last year, there was a major study where they had experienced developers try to close pull requests in repositories that they were familiar with. Those developers reported an uplift. They reported that they felt more productive with the use of these models. But in fact, if you look at their output and how much was actually merged back in, there was a 20% downlift. They were less productive as a result of using these models.\n# \u30102\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn6file2\ue201\nMclick Target: \"6:2\"\nContent Snippet:\n[L1] Dwarkesh Patel\n[L2] \n[L3] Maybe stepping back, I\u2019m not saying I think the \u201ccountry of geniuses\u201d is going to come in two years and therefore you should buy this compute. To me, the end conclusion you\u2019re arriving at makes a lot of sense. But that\u2019s because it seems like \u201ccountry of geniuses\u201d is hard and there\u2019s a long way to go. So stepping back, the thing I\u2019m trying to get at is more that it seems like your worldview is compatible with somebody who says, \u201cWe\u2019re like 10 years away from a world in which we\u2019re generating trillions of dollars of value.\u201d\n[L4] \n[L5] Dario Amodei\n[L6] \n[L7] That\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.\n[L8] \n[L9] Like in 2028, we get the real \u201ccountry of geniuses in the data center\u201d. The revenue\u2019s going into the low hundreds of billions by 2028, and then the country of geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. It takes two years to get to the trillions. That would be the world where it takes until 2030. I suspect even composing the technical exponential and diffusion exponential, we\u2019ll get there before 2030.\n[L10] \n[L11] Dwarkesh Patel\n[L12] \n[L13] So you laid out a model where Anthropic makes profit because it seems like fundamentally we\u2019re in a compute-constrained world. So eventually we keep growing compute\u2014\n[L14] \n[L15] Dario Amodei\n[L16] \n[L17] I think the way the profit comes is\u2026 Again, let\u2019s just abstract the whole industry here. Let\u2019s just imagine we\u2019re in an economics textbook. We have a small number of firms. Each can invest a limited amount. Each can invest some fraction in R&D. They have some marginal cost to serve. The gross profit margins on that marginal cost are very high because inference is efficient. There\u2019s some competition, but the models are also differentiated.\n[L18] \n[L19] Companies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.\n[L20] \n[L21] Dwarkesh Patel\n[L22] \n[L23] Help me understand that, because right now we do have three leading firms and they\u2019re not making profit. So what is changing?\n[L24] \n[L25] Dario Amodei\n[L26] \n[L27] Again, the gross margins right now are very positive. What\u2019s happening is a combination of two things. One is that we\u2019re still in the exponential scale-up phase of compute. A model gets trained. Let\u2019s say a model got trained that costs $1 billion last year. Then this year it produced $4 billion of revenue and cost $1 billion to inference from. Again, I\u2019m using stylized numbers here, but that would be 75% gross margins and this 25% tax. So that model as a whole makes $2 billion.\n[L28] \n[L29] But at the same time, we\u2019re spending $10 billion to train the next model because there\u2019s an exponential scale-up. So the company loses money. Each model makes money, but the company loses money.\n[L30] \n[L31] The equilibrium I\u2019m talking about is an equilibrium where we have the \u201ccountry of geniuses in a data center\u201d, but that model training scale-up has equilibrated more. Maybe it\u2019s still going up. We\u2019re still trying to predict the demand, but it\u2019s more leveled out.\n[L32] \n[L33] Dwarkesh Patel\n# \u30103\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn6file3\ue201\nMclick Target: \"6:3\"\nContent Snippet:\n[L1] We\u2019ve thought carefully about it. We\u2019re an enterprise business. Therefore, we can rely more on revenue. It\u2019s less fickle than consumer. We have better margins, which is the buffer between buying too much and buying too little. I think we bought an amount that allows us to capture pretty strong upside worlds. It won\u2019t capture the full 10x a year. Things would have to go pretty badly for us to be in financial trouble. So we\u2019ve thought carefully and we\u2019ve made that balance. That\u2019s what I mean when I say that we\u2019re being responsible.\n[L2] \n[L3] Dwarkesh Patel\n[L4] \n[L5] So it seems like it\u2019s possible that we actually just have different definitions of the \u201ccountry of a genius in a data center\u201d. Because when I think of actual human geniuses, an actual country of human geniuses in a data center, I would happily buy $5 trillion worth of compute to run an actual country of human geniuses in a data center.\n[L6] \n[L7] Let\u2019s say JPMorgan or Moderna or whatever doesn\u2019t want to use them. I\u2019ve got a country of geniuses. They\u2019ll start their own company. If they can\u2019t start their own company and they\u2019re bottlenecked by clinical trials\u2026 It is worth stating that with clinical trials, most clinical trials fail because the drug doesn\u2019t work. There\u2019s not efficacy.\n[L8] \n[L9] Dario Amodei\n[L10] \n[L11] I make exactly that point in \u201cMachines of Loving Grace\u201d, I say the clinical trials are going to go much faster than we\u2019re used to, but not infinitely fast.\n[L12] \n[L13] Dwarkesh Patel\n[L14] \n[L15] Okay, and then suppose it takes a year for the clinical trials to work out so that you\u2019re getting revenue from that and can make more drugs. Okay, well, you\u2019ve got a country of geniuses and you\u2019re an AI lab. You could use many more AI researchers. You also think there are these self-reinforcing gains from smart people working on AI tech. You can have the data center working on AI progress.\n[L16] \n[L17] Dario Amodei\n[L18] \n[L19] Are there substantially more gains from buying $1 trillion a year of compute versus $300 billion a year of compute?\n[L20] \n[L21] Dwarkesh Patel\n[L22] \n[L23] If your competitor is buying a trillion, yes there is.\n[L24] \n[L25] Dario Amodei\n[L26] \n[L27] Well, no, there\u2019s some gain, but then again, there\u2019s this chance that they go bankrupt before. Again, if you\u2019re off by only a year, you destroy yourselves. That\u2019s the balance. We\u2019re buying a lot. We\u2019re buying a hell of a lot. We\u2019re buying an amount that\u2019s comparable to what the biggest players in the game are buying.\n[L28] \n[L29] But if you\u2019re asking me, \u201cWhy haven\u2019t we signed $10 trillion of compute starting in mid-2027?\u201d... First of all, it can\u2019t be produced. There isn\u2019t that much in the world. But second, what if the country of geniuses comes, but it comes in mid-2028 instead of mid-2027? You go bankrupt.\n[L30] \n[L31] Dwarkesh Patel\n[L32] \n[L33] So if your projection is one to three years, it seems like you should want $10 trillion of compute by 2029 at the latest? Even in the longest version of the timelines you state, the compute you are ramping up to build doesn\u2019t seem in accordance.\n[L34] \n[L35] Dario Amodei\n[L36] \n[L37] What makes you think that?\n[L38] \n[L39] Dwarkesh Patel\n[L40] \n[L41] Human wages, let\u2019s say, are on the order of $50 trillion a year\u2014\n[L42] \n[L43] Dario Amodei\n# \u30104\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn6file4\ue201\nMclick Target: \"6:4\"\nContent Snippet:\n[L1] Dwarkesh Patel\n[L2] \n[L3] This is a point I\u2019m sure you would make, but diminishing returns on a genius could be quite high.\n[L4] \n[L5] More generally, what is profit in a market economy? Profit is basically saying other companies in the market can do more things with this money than I can.\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Put aside Anthropic. I don\u2019t want to give information about Anthropic. That\u2019s why I\u2019m giving these stylized numbers. But let\u2019s just derive the equilibrium of the industry. Why doesn\u2019t everyone spend 100% of their compute on training and not serve any customers? It\u2019s because if they didn\u2019t get any revenue, they couldn\u2019t raise money, they couldn\u2019t do compute deals, they couldn\u2019t buy more compute the next year.\n[L10] \n[L11] So there\u2019s going to be an equilibrium where every company spends less than 100% on training and certainly less than 100% on inference. It should be clear why you don\u2019t just serve the current models and never train another model, because then you don\u2019t have any demand because you\u2019ll fall behind. So there\u2019s some equilibrium. It\u2019s not gonna be 10%, it\u2019s not gonna be 90%. Let\u2019s just say as a stylized fact, it\u2019s 50%. That\u2019s what I\u2019m getting at.\n[L12] \n[L13] I think we\u2019re gonna be in a position where that equilibrium of how much you spend on training is less than the gross margins that you\u2019re able to get on compute. So the underlying economics are profitable. The problem is you have this hellish demand prediction problem when you\u2019re buying the next year of compute and you might guess under and be very profitable but have no compute for research. Or you might guess over and you are not profitable and you have all the compute for research in the world. Does that make sense? Just as a dynamic model of the industry?\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] Maybe stepping back, I\u2019m not saying I think the \u201ccountry of geniuses\u201d is going to come in two years and therefore you should buy this compute. To me, the end conclusion you\u2019re arriving at makes a lot of sense. But that\u2019s because it seems like \u201ccountry of geniuses\u201d is hard and there\u2019s a long way to go. So stepping back, the thing I\u2019m trying to get at is more that it seems like your worldview is compatible with somebody who says, \u201cWe\u2019re like 10 years away from a world in which we\u2019re generating trillions of dollars of value.\u201d\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] That\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.\n[L22] \n[L23] Like in 2028, we get the real \u201ccountry of geniuses in the data center\u201d. The revenue\u2019s going into the low hundreds of billions by 2028, and then the country of geniuses accelerates it to trillions. We\u2019re basically on the slow end of diffusion. It takes two years to get to the trillions. That would be the world where it takes until 2030. I suspect even composing the technical exponential and diffusion exponential, we\u2019ll get there before 2030.\n[L24] \n[L25] Dwarkesh Patel\n[L26] \n[L27] So you laid out a model where Anthropic makes profit because it seems like fundamentally we\u2019re in a compute-constrained world. So eventually we keep growing compute\u2014\n[L28] \n[L29] Dario Amodei\nVisible: 0% - 100%","summary":null,"assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"msearch","cloud_doc_urls":[null,null,null,null,null],"retrieval_turn_number":6,"is_contextual_answers_supported":true,"contextual_answers_available_sources":["slurm_github_issues","slurm_github_pulls","slurm_github_commits","slurm_github","files_uploaded_in_conversation"],"retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"connectors_file_search":{"tool_call_message_id":"cb30ae60-2447-4afa-850a-c8be28913231","source_filters":["files_uploaded_in_conversation"],"call_tool_latency_ms":1649.0148566663265},"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"2e24a289-5523-43a2-9a43-e98254f84b90","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","can_save":true},"recipient":"all","channel":null},"parent":"a251c433-d015-467b-af7f-759a5b58a433","children":["4e6d44eb-ed1f-4039-9894-fc6455b7d240"]},"4e6d44eb-ed1f-4039-9894-fc6455b7d240":{"id":"4e6d44eb-ed1f-4039-9894-fc6455b7d240","message":{"id":"4e6d44eb-ed1f-4039-9894-fc6455b7d240","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771115439.389839,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Searching for scaling in reinforcement learning","content":"The search results show relevant lines, but I need more details\u2014especially about scaling in reinforcement learning, including the end of exponential growth and the transition to log-linear stages. I\u2019ll try another search focusing on terms like \u201cRL scaling,\u201d \u201cpretraining,\u201d and \u201clog-linear\u201d for more insights.","chunks":["The search results show relevant lines, but I need more details\u2014especially about scaling in reinforcement learning, including the end of exponential growth and the transition to log-linear stages.","I\u2019ll try another search focusing on terms like \u201cRL scaling,\u201d \u201cpretraining,\u201d and \u201clog-linear\u201d for more insights."],"finished":true}],"source_analysis_msg_id":"2e24a289-5523-43a2-9a43-e98254f84b90"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"6c9af34f-0ed8-4ad7-97a5-8eaf62de12e7","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","classifier_response":"default","skip_reasoning_title":"Skip","can_save":true},"recipient":"all","channel":null},"parent":"9bb9f4d0-62ab-4ee9-b9ac-e288ff05d028","children":["abcbb4d2-ffb0-4e98-a955-454e6c282bfb"]},"abcbb4d2-ffb0-4e98-a955-454e6c282bfb":{"id":"abcbb4d2-ffb0-4e98-a955-454e6c282bfb","message":{"id":"abcbb4d2-ffb0-4e98-a955-454e6c282bfb","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771115436.064278,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"queries\":[\"reinforcement learning log-linear training time scaling math contests Amodei\",\"pre-training continues to get better scaling works\",\"engineers at Anthropic who don\u2019t write any code Claude Code\",\"near end of the exponential smart high school student smart college student PhD\",\"export controls China national security\"],\"source_filter\":[\"files_uploaded_in_conversation\"]}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"4e6d44eb-ed1f-4039-9894-fc6455b7d240","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","can_save":true},"recipient":"file_search.msearch","channel":null},"parent":"4e6d44eb-ed1f-4039-9894-fc6455b7d240","children":["9b6c14e6-78cc-400f-a3be-327c5b05b2e8"]},"9b6c14e6-78cc-400f-a3be-327c5b05b2e8":{"id":"9b6c14e6-78cc-400f-a3be-327c5b05b2e8","message":{"id":"9b6c14e6-78cc-400f-a3be-327c5b05b2e8","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771115439.7185078,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"spinner","status":"running","retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"94a04bf8-1fbb-4667-be56-c38d58befde1","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","can_save":false},"recipient":"all","channel":null},"parent":"abcbb4d2-ffb0-4e98-a955-454e6c282bfb","children":["bf54efd6-73d5-45ec-ba17-4b35501bafb4"]},"bf54efd6-73d5-45ec-ba17-4b35501bafb4":{"id":"bf54efd6-73d5-45ec-ba17-4b35501bafb4","message":{"id":"bf54efd6-73d5-45ec-ba17-4b35501bafb4","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1771115440.8399348,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn7file0\ue201\nMclick Target: \"7:0\"\nContent Snippet:\n[L1] Dario Amodei\n[L2] \n[L3] I actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.\n[L4] \n[L5] That was one among many things. Back in those days there was robotics. People tried to work on reasoning as a separate thing from language models, and there was scaling of the kind of RL that happened in AlphaGo and in Dota at OpenAI. People remember StarCraft at DeepMind, AlphaStar.\n[L6] \n[L7] It was written as a more general document. Rich Sutton put out \u201cThe Bitter Lesson\u201d a couple years later. The hypothesis is basically the same. What it says is that all the cleverness, all the techniques, all the \u201cwe need a new method to do something\u201d, that doesn\u2019t matter very much. There are only a few things that matter. I think I listed seven of them.\n[L8] \n[L9] One is how much raw compute you have. The second is the quantity of data. The third is the quality and distribution of data. It needs to be a broad distribution. The fourth is how long you train for. The fifth is that you need an objective function that can scale to the moon. The pre-training objective function is one such objective function. Another is the RL objective function that says you have a goal, you\u2019re going to go out and reach the goal.\n[L10] \n[L11] Within that, there\u2019s objective rewards like you see in math and coding, and there\u2019s more subjective rewards like you see in RLHF or higher-order versions of that. Then the sixth and seventh were things around normalization or conditioning, just getting the numerical stability so that the big blob of compute flows in this laminar way instead of running into problems.\n[L12] \n[L13] That was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\n[L14] \n[L15] What has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d\n[L16] \n[L17] We see that as well, and it\u2019s not just math contests. It\u2019s a wide variety of RL tasks. We\u2019re seeing the same scaling in RL that we saw for pre-training.\n[L18] \n[L19] Dwarkesh Patel\n[L20] \n[L21] You mentioned Rich Sutton and \u201cThe Bitter Lesson\u201d. I interviewed him last year, and he\u2019s actually very non-LLM-pilled. I don\u2019t know if this is his perspective, but one way to paraphrase his objection is: Something which possesses the true core of human learning would not require all these billions of dollars of data and compute and these bespoke environments, to learn how to use Excel, how to use PowerPoint, how to navigate a web browser. The fact that we have to build in these skills using these RL environments hints that we are actually lacking a core human learning algorithm. So we\u2019re scaling the wrong thing.\n[L22] \n[L23] That does raise the question. Why are we doing all this RL scaling if we think there\u2019s something that\u2019s going to be human-like in its ability to learn on the fly?\n[L24] \n[L25] Dario Amodei\n# \u30101\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn7file1\ue201\nMclick Target: \"7:1\"\nContent Snippet:\n[L1] Transcript\n[L2] 00:00:00 - What exactly are we scaling?\n[L3] Dwarkesh Patel\n[L4] \n[L5] We talked three years ago. In your view, what has been the biggest update over the last three years? What has been the biggest difference between what it felt like then versus now?\n[L6] \n[L7] Dario Amodei\n[L8] \n[L9] Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\n[L10] \n[L11] But when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.\n[L12] \n[L13] What has been the most surprising thing is the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that you have people \u2014 within the bubble and outside the bubble \u2014 talking about the same tired, old hot-button political issues, when we are near the end of the exponential.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] I want to understand what that exponential looks like right now. The first question I asked you when we recorded three years ago was, \u201cwhat\u2019s up with scaling and why does it work?\u201d I have a similar question now, but it feels more complicated. At least from the public\u2019s point of view, three years ago there were well-known public trends across many orders of magnitude of compute where you could see how the loss improves.\n[L18] \n[L19] Now we have RL scaling and there\u2019s no publicly known scaling law for it. It\u2019s not even clear what the story is. Is this supposed to be teaching the model skills? Is it supposed to be teaching meta-learning? What is the scaling hypothesis at this point?\n[L20] \n[L21] Dario Amodei\n[L22] \n[L23] I actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.\n[L24] \n[L25] That was one among many things. Back in those days there was robotics. People tried to work on reasoning as a separate thing from language models, and there was scaling of the kind of RL that happened in AlphaGo and in Dota at OpenAI. People remember StarCraft at DeepMind, AlphaStar.\n[L26] \n[L27] It was written as a more general document. Rich Sutton put out \u201cThe Bitter Lesson\u201d a couple years later. The hypothesis is basically the same. What it says is that all the cleverness, all the techniques, all the \u201cwe need a new method to do something\u201d, that doesn\u2019t matter very much. There are only a few things that matter. I think I listed seven of them.\n[L28] \n[L29] One is how much raw compute you have. The second is the quantity of data. The third is the quality and distribution of data. It needs to be a broad distribution. The fourth is how long you train for. The fifth is that you need an objective function that can scale to the moon. The pre-training objective function is one such objective function. Another is the RL objective function that says you have a goal, you\u2019re going to go out and reach the goal.\n[L30] \n[L31] Within that, there\u2019s objective rewards like you see in math and coding, and there\u2019s more subjective rewards like you see in RLHF or higher-order versions of that. Then the sixth and seventh were things around normalization or conditioning, just getting the numerical stability so that the big blob of compute flows in this laminar way instead of running into problems.\n# \u30102\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn7file2\ue201\nMclick Target: \"7:2\"\nContent Snippet:\n[L1] Dario Amodei\n[L2] \n[L3] I don\u2019t actually agree that much regarding the developed world. I feel like in the developed world, markets function pretty well. When there\u2019s a lot of money to be made on something and it\u2019s clearly the best available alternative, it\u2019s actually hard for the regulatory system to stop it.\n[L4] \n[L5] We\u2019re seeing that in AI itself. A thing I\u2019ve been trying to fight for is export controls on chips to China. That\u2019s in the national security interest of the US. That\u2019s squarely within the policy beliefs of almost everyone in Congress of both parties. The case is very clear. The counterarguments against it, I\u2019ll politely call them fishy. Yet it doesn\u2019t happen and we sell the chips because there\u2019s so much money riding on it. That money wants to be made. In that case, in my opinion, that\u2019s a bad thing. But it also applies when it\u2019s a good thing.\n[L6] \n[L7] So if we\u2019re talking about drugs and benefits of the technology, I am not as worried about those benefits being hampered in the developed world. I am a little worried about them going too slow. As I said, I do think we should work to speed the approval process in the FDA. I do think we should fight against these chatbot bills that you\u2019re describing. Described individually, I\u2019m against them. I think they\u2019re stupid.\n[L8] \n[L9] But I actually think the bigger worry is the developing world, where we don\u2019t have functioning markets and where we often can\u2019t build on the technology that we\u2019ve had. I worry more that those folks will get left behind. And I worry that even if the cures are developed, maybe there\u2019s someone in rural Mississippi who doesn\u2019t get it as well. That\u2019s a smaller version of the concern we have in the developing world.\n[L10] \n[L11] So the things we\u2019ve been doing are working with philanthropists. We work with folks who deliver medicine and health interventions to the developing world, to sub-Saharan Africa, India, Latin America, and other developing parts of the world. That\u2019s the thing I think that won\u2019t happen on its own.\n[L12] \n[L13] 01:47:41 - Why can\u2019t China and America both have a country of geniuses in a datacenter?\n[L14] Dwarkesh Patel\n[L15] \n[L16] You mentioned export controls. Why shouldn\u2019t the US and China both have a \u201ccountry of geniuses in a data center\u201d?\n[L17] \n[L18] Dario Amodei\n[L19] \n[L20] Why won\u2019t it happen or why shouldn\u2019t it happen?\n[L21] \n[L22] Dwarkesh Patel\n[L23] \n[L24] Why shouldn\u2019t it happen.\n[L25] \n[L26] Dario Amodei\n[L27] \n[L28] If this does happen, we could have a few situations. If we have an offense-dominant situation, we could have a situation like nuclear weapons, but more dangerous. Either side could easily destroy everything.\n[L29] \n[L30] We could also have a world where it\u2019s unstable. The nuclear equilibrium is stable because it\u2019s deterrence. But let\u2019s say there was uncertainty about, if the two AIs fought, which AI would win? That could create instability. You often have conflict when the two sides have a different assessment of their likelihood of winning. If one side is like, \u201cOh yeah, there\u2019s a 90% chance I\u2019ll win,\u201d and the other side thinks the same, then a fight is much more likely. They can\u2019t both be right, but they can both think that.\n[L31] \n[L32] Dwarkesh Patel\n[L33] \n[L34] But this seems like a fully general argument against the diffusion of AI technology. That\u2019s the implication of this world.\n[L35] \n[L36] Dario Amodei\n# \u30103\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn7file3\ue201\nMclick Target: \"7:3\"\nContent Snippet:\n[L1] That was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\n[L2] \n[L3] What has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d\n[L4] \n[L5] We see that as well, and it\u2019s not just math contests. It\u2019s a wide variety of RL tasks. We\u2019re seeing the same scaling in RL that we saw for pre-training.\n[L6] \n[L7] Dwarkesh Patel\n[L8] \n[L9] You mentioned Rich Sutton and \u201cThe Bitter Lesson\u201d. I interviewed him last year, and he\u2019s actually very non-LLM-pilled. I don\u2019t know if this is his perspective, but one way to paraphrase his objection is: Something which possesses the true core of human learning would not require all these billions of dollars of data and compute and these bespoke environments, to learn how to use Excel, how to use PowerPoint, how to navigate a web browser. The fact that we have to build in these skills using these RL environments hints that we are actually lacking a core human learning algorithm. So we\u2019re scaling the wrong thing.\n[L10] \n[L11] That does raise the question. Why are we doing all this RL scaling if we think there\u2019s something that\u2019s going to be human-like in its ability to learn on the fly?\n[L12] \n[L13] Dario Amodei\n[L14] \n[L15] I think this puts together several things that should be thought of differently. There is a genuine puzzle here, but it may not matter. In fact, I would guess it probably doesn\u2019t matter. There is an interesting thing. Let me take the RL out of it for a second, because I actually think it\u2019s a red herring to say that RL is any different from pre-training in this matter.\n[L16] \n[L17] If we look at pre-training scaling, it was very interesting back in 2017 when Alec Radford was doing GPT-1. The models before GPT-1 were trained on datasets that didn\u2019t represent a wide distribution of text. You had very standard language modeling benchmarks. GPT-1 itself was trained on a bunch of fanfiction, I think actually.\n[L18] \n[L19] It was literary text, which is a very small fraction of the text you can get. In those days it was like a billion words or something, so small datasets representing a pretty narrow distribution of what you can see in the world. It didn\u2019t generalize well. If you did better on some fanfiction corpus, it wouldn\u2019t generalize that well to other tasks.\n[L20] \n[L21] We had all these measures. We had all these measures of how well it did at predicting all these other kinds of texts. It was only when you trained over all the tasks on the internet \u2014 when you did a general internet scrape from something like Common Crawl or scraping links in Reddit, which is what we did for GPT-2 \u2014 that you started to get generalization.\n[L22] \n[L23] I think we\u2019re seeing the same thing on RL. We\u2019re starting first with simple RL tasks like training on math competitions, then moving to broader training that involves things like code. Now we\u2019re moving to many other tasks. I think then we\u2019re going to increasingly get generalization. So that kind of takes out the RL vs. pre-training side of it.\n# \u30104\u2020amodei-patel-interview.txt\u2020file_00000000b50c71fab1f803bf26e09f78\u3011\nContent source: Source.file\nTitle: amodei-patel-interview.txt\nCitation Marker: \ue200filecite\ue202turn7file4\ue201\nMclick Target: \"7:4\"\nContent Snippet:\n[L1] I don\u2019t know if it takes that much intelligence to remain an authoritarian country that continues to coalesce its own power. You can imagine a North Korea with an AI that\u2019s much worse than everybody else\u2019s, but still enough to keep power.\n[L2] \n[L3] In general, it seems like we should just have this attitude that the benefits of AI\u2014in the form of all these empowerments of humanity and health\u2014will be big. Historically, we have decided it\u2019s good to spread the benefits of technology widely, even to people whose governments are authoritarian. It is a tough question, how to think about it with AI, but historically we have said, \u201cyes, this is a positive-sum world, and it\u2019s still worth diffusing the technology.\u201d\n[L4] \n[L5] Dario Amodei\n[L6] \n[L7] There are a number of choices we have. Framing this as a government-to-government decision in national security terms is one lens, but there are a lot of other lenses. You could imagine a world where we produce all these cures to diseases. The cures are fine to sell to authoritarian countries, but the data centers just aren\u2019t. The chips and the data centers aren\u2019t, and the AI industry itself isn\u2019t.\n[L8] \n[L9] Another possibility I think folks should think about is this. Could there be developments we can make\u2014either that naturally happen as a result of AI, or that we could make happen by building technology on AI\u2014that create an equilibrium where it becomes infeasible for authoritarian countries to deny their people private use of the benefits of the technology? Are there equilibria where we can give everyone in an authoritarian country their own AI model that defends them from surveillance and there isn\u2019t a way for the authoritarian country to crack down on this while retaining power?\n[L10] \n[L11] I don\u2019t know. That sounds to me like if that went far enough, it would be a reason why authoritarian countries would disintegrate from the inside. But maybe there\u2019s a middle world where there\u2019s an equilibrium where, if they want to hold on to power, the authoritarians can\u2019t deny individualized access to the technology.\n[L12] \n[L13] But I actually do have a hope for the more radical version. Is it possible that the technology might inherently have properties\u2014or that by building on it in certain ways we could create properties\u2014that have this dissolving effect on authoritarian structures? Now, we hoped originally\u2014think back to the beginning of the Obama administration\u2014that social media and the internet would have that property, and it turns out not to. But what if we could try again with the knowledge of how many things could go wrong, and that this is a different technology? I don\u2019t know if it would work, but it\u2019s worth a try.\n[L14] \n[L15] Dwarkesh Patel\n[L16] \n[L17] It\u2019s just very unpredictable. There are first principles reasons why authoritarianism might be privileged.\n[L18] \n[L19] Dario Amodei\n[L20] \n[L21] It\u2019s all very unpredictable. We just have to recognize the problem and come up with 10 things we can try, try those, and then assess which ones are working, if any. Then try new ones if the old ones aren\u2019t working.\n[L22] \n[L23] Dwarkesh Patel\n[L24] \n[L25] But I guess that nets out to today, as you say, that we will not sell data centers, or chips, and the ability to make chips to China. So in some sense, you are denying\u2026 There would be some benefits to the Chinese economy, Chinese people, et cetera, because we\u2019re doing that. Then there\u2019d also be benefits to the American economy because it\u2019s a positive-sum world. We could trade. They could have their country\u2019s data centers doing one thing. We could have ours doing another. Already, you\u2019re saying it\u2019s not worth that positive-sum stipend to empower those countries?\n[L26] \n[L27] Dario Amodei\nVisible: 0% - 100%","summary":null,"assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"msearch","cloud_doc_urls":[null,null,null,null,null],"retrieval_turn_number":7,"is_contextual_answers_supported":true,"contextual_answers_available_sources":["slurm_github_issues","slurm_github_pulls","slurm_github_commits","slurm_github","files_uploaded_in_conversation"],"retrieval_search_sources":[{"product_type":"threads","source_name":"files_uploaded_in_conversation"}],"connectors_file_search":{"tool_call_message_id":"6c9af34f-0ed8-4ad7-97a5-8eaf62de12e7","source_filters":["files_uploaded_in_conversation"],"call_tool_latency_ms":1273.1374967843294},"reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"classifier_response":"default","request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"9b6c14e6-78cc-400f-a3be-327c5b05b2e8","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","can_save":true},"recipient":"all","channel":null},"parent":"9b6c14e6-78cc-400f-a3be-327c5b05b2e8","children":["c24d6639-2b37-44ad-bc3f-2b573c5783e7"]},"c24d6639-2b37-44ad-bc3f-2b573c5783e7":{"id":"c24d6639-2b37-44ad-bc3f-2b573c5783e7","message":{"id":"c24d6639-2b37-44ad-bc3f-2b573c5783e7","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771115447.08344,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Examining main messages from Amodei","content":"Amodei seems to be signaling that we're approaching major capability thresholds in AI, with scaling and compute as key drivers. He highlights concerns like economic diffusion, AI impacting markets and regulation, and global competition (e.g., with China). Additionally, he discusses reliability, fairness, and potential geopolitical implications.","chunks":["Amodei seems to be signaling that we're approaching major capability thresholds in AI, with scaling and compute as key drivers.","He highlights concerns like economic diffusion, AI impacting markets and regulation, and global competition (e.g., with China).","Additionally, he discusses reliability, fairness, and potential geopolitical implications."],"finished":true},{"summary":"Summarizing and organizing key messages","content":"We\u2019ll answer concisely and structure the points logically, citing relevant lines for each message. The top 7-10 points will include brief explanations for each and citations like \"engineers at Anthropic\" (lines 13-15). We\u2019ll also cite messages on the timeline, revenue growth, compute economics, and points on exponential growth and OSWorld improvement.","chunks":["We\u2019ll answer concisely and structure the points logically, citing relevant lines for each message.","The top 7-10 points will include brief explanations for each and citations like \"engineers at Anthropic\" (lines 13-15). We\u2019ll also cite messages on the timeline, revenue growth, compute economics, and points on exponential growth and OSWorld improvement."],"finished":true},{"summary":"Identifying key messages from Amodei","content":"We need to highlight important themes Amodei is conveying, like his views on compute scaling, policy regarding export controls and China, and the potential for authoritarian disintegration. I\u2019ll also mention his thoughts on diffusion, computer use reliability, and models learning in context. These points indicate major strategic shifts.","chunks":["We need to highlight important themes Amodei is conveying, like his views on compute scaling, policy regarding export controls and China, and the potential for authoritarian disintegration.","I\u2019ll also mention his thoughts on diffusion, computer use reliability, and models learning in context.","These points indicate major strategic shifts."],"finished":true},{"summary":"Summarizing key messages from Amodei","content":"We need to structure the response with a bullet point list titled \"Key Messages\" to cover topics like computer use reliability, exponential revenue growth, national security risks (e.g., export controls on China), potential social and political impacts, and economic considerations. Each point will include a critical analysis, highlighting uncertainties and rhetorical nuances.","chunks":["We need to structure the response with a bullet point list titled \"Key Messages\" to cover topics like computer use reliability, exponential revenue growth, national security risks (e.g., export controls on China), potential social and political impacts, and economic considerations. Each point will include a critical analysis, highlighting uncertainties and rhetorical nuances."],"finished":true},{"summary":"Crafting concise message points","content":"I\u2019ll summarize the key messages in 8 bullet points, each with a couple of sentences. I\u2019m mindful of the desired length, keeping it clear but not too long. No new info is needed, but I\u2019ll define terms like RLHF (Reinforcement Learning from Human Feedback) and OSWorld as necessary. Citations should be properly placed outside bold or code.","chunks":["I\u2019ll summarize the key messages in 8 bullet points, each with a couple of sentences.","I\u2019m mindful of the desired length, keeping it clear but not too long.","No new info is needed, but I\u2019ll define terms like RLHF (Reinforcement Learning from Human Feedback) and OSWorld as necessary.","Citations should be properly placed outside bold or code."],"finished":true},{"summary":"Summarizing key messages with citations","content":"I\u2019m organizing the points into a clear, concise structure for easy understanding. Each message includes its implication and sources. I'll also critically analyze any assumptions or uncertainties in the claims, especially around revenue projections, reliability, and the geopolitical scenario. Citations will be properly placed.","chunks":["I\u2019m organizing the points into a clear, concise structure for easy understanding.","Each message includes its implication and sources.","I'll also critically analyze any assumptions or uncertainties in the claims, especially around revenue projections, reliability, and the geopolitical scenario.","Citations will be properly placed."],"finished":true}],"source_analysis_msg_id":"94a04bf8-1fbb-4667-be56-c38d58befde1"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"resolved_model_slug":"gpt-5-2-thinking","hide_inline_actions":true,"disable_turn_actions":true,"request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"bf54efd6-73d5-45ec-ba17-4b35501bafb4","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","classifier_response":"default","skip_reasoning_title":"Skip","can_save":true},"recipient":"all","channel":null},"parent":"bf54efd6-73d5-45ec-ba17-4b35501bafb4","children":["588795c2-3d59-46dc-8a76-8ccef9e690ad"]},"588795c2-3d59-46dc-8a76-8ccef9e690ad":{"id":"588795c2-3d59-46dc-8a76-8ccef9e690ad","message":{"id":"588795c2-3d59-46dc-8a76-8ccef9e690ad","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771115483.990368,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 57s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":57,"request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"c24d6639-2b37-44ad-bc3f-2b573c5783e7","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","can_save":true},"recipient":"all","channel":null},"parent":"c24d6639-2b37-44ad-bc3f-2b573c5783e7","children":["4b0675a6-fc6b-476a-8a79-e34bc26f2c26"]},"4b0675a6-fc6b-476a-8a79-e34bc26f2c26":{"id":"4b0675a6-fc6b-476a-8a79-e34bc26f2c26","message":{"id":"4b0675a6-fc6b-476a-8a79-e34bc26f2c26","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771115440.838676,"update_time":null,"content":{"content_type":"text","parts":["Here are the **most consequential \u201csignals\u201d** he\u2019s sending (i.e., the takeaways he\u2019s trying to imprint on listeners), with the passages that carry them:\n\n1) **\u201cStop debating small stuff \u2014 we\u2019re close to the endgame.\u201d**  \nHe frames the current moment as \u201cnear the end of the exponential,\u201d and is shocked the public is still focused on \u201ctired, old hot-button political issues.\u201d \ue200filecite\ue202turn7file1\ue202L9-L14\ue201  \n**Consequence:** this is an *urgency-setting move* meant to pull attention (and policy/compute capital) forward.\n\n2) **The core research thesis is still: scale wins (compute/data/objectives), not clever tricks.**  \nHe re-asserts his \u201cBig Blob of Compute\u201d style view: a small set of scaling inputs matters most (raw compute, data quantity/quality, training time, scalable objectives, stability). \ue200filecite\ue202turn7file0\ue202L1-L12\ue201  \n**Consequence:** this legitimizes \u201ckeep scaling\u201d and implicitly favors actors who can secure compute + data pipelines.\n\n3) **Reinforcement learning is now scaling like pre-training \u2014 so there\u2019s no obvious wall yet.**  \nHe claims pre-training \u201ccontinues to give\u2026 gains,\u201d and that reinforcement learning (RL) is showing similar scaling, including \u201clog-linear\u201d improvement with training time on math-contest training (AIME etc.). \ue200filecite\ue202turn7file0\ue202L13-L17\ue201  \n**Consequence:** he\u2019s telling you the frontier is not stuck; it\u2019s *still a capital + execution race*.\n\n4) **Agentic deployment hinges on \u201ccomputer use\u201d reliability, and it\u2019s improving fast.**  \nHe explicitly calls computer-use mastery a blocker for deployment and cites OSWorld rising from ~15% to ~65\u201370% over ~1.25 years (with the caveat he doesn\u2019t remember exactly). \ue200filecite\ue202turn6file1\ue202L1-L4\ue201  \n**Consequence:** \u201cagents in the real world\u201d isn\u2019t sci-fi; it\u2019s a reliability threshold problem that could flip quickly.\n\n5) **Coding is already transforming workflows; \u201clearning on the job\u201d isn\u2019t the main bottleneck (in that domain).**  \nHe says they have engineers who \u201cdon\u2019t write any code,\u201d and argues that in coding, missing \u201cworked here a year\u201d isn\u2019t a top complaint; the model can effectively ingest the codebase \u201cin the context.\u201d \ue200filecite\ue202turn6file1\ue202L13-L17\ue202turn6file1\ue202L25-L26\ue201  \n**Consequence:** he\u2019s signaling near-term labor substitution/augmentation in at least one high-value domain.\n\n6) **Two exponentials: capability growth + economic diffusion \u2014 and both are fast.**  \nHe describes a \u201ccapability exponential\u201d plus a downstream \u201cdiffusion exponential\u201d into the economy (faster than prior tech, but not instantaneous), tied to fiddly enterprise change-management realities. \ue200filecite\ue202turn6file0\ue202L11-L13\ue202turn6file0\ue202L29-L31\ue201  \nHe also points to Anthropic\u2019s revenue trajectory as evidence of speed (with his own \u201ccan\u2019t go forever\u201d caveat). \ue200filecite\ue202turn6file0\ue202L1-L13\ue201  \n**Consequence:** this is a direct argument that \u201ceven if you doubt the tech curve, the business curve is already screaming.\u201d\n\n7) **A very aggressive macro forecast: \u201ctrillions\u201d before 2030; \u201ccountry of geniuses\u201d plausibly by ~2028.**  \nHe says it\u2019s hard to see there *won\u2019t* be trillions in revenue before 2030, and sketches a world where \u201ccountry of geniuses in the data center\u201d arrives around 2028 and accelerates revenue into the trillions. \ue200filecite\ue202turn6file2\ue202L5-L10\ue201  \n**Consequence:** this is an explicit timeline bet meant to justify enormous compute commitments now.\n\n8) **Compute economics & strategy: high gross margins, but losses now because training spend is still exponentially ramping.**  \nHe argues today\u2019s losses can coexist with strong per-model gross margins because companies keep plowing far more into the next training run during the scale-up phase; profitability comes when training scale-up \u201cequilibrates\u201d more. \ue200filecite\ue202turn6file2\ue202L17-L31\ue201  \nHe emphasizes the \u201chellish demand prediction problem\u201d in buying next year\u2019s compute. \ue200filecite\ue202turn6file4\ue202L13-L14\ue201  \n**Consequence:** this is basically \u201cyes the burn is rational; the winners are those who don\u2019t bankrupt themselves while scaling.\u201d\n\n9) **Geopolitics: he\u2019s advocating hard-line chip export controls, and he thinks money is the main political obstacle.**  \nHe says he\u2019s been fighting for export controls on chips to China; claims the case is clear and that counterarguments are \u201cfishy,\u201d yet it doesn\u2019t happen because of money. \ue200filecite\ue202turn7file2\ue202L3-L6\ue201  \n**Consequence:** he\u2019s pushing a national-security framing that implies *restricted diffusion* of frontier capability.\n\n10) **\u201cUS + China both having \u2018countries of geniuses\u2019 could be catastrophically unstable.\u201d**  \nHe argues an offense-dominant world could be \u201clike nuclear weapons, but more dangerous,\u201d and worries about instability if each side misjudges which AI would win. \ue200filecite\ue202turn7file2\ue202L28-L31\ue201  \n**Consequence:** this is a direct rationale for containment/advantage-seeking policy rather than \u201copen diffusion.\u201d\n\n---\n\n### The uncomfortable critique (worth keeping in mind)\nA lot of this is **agenda-setting more than proof**: he frequently uses **assertive timelines**, selective benchmarks (\u201cI don\u2019t remember exactly\u201d), and **internal anecdotes** to justify extremely strong conclusions. \ue200filecite\ue202turn6file1\ue202L3-L3\ue201  \nThat doesn\u2019t make him wrong \u2014 but it does mean the *most consequential message* is also the most contestable: **\u201cthe decisive phase is imminent, so act now.\u201d** \ue200filecite\ue202turn7file1\ue202L13-L14\ue201"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[{"start_ix":376,"end_ix":404,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\nBut when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.","extra":{"retrieval_turn":7,"retrieval_file_index":1,"cited_message_idx":41,"cited_message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","search_result_idx":1,"line_range":[9,14],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":802,"end_ix":830,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"Dario Amodei\nI actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.","extra":{"retrieval_turn":7,"retrieval_file_index":0,"cited_message_idx":41,"cited_message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","search_result_idx":0,"line_range":[1,12],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":1260,"end_ix":1289,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"That was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\nWhat has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d","extra":{"retrieval_turn":7,"retrieval_file_index":0,"cited_message_idx":41,"cited_message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","search_result_idx":0,"line_range":[13,17],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":1662,"end_ix":1689,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"I think that\u2019s dependent on several things. I think this is one of the things that\u2019s actually blocking deployment: getting to the point on computer use where the models are really masters at using the computer.\nWe\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.","extra":{"retrieval_turn":6,"retrieval_file_index":1,"cited_message_idx":37,"cited_message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","search_result_idx":1,"line_range":[1,4],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":2743,"end_ix":2771,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"But what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\nDwarkesh Patel","extra":{"retrieval_turn":6,"retrieval_file_index":0,"cited_message_idx":37,"cited_message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","search_result_idx":0,"line_range":[1,13],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":3218,"end_ix":3246,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"Dario Amodei\nThat\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.","extra":{"retrieval_turn":6,"retrieval_file_index":2,"cited_message_idx":37,"cited_message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","search_result_idx":2,"line_range":[5,10],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":3711,"end_ix":3740,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"I think the way the profit comes is\u2026 Again, let\u2019s just abstract the whole industry here. Let\u2019s just imagine we\u2019re in an economics textbook. We have a small number of firms. Each can invest a limited amount. Each can invest some fraction in R&D. They have some marginal cost to serve. The gross profit margins on that marginal cost are very high because inference is efficient. There\u2019s some competition, but the models are also differentiated.\nCompanies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.","extra":{"retrieval_turn":6,"retrieval_file_index":2,"cited_message_idx":37,"cited_message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","search_result_idx":2,"line_range":[17,31],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":3828,"end_ix":3857,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"I think we\u2019re gonna be in a position where that equilibrium of how much you spend on training is less than the gross margins that you\u2019re able to get on compute. So the underlying economics are profitable. The problem is you have this hellish demand prediction problem when you\u2019re buying the next year of compute and you might guess under and be very profitable but have no compute for research. Or you might guess over and you are not profitable and you have all the compute for research in the world. Does that make sense? Just as a dynamic model of the industry?","extra":{"retrieval_turn":6,"retrieval_file_index":4,"cited_message_idx":37,"cited_message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","search_result_idx":4,"line_range":[13,14],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":4283,"end_ix":4310,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"I don\u2019t actually agree that much regarding the developed world. I feel like in the developed world, markets function pretty well. When there\u2019s a lot of money to be made on something and it\u2019s clearly the best available alternative, it\u2019s actually hard for the regulatory system to stop it.\nWe\u2019re seeing that in AI itself. A thing I\u2019ve been trying to fight for is export controls on chips to China. That\u2019s in the national security interest of the US. That\u2019s squarely within the policy beliefs of almost everyone in Congress of both parties. The case is very clear. The counterarguments against it, I\u2019ll politely call them fishy. Yet it doesn\u2019t happen and we sell the chips because there\u2019s so much money riding on it. That money wants to be made. In that case, in my opinion, that\u2019s a bad thing. But it also applies when it\u2019s a good thing.","extra":{"retrieval_turn":7,"retrieval_file_index":2,"cited_message_idx":41,"cited_message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","search_result_idx":2,"line_range":[3,6],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":4689,"end_ix":4718,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"If this does happen, we could have a few situations. If we have an offense-dominant situation, we could have a situation like nuclear weapons, but more dangerous. Either side could easily destroy everything.\nWe could also have a world where it\u2019s unstable. The nuclear equilibrium is stable because it\u2019s deterrence. But let\u2019s say there was uncertainty about, if the two AIs fought, which AI would win? That could create instability. You often have conflict when the two sides have a different assessment of their likelihood of winning. If one side is like, \u201cOh yeah, there\u2019s a 90% chance I\u2019ll win,\u201d and the other side thinks the same, then a fight is much more likely. They can\u2019t both be right, but they can both think that.","extra":{"retrieval_turn":7,"retrieval_file_index":2,"cited_message_idx":41,"cited_message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","search_result_idx":2,"line_range":[28,31],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":5113,"end_ix":5140,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"We\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.","extra":{"retrieval_turn":6,"retrieval_file_index":1,"cited_message_idx":37,"cited_message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","search_result_idx":1,"line_range":[3,3],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}},{"start_ix":5303,"end_ix":5332,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","text":"What has been the most surprising thing is the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that you have people \u2014 within the bubble and outside the bubble \u2014 talking about the same tired, old hot-button political issues, when we are near the end of the exponential.","extra":{"retrieval_turn":7,"retrieval_file_index":1,"cited_message_idx":41,"cited_message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","search_result_idx":1,"line_range":[13,14],"cloud_doc_url":null,"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null,"set_id":null,"spl_version":null,"run_id":null,"title":null,"effective_time":null}}}],"content_references":[{"matched_text":"\ue200filecite\ue202turn7file1\ue202L9-L14\ue201","start_idx":376,"end_idx":404,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. There\u2019s plus or minus a year or two here and there. I don\u2019t know that I would\u2019ve predicted the specific direction of code.\nBut when I look at the exponential, it is roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. The frontier is a little bit uneven, but it\u2019s roughly what I expected.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":7,"message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","file_index":1,"line_range_start":9,"line_range_end":14},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn7file0\ue202L1-L12\ue201","start_idx":802,"end_idx":830,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"Dario Amodei\nI actually have the same hypothesis I had even all the way back in 2017. I think I talked about it last time, but I wrote a doc called \u201cThe Big Blob of Compute Hypothesis\u201d. It wasn\u2019t about the scaling of language models in particular. When I wrote it GPT-1 had just come out.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":7,"message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","file_index":0,"line_range_start":1,"line_range_end":12},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn7file0\ue202L13-L17\ue201","start_idx":1260,"end_idx":1289,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"That was the hypothesis, and it\u2019s a hypothesis I still hold. I don\u2019t think I\u2019ve seen very much that is not in line with it. The pre-training scaling laws were one example of what we see there. Those have continued going. Now it\u2019s been widely reported, we feel good about pre-training. It\u2019s continuing to give us gains.\nWhat has changed is that now we\u2019re also seeing the same thing for RL. We\u2019re seeing a pre-training phase and then an RL phase on top of that. With RL, it\u2019s actually just the same. Even other companies have published things in some of their releases that say, \u201cWe train the model on math contests \u2014 AIME or other things \u2014 and how well the model does is log-linear in how long we\u2019ve trained it.\u201d","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":7,"message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","file_index":0,"line_range_start":13,"line_range_end":17},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn6file1\ue202L1-L4\ue201","start_idx":1662,"end_idx":1689,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"I think that\u2019s dependent on several things. I think this is one of the things that\u2019s actually blocking deployment: getting to the point on computer use where the models are really masters at using the computer.\nWe\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":6,"message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","file_index":1,"line_range_start":1,"line_range_end":4},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn6file1\ue202L13-L17\ue202turn6file1\ue202L25-L26\ue201","start_idx":2129,"end_idx":2177,"safe_urls":[],"refs":[],"alt":null,"prompt_text":null,"type":"hidden","invalid":true},{"matched_text":"\ue200filecite\ue202turn6file0\ue202L11-L13\ue202turn6file0\ue202L29-L31\ue201","start_idx":2580,"end_idx":2628,"safe_urls":[],"refs":[],"alt":null,"prompt_text":null,"type":"hidden","invalid":true},{"matched_text":"\ue200filecite\ue202turn6file0\ue202L1-L13\ue201","start_idx":2743,"end_idx":2771,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"But what we\u2019ve seen from the beginning, at least if you look within Anthropic, there\u2019s this bizarre 10x per year growth in revenue that we\u2019ve seen. So in 2023, it was zero to $100 million. In 2024, it was $100 million to $1 billion. In 2025, it was $1 billion to $ 9-10 billion.\nDwarkesh Patel","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":6,"message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","file_index":0,"line_range_start":1,"line_range_end":13},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn6file2\ue202L5-L10\ue201","start_idx":3218,"end_idx":3246,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"Dario Amodei\nThat\u2019s just not my view. So I\u2019ll make another prediction. It is hard for me to see that there won\u2019t be trillions of dollars in revenue before 2030. I can construct a plausible world. It takes maybe three years. That would be the end of what I think it\u2019s plausible.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":6,"message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","file_index":2,"line_range_start":5,"line_range_end":10},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn6file2\ue202L17-L31\ue201","start_idx":3711,"end_idx":3740,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"I think the way the profit comes is\u2026 Again, let\u2019s just abstract the whole industry here. Let\u2019s just imagine we\u2019re in an economics textbook. We have a small number of firms. Each can invest a limited amount. Each can invest some fraction in R&D. They have some marginal cost to serve. The gross profit margins on that marginal cost are very high because inference is efficient. There\u2019s some competition, but the models are also differentiated.\nCompanies will compete to push their research budgets up. But because there\u2019s a small number of players, we have the... What is it called? The Cournot equilibrium, I think, is what the small number of firm equilibrium is. The point is it doesn\u2019t equilibrate to perfect competition with zero margins. If there\u2019s three firms in the economy and all are kind of independently behaving rationally, it doesn\u2019t equilibrate to zero.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":6,"message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","file_index":2,"line_range_start":17,"line_range_end":31},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn6file4\ue202L13-L14\ue201","start_idx":3828,"end_idx":3857,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"I think we\u2019re gonna be in a position where that equilibrium of how much you spend on training is less than the gross margins that you\u2019re able to get on compute. So the underlying economics are profitable. The problem is you have this hellish demand prediction problem when you\u2019re buying the next year of compute and you might guess under and be very profitable but have no compute for research. Or you might guess over and you are not profitable and you have all the compute for research in the world. Does that make sense? Just as a dynamic model of the industry?","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":6,"message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","file_index":4,"line_range_start":13,"line_range_end":14},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn7file2\ue202L3-L6\ue201","start_idx":4283,"end_idx":4310,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"I don\u2019t actually agree that much regarding the developed world. I feel like in the developed world, markets function pretty well. When there\u2019s a lot of money to be made on something and it\u2019s clearly the best available alternative, it\u2019s actually hard for the regulatory system to stop it.\nWe\u2019re seeing that in AI itself. A thing I\u2019ve been trying to fight for is export controls on chips to China. That\u2019s in the national security interest of the US. That\u2019s squarely within the policy beliefs of almost everyone in Congress of both parties. The case is very clear. The counterarguments against it, I\u2019ll politely call them fishy. Yet it doesn\u2019t happen and we sell the chips because there\u2019s so much money riding on it. That money wants to be made. In that case, in my opinion, that\u2019s a bad thing. But it also applies when it\u2019s a good thing.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":7,"message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","file_index":2,"line_range_start":3,"line_range_end":6},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn7file2\ue202L28-L31\ue201","start_idx":4689,"end_idx":4718,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"If this does happen, we could have a few situations. If we have an offense-dominant situation, we could have a situation like nuclear weapons, but more dangerous. Either side could easily destroy everything.\nWe could also have a world where it\u2019s unstable. The nuclear equilibrium is stable because it\u2019s deterrence. But let\u2019s say there was uncertainty about, if the two AIs fought, which AI would win? That could create instability. You often have conflict when the two sides have a different assessment of their likelihood of winning. If one side is like, \u201cOh yeah, there\u2019s a 90% chance I\u2019ll win,\u201d and the other side thinks the same, then a fight is much more likely. They can\u2019t both be right, but they can both think that.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":7,"message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","file_index":2,"line_range_start":28,"line_range_end":31},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn6file1\ue202L3-L3\ue201","start_idx":5113,"end_idx":5140,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"We\u2019ve seen this climb in benchmarks, and benchmarks are always imperfect measures. But I think when we first released computer use a year and a quarter ago, OSWorld was at maybe 15%. I don\u2019t remember exactly, but we\u2019ve climbed from that to 65-70%. There may be harder measures as well, but I think computer use has to pass a point of reliability.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":6,"message_id":"88b00e63-7d76-4acf-ba43-b5aac6921767","file_index":1,"line_range_start":3,"line_range_end":3},"fff_metadata":null,"connector_id":null},{"matched_text":"\ue200filecite\ue202turn7file1\ue202L13-L14\ue201","start_idx":5303,"end_idx":5332,"alt":null,"type":"file","name":"amodei-patel-interview.txt","id":"file_00000000b50c71fab1f803bf26e09f78","source":"my_files","snippet":"What has been the most surprising thing is the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that you have people \u2014 within the bubble and outside the bubble \u2014 talking about the same tired, old hot-button political issues, when we are near the end of the exponential.","cloud_doc_url":null,"medical_file_reference":{"authors_display":null,"doi":null,"publication_year":"None","journal_name":null,"journal_homepage_url":null},"drug_file_reference":null,"input_pointer":{"message_index":7,"message_id":"9c88bf1c-3e1e-408c-a4c2-485f6153330f","file_index":1,"line_range_start":13,"line_range_end":14},"fff_metadata":null,"connector_id":null}],"resolved_model_slug":"gpt-5-2-thinking","search_result_groups":[],"request_id":"dff07c78-3f21-4a28-b9b6-d24d7e5eda1c","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"588795c2-3d59-46dc-8a76-8ccef9e690ad","turn_exchange_id":"c2d5aa4e-26fa-4851-9f1c-3fbd1ba910bb","can_save":true},"recipient":"all","channel":"final"},"parent":"588795c2-3d59-46dc-8a76-8ccef9e690ad","children":[]}}}