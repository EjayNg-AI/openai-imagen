{"id":"67b2a062-091c-8007-85eb-9ba57a31ba38","title":"LLM Pre-training Data Sizes","create_time":1739759714.192224,"update_time":1739759800.661144,"current_node":"8b104701-da70-45cd-9221-a1eaf3422221","mapping":{"aaa14812-b7e7-41ec-9df0-e4bdd79af7ff":{"id":"aaa14812-b7e7-41ec-9df0-e4bdd79af7ff","message":null,"parent":null,"children":["a674aeda-02ba-4861-b940-0bb8bd04911b"]},"a674aeda-02ba-4861-b940-0bb8bd04911b":{"id":"a674aeda-02ba-4861-b940-0bb8bd04911b","message":{"id":"a674aeda-02ba-4861-b940-0bb8bd04911b","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"can_save":false},"recipient":"all","channel":null},"parent":"aaa14812-b7e7-41ec-9df0-e4bdd79af7ff","children":["84c36d21-5613-4895-849b-81de0451b494"]},"84c36d21-5613-4895-849b-81de0451b494":{"id":"84c36d21-5613-4895-849b-81de0451b494","message":{"id":"84c36d21-5613-4895-849b-81de0451b494","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view."},"is_user_system_message":true,"can_save":true},"recipient":"all","channel":null},"parent":"a674aeda-02ba-4861-b940-0bb8bd04911b","children":["c14831d3-16ae-48fd-ae6b-955a3a9b0947"]},"c14831d3-16ae-48fd-ae6b-955a3a9b0947":{"id":"c14831d3-16ae-48fd-ae6b-955a3a9b0947","message":{"id":"c14831d3-16ae-48fd-ae6b-955a3a9b0947","author":{"role":"user","name":null,"metadata":{}},"create_time":1739759714.197212,"update_time":null,"content":{"content_type":"text","parts":["What is the amount of data used in pre-training by typical production grade, frontier LLMs?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"system_hints":["search"],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"91326203a9035ff9-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"84c36d21-5613-4895-849b-81de0451b494","children":["567800fb-3149-46a5-94b9-6d0eb6d9497e"]},"567800fb-3149-46a5-94b9-6d0eb6d9497e":{"id":"567800fb-3149-46a5-94b9-6d0eb6d9497e","message":{"id":"567800fb-3149-46a5-94b9-6d0eb6d9497e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1739759720.735498,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"c14831d3-16ae-48fd-ae6b-955a3a9b0947","request_id":"91326203a9035ff9-SIN","timestamp_":"absolute","can_save":false},"recipient":"all","channel":null},"parent":"c14831d3-16ae-48fd-ae6b-955a3a9b0947","children":["e8d49ef5-2214-4923-a3d0-7e866ef8a179"]},"e8d49ef5-2214-4923-a3d0-7e866ef8a179":{"id":"e8d49ef5-2214-4923-a3d0-7e866ef8a179","message":{"id":"e8d49ef5-2214-4923-a3d0-7e866ef8a179","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1739759720.735609,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"What is the amount of data used in pre-training by typical production grade, frontier LLMs?\")"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"citations":[],"content_references":[],"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"567800fb-3149-46a5-94b9-6d0eb6d9497e","request_id":"91326203a9035ff9-SIN","timestamp_":"absolute","can_save":true},"recipient":"web","channel":null},"parent":"567800fb-3149-46a5-94b9-6d0eb6d9497e","children":["53608457-01dc-4f88-ab77-8796a00a3c09"]},"53608457-01dc-4f88-ab77-8796a00a3c09":{"id":"53608457-01dc-4f88-ab77-8796a00a3c09","message":{"id":"53608457-01dc-4f88-ab77-8796a00a3c09","author":{"role":"tool","name":"web","metadata":{}},"create_time":1739759720.735691,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":""},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"command":"search","status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"e8d49ef5-2214-4923-a3d0-7e866ef8a179","request_id":"91326203a9035ff9-SIN","timestamp_":"absolute","can_save":false},"recipient":"assistant","channel":null},"parent":"e8d49ef5-2214-4923-a3d0-7e866ef8a179","children":["d390b7f7-3721-43f3-8070-dfdd589532ea"]},"d390b7f7-3721-43f3-8070-dfdd589532ea":{"id":"d390b7f7-3721-43f3-8070-dfdd589532ea","message":{"id":"d390b7f7-3721-43f3-8070-dfdd589532ea","author":{"role":"assistant","name":null,"metadata":{"real_author":"tool:web"}},"create_time":1739759720.735745,"update_time":null,"content":{"content_type":"text","parts":["\ue203Production-grade, frontier large language models (LLMs) are typically pre-trained on datasets comprising hundreds of billions to several trillion tokens.\ue204 \ue203For instance, OpenAI's GPT-3 was trained on approximately 500 billion tokens, while Google's PaLM utilized a dataset of about 780 billion tokens.\ue204 \ue203Meta's Llama 3, released in April 2024, was pre-trained on an extensive dataset of approximately 15 trillion tokens.\ue204 \ue200cite\ue202turn0search16\ue202turn0search18\ue202turn0search13\ue201\ue206\n\n\ue203The size of the pre-training dataset is a critical factor influencing an LLM's performance.\ue204 \ue203Larger datasets enable models to capture a broader range of linguistic patterns and knowledge, enhancing their ability to understand and generate human-like text.\ue204 \ue203However, the availability of high-quality, diverse training data is becoming a significant challenge.\ue204 \ue203Estimates suggest that the supply of publicly available textual data suitable for training LLMs could be exhausted by 2028, posing a substantial hurdle for future model development.\ue204 \ue200cite\ue202turn0news21\ue201\ue206\n\n\ue203In response to these data limitations, researchers are exploring alternative strategies, such as leveraging synthetic data generation and focusing on data quality over quantity.\ue204 \ue203These approaches aim to sustain the advancement of LLMs despite the constraints on accessible training data.\ue204\ue206\n\n\ue200navlist\ue202Challenges and Innovations in AI Training Data\ue202turn0news21,turn0news22,turn0news24\ue201 "]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"search_source":"composer_search","client_reported_search_source":"conversation_composer_web_icon","search_result_groups":[{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40jelkhoury880/how-have-pre-training-datasets-for-large-language-models-evolved-13d74c01f8e8","title":"How Have Pre-Training Datasets for Large Language Models ...","snippet":"The evolution of pre-training datasets for large language models reflects the rapid advancement and growing complexity of the field.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"content_type":null,"pub_date":1720828800.0,"attributions":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40aydinKerem/everything-to-learn-about-large-language-models-part-2-3-pre-training-and-fine-tuning-5fc68700701a","title":"Everything to learn about Large Language Models(Part 2/3 Pre ...","snippet":"Pre-training a large language model is done in a self-supervised way, meaning it is trained with unlabeled data which is just text from the internet.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"content_type":null,"pub_date":1701475200.0,"attributions":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"amazon.com","entries":[{"type":"search_result","url":"https://aws.amazon.com/what-is/large-language-model/","title":"What is LLM? - Large Language Models Explained - AWS","snippet":"Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"content_type":null,"pub_date":null,"attributions":null,"attribution":"aws.amazon.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2310.16789v3","title":"Detecting Pretraining Data from Large Language Models - arXiv","snippet":"Model size.\u200b\u200b This is likely because larger models have more parameters and thus are more likely to memorize the pretraining data.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"content_type":null,"pub_date":null,"attributions":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"kili-technology.com","entries":[{"type":"search_result","url":"https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models","title":"Open-Sourced Training Datasets for Large Language Models (LLMs)","snippet":"Training a large language model requires an enormous size of datasets. For example, OpenAI trained GPT-3 with 45 TB of textual data curated from various sources ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"content_type":null,"pub_date":null,"attributions":null,"attribution":"kili-technology.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=DimPeeCxKO","title":"Simple and Scalable Strategies to Continually Pre-train Large...","snippet":"Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"content_type":null,"pub_date":1710720000.0,"attributions":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2024.findings-acl.35.pdf","title":"[PDF] DPDLLM: A Black-box Framework for Detecting Pre-training Data ...","snippet":"The success of large language models (LLM) benefits from large-scale model parameters and large amounts of pre-training data. However, the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"content_type":null,"pub_date":1723334400.0,"attributions":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/2024.emnlp-main.300.pdf","title":"[PDF] Pretraining Data Detection for Large Language Models","snippet":"As the scale of training corpora for large lan- guage models (LLMs) grows, model devel- opers become increasingly reluctant to dis- close ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"content_type":null,"pub_date":1731369600.0,"attributions":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://gist.github.com/ritwikraha/77e79990992043f60a9588610b2781c5","title":"Pretraining of Large Language Models - GitHub Gist","snippet":"Since the goal of an llm is ambitious in nature, the data needed for pretraining should be high quality and voluminous. There are two main types of data used to ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"content_type":null,"pub_date":null,"attributions":null,"attribution":"gist.github.com"}]}],"safe_urls":["https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693?utm_source=chatgpt.com","https://en.wikipedia.org/wiki/GPT-3?utm_source=chatgpt.com","https://www.businessinsider.com/generative-ai-wall-scaling-laws-training-data-chatgpt-gemini-claude-2024-11?utm_source=chatgpt.com","https://nymag.com/intelligencer/article/the-future-of-ai-shouldnt-be-taken-at-face-value.html?utm_campaign=feed-part&utm_medium=social_acct&utm_source=chatgpt.com","https://images.wsj.net/im-68847617/?resize=500&size=1&width=1280","https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com","https://en.wikipedia.org/wiki/PaLM?utm_source=chatgpt.com","https://pyxis.nymag.com/v1/imgs/cdc/2c1/584a5b8a4f5cd3edf8ec7553da7e988db3-microsoft-chat-gpt.2x.rhorizontal.w710.jpg","https://i.insider.com/673e124cede4eeae39289e10?format=jpeg"],"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"53608457-01dc-4f88-ab77-8796a00a3c09","message_locale":"en-US","image_results":[],"content_references":[{"matched_text":"\ue203","start_idx":0,"end_idx":1,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":154,"end_idx":155,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":156,"end_idx":157,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":302,"end_idx":303,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":304,"end_idx":305,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":421,"end_idx":422,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0search16\ue202turn0search18\ue202turn0search13\ue201","start_idx":423,"end_idx":471,"refs":[],"alt":"([en.wikipedia.org](https://en.wikipedia.org/wiki/GPT-3?utm_source=chatgpt.com), [en.wikipedia.org](https://en.wikipedia.org/wiki/PaLM?utm_source=chatgpt.com), [en.wikipedia.org](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"GPT-3","url":"https://en.wikipedia.org/wiki/GPT-3?utm_source=chatgpt.com","pub_date":1739503193.0,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"en.wikipedia.org"},{"title":"PaLM","url":"https://en.wikipedia.org/wiki/PaLM?utm_source=chatgpt.com","pub_date":1737875888.0,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":18}],"hue":null,"attributions":null,"attribution":"en.wikipedia.org"},{"title":"Llama (language model)","url":"https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com","pub_date":1739632103.0,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"en.wikipedia.org"}],"status":"done","error":null,"style":null},{"matched_text":"\ue206","start_idx":471,"end_idx":472,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":474,"end_idx":475,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":566,"end_idx":567,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":568,"end_idx":569,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":731,"end_idx":732,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":733,"end_idx":734,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":835,"end_idx":836,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":837,"end_idx":838,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1019,"end_idx":1020,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0news21\ue201","start_idx":1021,"end_idx":1039,"refs":[],"alt":"([businessinsider.com](https://www.businessinsider.com/generative-ai-wall-scaling-laws-training-data-chatgpt-gemini-claude-2024-11?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI improvements are slowing down. Companies have a plan to break through the wall.","url":"https://www.businessinsider.com/generative-ai-wall-scaling-laws-training-data-chatgpt-gemini-claude-2024-11?utm_source=chatgpt.com","pub_date":1732698002.0,"snippet":"There is an ongoing debate in the tech industry about whether improvements to AI models have reached a plateau. Prominent AI leaders like Sam Altman of OpenAI and CEOs from companies like Anthropic and Nvidia have disputed claims of slowed progress, while figures like Marc Andreessen argue that advancements are becoming less noticeable. The challenge includes limited access to computing power and the exhaustion of publicly available training data. To address these issues, companies are exploring new data sources, such as multimodal and private data, and improving data quality through synthetic data. There's also increased focus on building reasoning capabilities into AI, where models can take longer to process prompts for better accuracy. The industry faces slower improvement rates and higher costs for future advancements, which may require adjusting expectations and timelines for the development of more sophisticated AI models.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"news","ref_index":21}],"hue":null,"attributions":null,"attribution":"businessinsider.com"}],"status":"done","error":null,"style":null},{"matched_text":"\ue206","start_idx":1039,"end_idx":1040,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":1042,"end_idx":1043,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1220,"end_idx":1221,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":1222,"end_idx":1223,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":1331,"end_idx":1332,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue206","start_idx":1332,"end_idx":1333,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200navlist\ue202Challenges and Innovations in AI Training Data\ue202turn0news21,turn0news22,turn0news24\ue201","start_idx":1335,"end_idx":1427,"refs":[{"turn_index":0,"ref_type":"news","ref_index":21},{"turn_index":0,"ref_type":"news","ref_index":22},{"turn_index":0,"ref_type":"news","ref_index":24}],"alt":"- [businessinsider.com](https://www.businessinsider.com/generative-ai-wall-scaling-laws-training-data-chatgpt-gemini-claude-2024-11?utm_source=chatgpt.com)\n- [wsj.com](https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693?utm_source=chatgpt.com)\n- [nymag.com](https://nymag.com/intelligencer/article/the-future-of-ai-shouldnt-be-taken-at-face-value.html?utm_campaign=feed-part&utm_medium=social_acct&utm_source=chatgpt.com)","prompt_text":null,"type":"nav_list","title":"Challenges and Innovations in AI Training Data","items":[{"title":"AI improvements are slowing down. Companies have a plan to break through the wall.","url":"https://www.businessinsider.com/generative-ai-wall-scaling-laws-training-data-chatgpt-gemini-claude-2024-11?utm_source=chatgpt.com","thumbnail_url":"https://i.insider.com/673e124cede4eeae39289e10?format=jpeg","pub_date":1732698002.0,"attribution":"businessinsider.com"},{"title":"The Next Great Leap in AI Is Behind Schedule and Crazy Expensive","url":"https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693?utm_source=chatgpt.com","thumbnail_url":"https://images.wsj.net/im-68847617/?resize=500&size=1&width=1280","pub_date":1734746400.0,"attribution":"wsj.com"},{"title":"The Future of AI Shouldn't Be Taken at Face Value","url":"https://nymag.com/intelligencer/article/the-future-of-ai-shouldnt-be-taken-at-face-value.html?utm_campaign=feed-part&utm_medium=social_acct&utm_source=chatgpt.com","thumbnail_url":"https://pyxis.nymag.com/v1/imgs/cdc/2c1/584a5b8a4f5cd3edf8ec7553da7e988db3-microsoft-chat-gpt.2x.rhorizontal.w710.jpg","pub_date":1735380010.0,"attribution":"nymag.com"}]},{"matched_text":" ","start_idx":1427,"end_idx":1427,"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"GPT-3","url":"https://en.wikipedia.org/wiki/GPT-3?utm_source=chatgpt.com","attribution":"en.wikipedia.org"},{"title":"AI improvements are slowing down. Companies have a plan to break through the wall.","url":"https://www.businessinsider.com/generative-ai-wall-scaling-laws-training-data-chatgpt-gemini-claude-2024-11?utm_source=chatgpt.com","attribution":"businessinsider.com"}],"has_images":false}],"citations":[{"start_ix":0,"end_ix":1,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":154,"end_ix":155,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":156,"end_ix":157,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":302,"end_ix":303,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":304,"end_ix":305,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":421,"end_ix":422,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":423,"end_ix":471,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":471,"end_ix":472,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":474,"end_ix":475,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":566,"end_ix":567,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":568,"end_ix":569,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":731,"end_ix":732,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":733,"end_ix":734,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":835,"end_ix":836,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":837,"end_ix":838,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1019,"end_ix":1020,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1021,"end_ix":1039,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1039,"end_ix":1040,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1042,"end_ix":1043,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1220,"end_ix":1221,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1222,"end_ix":1223,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1331,"end_ix":1332,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1332,"end_ix":1333,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1335,"end_ix":1427,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":1427,"end_ix":1427,"invalid_reason":"Unsupported, please upgrade"}],"request_id":"91326203a9035ff9-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"53608457-01dc-4f88-ab77-8796a00a3c09","children":["7bf96ab3-948a-43b2-86bb-35ba1a793820"]},"7bf96ab3-948a-43b2-86bb-35ba1a793820":{"id":"7bf96ab3-948a-43b2-86bb-35ba1a793820","message":{"id":"7bf96ab3-948a-43b2-86bb-35ba1a793820","author":{"role":"user","name":null,"metadata":{}},"create_time":1739759795.448194,"update_time":null,"content":{"content_type":"text","parts":["Search on much pre-training data was used for Deepseek"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"system_hints":["search"],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"913263fe6e885ff9-SIN","message_source":null,"timestamp_":"absolute","message_type":null,"can_save":true},"recipient":"all","channel":null},"parent":"d390b7f7-3721-43f3-8070-dfdd589532ea","children":["ebeccf75-3552-4263-9f54-07ba480ab93c"]},"ebeccf75-3552-4263-9f54-07ba480ab93c":{"id":"ebeccf75-3552-4263-9f54-07ba480ab93c","message":{"id":"ebeccf75-3552-4263-9f54-07ba480ab93c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1739759800.068401,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"Search on much pre-training data was used for Deepseek\")"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"citations":[],"content_references":[],"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"7bf96ab3-948a-43b2-86bb-35ba1a793820","request_id":"913263fe6e885ff9-SIN","timestamp_":"absolute","can_save":true},"recipient":"web","channel":null},"parent":"7bf96ab3-948a-43b2-86bb-35ba1a793820","children":["23b11341-f062-4c1a-83e7-952ad80ec754"]},"23b11341-f062-4c1a-83e7-952ad80ec754":{"id":"23b11341-f062-4c1a-83e7-952ad80ec754","message":{"id":"23b11341-f062-4c1a-83e7-952ad80ec754","author":{"role":"tool","name":"web","metadata":{}},"create_time":1739759800.068559,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":""},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true,"command":"search","status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"ebeccf75-3552-4263-9f54-07ba480ab93c","request_id":"913263fe6e885ff9-SIN","timestamp_":"absolute","can_save":false},"recipient":"assistant","channel":null},"parent":"ebeccf75-3552-4263-9f54-07ba480ab93c","children":["8b104701-da70-45cd-9221-a1eaf3422221"]},"8b104701-da70-45cd-9221-a1eaf3422221":{"id":"8b104701-da70-45cd-9221-a1eaf3422221","message":{"id":"8b104701-da70-45cd-9221-a1eaf3422221","author":{"role":"assistant","name":null,"metadata":{"real_author":"tool:web"}},"create_time":1739759800.06866,"update_time":null,"content":{"content_type":"text","parts":["\ue203DeepSeek, a Chinese AI startup, has developed several large language models (LLMs) with varying amounts of pre-training data:\ue204\ue206\n\n- **DeepSeek-V2**: \ue203Pre-trained on 8.1 trillion tokens.\ue204 \ue200cite\ue202turn0academia11\ue201\ue206\n\n- **DeepSeek-V3**: \ue203Pre-trained on 14.8 trillion tokens.\ue204 \ue200cite\ue202turn0search4\ue201\ue206\n\n- **DeepSeek-LLM**: \ue203Initially pre-trained on a dataset of 2 trillion tokens, with plans for continuous expansion.\ue204 \ue200cite\ue202turn0academia9\ue201\ue206\n\n\ue203These models have been trained on diverse and high-quality datasets, enabling them to achieve performance comparable to leading closed-source models.\ue204\ue206 "]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"search_source":"composer_search","client_reported_search_source":"conversation_composer_previous_web_mode","search_result_groups":[{"type":"search_result_group","domain":"stratechery.com","entries":[{"type":"search_result","url":"https://stratechery.com/2025/deepseek-faq/","title":"DeepSeek FAQ \u2013 Stratechery by Ben Thompson","snippet":"During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"content_type":null,"pub_date":1737936000.0,"attributions":null,"attribution":"stratechery.com"}]},{"type":"search_result_group","domain":"nextplatform.com","entries":[{"type":"search_result","url":"https://www.nextplatform.com/2025/01/27/how-did-deepseek-train-its-ai-model-on-a-lot-less-and-crippled-hardware/","title":"How Did DeepSeek Train Its AI Model On A Lot Less \u2013 And Crippled","snippet":"In the DeepSeek-V3 paper, DeepSeek says that it spent 2.66 million GPU-hours on H800 accelerators to do the pretraining, 119,000 GPU-hours on ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"content_type":null,"pub_date":1737936000.0,"attributions":null,"attribution":"nextplatform.com"}]},{"type":"search_result_group","domain":"martinfowler.com","entries":[{"type":"search_result","url":"https://martinfowler.com/articles/deepseek-papers.html","title":"The DeepSeek Series: A Technical Overview - Martin Fowler","snippet":"Training & Outcomes\u200b\u200b DeepSeek-V2, with ~236B total params (21B activated), is pre-trained on 8.1T tokens. They do Supervised Fine Tuning (SFT) ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"content_type":null,"pub_date":1738800000.0,"attributions":null,"attribution":"martinfowler.com"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/LocalLLaMA/comments/1hn8ams/deepseek_v3_was_trained_on_811x_less_the_normal/","title":"Deepseek v3 was trained on 8-11x less the normal budget ... - Reddit","snippet":"It has a total of 671 billion parameters, with only 37 billion parameters activated for each token, which makes it efficient despite its size.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"content_type":null,"pub_date":1735257600.0,"attributions":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/deepseek-ai/DeepSeek-V3","title":"deepseek-ai/DeepSeek-V3 - Hugging Face","snippet":"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"content_type":null,"pub_date":1736121600.0,"attributions":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2401.02954v1","title":"DeepSeek LLM Scaling Open-Source Language Models with ... - arXiv","snippet":"To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"content_type":null,"pub_date":null,"attributions":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"inferless.com","entries":[{"type":"search_result","url":"https://www.inferless.com/learn/the-ultimate-guide-to-deepseek-models","title":"The Ultimate Guide to DeepSeek Models - Inferless","snippet":"Building upon the DeepSeek-Coder-Base-v1. 5 7B model, it undergoes continuous pre-training on a substantial dataset comprising 120 billion math ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"content_type":null,"pub_date":1738281600.0,"attributions":null,"attribution":"inferless.com"}]},{"type":"search_result_group","domain":"labelstud.io","entries":[{"type":"search_result","url":"https://labelstud.io/blog/top-5-most-successful-data-curation-strategies-in-deepseek/","title":"Top 5 Most Successful Data Curation Strategies in DeepSeek","snippet":"DeepSeek-R1's pretraining dataset contains 14.8 trillion tokens, surpassing previous versions and introducing variety across multiple dimensions ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"content_type":null,"pub_date":1738195200.0,"attributions":null,"attribution":"labelstud.io"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/deepseek-ai/DeepSeek-Coder/issues/121","title":"How many tokens of code in pretraining \u00b7 Issue #121 - GitHub","snippet":"According to the README, DeepSeek-Coder is trained on 2T tokens, where 87% is code. So there should be around 1.74T tokens of code in the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"content_type":null,"pub_date":1708128000.0,"attributions":null,"attribution":"github.com"}]}],"safe_urls":["https://arxiv.org/abs/2401.02954?utm_source=chatgpt.com","https://arxiv.org/abs/2405.04434?utm_source=chatgpt.com","https://huggingface.co/deepseek-ai/DeepSeek-V3?utm_source=chatgpt.com"],"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"23b11341-f062-4c1a-83e7-952ad80ec754","message_locale":"en-US","image_results":[],"content_references":[{"matched_text":"\ue203","start_idx":0,"end_idx":1,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":126,"end_idx":127,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue206","start_idx":127,"end_idx":128,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":149,"end_idx":150,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":185,"end_idx":186,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0academia11\ue201","start_idx":187,"end_idx":209,"refs":[],"alt":"([arxiv.org](https://arxiv.org/abs/2405.04434?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model","url":"https://arxiv.org/abs/2405.04434?utm_source=chatgpt.com","pub_date":1715097403.0,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"academia","ref_index":11}],"hue":null,"attributions":null,"attribution":"arxiv.org"}],"status":"done","error":null,"style":null},{"matched_text":"\ue206","start_idx":209,"end_idx":210,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":231,"end_idx":232,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":268,"end_idx":269,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0search4\ue201","start_idx":270,"end_idx":289,"refs":[],"alt":"([huggingface.co](https://huggingface.co/deepseek-ai/DeepSeek-V3?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"deepseek-ai/DeepSeek-V3 - Hugging Face","url":"https://huggingface.co/deepseek-ai/DeepSeek-V3?utm_source=chatgpt.com","pub_date":1736121600.0,"snippet":"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"huggingface.co"}],"status":"done","error":null,"style":null},{"matched_text":"\ue206","start_idx":289,"end_idx":290,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":312,"end_idx":313,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":406,"end_idx":407,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue200cite\ue202turn0academia9\ue201","start_idx":408,"end_idx":429,"refs":[],"alt":"([arxiv.org](https://arxiv.org/abs/2401.02954?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism","url":"https://arxiv.org/abs/2401.02954?utm_source=chatgpt.com","pub_date":1704481153.0,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"academia","ref_index":9}],"hue":null,"attributions":null,"attribution":"arxiv.org"}],"status":"done","error":null,"style":null},{"matched_text":"\ue206","start_idx":429,"end_idx":430,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue203","start_idx":432,"end_idx":433,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue204","start_idx":582,"end_idx":583,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":"\ue206","start_idx":583,"end_idx":584,"refs":["hidden"],"alt":null,"prompt_text":null,"type":"hidden","invalid":false},{"matched_text":" ","start_idx":584,"end_idx":584,"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model","url":"https://arxiv.org/abs/2405.04434?utm_source=chatgpt.com","attribution":"arxiv.org"},{"title":"deepseek-ai/DeepSeek-V3 - Hugging Face","url":"https://huggingface.co/deepseek-ai/DeepSeek-V3?utm_source=chatgpt.com","attribution":"huggingface.co"}],"has_images":false}],"citations":[{"start_ix":0,"end_ix":1,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":126,"end_ix":127,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":127,"end_ix":128,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":149,"end_ix":150,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":185,"end_ix":186,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":187,"end_ix":209,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":209,"end_ix":210,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":231,"end_ix":232,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":268,"end_ix":269,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":270,"end_ix":289,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":289,"end_ix":290,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":312,"end_ix":313,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":406,"end_ix":407,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":408,"end_ix":429,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":429,"end_ix":430,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":432,"end_ix":433,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":582,"end_ix":583,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":583,"end_ix":584,"invalid_reason":"Unsupported, please upgrade"},{"start_ix":584,"end_ix":584,"invalid_reason":"Unsupported, please upgrade"}],"request_id":"913263fe6e885ff9-SIN","timestamp_":"absolute","can_save":true},"recipient":"all","channel":null},"parent":"23b11341-f062-4c1a-83e7-952ad80ec754","children":[]}}}