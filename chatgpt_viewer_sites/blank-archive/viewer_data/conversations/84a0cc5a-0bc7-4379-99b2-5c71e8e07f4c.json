{"id":"84a0cc5a-0bc7-4379-99b2-5c71e8e07f4c","title":"Check the following markdown document for factual accuracy, logical flow, cor...","create_time":1771969920.4364064,"update_time":1771969920.4364064,"current_node":"553e6b8d-0723-4efe-85c8-358d8d8e8fcf","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["2539351f-e0f1-415d-9b1d-68b388628e59"]},"2539351f-e0f1-415d-9b1d-68b388628e59":{"id":"2539351f-e0f1-415d-9b1d-68b388628e59","message":{"id":"2539351f-e0f1-415d-9b1d-68b388628e59","author":{"role":"user","name":null,"metadata":{}},"create_time":1771969920.4364064,"update_time":null,"content":{"content_type":"text","parts":["Check the following markdown document for factual accuracy, logical flow, correct semantics and explanations, correct use of symbols and regexes, and overall coherency. Check also for the accuracy of the mathematics underpinning the algorithms and the code that students are asked to produce.\n\n---\n\n# CS336 Assignment 1 (basics): Building a Transformer LM\n\n**Version 1.0.6**\n\nCS336 Staff\n\nSpring 2025\n\n---\n\n## 1 Assignment Overview\n\nIn this assignment, you will build all the components needed to train a standard Transformer language model (LM) from scratch and train some models.\n\n**What you will implement**\n\n1. Byte-pair encoding (BPE) tokenizer (\u00a72)\n2. Transformer language model (LM) (\u00a73)\n3. The cross-entropy loss function and the AdamW optimizer (\u00a74)\n4. The training loop, with support for serializing and loading model and optimizer state (\u00a75)\n\n**What you will run**\n\n1. Train a BPE tokenizer on the TinyStories dataset.\n2. Run your trained tokenizer on the dataset to convert it into a sequence of integer IDs.\n3. Train a Transformer LM on the TinyStories dataset.\n4. Generate samples and evaluate perplexity using the trained Transformer LM.\n5. Train models on OpenWebText and submit your attained perplexities to a leaderboard.\n\n**What you can use** We expect you to build these components from scratch. In particular, you may *not* use any definitions from `torch.nn`, `torch.nn.functional`, or `torch.optim` except for the following:\n\n- `torch.nn.Parameter`\n- Container classes in `torch.nn` (e.g., `Module`, `ModuleList`, `Sequential`, etc.)[^1]\n- `torch.nn.init.trunc_normal_`\n- The `torch.optim.Optimizer` base class\n\nYou may use any other PyTorch definitions. If you would like to use a function or class and are not sure whether it is permitted, feel free to ask on Slack. When in doubt, consider if using it compromises the \"from-scratch\" ethos of the assignment.\n\n[^1]: See [PyTorch.org/docs/stable/nn.html#containers](https://PyTorch.org/docs/stable/nn.html#containers) for a full list.\n\n**Statement on AI tools** Prompting LLMs such as ChatGPT is permitted for low-level programming questions or high-level conceptual questions about language models, but using it directly to solve the problem is prohibited.\n\nWe strongly encourage you to disable AI autocomplete (e.g., Cursor Tab, GitHub CoPilot) in your IDE when completing assignments (though non-AI autocomplete, e.g., autocompleting function names is totally fine). We have found that AI autocomplete makes it much harder to engage deeply with the content.\n\n**What the code looks like** All the assignment code as well as this writeup are available on GitHub at:\n\n> [github.com/stanford-cs336/assignment1-basics](https://github.com/stanford-cs336/assignment1-basics)\n\nPlease `git clone` the repository. If there are any updates, we will notify you so you can `git pull` to get the latest.\n\n1. `cs336_basics/*`: This is where you write your code. Note that there's no code in here---you can do whatever you want from scratch!\n\n2. `adapters.py`: There is a set of functionality that your code must have. For each piece of functionality (e.g., scaled dot product attention), fill out its implementation (e.g., `run_scaled_dot_product_attention`) by simply invoking your code. Note: your changes to `adapters.py` should not contain any substantive logic; this is glue code.\n\n3. `test_*.py`: This contains all the tests that you must pass (e.g., `test_scaled_dot_product_attention`), which will invoke the hooks defined in `adapters.py`. Don't edit the test files.\n\n**How to submit** You will submit the following files to Gradescope:\n\n- `writeup.pdf`: Answer all the written questions. Please typeset your responses.\n- `code.zip`: Contains all the code you've written.\n\nTo submit to the leaderboard, submit a PR to:\n\n> [github.com/stanford-cs336/assignment1-basics-leaderboard](https://github.com/stanford-cs336/assignment1-basics-leaderboard)\n\nSee the `README.md` in the leaderboard repository for detailed submission instructions.\n\n**Where to get datasets** This assignment will use two pre-processed datasets: TinyStories [Eldan and Li, 2023] and OpenWebText [Gokaslan et al., 2019]. Both datasets are single, large plaintext files. If you are doing the assignment with the class, you can find these files at `/data` of any non-head node machine.\n\nIf you are following along at home, you can download these files with the commands inside the `README.md`.\n\n> **Low-Resource/Downscaling Tip: Init**\n>\n> Throughout the course's assignment handouts, we will give advice for working through parts of the assignment with fewer or no GPU resources. For example, we will sometimes suggest **downscaling** your dataset or model size, or explain how to run training code on a MacOS integrated GPU or CPU. You'll find these \"low-resource tips\" in a blue box (like this one). Even if you are an enrolled Stanford student with access to the course machines, these tips may help you iterate faster and save time, so we recommend you to read them!\n\n> **Low-Resource/Downscaling Tip: Assignment 1 on Apple Silicon or CPU**\n>\n> With the staff solution code, we can train an LM to generate reasonably fluent text on an Apple M3 Max chip with 36 GB RAM, in under 5 minutes on Metal GPU (MPS) and about 30 minutes using the CPU. If these words don't mean much to you, don't worry! Just know that if you have a reasonably up-to-date laptop and your implementation is correct and efficient, you will be able to train a small LM that generates simple children's stories with decent fluency.\n>\n> Later in the assignment, we will explain what changes to make if you are on CPU or MPS.\n\n---\n\n## 2 Byte-Pair Encoding (BPE) Tokenizer\n\nIn the first part of the assignment, we will train and implement a byte-level byte-pair encoding (BPE) tokenizer [Sennrich et al., 2016; Wang et al., 2019]. In particular, we will represent arbitrary (Unicode) strings as a sequence of bytes and train our BPE tokenizer on this byte sequence. Later, we will use this tokenizer to encode text (a string) into tokens (a sequence of integers) for language modeling.\n\n### 2.1 The Unicode Standard\n\nUnicode is a text encoding standard that maps characters to integer *code points*. As of Unicode 16.0 (released in September 2024), the standard defines 154,998 characters across 168 scripts. For example, the character \"s\" has the code point 115 (typically notated as `U+0073`, where `U+` is a conventional prefix and `0073` is 115 in hexadecimal), and the character \"\u725b\" has the code point 29275. In Python, you can use the `ord()` function to convert a single Unicode character into its integer representation. The `chr()` function converts an integer Unicode code point into a string with the corresponding character.\n\n```python\n>>> ord('\u725b')\n29275\n>>> chr(29275)\n'\u725b'\n```\n\n> **Problem (`unicode1`): Understanding Unicode (1 point)**\n>\n> (a) What Unicode character does `chr(0)` return?\n>\n> **Deliverable**: A one-sentence response.\n>\n> (b) How does this character's string representation (`__repr__()`) differ from its printed representation?\n>\n> **Deliverable**: A one-sentence response.\n>\n> (c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:\n>\n> ```python\n> >>> chr(0)\n> >>> print(chr(0))\n> >>> \"this is a test\" + chr(0) + \"string\"\n> >>> print(\"this is a test\" + chr(0) + \"string\")\n> ```\n>\n> **Deliverable**: A one-sentence response.\n\n### 2.2 Unicode Encodings\n\nWhile the Unicode standard defines a mapping from characters to code points (integers), it's impractical to train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around 150K items) and sparse (since many characters are quite rare). Instead, we'll use a Unicode encoding, which converts a Unicode character into a sequence of bytes. The Unicode standard itself defines three encodings: UTF-8, UTF-16, and UTF-32, with UTF-8 being the dominant encoding for the Internet (more than 98% of all webpages).\n\nTo encode a Unicode string into UTF-8, we can use the `encode()` function in Python. To access the underlying byte values for a Python `bytes` object, we can iterate over it (e.g., call `list()`). Finally, we can use the `decode()` function to decode a UTF-8 byte string into a Unicode string.\n\n```python\n>>> test_string = \"hello! \u3053\u3093\u306b\u3061\u306f!\"\n>>> utf8_encoded = test_string.encode(\"utf-8\")\n>>> print(utf8_encoded)\nb'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n>>> print(type(utf8_encoded))\n<class 'bytes'>\n>>> # Get the byte values for the encoded string (integers from 0 to 255).\n>>> list(utf8_encoded)\n[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n>>> # One byte does not necessarily correspond to one Unicode character!\n>>> print(len(test_string))\n13\n>>> print(len(utf8_encoded))\n23\n>>> print(utf8_encoded.decode(\"utf-8\"))\nhello! \u3053\u3093\u306b\u3061\u306f!\n```\n\nBy converting our Unicode codepoints into a sequence of bytes (e.g., via the UTF-8 encoding), we are essentially taking a sequence of codepoints (integers in the range 0 to 1,114,111) and transforming it into a sequence of byte values (integers in the range 0 to 255). The 256-length byte vocabulary is *much* more manageable to deal with. When using byte-level tokenization, we do not need to worry about out-of-vocabulary tokens, since we know that *any* input text can be expressed as a sequence of integers from 0 to 255.\n\n> **Problem (`unicode2`): Unicode Encodings (3 points)**\n>\n> (a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n>\n> **Deliverable**: A one-to-two sentence response.\n>\n> (b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n>\n> ```python\n> def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n>     return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n>\n> >>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n> 'hello'\n> ```\n>\n> **Deliverable**: An example input byte string for which `decode_utf8_bytes_to_str_wrong` produces incorrect output, with a one-sentence explanation of why the function is incorrect.\n>\n> (c) Give a two byte sequence that does not decode to any Unicode character(s).\n>\n> **Deliverable**: An example, with a one-sentence explanation.\n\n### 2.3 Subword Tokenization\n\nWhile byte-level tokenization can alleviate the out-of-vocabulary issues faced by word-level tokenizers, tokenizing text into bytes results in extremely long input sequences. This slows down model training, since a sentence with 10 words might only be 10 tokens long in a word-level language model, but could be 50 or more tokens long in a character-level model (depending on the length of the words). Processing these longer sequences requires more computation at each step of the model. Furthermore, language modeling on byte sequences is difficult because the longer input sequences create long-term dependencies in the data.\n\nSubword tokenization is a midpoint between word-level tokenizers and byte-level tokenizers. Note that a byte-level tokenizer's vocabulary has 256 entries (byte values are 0 to 255). A subword tokenizer trades-off a larger vocabulary size for better compression of the input byte sequence. For example, if the byte sequence `b'the'` often occurs in our raw text training data, assigning it an entry in the vocabulary would reduce this 3-token sequence to a single token.\n\nHow do we select these subword units to add to our vocabulary? Sennrich et al. [2016] propose to use byte-pair encoding (BPE; Gage, 1994), a compression algorithm that iteratively replaces (\"merges\") the most frequent pair of bytes with a single, new unused index. Note that this algorithm adds subword tokens to our vocabulary to maximize the compression of our input sequences---if a word occurs in our input text enough times, it'll be represented as a single subword unit.\n\nSubword tokenizers with vocabularies constructed via BPE are often called BPE tokenizers. In this assignment, we'll implement a byte-level BPE tokenizer, where the vocabulary items are bytes or merged sequences of bytes, which give us the best of both worlds in terms of out-of-vocabulary handling and manageable input sequence lengths. The process of constructing the BPE tokenizer vocabulary is known as \"training\" the BPE tokenizer.\n\n### 2.4 BPE Tokenizer Training\n\nThe BPE tokenizer training procedure consists of three main steps.\n\n**Vocabulary initialization** In this assignment, we represent the tokenizer vocabulary as `id_to_token: dict[int, bytes]` (a one-to-one mapping from integer ID to bytestring token). Since we're training a byte-level BPE tokenizer, our initial vocabulary is simply the set of all bytes. Since there are 256 possible byte values, our initial vocabulary is of size 256. During tokenization, you can build the inverse mapping `token_to_id: dict[bytes, int]` from this dictionary for fast encoding.\n\n**Pre-tokenization** Once you have a vocabulary, you could, in principle, count how often bytes occur next to each other in your text and begin merging them starting with the most frequent pair of bytes. However, this is quite computationally expensive, since we'd have to go take a full pass over the corpus each time we merge. In addition, directly merging bytes across the corpus may result in tokens that differ only in punctuation (e.g., `dog!` vs. `dog.`). These tokens would get completely different token IDs, even though they are likely to have high semantic similarity (since they differ only in punctuation).\n\nTo avoid this, we *pre-tokenize* the corpus. You can think of this as a coarse-grained tokenization over the corpus that helps us count how often pairs of characters appear. For example, the word `'text'` might be a pre-token that appears 10 times. In this case, when we count how often the characters 't' and 'e' appear next to each other, we will see that the word 'text' has 't' and 'e' adjacent and we can increment their count by 10 instead of looking through the corpus. Since we're training a byte-level BPE model, each pre-token is represented as a sequence of UTF-8 bytes.\n\nThe original BPE implementation of Sennrich et al. [2016] pre-tokenizes by simply splitting on whitespace (i.e., `s.split(\" \")`). In contrast, we'll use a regex-based pre-tokenizer (used by GPT-2; Radford et al., 2019) from [github.com/openai/tiktoken/pull/234/files](https://github.com/openai/tiktoken/pull/234/files):\n\n```python\n>>> PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n```\n\nIt may be useful to interactively split some text with this pre-tokenizer to get a better sense of its behavior:\n\n```python\n>>> # requires `regex` package\n>>> import regex as re\n>>> re.findall(PAT, \"some text that i'll pre-tokenize\")\n['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']\n```\n\nWhen using it in your code, however, you should use `re.finditer` to avoid storing the pre-tokenized words as you construct your mapping from pre-tokens to their counts.\nBecause the pattern includes an optional leading space (` ?`), this pre-tokenization is context-sensitive at chunk boundaries: splitting text arbitrarily can change how the first token in a chunk is segmented.\n\n**Compute BPE merges** Now that we've converted our input text into pre-tokens and represented each pre-token as a sequence of UTF-8 bytes, we can compute the BPE merges (i.e., train the BPE tokenizer). At a high level, the BPE algorithm iteratively counts every adjacent pair of symbols (initially bytes; later merged byte sequences) and identifies the pair with the highest frequency (\"A\", \"B\"). Every occurrence of this most frequent pair (\"A\", \"B\") is then *merged*, i.e., replaced with a new token \"AB\". This new merged token is added to our vocabulary; as a result, the final vocabulary after BPE training is the size of the initial vocabulary (256 in our case), plus the number of BPE merge operations performed during training. For efficiency during BPE training, we do not consider pairs that cross pre-token boundaries.[^2] When computing merges, deterministically break ties in pair frequency by *preferring the lexicographically greater pair*. Interpret this ordering directly on byte-sequence pairs (e.g., `tuple[bytes, bytes]` in Python), not on token IDs. For example, if the pairs (\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), and (\"BA\", \"A\") all have the highest frequency, we'd merge (\"BA\", \"A\"):\n\n```python\n>>> max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])\n('BA', 'A')\n```\n\n[^2]: Note that the original BPE formulation [Sennrich et al., 2016] specifies the inclusion of an end-of-word token. We do not add an end-of-word-token when training byte-level BPE models because all bytes (including whitespace and punctuation) are included in the model's vocabulary. Since we're explicitly representing spaces and punctuation, the learned BPE merges will naturally reflect these word boundaries.\n\n**Special tokens** Often, some strings (e.g., `<|endoftext|>`) are used to encode metadata (e.g., boundaries between documents). When encoding text, it's often desirable to treat some strings as \"special tokens\" that should never be split into multiple tokens (i.e., will always be preserved as a single token). For example, the end-of-sequence string `<|endoftext|>` should always be preserved as a single token (i.e., a single integer ID), so we know when to stop generating from the language model. These special tokens must be added to the vocabulary, so they have a corresponding fixed token ID.\n\nAlgorithm 1 of Sennrich et al. [2016] contains an inefficient implementation of BPE tokenizer training (essentially following the steps that we outlined above). As a first exercise, it may be useful to implement and test this function to test your understanding.\n\n> **Example (`bpe_example`): BPE training example**\n>\n> Here is a stylized example from Sennrich et al. [2016]. Consider a corpus consisting of the following text\n>\n> ```\n> low low low low low\n> lower lower widest widest widest\n> newest newest newest newest newest newest\n> ```\n>\n> and the vocabulary has a special token `<|endoftext|>`.\n>\n> **Vocabulary** We initialize our vocabulary with our special token `<|endoftext|>` and the 256 byte values.\n>\n> **Pre-tokenization** For simplicity and to focus on the merge procedure, we assume in this example that pretokenization simply splits on whitespace. When we pretokenize and count, we end up with the frequency table:\n>\n> ```\n> {low: 5, lower: 2, widest: 3, newest: 6}\n> ```\n>\n> It is convenient to represent this as a `dict[tuple[bytes], int]`, e.g. `{(b\"l\", b\"o\", b\"w\"): 5 ...}`. Note that even a single byte is a `bytes` object in Python. There is no `byte` type in Python to represent a single byte, just as there is no `char` type in Python to represent a single character.\n>\n> **Merges** We first look at every successive pair of bytes and sum the frequency of the words where they appear {`b\"lo\"`: 7, `b\"ow\"`: 7, `b\"we\"`: 8, `b\"er\"`: 2, `b\"wi\"`: 3, `b\"id\"`: 3, `b\"de\"`: 3, `b\"es\"`: 9, `b\"st\"`: 9, `b\"ne\"`: 6, `b\"ew\"`: 6}. The pair (`b\"es\"`) and (`b\"st\"`) are tied, so we take the lexicographically greater pair, (`b\"st\"`). We would then merge the pre-tokens so that we end up with {(b\"l\", b\"o\", b\"w\"): 5, (b\"l\", b\"o\", b\"w\", b\"e\", b\"r\"): 2, (b\"w\", b\"i\", b\"d\", b\"e\", b\"st\"): 3, (b\"n\", b\"e\", b\"w\", b\"e\", b\"st\"): 6}.\n>\n> In the second round, we see that (`b\"e\"`, `b\"st\"`) is the most common pair (with a count of 9) and we would merge into {(b\"l\", b\"o\", b\"w\"): 5, (b\"l\", b\"o\", b\"w\", b\"e\", b\"r\"): 2, (b\"w\", b\"i\", b\"d\", b\"est\"): 3, (b\"n\", b\"e\", b\"w\", b\"est\"): 6}. Continuing this, the sequence of merges we get in the end will be ['s t', 'e st', 'o w', 'l ow', 'w est', 'n e', 'ne west', 'w i', 'wi d', 'wid est', 'low e', 'lowe r'].\n>\n> If we take 6 merges, we have ['s t', 'e st', 'o w', 'l ow', 'w est', 'n e'] and our vocabulary elements would be [`<|endoftext|>`, [...256 BYTE CHARS], st, est, ow, low, west, ne].\n>\n> With this vocabulary and set of merges, the word `newest` would tokenize as [ne, west].\n\n### 2.5 Experimenting with BPE Tokenizer Training\n\nLet's train a byte-level BPE tokenizer on the TinyStories dataset. Instructions to find / download the dataset can be found in Section 1. Before you start, we recommend taking a look at the TinyStories dataset to get a sense of what's in the data.\n\n**Parallelizing pre-tokenization** You will find that a major bottleneck is the pre-tokenization step. You can speed up pre-tokenization by parallelizing your code with the built-in library `multiprocessing`. Concretely, we recommend that in parallel implementations of pre-tokenization, you chunk the corpus while ensuring your chunk boundaries occur at the beginning of a special token. You are free to use the starter code at the following link verbatim to obtain chunk boundaries, which you can then use to distribute work across your processes:\n\n> [https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336\\_basics/pretokenization\\_example.py](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py)\n\nThis chunking will always be valid, since we never want to merge across document boundaries. For the purposes of the assignment, you can always split in this way. Don't worry about the edge case of receiving a very large corpus that does not contain `<|endoftext|>`.\n\n**Removing special tokens before pre-tokenization** Before running pre-tokenization with the regex pattern (using `re.finditer`), you should strip out all special tokens from your corpus (or your chunk, if using a parallel implementation). Make sure that you **split** on your special tokens, so that no merging can occur across the text they delimit. For example, if you have a corpus (or chunk) like [Doc 1]`<|endoftext|>`[Doc 2], you should split on the special token `<|endoftext|>`, and pre-tokenize [Doc 1] and [Doc 2] separately, so that no merging can occur across the document boundary. This can be done using `re.split` with `\"|\".join(special_tokens)` as the delimiter (with careful use of `re.escape` since `|` may occur in the special tokens). The test `test_train_bpe_special_tokens` will test for this.\n\n**Optimizing the merging step** The naive implementation of BPE training in the stylized example above is slow because for every merge, it iterates over all byte pairs to identify the most frequent pair. However, the only pair counts that change after each merge are those that overlap with the merged pair. Thus, BPE training speed can be improved by indexing the counts of all pairs and incrementally updating these counts, rather than explicitly iterating over each pair of bytes to count pair frequencies. You can get significant speedups with this caching procedure, though we note that the merging part of BPE training is *not* parallelizable in Python.\n\n> **Low-Resource/Downscaling Tip: Profiling**\n>\n> You should use profiling tools like `cProfile` or `scalene` to identify the bottlenecks in your implementation, and focus on optimizing those.\n\n> **Low-Resource/Downscaling Tip: \"Downscaling\"**\n>\n> Instead of jumping to training your tokenizer on the full TinyStories dataset, we recommend you first train on a small subset of the data: a \"debug dataset\". For example, you could train your tokenizer on the TinyStories validation set instead, which is 22K documents instead of 2.12M. This illustrates a general strategy of downscaling whenever possible to speed up development: for example, using smaller datasets, smaller model sizes, etc. Choosing the size of the debug dataset or hyperparameter config requires careful consideration: you want your debug set to be large enough to have the same bottlenecks as the full configuration (so that the optimizations you make will generalize), but not so big that it takes forever to run.\n\n> **Problem (`train_bpe`): BPE Tokenizer Training (15 points)**\n>\n> **Deliverable**: Write a function that, given a path to an input text file, trains a (byte-level) BPE tokenizer. Your BPE training function should handle (at least) the following input parameters:\n>\n> **`input_path`**: `str` Path to a text file with BPE tokenizer training data.\n>\n> **`vocab_size`**: `int` A positive integer that defines the maximum final vocabulary size (including the initial byte vocabulary, vocabulary items produced from merging, and any special tokens).\n>\n> **`special_tokens`**: `list[str]` A list of strings to add to the vocabulary. These special tokens do not otherwise affect BPE training.\n>\n> Your BPE training function should return the resulting vocabulary and merges:\n>\n> **`id_to_token`** (`vocab` in tests/adapters): `dict[int, bytes]` The tokenizer vocabulary, a mapping from `int` (token ID in the vocabulary) to `bytes` (token bytes).\n>\n> **`merges`**: `list[tuple[bytes, bytes]]` A list of BPE merges produced from training. Each list item is a `tuple` of `bytes` (`<token1>`, `<token2>`), representing that `<token1>` was merged with `<token2>`. The merges should be ordered by order of creation.\n>\n> To test your BPE training function against our provided tests, you will first need to implement the test adapter at \\[`adapters.run_train_bpe`\\]. Then, run `uv run pytest tests/test_train_bpe.py`. Your implementation should be able to pass all tests. Optionally (this could be a large time-investment), you can implement the key parts of your training method using some systems language, for instance C++ (consider `cppyy` for this) or Rust (using PyO3). If you do this, be aware of which operations require copying vs reading directly from Python memory, and make sure to leave build instructions, or make sure it builds using only `pyproject.toml`. Also note that the GPT-2 regex is not well-supported in most regex engines and will be too slow in most that do. We have verified that Oniguruma is reasonably fast and supports negative lookahead, but the `regex` package in Python is, if anything, even faster.\n\n> **Problem (`train_bpe_tinystories`): BPE Training on TinyStories (2 points)**\n>\n> (a) Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories `<|endoftext|>` special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n>\n> **Resource requirements**: $\\leq$ 30 minutes (no GPUs), $\\leq$ 30GB RAM\n>\n> **Hint** You should be able to get under 2 minutes for BPE training using `multiprocessing` during pretokenization and the following two facts:\n>\n> (a) The `<|endoftext|>` token delimits documents in the data files.\n>\n> (b) The `<|endoftext|>` token is handled as a special case before the BPE merges are applied.\n>\n> **Deliverable**: A one-to-two sentence response.\n>\n> (b) Profile your code. What part of the tokenizer training process takes the most time?\n>\n> **Deliverable**: A one-to-two sentence response.\n\nNext, we'll try training a byte-level BPE tokenizer on the OpenWebText dataset. As before, we recommend taking a look at the dataset to better understand its contents.\n\n> **Problem (`train_bpe_expts_owt`): BPE Training on OpenWebText (2 points)**\n>\n> (a) Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What is the longest token in the vocabulary? Does it make sense?\n>\n> **Resource requirements**: $\\leq$ 12 hours (no GPUs), $\\leq$ 100GB RAM\n>\n> **Deliverable**: A one-to-two sentence response.\n>\n> (b) Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\n>\n> **Deliverable**: A one-to-two sentence response.\n\n### 2.6 BPE Tokenizer: Encoding and Decoding\n\nIn the previous part of the assignment, we implemented a function to train a BPE tokenizer on input text to obtain a tokenizer vocabulary and a list of BPE merges. Now, we will implement a BPE tokenizer that loads a provided vocabulary and list of merges and uses them to encode and decode text to/from token IDs.\n\n#### 2.6.1 Encoding text\n\nThe process of encoding text by BPE mirrors how we train the BPE vocabulary. There are a few major steps.\n\n**Step 1: Pre-tokenize.** We first pre-tokenize the sequence and represent each pre-token as a sequence of UTF-8 bytes, just as we did in BPE training. We will be merging these bytes within each pre-token into vocabulary elements, handling each pre-token independently (no merges across pre-token boundaries).\n\n**Step 2: Apply the merges.** We then take the sequence of vocabulary element merges created during BPE training, and apply it to our pre-tokens *in the same order of creation*. For each merge rule `(A, B)`, merge all adjacent occurrences of `(A, B)` in the current pre-token before moving to the next merge rule.\n\n> **Example (`bpe_encoding`): BPE encoding example**\n>\n> For example, suppose our input string is `'the cat ate'`, our vocabulary is {0: `b' '`, 1: `b'a'`, 2: `b'c'`, 3: `b'e'`, 4: `b'h'`, 5: `b't'`, 6: `b'th'`, 7: `b' c'`, 8: `b' a'`, 9: `b'the'`, 10: `b' at'`}, and our learned merges are [(`b't'`, `b'h'`), (`b' '`, `b'c'`), (`b' '`, `b'a'`), (`b'th'`, `b'e'`), (`b' a'`, `b't'`)]. First, our pre-tokenizer would split this string into `['the', ' cat', ' ate']`. Then, we'll look at each pre-token and apply the BPE merges.\n>\n> The first pre-token `'the'` is initially represented as [`b't'`, `b'h'`, `b'e'`]. Looking at our list of merges, we identify the first applicable merge to be (`b't'`, `b'h'`), and use that to transform the pre-token into [`b'th'`, `b'e'`]. Then, we go back to the list of merges and identify the next applicable merge to be (`b'th'`, `b'e'`), which transforms the pre-token into [`b'the'`]. Finally, looking back at the list of merges, we see that there are no more that apply to the string (since the entire pre-token has been merged into a single token), so we are done applying the BPE merges. The corresponding integer sequence is [9].\n>\n> Repeating this process for the remaining pre-tokens, we see that the pre-token `' cat'` is represented as [`b' c'`, `b'a'`, `b't'`] after applying the BPE merges, which becomes the integer sequence [7, 1, 5]. The final pre-token `' ate'` is [`b' at'`, `b'e'`] after applying the BPE merges, which becomes the integer sequence [10, 3].\n>\n> Thus, the final result of encoding our input string is [9, 7, 1, 5, 10, 3].\n\n**Special tokens.** Your tokenizer should be able to properly handle user-defined special tokens when encoding text (provided when constructing the tokenizer).\n\n**Memory considerations.** Suppose we want to tokenize a large text file that we cannot fit in memory. To efficiently tokenize this large file (or any other stream of data), we need to break it up into manageable chunks and process each chunk in-turn, so that the memory complexity is constant as opposed to linear in the size of the text. In doing so, we need to make sure that a token doesn't cross chunk boundaries, else we'll get a different tokenization than the naive method of tokenizing the entire sequence in-memory.\n\n#### 2.6.2 Decoding text\n\nTo decode a sequence of integer token IDs back to raw text, we can simply look up each ID's corresponding entries in the vocabulary (a byte sequence), concatenate them together, and then decode the bytes to a Unicode string. Note that input IDs are not guaranteed to map to valid Unicode strings (since a user could input any sequence of integer IDs). In the case that the input token IDs do not produce a valid Unicode string, you should replace the malformed bytes with the official Unicode replacement character U+FFFD.[^3] The `errors` argument of `bytes.decode` controls how Unicode decoding errors are handled, and using `errors='replace'` will automatically replace malformed data with the replacement marker.\n\n[^3]: See [en.wikipedia.org/wiki/Specials\\_(Unicode\\_block)#Replacement\\_character](https://en.wikipedia.org/wiki/Specials_(Unicode_block)#Replacement_character) for more information about the Unicode replacement character.\n\n> **Problem (`tokenizer`): Implementing the tokenizer (15 points)**\n>\n> **Deliverable**: Implement a `Tokenizer` class that, given a vocabulary and a list of merges, encodes text into integer IDs and decodes integer IDs into text. Your tokenizer should also support user-provided special tokens (appending them to the vocabulary if they aren't already there). We recommend the following interface:\n>\n> **`def __init__(self, vocab, merges, special_tokens=None)`** Construct a tokenizer from a given vocabulary, list of merges, and (optionally) a list of special tokens. This function should accept the following parameters:\n>\n> ```\n> vocab: dict[int, bytes]\n> merges: list[tuple[bytes, bytes]]\n> special_tokens: list[str] | None = None\n> ```\n>\n> Here, `vocab` is your `id_to_token` mapping. In `__init__`, build `token_to_id: dict[bytes, int]` from `id_to_token` for encoding, and use `id_to_token` for decoding.\n>\n> **`def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)`** Class method that constructs and return a `Tokenizer` from a serialized vocabulary and list of merges (in the same format that your BPE training code output) and (optionally) a list of special tokens. This method should accept the following additional parameters:\n>\n> ```\n> vocab_filepath: str\n> merges_filepath: str\n> special_tokens: list[str] | None = None\n> ```\n>\n> **`def encode(self, text: str) -> list[int]`** Encode an input text into a sequence of token IDs.\n>\n> **`def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]`** Given an iterable of strings (e.g., a Python file handle), return a generator that lazily yields token IDs. This is required for memory-efficient tokenization of large files that we cannot directly load into memory.\n>\n> **`def decode(self, ids: list[int]) -> str`** Decode a sequence of token IDs into text.\n>\n> To test your `Tokenizer` against our provided tests, you will first need to implement the test adapter at \\[`adapters.get_tokenizer`\\]. Then, run `uv run pytest tests/test_tokenizer.py`. Your implementation should be able to pass all tests.\n\n### 2.7 Experiments\n\n> **Problem (`tokenizer_experiments`): Experiments with tokenizers (4 points)**\n>\n> (a) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer's compression ratio (bytes/token)?\n>\n> **Deliverable**: A one-to-two sentence response.\n>\n> (b) What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\n>\n> **Deliverable**: A one-to-two sentence response.\n>\n> (c) Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\n>\n> **Deliverable**: A one-to-two sentence response.\n>\n> (d) Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We'll use this later to train our language model. We recommend serializing the token IDs as a NumPy array of datatype `uint16`. Why is `uint16` an appropriate choice?\n>\n> **Deliverable**: A one-to-two sentence response.\n\n---\n\n## 3 Transformer Language Model Architecture\n\nA language model takes as input a batched sequence of integer token IDs (i.e., `torch.Tensor` of shape (`batch_size`, `sequence_length`)), and returns a (batched) tensor of next-token logits over the vocabulary (i.e., a PyTorch tensor of shape (`batch_size`, `sequence_length`, `vocab_size`)). Probabilities are obtained by applying softmax to these logits when needed (e.g., for loss computation or decoding). When generating text from the language model during inference, we take the predicted next-token distribution from the final time step (i.e., the last item in the sequence) to generate the next token in the sequence (e.g., by taking the token with the highest probability, sampling from the distribution, etc.), add the generated token to the input sequence, and repeat.\n\nIn this part of the assignment, you will build this Transformer language model from scratch. We will begin with a high-level description of the model before progressively detailing the individual components.\n\n### 3.1 Transformer LM\n\nGiven a sequence of token IDs, the Transformer language model uses an input embedding to convert token IDs to dense vectors, passes the embedded tokens through `num_layers` Transformer blocks, and then applies a learned linear projection (the \"output embedding\" or \"LM head\") to produce the predicted next-token logits. See Figure 1 for a schematic representation.\n\n![Figure 1: An overview of the Transformer language model.](images/figure1_transformer_lm_overview.png)\n\n#### 3.1.1 Token Embeddings\n\nIn the very first step, the Transformer *embeds* the (batched) sequence of token IDs into a sequence of vectors containing information on the token identity (red blocks in Figure 1).\n\nMore specifically, given a sequence of token IDs, the Transformer language model uses a token embedding layer to produce a sequence of vectors. Each embedding layer takes in a tensor of integers of shape (`batch_size`, `sequence_length`) and produces a sequence of vectors of shape (`batch_size`, `sequence_length`, `d_model`).\n\n#### 3.1.2 Pre-norm Transformer Block\n\nAfter embedding, the activations are processed by several identically structured neural net layers. A standard decoder-only Transformer language model consists of `num_layers` identical layers (commonly called Transformer \"blocks\"). Each Transformer block takes in an input of shape (`batch_size`, `sequence_length`, `d_model`) and returns an output of shape (`batch_size`, `sequence_length`, `d_model`). Each block aggregates information across the sequence (via self-attention) and non-linearly transforms it (via the feed-forward layers).\n\n### 3.2 Output Normalization and Embedding\n\nAfter `num_layers` Transformer blocks, we will take the final activations and turn them into logits over the vocabulary.\n\nWe will implement the \"pre-norm\" Transformer block (detailed in \u00a73.5), which additionally requires the use of layer normalization (detailed below) after the final Transformer block to ensure its outputs are properly scaled.\n\nAfter this normalization, we will use a standard learned linear transformation to convert the output of the Transformer blocks into predicted next-token logits (see, e.g., Radford et al., 2018, equation 2).\n\n### 3.3 Remark: Batching, Einsum and Efficient Computation\n\nThroughout the Transformer, we will be performing the same computation applied to many batch-like inputs. Here are a few examples:\n\n- **Elements of a batch**: we apply the same Transformer `forward` operation on each batch element.\n- **Sequence length**: the \"position-wise\" operations like RMSNorm and feed-forward operate identically on each position of a sequence.\n- **Attention heads**: the attention operation is batched across attention heads in a \"multi-headed\" attention operation.\n\nIt is useful to have an ergonomic way of performing such operations in a way that fully utilizes the GPU, and is easy to read and understand. Many PyTorch operations can take in excess \"batch-like\" dimensions at the start of a tensor and repeat/broadcast the operation across these dimensions efficiently.\n\nFor instance, say we are doing a position-wise, batched operation. We have a \"data tensor\" $D$ of shape (`batch_size`, `sequence_length`, `d_model`), and we would like to do a batched vector-matrix multiply against a matrix $A$ of shape (`d_model`, `d_model`). In this case, `D @ A` will do a batched matrix multiply, which is an efficient primitive in PyTorch, where the (`batch_size`, `sequence_length`) dimensions are batched over.\n\nBecause of this, it is helpful to assume that your functions may be given additional batch-like dimensions and to keep those dimensions at the start of the PyTorch shape. To organize tensors so they can be batched in this manner, they might need to be shaped using many steps of `view`, `reshape` and `transpose`. This can be a bit of a pain, and it often gets hard to read what the code is doing and what the shapes of your tensors are.\n\nA more ergonomic option is to use *einsum notation* within `torch.einsum`, or rather use framework agnostic libraries like `einops` or `einx`. The two key ops are `einsum`, which can do tensor contractions with arbitrary dimensions of input tensors, and `rearrange`, which can reorder, concatenate, and split arbitrary dimensions. It turns out almost all operations in machine learning are some combination of dimension juggling and tensor contraction with the occasional (usually pointwise) nonlinear function. This means that a lot of your code can be more readable and flexible when using einsum notation.\n\nWe **strongly** recommend learning and using einsum notation for the class. Students who have not been exposed to einsum notation before should use `einops` (docs [here](https://einops.rocks/)), and students who are already comfortable with `einops` should learn the more general `einx` ([here](https://einx.readthedocs.io/)).[^4] Both packages are already installed in the environment we've supplied.\n\n[^4]: It's worth noting that while `einops` has a great amount of support, `einx` is not as battle-tested. You should feel free to fall back to using `einops` with some more plain PyTorch if you find any limitations or bugs in `einx`.\n\nHere we give some examples of how einsum notation can be used. These are a supplement to the documentation for einops, which you should read first.\n\n> **Example (`einstein_example1`): Batched matrix multiplication with `einops.einsum`**\n>\n> ```python\n> import torch\n> from einops import rearrange, einsum\n>\n> ## Basic implementation\n> Y = D @ A.T\n> # Hard to tell the input and output shapes and what they mean.\n> # What shapes can D and A have, and do any of these have unexpected behavior?\n>\n> ## Einsum is self-documenting and robust\n> #                   D                       A          ->          Y\n> Y = einsum(D, A, \"batch sequence d_in, d_out d_in -> batch sequence d_out\")\n>\n> ## Or, a batched version where D can have any leading dimensions but A is constrained.\n> Y = einsum(D, A, \"... d_in, d_out d_in -> ... d_out\")\n> ```\n\n> **Example (`einstein_example2`): Broadcasted operations with `einops.rearrange`**\n>\n> We have a batch of images, and for each image we want to generate 10 dimmed versions based on some scaling factor:\n>\n> ```python\n> images = torch.randn(64, 128, 128, 3)  # (batch, height, width, channel)\n> dim_by = torch.linspace(start=0.0, end=1.0, steps=10)\n>\n> ## Reshape and multiply\n> dim_value = rearrange(dim_by,      \"dim_value                -> 1 dim_value 1 1 1\")\n> images_rearr = rearrange(images, \"b height width channel -> b 1 height width channel\")\n> dimmed_images = images_rearr * dim_value\n>\n> ## Or in one go:\n> dimmed_images = einsum(\n>     images, dim_by,\n>     \"batch height width channel, dim_value -> batch dim_value height width channel\"\n> )\n> ```\n\n> **Example (`einstein_example3`): Pixel mixing with `einops.rearrange`**\n>\n> Suppose we have a batch of images represented as a tensor of shape (`batch`, `height`, `width`, `channel`), and we want to perform a linear transformation across all pixels of the image, but this transformation should happen independently for each channel. Our linear transformation is represented as a matrix $B$ of shape (`height` $\\times$ `width`, `height` $\\times$ `width`).\n>\n> ```python\n> channels_last = torch.randn(64, 32, 32, 3)  # (batch, height, width, channel)\n> B = torch.randn(32*32, 32*32)\n>\n> ## Rearrange an image tensor for mixing across all pixels\n> channels_last_flat = channels_last.view(\n>     -1, channels_last.size(1) * channels_last.size(2), channels_last.size(3)\n> )\n>\n> channels_first_flat = channels_last_flat.transpose(1, 2)\n>\n> channels_first_flat_transformed = channels_first_flat @ B.T\n>\n> channels_last_flat_transformed = channels_first_flat_transformed.transpose(1, 2)\n>\n> channels_last_transformed = channels_last_flat_transformed.view(*channels_last.shape)\n> ```\n>\n> Instead, using einops:\n>\n> ```python\n> height = width = 32\n> ## Rearrange replaces clunky torch view + transpose\n> channels_first = rearrange(\n>     channels_last,\n>     \"batch height width channel -> batch channel (height width)\"\n> )\n> channels_first_transformed = einsum(\n>     channels_first, B,\n>     \"batch channel pixel_in, pixel_out pixel_in -> batch channel pixel_out\"\n> )\n> channels_last_transformed = rearrange(\n>     channels_first_transformed,\n>     \"batch channel (height width) -> batch height width channel\",\n>     height=height, width=width\n> )\n> ```\n>\n> Or, if you're feeling crazy: all in one go using `einx.dot` (einx equivalent of `einops.einsum`)\n>\n> ```python\n> height = width = 32\n> channels_last_transformed = einx.dot(\n>     \"batch row_in col_in channel, (row_out col_out) (row_in col_in)\"\n>     \"-> batch row_out col_out channel\",\n>     channels_last, B,\n>     col_in=width, col_out=width\n> )\n> ```\n\nThe first implementation here could be improved by placing comments before and after to indicate what the input and output shapes are, but this is clunky and susceptible to bugs. With einsum notation, documentation *is* implementation!\n\nEinsum notation can handle arbitrary input batching dimensions, but also has the key benefit of being *self-documenting*. It's much clearer what the relevant shapes of your input and output tensors are in code that uses einsum notation. For the remaining tensors, you can consider using Tensor type hints, for instance using the [jaxtyping library](https://github.com/google/jaxtyping) (not specific to Jax).\n\nWe will talk more about the performance implications of using einsum notation in assignment 2, but for now know that they're almost always better than the alternative!\n\n#### 3.3.1 Mathematical Notation and Memory Ordering\n\nIn the Transformer sections below, we will use the common ML convention that **tokens are rows**. This is a tensor-shape convention, not a requirement imposed by row-major memory layout. With row vectors, a linear transformation looks like\n\n$$y = xW^{\\top},\\tag{1}$$\n\nfor $W \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}$ and row-vector $x \\in \\mathbb{R}^{1 \\times d_{\\text{in}}}$. In PyTorch code, activations are usually stored with feature dimension last (`(..., d_in)`), so with weights stored as `W` of shape (`d_out`, `d_in`), the same map is typically written as `x @ W.T`.\n\nThe equivalent column-vector form is\n\n$$y = Wx,\\tag{2}$$\n\nbut we will keep using row-vector notation in this handout for consistency with implementation shapes.\n\n### 3.4 Basic Building Blocks: Linear and Embedding Modules\n\n#### 3.4.1 Parameter Initialization\n\nTraining neural networks effectively often requires careful initialization of the model parameters---bad initializations can lead to undesirable behavior such as vanishing or exploding gradients. Pre-norm transformers are unusually robust to initializations, but they can still have a significant impact on training speed and convergence. Since this assignment is already long, we will save the details for assignment 3, and instead give you some approximate initializations that should work well for most cases. For now, use:\n\n- Linear weights: $\\mathcal{N}\\left(\\mu = 0, \\sigma^2 = \\frac{2}{d_{\\text{in}} + d_{\\text{out}}}\\right)$ truncated at $[-3\\sigma, 3\\sigma]$.\n- Embedding: $\\mathcal{N}\\left(\\mu = 0, \\sigma^2 = 1\\right)$ truncated at $[-3, 3]$\n- RMSNorm: $\\mathbb{1}$\n\nYou should use `torch.nn.init.trunc_normal_` to initialize the truncated normal weights.\n\n#### 3.4.2 Linear Module\n\nLinear layers are a fundamental building block of Transformers and neural nets in general. First, you will implement your own `Linear` class that inherits from `torch.nn.Module` and performs a linear transformation:\n\n$$y = xW^{\\top}.\\tag{3}$$\n\nNote that we do not include a bias term, following most modern LLMs.\n\n> **Problem (`linear`): Implementing the linear module (1 point)**\n>\n> **Deliverable**: Implement a `Linear` class that inherits from `torch.nn.Module` and performs a linear transformation. Your implementation should follow the interface of PyTorch's built-in `nn.Linear` module, except for not having a `bias` argument or parameter. We recommend the following interface:\n>\n> **`def __init__(self, in_features, out_features, device=None, dtype=None)`** Construct a linear transformation module. This function should accept the following parameters:\n>\n> > **`in_features`**: `int` final dimension of the input\n> >\n> > **`out_features`**: `int` final dimension of the output\n> >\n> > **`device`**: `torch.device | None = None` Device to store the parameters on\n> >\n> > **`dtype`**: `torch.dtype | None = None` Data type of the parameters\n>\n> **`def forward(self, x: torch.Tensor) -> torch.Tensor`** Apply the linear transformation to the input.\n>\n> Make sure to:\n>\n> - subclass `nn.Module`\n> - call the superclass constructor\n> - construct and store your parameter as $W$ (not $W^{\\top}$) for memory ordering reasons, putting it in an `nn.Parameter`\n> - of course, don't use `nn.Linear` or `nn.functional.linear`\n>\n> For initializations, use the settings from above along with `torch.nn.init.trunc_normal_` to initialize the weights.\n>\n> To test your `Linear` module, implement the test adapter at \\[`adapters.run_linear`\\]. The adapter should load the given weights into your `Linear` module. You can use `Module.load_state_dict` for this purpose. Then, run `uv run pytest -k test_linear`.\n\n#### 3.4.3 Embedding Module\n\nAs discussed above, the first layer of the Transformer is an embedding layer that maps integer token IDs into a vector space of dimension `d_model`. We will implement a custom `Embedding` class that inherits from `torch.nn.Module` (so you should not use `nn.Embedding`). The `forward` method should select the embedding vector for each token ID by indexing into an embedding matrix of shape (`vocab_size`, `d_model`) using a `torch.LongTensor` of token IDs with shape (`batch_size`, `sequence_length`).\n\n> **Problem (`embedding`): Implement the embedding module (1 point)**\n>\n> **Deliverable**: Implement the `Embedding` class that inherits from `torch.nn.Module` and performs an embedding lookup. Your implementation should follow the interface of PyTorch's built-in `nn.Embedding` module. We recommend the following interface:\n>\n> **`def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None)`** Construct an embedding module. This function should accept the following parameters:\n>\n> > **`num_embeddings`**: `int` Size of the vocabulary\n> >\n> > **`embedding_dim`**: `int` Dimension of the embedding vectors, i.e., $d_{\\text{model}}$\n> >\n> > **`device`**: `torch.device | None = None` Device to store the parameters on\n> >\n> > **`dtype`**: `torch.dtype | None = None` Data type of the parameters\n>\n> **`def forward(self, token_ids: torch.Tensor) -> torch.Tensor`** Lookup the embedding vectors for the given token IDs.\n>\n> Make sure to:\n>\n> - subclass `nn.Module`\n> - call the superclass constructor\n> - initialize your embedding matrix as a `nn.Parameter`\n> - store the embedding matrix with the `d_model` being the final dimension\n> - of course, don't use `nn.Embedding` or `nn.functional.embedding`\n>\n> Again, use the settings from above for initialization, and use `torch.nn.init.trunc_normal_` to initialize the weights.\n>\n> To test your implementation, implement the test adapter at \\[`adapters.run_embedding`\\]. Then, run `uv run pytest -k test_embedding`.\n\n### 3.5 Pre-Norm Transformer Block\n\nEach Transformer block has two sub-layers: a multi-head self-attention mechanism and a position-wise feed-forward network (Vaswani et al., 2017, section 3.1).\n\nIn the original Transformer paper, the model uses a residual connection around each of the two sub-layers, followed by layer normalization. This architecture is commonly known as the \"post-norm\" Transformer, since layer normalization is applied to the sublayer output. However, a variety of work has found that moving layer normalization from the output of each sub-layer to the input of each sub-layer (with an additional layer normalization after the final Transformer block) improves Transformer training stability [Nguyen and Salazar, 2019; Xiong et al., 2020]---see Figure 2 for a visual representation of this \"pre-norm\" Transformer block. The output of each Transformer block sub-layer is then added to the sub-layer input via the residual connection (Vaswani et al., 2017, section 5.4). An intuition for pre-norm is that there is a clean \"residual stream\" without any normalization going from the input embeddings to the final output of the Transformer, which is purported to improve gradient flow. This pre-norm Transformer is now the standard used in language models today (e.g., GPT-3, LLaMA, PaLM, etc.), so we will implement this variant. We will walk through each of the components of a pre-norm Transformer block, implementing them in sequence.\n\n![Figure 2: A pre-norm Transformer block.](images/figure2_pre_norm_transformer_block.png)\n\n#### 3.5.1 Root Mean Square Layer Normalization\n\nThe original Transformer implementation of Vaswani et al. [2017] uses layer normalization [Ba et al., 2016] to normalize activations. Following Touvron et al. [2023], we will use root mean square layer normalization (RMSNorm; Zhang and Sennrich, 2019, equation 4) for layer normalization. Given a vector $a \\in \\mathbb{R}^{d_{\\text{model}}}$ of activations, RMSNorm will rescale each activation $a_i$ as follows:\n\n$$\\text{RMSNorm}(a_i) = \\frac{a_i}{\\text{RMS}(a)} g_i,\\tag{4}$$\n\nwhere $\\text{RMS}(a) = \\sqrt{\\frac{1}{d_{\\text{model}}} \\sum_{i=1}^{d_{\\text{model}}} a_i^2 + \\varepsilon}$. Here, $g_i$ is a learnable \"gain\" parameter (there are `d_model` such parameters total), and $\\varepsilon$ is a hyperparameter that is often fixed at 1e-5.\n\nYou should upcast your input to `torch.float32` to prevent overflow when you square the input. Overall, your `forward` method should look like:\n\n```python\nin_dtype = x.dtype\nx = x.to(torch.float32)\n\n# Your code here performing RMSNorm\n...\nresult = ...\n\n# Return the result in the original dtype\nreturn result.to(in_dtype)\n```\n\n> **Problem (`rmsnorm`): Root Mean Square Layer Normalization (1 point)**\n>\n> **Deliverable**: Implement RMSNorm as a `torch.nn.Module`. We recommend the following interface:\n>\n> **`def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None)`** Construct the RMSNorm module. This function should accept the following parameters:\n>\n> > **`d_model`**: `int` Hidden dimension of the model\n> >\n> > **`eps`**: `float = 1e-5` Epsilon value for numerical stability\n> >\n> > **`device`**: `torch.device | None = None` Device to store the parameters on\n> >\n> > **`dtype`**: `torch.dtype | None = None` Data type of the parameters\n>\n> **`def forward(self, x: torch.Tensor) -> torch.Tensor`** Process an input tensor of shape (`batch_size`, `sequence_length`, `d_model`) and return a tensor of the same shape.\n>\n> **Note:** Remember to upcast your input to `torch.float32` before performing the normalization (and later downcast to the original dtype), as described above.\n>\n> To test your implementation, implement the test adapter at \\[`adapters.run_rmsnorm`\\]. Then, run `uv run pytest -k test_rmsnorm`.\n\n#### 3.5.2 Position-Wise Feed-Forward Network\n\nIn the original Transformer paper (section 3.3 of Vaswani et al. [2017]), the Transformer feed-forward network consists of two linear transformations with a ReLU activation ($\\text{ReLU}(x) = \\max(0, x)$) between them. The dimensionality of the inner feed-forward layer is typically 4x the input dimensionality.\n\nHowever, modern language models tend to incorporate two main changes compared to this original design: they use another activation function and employ a gating mechanism. Specifically, we will implement the \"SwiGLU\" activation function adopted in LLMs like Llama 3 [Grattafiori et al., 2024] and Qwen 2.5 [Yang et al., 2024], which combines the SiLU (often called Swish) activation with a Gated Linear Unit (GLU). We will also omit the bias terms sometimes used in linear layers, following most modern LLMs since PaLM [Chowdhery et al., 2022] and LLaMA [Touvron et al., 2023].\n\nThe SiLU or Swish activation function [Elfwing et al., 2017] is defined as follows:\n\n$$\\text{SiLU}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\\tag{5}$$\n\nAs can be seen in Figure 3, the SiLU activation function is similar to the ReLU activation function, but is smooth at zero.\n\n![Figure 3: Comparing the SiLU (aka Swish) and ReLU activation functions.](images/figure3_silu_vs_relu.png)\n\nGated Linear Units (GLUs) were originally defined by Dauphin et al. [2017] as the element-wise product of a linear transformation passed through a sigmoid function and another linear transformation:\n\n$$\\text{GLU}(x, W_{\\text{gate}}, W_{\\text{up}}) = \\sigma(xW_{\\text{gate}}^{\\top}) \\odot (xW_{\\text{up}}^{\\top}),\\tag{6}$$\n\nwhere $\\odot$ represents element-wise multiplication. Gated Linear Units are suggested to \"reduce the vanishing gradient problem for deep architectures by providing a linear path for the gradients while retaining non-linear capabilities.\"\n\nPutting the SiLU/Swish and GLU together, we get the SwiGLU, which we will use for our feed-forward networks:\n\n$$\\text{FFN}(x) = \\text{SwiGLU}(x, W_1, W_2, W_3) = (\\text{SiLU}(xW_1^{\\top}) \\odot xW_3^{\\top})W_2^{\\top},\\tag{7}$$\n\nwhere $x \\in \\mathbb{R}^{1 \\times d_{\\text{model}}}$, $W_1, W_3 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$, $W_2 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$, and canonically, $d_{\\text{ff}} = \\frac{8}{3} d_{\\text{model}}$. In this notation, $W_1$ is the gate projection, $W_3$ is the up projection, and $W_2$ is the down/output projection.\n\nShazeer [2020] first proposed combining the SiLU/Swish activation with GLUs and conducted experiments showing that SwiGLU outperforms baselines like ReLU and SiLU (without gating) on language modeling tasks. Later in the assignment, you will compare SwiGLU and SiLU. Though we've mentioned some heuristic arguments for these components (and the papers provide more supporting evidence), it's good to keep an empirical perspective: a now famous quote from Shazeer's paper is\n\n> We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence.\n\n> **Problem (`positionwise_feedforward`): Implement the position-wise feed-forward network (2 points)**\n>\n> **Deliverable**: Implement the SwiGLU feed-forward network, composed of a SiLU activation function and a GLU.\n>\n> **Note:** in this particular case, you should feel free to use `torch.sigmoid` in your implementation for numerical stability.\n>\n> You should set $d_{\\text{ff}}$ to approximately $\\frac{8}{3} \\times d_{\\text{model}}$ in your implementation, while ensuring that the dimensionality of the inner feed-forward layer is a multiple of 64 to make good use of your hardware. To test your implementation against our provided tests, you will need to implement the test adapter at \\[`adapters.run_swiglu`\\]. Then, run `uv run pytest -k test_swiglu` to test your implementation.\n\n#### 3.5.3 Relative Positional Embeddings\n\nTo inject positional information into the model, we will implement Rotary Position Embeddings [Su et al., 2021], often called RoPE. For a given query token row vector $q^{(i)} = x^{(i)}W_q^{\\top} \\in \\mathbb{R}^{1 \\times d}$ at token position $i$, we apply the RoPE rotation on the right, giving $q'^{(i)} = q^{(i)}(R^i)^{\\top} = x^{(i)}W_q^{\\top}(R^i)^{\\top}$. Here, $R^i$ rotates pairs of embedding elements $q_{2k-1:2k}^{(i)}$ as 2d vectors by the angle $\\theta_{i,k} = \\frac{i}{\\Theta^{(2k-2)/d}}$ for $k \\in \\{1, \\ldots, d/2\\}$ and some constant $\\Theta$. This requires $d$ (and therefore `d_k` in implementation) to be even. Thus, we can consider $R^i$ to be a block-diagonal matrix of size $d \\times d$, with blocks $R_k^i$ for $k \\in \\{1, \\ldots, d/2\\}$, with\n\n$$R_k^i = \\begin{bmatrix} \\cos(\\theta_{i,k}) & -\\sin(\\theta_{i,k}) \\\\ \\sin(\\theta_{i,k}) & \\cos(\\theta_{i,k}) \\end{bmatrix}.\\tag{8}$$\n\nThus we get the full rotation matrix\n\n$$R^i = \\begin{bmatrix} R_1^i & 0 & 0 & \\ldots & 0 \\\\ 0 & R_2^i & 0 & \\ldots & 0 \\\\ 0 & 0 & R_3^i & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & R_{d/2}^i \\end{bmatrix},\\tag{9}$$\n\nwhere 0s represent $2 \\times 2$ zero matrices. While one could construct the full $d \\times d$ matrix, a good solution should use the properties of this matrix to implement the transformation more efficiently. Since we only care about the relative rotation of tokens within a given sequence, we can reuse the values we compute for $\\cos(\\theta_{i,k})$ and $\\sin(\\theta_{i,k})$ across layers, and different batches. If you would like to optimize it, you may use a single RoPE module referenced by all layers, and it can have a 2d pre-computed buffer of sin and cos values created during init with `self.register_buffer(persistent=False)`, instead of a `nn.Parameter` (because we do not want to learn these fixed cosine and sine values). The exact same rotation process we did for our $q^{(i)}$ is then done for $k^{(j)}$, rotating by the corresponding $R^j$. Notice that this layer has no learnable parameters.\n\n> **Problem (`rope`): Implement RoPE (2 points)**\n>\n> **Deliverable**: Implement a class `RotaryPositionalEmbedding` that applies RoPE to the input tensor. The following interface is recommended:\n>\n> **`def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None)`** Construct the RoPE module and create buffers if needed.\n>\n> > **`theta`**: `float` $\\Theta$ value for the RoPE\n> >\n> > **`d_k`**: `int` dimension of query and key vectors\n> >\n> > **`max_seq_len`**: `int` Maximum sequence length that will be inputted\n> >\n> > **`device`**: `torch.device | None = None` Device to store the buffer on\n>\n> **`def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor`** Process an input tensor of shape (`...`, `seq_len`, `d_k`) and return a tensor of the same shape. Note that you should tolerate $x$ with an arbitrary number of batch dimensions. You should assume that the token positions are a tensor of shape (`...`, `seq_len`) specifying the token positions of $x$ along the sequence dimension.\n>\n> **Note:** `d_k` must be even so that RoPE can rotate pairs of features.\n>\n> You should use the token positions to slice your (possibly precomputed) cos and sin tensors along the sequence dimension.\n>\n> To test your implementation, complete \\[`adapters.run_rope`\\] and make sure it passes `uv run pytest -k test_rope`.\n\n#### 3.5.4 Scaled Dot-Product Attention\n\nWe will now implement scaled dot-product attention as described in Vaswani et al. [2017] (section 3.2.1). As a preliminary step, the definition of the Attention operation will make use of softmax, an operation that takes an unnormalized vector of scores and turns it into a normalized distribution:\n\n$$\\text{softmax}(v)_i = \\frac{\\exp(v_i)}{\\sum_{j=1}^{n} \\exp(v_j)}.\\tag{10}$$\n\nNote that $\\exp(v_i)$ can become `inf` for large values (then, `inf`/`inf` = `NaN`). We can avoid this by noticing that the softmax operation is invariant to adding any constant $c$ to all inputs. We can leverage this property for numerical stability---typically, we will subtract the largest entry of $v$ from all elements of $v$, making the new largest entry 0. You will now implement softmax, using this trick for numerical stability.\n\n> **Problem (`softmax`): Implement softmax (1 point)**\n>\n> **Deliverable**: Write a function to apply the softmax operation on a tensor. Your function should take two parameters: a tensor and a *dimension* $i$, and apply softmax to the $i$-th dimension of the input tensor. The output tensor should have the same shape as the input tensor, but its $i$-th dimension will now have a normalized probability distribution. Use the trick of subtracting the maximum value in the $i$-th dimension from all elements of the $i$-th dimension to avoid numerical stability issues.\n>\n> To test your implementation, complete \\[`adapters.run_softmax`\\] and make sure it passes `uv run pytest -k test_softmax_matches_pytorch`.\n\nWe can now define the Attention operation mathematically as follows:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right) V\\tag{11}$$\n\nwhere $Q \\in \\mathbb{R}^{n \\times d_k}$, $K \\in \\mathbb{R}^{m \\times d_k}$, and $V \\in \\mathbb{R}^{m \\times d_v}$. Here, $Q$, $K$ and $V$ are all inputs to this operation---note that these are not the learnable parameters. If you're wondering why this is $QK^{\\top}$ (rather than $Q^{\\top}K$), see \u00a73.3.1.\n\n**Masking:** It is sometimes convenient to *mask* the output of an attention operation. A mask should have the shape $M \\in \\{\\text{True}, \\text{False}\\}^{n \\times m}$, and each row $i$ of this boolean matrix indicates which keys the query $i$ should attend to. Canonically (and slightly confusingly), a value of `True` at position $(i, j)$ indicates that the query $i$ *does* attend to the key $j$, and a value of `False` indicates that the query *does not* attend to the key. In other words, \"information flows\" at $(i, j)$ pairs with value `True`. For example, consider a $1 \\times 3$ mask matrix with entries \\[`True`, `True`, `False`\\]. The single query vector attends only to the first two keys.\n\nComputationally, it will be much more efficient to use masking than to compute attention on subsequences, and we can do this by taking the pre-softmax values $\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)$ and adding a $-\\infty$ in any entry of the mask matrix that is `False`. In finite-precision implementations, avoid rows that are entirely masked (all `False`), since this can produce `NaN` values during softmax stabilization.\n\n> **Problem (`scaled_dot_product_attention`): Implement scaled dot-product attention (5 points)**\n>\n> **Deliverable**: Implement the scaled dot-product attention function. Your implementation should handle keys and queries of shape (`batch_size`, `...`, `seq_len`, `d_k`) and values of shape (`batch_size`, `...`, `seq_len`, `d_v`), where `...` represents any number of other batch-like dimensions (if provided). The implementation should return an output with the shape (`batch_size`, `...`, `seq_len`, `d_v`). See section 3.3 for a discussion on batch-like dimensions.\n>\n> Your implementation should also support an optional user-provided boolean mask of shape (`seq_len`, `seq_len`). The attention probabilities of positions with a mask value of `True` should collectively sum to 1, and the attention probabilities of positions with a mask value of `False` should be zero.\n>\n> To test your implementation against our provided tests, you will need to implement the test adapter at \\[`adapters.run_scaled_dot_product_attention`\\].\n>\n> `uv run pytest -k test_scaled_dot_product_attention` tests your implementation on third-order input tensors, while `uv run pytest -k test_4d_scaled_dot_product_attention` tests your implementation on fourth-order input tensors.\n\n#### 3.5.5 Causal Multi-Head Self-Attention\n\nWe will implement multi-head self-attention as described in section 3.2.2 of Vaswani et al. [2017]. Recall that, mathematically, the operation of applying multi-head attention is defined as follows:\n\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\tag{12}$$\n\n$$\\text{for head}_i = \\text{Attention}(Q_i, K_i, V_i)\\tag{13}$$\n\nwith $Q_i$, $K_i$, $V_i$ being slice number $i \\in \\{1, \\ldots, h\\}$ of size $d_k$ or $d_v$ of the embedding dimension for $Q$, $K$, and $V$ respectively. With Attention being the scaled dot-product attention operation defined in \u00a73.5.4. From this we can form the multi-head *self*-attention operation:\n\n$$\\text{MultiHeadSelfAttention}(x) = \\text{MultiHead}(xW_Q^{\\top}, xW_K^{\\top}, xW_V^{\\top})W_O^{\\top}\\tag{14}$$\n\nHere, the learnable parameters are $W_Q \\in \\mathbb{R}^{hd_k \\times d_{\\text{model}}}$, $W_K \\in \\mathbb{R}^{hd_k \\times d_{\\text{model}}}$, $W_V \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$, and $W_O \\in \\mathbb{R}^{d_{\\text{model}} \\times hd_v}$. Since the $Q$s, $K$s, and $V$s are sliced in the multi-head attention operation, we can think of $W_Q$, $W_K$ and $W_V$ as being separated for each head along the output dimension. When you have this working, you should be computing the key, value, and query projections in a total of three matrix multiplies.[^5]\n\n[^5]: As a stretch goal, try combining the key, query, and value projections into a single weight matrix so you only need a single matrix multiply.\n\n**Causal masking.** Your implementation should prevent the model from attending to future tokens in the sequence. In other words, if the model is given a token sequence $t_1, \\ldots, t_n$, and we want to calculate the next-word predictions for the prefix $t_1, \\ldots, t_i$ (where $i < n$), the model should *not* be able to access (attend to) the token representations at positions $t_{i+1}, \\ldots, t_n$ since it will not have access to these tokens when generating text during inference (and these future tokens leak information about the identity of the true next word, trivializing the language modeling pre-training objective). For an input token sequence $t_1, \\ldots, t_n$ we can naively prevent access to future tokens by running multi-head self-attention $n$ times (for the $n$ unique prefixes in the sequence). Instead, we'll use causal attention masking, which allows token $i$ to attend to all positions $j \\leq i$ in the sequence. You can use `torch.triu` or a broadcasted index comparison to construct this mask, and you should take advantage of the fact that your scaled dot-product attention implementation from \u00a73.5.4 already supports attention masking.\n\n**Applying RoPE.** RoPE should be applied to the query and key vectors, but not the value vectors. Also, the head dimension should be handled as a batch dimension, because in multi-head attention, attention is being applied independently for each head. This means that precisely the same RoPE rotation should be applied to the query and key vectors for each head.\n\n> **Problem (`multihead_self_attention`): Implement causal multi-head self-attention (5 points)**\n>\n> **Deliverable**: Implement causal multi-head self-attention as a `torch.nn.Module`. Your implementation should accept (at least) the following parameters:\n>\n> **`d_model`**: `int` Dimensionality of the Transformer block inputs.\n>\n> **`num_heads`**: `int` Number of heads to use in multi-head self-attention.\n>\n> Following Vaswani et al. [2017], set $d_k = d_v = d_{\\text{model}} / h$. To test your implementation against our provided tests, implement the test adapter at \\[`adapters.run_multihead_self_attention`\\]. Then, run `uv run pytest -k test_multihead_self_attention` to test your implementation.\n\n### 3.6 The Full Transformer LM\n\nLet's begin by assembling the Transformer block (it will be helpful to refer back to Figure 2). A Transformer block contains two 'sublayers', one for the multihead self attention, and another for the feed-forward network. In each sublayer, we first perform RMSNorm, then the main operation (MHA/FF), finally adding in the residual connection.\n\nTo be concrete, the first half (the first 'sub-layer') of the Transformer block should be implementing the following set of updates to produce an output $y$ from an input $x$,\n\n$$y = x + \\text{MultiHeadSelfAttention}(\\text{RMSNorm}(x)).\\tag{15}$$\n\n> **Problem (`transformer_block`): Implement the Transformer block (3 points)**\n>\n> Implement the pre-norm Transformer block as described in \u00a73.5 and illustrated in Figure 2. Your Transformer block should accept (at least) the following parameters.\n>\n> **`d_model`**: `int` Dimensionality of the Transformer block inputs.\n>\n> **`num_heads`**: `int` Number of heads to use in multi-head self-attention.\n>\n> **`d_ff`**: `int` Dimensionality of the position-wise feed-forward inner layer.\n>\n> To test your implementation, implement the adapter \\[`adapters.run_transformer_block`\\]. Then run `uv run pytest -k test_transformer_block` to test your implementation.\n>\n> **Deliverable**: Transformer block code that passes the provided tests.\n\nNow we put the blocks together, following the high level diagram in Figure 1. Follow our description of the embedding in Section 3.1.1, feed this into `num_layers` Transformer blocks, and then pass that into the three output layers to obtain logits over the vocabulary.\n\n> **Problem (`transformer_lm`): Implementing the Transformer LM (3 points)**\n>\n> Time to put it all together! Implement the Transformer language model as described in \u00a73.1 and illustrated in Figure 1. At minimum, your implementation should accept all the aforementioned construction parameters for the Transformer block, as well as these additional parameters:\n>\n> **`vocab_size`**: `int` The size of the vocabulary, necessary for determining the dimensionality of the token embedding matrix.\n>\n> **`context_length`**: `int` The maximum context length, used for causal masking and for setting RoPE's maximum sequence length (e.g., precomputed sin/cos buffers).\n>\n> **`num_layers`**: `int` The number of Transformer blocks to use.\n>\n> To test your implementation against our provided tests, you will first need to implement the test adapter at \\[`adapters.run_transformer_lm`\\]. Then, run `uv run pytest -k test_transformer_lm` to test your implementation.\n>\n> **Deliverable**: A Transformer LM module that passes the above tests.\n\n**Resource accounting.** It is useful to be able to understand how the various parts of the Transformer consume compute and memory. We will go through the steps to do some basic \"FLOPs accounting.\" The *vast* majority of FLOPS in a Transformer are matrix multiplies, so our core approach is simple:\n\n1. Write down all the matrix multiplies in a Transformer forward pass.\n2. Convert each matrix multiply into FLOPs required.\n\nFor this second step, the following fact will be useful:\n\n**Rule:** Given $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$, the matrix-matrix product $AB$ requires $2mnp$ FLOPs.\n\nTo see this, note that $(AB)[i, j] = A[i, :] \\cdot B[:, j]$, and that this dot product requires $n$ additions and $n$ multiplications ($2n$ FLOPs). Then, since the matrix-matrix product $AB$ has $m \\times p$ entries, the total number of FLOPS is $(2n)(mp) = 2mnp$.\n\nNow, before you do the next problem, it can be helpful to go through each component of your Transformer block and Transformer LM, and list out all the matrix multiplies and their associated FLOPs costs.\n\n> **Problem (`transformer_accounting`): Transformer LM resource accounting (5 points)**\n>\n> (a) Consider GPT-2 XL, which has the following configuration:\n>\n> > **`vocab_size`** : 50,257\n> >\n> > **`context_length`** : 1,024\n> >\n> > **`num_layers`** : 48\n> >\n> > **`d_model`** : 1,600\n> >\n> > **`num_heads`** : 25\n> >\n> > **`d_ff`** : 6,400\n>\n> Suppose we constructed our model using this configuration. How many trainable parameters would our model have? Assuming each parameter is represented using single-precision floating point, how much memory is required to just load this model?\n>\n> **Deliverable**: A one-to-two sentence response.\n>\n> (b) Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped model. How many FLOPs do these matrix multiplies require in total? Assume that our input sequence has `context_length` tokens.\n>\n> **Deliverable**: A list of matrix multiplies (with descriptions), and the total number of FLOPs required.\n>\n> (c) Based on your analysis above, which parts of the model require the most FLOPs?\n>\n> **Deliverable**: A one-to-two sentence response.\n>\n> (d) Repeat your analysis with GPT-2 small (12 layers, 768 `d_model`, 12 heads), GPT-2 medium (24 layers, 1024 `d_model`, 16 heads), and GPT-2 large (36 layers, 1280 `d_model`, 20 heads). As the model size increases, which parts of the Transformer LM take up proportionally more or less of the total FLOPs?\n>\n> **Deliverable**: For each model, provide a breakdown of model components and its associated FLOPs (as a proportion of the total FLOPs required for a forward pass). In addition, provide a one-to-two sentence description of how varying the model size changes the proportional FLOPs of each component.\n>\n> (e) Take GPT-2 XL and increase the context length to 16,384. How does the total FLOPs for one forward pass change? How do the relative contribution of FLOPs of the model components change?\n>\n> **Deliverable**: A one-to-two sentence response.\n\n---\n\n## 4 Training a Transformer LM\n\nWe now have the steps to preprocess the data (via tokenizer) and the model (Transformer). What remains is to build all of the code to support training. This consists of the following:\n\n- **Loss:** we need to define the loss function (cross-entropy).\n- **Optimizer:** we need to define the optimizer to minimize this loss (AdamW).\n- **Training loop:** we need all the supporting infrastructure that loads data, saves checkpoints, and manages training.\n\n### 4.1 Cross-entropy loss\n\nRecall that the Transformer language model defines a distribution $p_\\theta(x_{i+1} \\mid x_{1:i})$ for each sequence $x$ of length $m + 1$ and $i = 1, \\ldots, m$. Given a training set $D$ consisting of sequences of length $m + 1$ (or windows of that length from a longer stream), we define the standard cross-entropy (negative log-likelihood) loss function:\n\n$$\\ell(\\theta; D) = \\frac{1}{|D|m} \\sum_{x \\in D} \\sum_{i=1}^{m} -\\log p_\\theta(x_{i+1} \\mid x_{1:i}).\\tag{16}$$\n\n(Note that a single forward pass in the Transformer yields $p_\\theta(x_{i+1} \\mid x_{1:i})$ for *all* $i = 1, \\ldots, m$.)\n\nIn particular, the Transformer computes logits $o_i \\in \\mathbb{R}^{\\text{vocab\\_size}}$ for each position $i$, which results in:[^6]\n\n$$p(x_{i+1} \\mid x_{1:i}) = \\text{softmax}(o_i)[x_{i+1}] = \\frac{\\exp(o_i[x_{i+1}])}{\\sum_{a=1}^{\\text{vocab\\_size}} \\exp(o_i[a])}.\\tag{17}$$\n\nThe cross entropy loss is generally defined with respect to the vector of logits $o_i \\in \\mathbb{R}^{\\text{vocab\\_size}}$ and target $x_{i+1}$.[^7]\n\n[^6]: Note that $o_i[k]$ refers to value at index $k$ of the vector $o_i$.\n[^7]: This corresponds to the cross entropy between the Dirac delta distribution over $x_{i+1}$ and the predicted $\\text{softmax}(o_i)$ distribution.\n\nImplementing the cross entropy loss requires some care with numerical issues, just like in the case of softmax.\n\n> **Problem (`cross_entropy`): Implement Cross entropy (unspecified points)**\n>\n> **Deliverable**: Write a function to compute the cross entropy loss, which takes in predicted logits ($o_i$) and targets ($x_{i+1}$) and computes the cross entropy $\\ell_i = -\\log \\text{softmax}(o_i)[x_{i+1}]$. Your function should handle the following:\n>\n> - Subtract the largest element for numerical stability.\n> - Cancel out log and exp whenever possible.\n> - Handle any additional batch dimensions and return the *average* across the batch. As with section 3.3, we assume batch-like dimensions always come first, before the vocabulary size dimension.\n>\n> Implement \\[`adapters.run_cross_entropy`\\], then run `uv run pytest -k test_cross_entropy` to test your implementation.\n\n**Perplexity** Cross entropy suffices for training, but when we evaluate the model, we also want to report perplexity. For a sequence of length $m$ where we suffer cross-entropy losses $\\ell_1, \\ldots, \\ell_m$:\n\n$$\\text{perplexity} = \\exp\\left(\\frac{1}{m} \\sum_{i=1}^{m} \\ell_i\\right).\\tag{18}$$\n\n### 4.2 The SGD Optimizer\n\nNow that we have a loss function, we will begin our exploration of optimizers. The simplest gradient-based optimizer is Stochastic Gradient Descent (SGD). We start with randomly initialized parameters $\\theta_0$. Then for each step $t = 0, \\ldots, T - 1$, we perform the following update:\n\n$$\\theta_{t+1} \\leftarrow \\theta_t - \\alpha_t \\nabla L(\\theta_t; B_t),\\tag{19}$$\n\nwhere $B_t$ is a random batch of data sampled from the dataset $D$, and the *learning rate* $\\alpha_t$ and *batch size* $|B_t|$ are hyperparameters.\n\n#### 4.2.1 Implementing SGD in PyTorch\n\nTo implement our optimizers, we will subclass the PyTorch `torch.optim.Optimizer` class. An `Optimizer` subclass must implement two methods:\n\n**`def __init__(self, params, ...)`** should initialize your optimizer. Here, `params` will be a collection of parameters to be optimized (or parameter groups, in case the user wants to use different hyperparameters, such as learning rates, for different parts of the model). Make sure to pass `params` to the `__init__` method of the base class, which will store these parameters for use in `step`. You can take additional arguments depending on the optimizer (e.g., the learning rate is a common one), and pass them to the base class constructor as a dictionary, where keys are the names (strings) you choose for these parameters.\n\n**`def step(self)`** should make one update of the parameters. During the training loop, this will be called after the backward pass, so you have access to the gradients on the last batch. This method should iterate through each parameter tensor `p` and modify them *in place*, i.e. setting `p.data`, which holds the tensor associated with that parameter based on the gradient `p.grad` (if it exists), the tensor representing the gradient of the loss with respect to that parameter.\n\nThe PyTorch optimizer API has a few subtleties, so it's easier to explain it with an example. To make our example richer, we'll implement a slight variation of SGD where the learning rate decays over training, starting with an initial learning rate $\\alpha$ and taking successively smaller steps over time:\n\n$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{t+1}} \\nabla L(\\theta_t; B_t)\\tag{20}$$\n\nLet's see how this version of SGD would be implemented as a PyTorch `Optimizer`:\n\n```python\nfrom collections.abc import Callable, Iterable\nfrom typing import Optional\nimport torch\nimport math\n\nclass SGD(torch.optim.Optimizer):\n    def __init__(self, params, lr=1e-3):\n        if lr < 0:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        defaults = {\"lr\": lr}\n        super().__init__(params, defaults)\n\n    def step(self, closure: Optional[Callable] = None):\n        loss = None if closure is None else closure()\n        for group in self.param_groups:\n            lr = group[\"lr\"]  # Get the learning rate.\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                state = self.state[p]  # Get state associated with p.\n                t = state.get(\"t\", 0)  # Get iteration number from the state, or initial value.\n                grad = p.grad.data  # Get the gradient of loss with respect to p.\n                p.data -= lr / math.sqrt(t + 1) * grad  # Update weight tensor in-place.\n                state[\"t\"] = t + 1  # Increment iteration number.\n\n        return loss\n```\n\nIn `__init__`, we pass the parameters to the optimizer, as well as default hyperparameters, to the base class constructor (the parameters might come in groups, each with different hyperparameters). In case the parameters are just a single collection of `torch.nn.Parameter` objects, the base constructor will create a single group and assign it the default hyperparameters. Then, in `step`, we iterate over each parameter group, then over each parameter in that group, and apply Eq. 20. Here, we keep the iteration number as a state associated with each parameter: we first read this value, use it in the gradient update, and then update it. The API specifies that the user might pass in a callable `closure` to re-compute the loss before the optimizer step. We won't need this for the optimizers we'll use, but we add it to comply with the API.\n\nTo see this working, we can use the following minimal example of a *training loop*:\n\n```python\nweights = torch.nn.Parameter(5 * torch.randn((10, 10)))\nopt = SGD([weights], lr=1)\n\nfor t in range(100):\n    opt.zero_grad()  # Reset the gradients for all learnable parameters.\n    loss = (weights**2).mean()  # Compute a scalar loss value.\n    print(loss.cpu().item())\n    loss.backward()  # Run backward pass, which computes gradients.\n    opt.step()  # Run optimizer step.\n```\n\nThis is the typical structure of a training loop: in each iteration, we will compute the loss and run a step of the optimizer. When training language models, our learnable parameters will come from the model (in PyTorch, `m.parameters()` gives us this collection). The loss will be computed over a sampled batch of data, but the basic structure of the training loop will be the same.\n\n> **Problem (`learning_rate_tuning`): Tuning the learning rate (1 point)**\n>\n> As we will see, one of the hyperparameters that affects training the most is the learning rate. Let's see that in practice in our toy example. Run the SGD example above with three other values for the learning rate: 1e1, 1e2, and 1e3, for just 10 training iterations. What happens with the loss for each of these learning rates? Does it decay faster, slower, or does it diverge (i.e., increase over the course of training)?\n>\n> **Deliverable**: A one-to-two sentence response with the behaviors you observed.\n\n### 4.3 AdamW\n\nModern language models are typically trained with more sophisticated optimizers, instead of SGD. Most optimizers used recently are derivatives of the Adam optimizer [Kingma and Ba, 2015]. We will use AdamW [Loshchilov and Hutter, 2019], which is in wide use in recent work. AdamW proposes a modification to Adam that improves regularization by adding *weight decay* (at each iteration, we pull the parameters towards 0), in a way that is decoupled from the gradient update. We will implement AdamW as described in algorithm 1 of Loshchilov and Hutter [2019].\n\nAdamW is *stateful*: for each parameter, it keeps track of a running estimate of its first and second moments. Thus, AdamW uses additional memory in exchange for improved stability and convergence. Besides the learning rate $\\alpha$, AdamW has a pair of hyperparameters $(\\beta_1, \\beta_2)$ that control the updates to the moment estimates, and a weight decay rate $\\lambda$. Typical applications set $(\\beta_1, \\beta_2)$ to $(0.9, 0.999)$, but large language models like LLaMA [Touvron et al., 2023] and GPT-3 [Brown et al., 2020] are often trained with $(0.9, 0.95)$. In Algorithm 1, $\\alpha$ is the scheduled learning rate at step $t$, $\\alpha_t$ is the bias-corrected effective step size used for the Adam update only, and the decoupled weight decay update uses $\\alpha$. The algorithm can be written as follows, where $\\epsilon$ is a small value (e.g., $10^{-8}$) used to improve numerical stability in case we get extremely small values in $v$:\n\n> **Algorithm 1** AdamW Optimizer\n>\n> $\\text{init}(\\theta)$ (Initialize learnable parameters)\n>\n> $m \\leftarrow 0$ (Initial value of the first moment vector; same shape as $\\theta$)\n>\n> $v \\leftarrow 0$ (Initial value of the second moment vector; same shape as $\\theta$)\n>\n> **for** $t = 1, \\ldots, T$ **do**\n>\n> $\\quad$ Sample batch of data $B_t$\n>\n> $\\quad$ $g \\leftarrow \\nabla_\\theta \\ell(\\theta; B_t)$ (Compute the gradient of the loss at the current time step)\n>\n> $\\quad$ $m \\leftarrow \\beta_1 m + (1 - \\beta_1) g$ (Update the first moment estimate)\n>\n> $\\quad$ $v \\leftarrow \\beta_2 v + (1 - \\beta_2) g^2$ (Update the second moment estimate)\n>\n> $\\quad$ $\\alpha_t \\leftarrow \\alpha \\frac{\\sqrt{1 - (\\beta_2)^t}}{1 - (\\beta_1)^t}$ (Compute bias-corrected effective step size for iteration $t$)\n>\n> $\\quad$ $\\theta \\leftarrow \\theta - \\alpha_t \\frac{m}{\\sqrt{v} + \\epsilon}$ (Update the parameters)\n>\n> $\\quad$ $\\theta \\leftarrow \\theta - \\alpha \\lambda \\theta$ (Apply decoupled weight decay using scheduled learning rate $\\alpha$)\n>\n> **end for**\n\nNote that $t$ starts at 1. You will now implement this optimizer.\n\n> **Problem (`adamw`): Implement AdamW (2 points)**\n>\n> **Deliverable**: Implement the AdamW optimizer as a subclass of `torch.optim.Optimizer`. Your class should take the learning rate $\\alpha$ in `__init__`, as well as the $\\beta$, $\\epsilon$ and $\\lambda$ hyperparameters. To help you keep state, the base `Optimizer` class gives you a dictionary `self.state`, which maps `nn.Parameter` objects to a dictionary that stores any information you need for that parameter (for AdamW, this would be the moment estimates). Implement \\[`adapters.get_adamw_cls`\\] and make sure it passes `uv run pytest -k test_adamw`.\n\n> **Problem (`adamwAccounting`): Resource accounting for training with AdamW (2 points)**\n>\n> Let us compute how much memory and compute running AdamW requires. Assume we are using float32 for every tensor.\n>\n> (a) How much peak memory does running AdamW require? Decompose your answer based on the memory usage of the parameters, activations, gradients, and optimizer state. Express your answer in terms of the `batch_size` and the model hyperparameters (`vocab_size`, `context_length`, `num_layers`, `d_model`, `num_heads`). Assume `d_ff` $= 4 \\times$ `d_model`.\n>\n> For simplicity, when calculating memory usage of activations, consider only the following components:\n>\n> - Transformer block\n>   - RMSNorm(s)\n>   - Multi-head self-attention sublayer: $QKV$ projections, $QK^{\\top}$ matrix multiply, softmax, weighted sum of values, output projection.\n>   - Position-wise feed-forward: $W_1$ matrix multiply, SiLU, $W_2$ matrix multiply\n> - final RMSNorm\n> - output embedding\n> - cross-entropy on logits\n>\n> **Deliverable**: An algebraic expression for each of parameters, activations, gradients, and optimizer state, as well as the total.\n>\n> (b) Instantiate your answer for a GPT-2 XL-shaped model to get an expression that only depends on the `batch_size`. What is the maximum batch size you can use and still fit within 80GB memory?\n>\n> **Deliverable**: An expression that looks like $a \\cdot$ `batch_size` $+ b$ for numerical values $a, b$, and a number representing the maximum batch size.\n>\n> (c) How many FLOPs does running one step of AdamW take?\n>\n> **Deliverable**: An algebraic expression, with a brief justification.\n>\n> (d) Model FLOPs utilization (MFU) is defined as the ratio of observed throughput (tokens per second) relative to the hardware's theoretical peak FLOP throughput [Chowdhery et al., 2022]. An NVIDIA A100 GPU has a theoretical peak of 19.5 teraFLOP/s for float32 operations. Assuming you are able to get 50% MFU, how long would it take to train a GPT-2 XL for 400K steps and a batch size of 1024 on a single A100? Following Kaplan et al. [2020] and Hoffmann et al. [2022], assume that the backward pass has twice the FLOPs of the forward pass.\n>\n> **Deliverable**: The number of days training would take, with a brief justification.\n\n### 4.4 Learning rate scheduling\n\nThe value for the learning rate that leads to the quickest decrease in loss often varies during training. In training Transformers, it is typical to use a learning rate *schedule*, where we start with a bigger learning rate, making quicker updates in the beginning, and slowly decay it to a smaller value as the model trains.[^8] In this assignment, we will implement the cosine annealing schedule used to train LLaMA [Touvron et al., 2023].\n\n[^8]: It's sometimes common to use a schedule where the learning rate rises back up (restarts) to help get past local minima.\n\nA scheduler is simply a function that takes the current step $t$ and other relevant parameters (such as the initial and final learning rates), and returns the learning rate to use for the gradient update at step $t$. The simplest schedule is the constant function, which will return the same learning rate given any $t$.\n\nThe cosine annealing learning rate schedule takes (i) the current iteration $t$, (ii) the maximum learning rate $\\alpha_{\\max}$, (iii) the minimum (final) learning rate $\\alpha_{\\min}$, (iv) the number of *warm-up* iterations $T_w$, and (v) the cosine annealing end step $T_{\\text{end}}$. The learning rate at iteration $t$ is defined as:\n\n**(Warm-up)** If $t < T_w$, then $\\alpha_t = \\frac{t}{T_w} \\alpha_{\\max}$.\n\n**(Cosine annealing)** If $T_w \\leq t \\leq T_{\\text{end}}$, then $\\alpha_t = \\alpha_{\\min} + \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{t - T_w}{T_{\\text{end}} - T_w}\\pi\\right)\\right)(\\alpha_{\\max} - \\alpha_{\\min})$.\n\n**(Post-annealing)** If $t > T_{\\text{end}}$, then $\\alpha_t = \\alpha_{\\min}$.\n\n> **Problem (`learning_rate_schedule`): Implement cosine learning rate schedule with warmup (unspecified points)**\n>\n> Write a function that takes $t$, $\\alpha_{\\max}$, $\\alpha_{\\min}$, $T_w$ and $T_{\\text{end}}$, and returns the learning rate $\\alpha_t$ according to the scheduler defined above. Then implement \\[`adapters.get_lr_cosine_schedule`\\] and make sure it passes `uv run pytest -k test_get_lr_cosine_schedule`.\n\n### 4.5 Gradient clipping\n\nDuring training, we can sometimes hit training examples that yield large gradients, which can destabilize training. To mitigate this, one technique often employed in practice is *gradient clipping*. The idea is to enforce a limit on the norm of the gradient after each backward pass before taking an optimizer step.\n\nGiven the gradient (for all parameters) $g$, we compute its $\\ell_2$-norm $\\|g\\|_2$. If this norm is less than a maximum value $M$, then we leave $g$ as is; otherwise, we scale $g$ down by a factor of $\\frac{M}{\\|g\\|_2 + \\epsilon}$ (where a small $\\epsilon$, like $10^{-6}$, is added for numeric stability). Note that the resulting norm will be just under $M$.\n\n> **Problem (`gradient_clipping`): Implement gradient clipping (1 point)**\n>\n> Write a function that implements gradient clipping. Your function should take a list of parameters and a maximum $\\ell_2$-norm. It should modify each parameter gradient in place. Use $\\epsilon = 10^{-6}$ (the PyTorch default). Then, implement the adapter \\[`adapters.run_gradient_clipping`\\] and make sure it passes `uv run pytest -k test_gradient_clipping`.\n\n---\n\n## 5 Training loop\n\nWe will now finally put together the major components we've built so far: the tokenized data, the model, and the optimizer.\n\n### 5.1 Data Loader\n\nThe tokenized data (e.g., that you prepared in `tokenizer_experiments`) is a single sequence of tokens $x = (x_1, \\ldots, x_n)$. Even though the source data might consist of separate documents (e.g., different web pages, or source code files), a common practice is to concatenate all of those into a single sequence of tokens, adding a delimiter between them (such as the `<|endoftext|>` token).\n\nA *data loader* turns this into a stream of *batches*, where each batch consists of $B$ sequences of length $m$, paired with the corresponding next tokens, also with length $m$. For example, for $B = 1, m = 3$, $([x_2, x_3, x_4], [x_3, x_4, x_5])$ would be one potential batch.\n\nLoading data in this way simplifies training for a number of reasons. First, any $1 \\leq i < n - m$ gives a valid training sequence, so sampling sequences are trivial. Since all training sequences have the same length, there's no need to pad input sequences, which improves hardware utilization (also by increasing batch size $B$). Finally, we also don't need to fully load the full dataset to sample training data, making it easy to handle large datasets that might not otherwise fit in memory.\n\n> **Problem (`data_loading`): Implement data loading (2 points)**\n>\n> **Deliverable**: Write a function that takes a numpy array $x$ (integer array with token IDs), a `batch_size`, a `context_length` and a PyTorch device string (e.g., `'cpu'` or `'cuda:0'`), and returns a pair of tensors: the sampled input sequences and the corresponding next-token targets. Both tensors should have shape (`batch_size`, `context_length`) containing token IDs, and both should be placed on the requested device. To test your implementation against our provided tests, you will first need to implement the test adapter at \\[`adapters.run_get_batch`\\]. Then, run `uv run pytest -k test_get_batch` to test your implementation.\n\n> **Low-Resource/Downscaling Tip: Data loading on CPU or Apple Silicon**\n>\n> If you are planning to train your LM on CPU or Apple Silicon, you need to move your data to the correct device (and similarly, you should use the same device for your model later on).\n>\n> If you are on CPU, you can use the `'cpu'` device string, and on Apple Silicon (M\\* chips), you can use the `'mps'` device string.\n>\n> For more on MPS, checkout these resources:\n>\n> - [https://developer.apple.com/metal/pytorch/](https://developer.apple.com/metal/pytorch/)\n> - [https://pytorch.org/docs/main/notes/mps.html](https://pytorch.org/docs/main/notes/mps.html)\n\nWhat if the dataset is too big to load into memory? We can use a Unix systemcall named `mmap` which maps a file on disk to virtual memory, and lazily loads the file contents when that memory location is accessed. Thus, you can \"pretend\" you have the entire dataset in memory. Numpy implements this through `np.memmap` (or the flag `mmap_mode='r'` to `np.load`, if you originally saved the array with `np.save`), which will return a numpy array-like object that loads the entries on-demand as you access them. **When sampling from your dataset (i.e., a numpy array) during training, be sure load the dataset in memory-mapped mode** (via `np.memmap` or the flag `mmap_mode='r'` to `np.load`, depending on how you saved the array). Make sure you also specify a `dtype` that matches the array that you're loading. It may be helpful to explicitly verify that the memory-mapped data looks correct (e.g., doesn't contain values beyond the expected vocabulary size).\n\n### 5.2 Checkpointing\n\nIn addition to loading data, we will also need to save models as we train. When running jobs, we often want to be able to resume a training run that for some reason stopped midway (e.g., due to your job timing out, machine failure, etc). Even when all goes well, we might also want to later have access to intermediate models (e.g., to study training dynamics post-hoc, take samples from models at different stages of training, etc).\n\nA checkpoint should have all the states that we need to resume training. We of course want to be able to restore model weights at a minimum. If using a stateful optimizer (such as AdamW), we will also need to save the optimizer's state (e.g., in the case of AdamW, the moment estimates). Finally, to resume the learning rate schedule, we will need to know the iteration number we stopped at. PyTorch makes it easy to save all of these: every `nn.Module` has a `state_dict()` method that returns a dictionary with all learnable weights; we can restore these weights later with the sister method `load_state_dict()`. The same goes for any `torch.optim.Optimizer`. Finally, `torch.save(obj, dest)` can dump an object (e.g., a dictionary containing tensors in some values, but also regular Python objects like integers) to a file (path) or file-like object, which can then be loaded back into memory with `torch.load(src)`.\n\n> **Problem (`checkpointing`): Implement model checkpointing (1 point)**\n>\n> Implement the following two functions to load and save checkpoints:\n>\n> **`def save_checkpoint(model, optimizer, iteration, out)`** should dump all the state from the first three arguments into the file-like object `out`. You can use the `state_dict` method of both the model and the optimizer to get their relevant states and use `torch.save(obj, out)` to dump `obj` into `out` (PyTorch supports either a path or a file-like object here). A typical choice is to have `obj` be a dictionary, but you can use whatever format you want as long as you can load your checkpoint later.\n>\n> This function expects the following parameters:\n>\n> > `model`: `torch.nn.Module`\n> >\n> > `optimizer`: `torch.optim.Optimizer`\n> >\n> > `iteration`: `int`\n> >\n> > `out`: `str | os.PathLike | typing.BinaryIO | typing.IO[bytes]`\n>\n> **`def load_checkpoint(src, model, optimizer)`** should load a checkpoint from `src` (path or file-like object), and then recover the model and optimizer states from that checkpoint. Your function should return the iteration number that was saved in your `save_checkpoint` implementation. You can use `torch.load(src)` to recover what you saved in your `save_checkpoint` implementation, and the `load_state_dict` method in both the model and optimizers to return them to their previous states.\n>\n> This function expects the following parameters:\n>\n> > `src`: `str | os.PathLike | typing.BinaryIO | typing.IO[bytes]`\n> >\n> > `model`: `torch.nn.Module`\n> >\n> > `optimizer`: `torch.optim.Optimizer`\n>\n> Implement the \\[`adapters.run_save_checkpoint`\\] and \\[`adapters.run_load_checkpoint`\\] adapters, and make sure they pass `uv run pytest -k test_checkpointing`.\n\n### 5.3 Training loop\n\nNow, it's finally time to put all of the components you implemented together into your main training script. It will pay off to make it easy to start training runs with different hyperparameters (e.g., by taking them as command-line arguments), since you will be doing these many times later to study how different choices impact training.\n\n> **Problem (`training_together`): Put it together (4 points)**\n>\n> **Deliverable**: Write a script that runs a training loop to train your model on user-provided input. In particular, we recommend that your training script allow for (at least) the following:\n>\n> - Ability to configure and control the various model and optimizer hyperparameters.\n> - Memory-efficient loading of training and validation datasets with `np.memmap`.\n> - Serializing checkpoints to a user-provided path.\n> - Periodically logging training and validation performance (e.g., to console and/or an external service like Weights and Biases).[^9]\n\n[^9]: [wandb.ai](https://wandb.ai)\n\n---\n\n## 6 Generating text\n\nNow that we can train models, the last piece we need is the ability to generate text from our model. Recall that a language model takes in a (possibly batched) integer sequence of length (`sequence_length`) and produces a matrix of size (`sequence_length` $\\times$ `vocab_size`), where each element of the sequence is a vector of logits predicting the next word after that position. We will now write a few functions to turn this into a sampling scheme for new sequences.\n\n**Softmax** By standard convention, the language model output is the output of the final linear layer (the \"logits\") and so we have to turn this into a normalized probability via the *softmax* operation, which we saw earlier in Eq. 10.\n\n**Decoding** To generate text (decode) from our model, we will provide the model with a sequence of prefix tokens (the \"prompt\"), get logits over the vocabulary that predict the next word in the sequence, convert them to a probability distribution with softmax, and sample from this distribution to determine the next output token.\n\nConcretely, one step of the decoding process should take in a sequence $x_{1 \\ldots t}$ and return a token $x_{t+1}$ via the following equation,\n\n$$P(x_{t+1} = i \\mid x_{1 \\ldots t}) = \\frac{\\exp(v_i)}{\\sum_j \\exp(v_j)}$$\n\n$$v = \\text{TransformerLM}(x_{1 \\ldots t})_t \\in \\mathbb{R}^{\\text{vocab\\_size}}$$\n\nwhere TransformerLM is our model which takes as input a sequence of `sequence_length` and produces a matrix of size (`sequence_length` $\\times$ `vocab_size`), and we take the last element of this matrix, as we are looking for the next word prediction at the $t$-th position.\n\nThis gives us a basic decoder by repeatedly sampling from these one-step conditionals (appending our previously-generated output token to the input of the next decoding timestep) until we generate the end-of-sequence token `<|endoftext|>` (or a user-specified maximum number of tokens to generate).\n\n**Decoder tricks** We will be experimenting with small models, and small models can sometimes generate very low quality texts. Two simple decoder tricks can help fix these issues. First, in *temperature scaling* we modify our softmax with a temperature parameter $\\tau$, where the new softmax is\n\n$$\\text{softmax}(v, \\tau)_i = \\frac{\\exp(v_i / \\tau)}{\\sum_{j=1}^{\\text{vocab\\_size}} \\exp(v_j / \\tau)}.\\tag{24}$$\n\n(Equation numbering follows the original handout and may skip numbers.)\n\nNote how setting $\\tau \\to 0$ makes it so that the largest element of $v$ dominates, and the output of the softmax becomes a one-hot vector concentrated at this maximal element.\n\nSecond, another trick is *nucleus* or *top-p* sampling, where we modify the sampling distribution by truncating low-probability words. Let $q$ be a probability distribution that we get from a (temperature-scaled) softmax of size (`vocab_size`). Nucleus sampling with hyperparameter $p$ produces the next token according to the equation\n\n$$P(x_{t+1} = i | q) = \\begin{cases} \\frac{q_i}{\\sum_{j \\in V(p)} q_j} & \\text{if } i \\in V(p) \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\nwhere $V(p)$ is the *smallest* set of indices such that $\\sum_{j \\in V(p)} q_j \\geq p$. You can compute this quantity easily by first sorting the probability distribution $q$ by magnitude, and selecting the largest vocabulary elements until you reach the target level of $p$.\n\n> **Problem (`decoding`): Decoding (3 points)**\n>\n> **Deliverable**: Implement a function to decode from your language model. We recommend that you support the following features:\n>\n> - Generate completions for a user-provided prompt (i.e., take in some $x_{1 \\ldots t}$ and sample a completion until you hit an `<|endoftext|>` token).\n> - Allow the user to control the maximum number of generated tokens.\n> - Given a desired temperature value, apply softmax temperature scaling to the predicted next-word distributions before sampling.\n> - Top-p sampling (Holtzman et al., 2020; also known as nucleus sampling), given a user-specified threshold value.\n\n---\n\n## 7 Experiments\n\nNow it is time to put everything together and train (small) language models on a pretraining dataset.\n\n### 7.1 How to Run Experiments and Deliverables\n\nThe best way to understand the rationale behind the architectural components of a Transformer is to actually modify it and run it yourself. There is no substitute for hands-on experience.\n\nTo this end, it's important to be able to experiment **quickly, consistently, and keep records** of what you did. To experiment quickly, we will be running many experiments on a small scale model (17M parameters) and simple dataset (TinyStories). To do things consistently, you will ablate components and vary hyperparameters in a systematic way, and to keep records we will ask you to submit a log of your experiments and learning curves associated with each experiment.\n\nTo make it possible to submit loss curves, **make sure to periodically evaluate validation losses and record both the number of steps and wallclock times**. You might find logging infrastructure such as Weights and Biases helpful.\n\n> **Problem (`experiment_log`): Experiment logging (3 points)**\n>\n> For your training and evaluation code, create experiment tracking infrastructure that allows you to track your experiments and loss curves with respect to gradient steps and wallclock time.\n>\n> **Deliverable**: Logging infrastructure code for your experiments and an experiment log (a document of all the things you tried) for the assignment problems below in this section.\n\n### 7.2 TinyStories\n\nWe are going to start with a very simple dataset (TinyStories; Eldan and Li, 2023) where models will train quickly, and we can see some interesting behaviors. The instructions for getting this dataset is at section 1. An example of what this dataset looks like is below.\n\n> **Example (`tinystories_example`): One example from TinyStories**\n>\n> Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed! He said, \"Wow, that is a really amazing vase! Can I buy it?\" The shopkeeper smiled and said, \"Of course you can. You can take it home and show all your friends how amazing it is!\" So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. And that's how Ben found an amazing vase in the store!\n\n**Hyperparameter tuning** We will tell you some very basic hyperparameters to start with and ask you to find some settings for others that work well.\n\n**`vocab_size`** 10000. Typical vocabulary sizes are in the tens to hundreds of thousands. You should vary this and see how the vocabulary and model behavior changes.\n\n**`context_length`** 256. Simple datasets such as TinyStories might not need long sequence lengths, but for the later OpenWebText data, you may want to vary this. Try varying this and seeing the impact on both the per-iteration runtime and the final perplexity.\n\n**`d_model`** 512. This is slightly smaller than the 768 dimensions used in many small Transformer papers, but this will make things faster.\n\n**`d_ff`** 1344. This is roughly $\\frac{8}{3}$ `d_model` while being a multiple of 64, which is good for GPU performance.\n\n**RoPE theta parameter** $\\Theta$ 10000.\n\n**number of layers and heads** 4 layers, 16 heads. Together, this will give about 17M non-embedding parameters which is a fairly small Transformer.\n\n**total tokens processed** 327,680,000 (your batch size $\\times$ total step count $\\times$ context length should equal roughly this value).\n\nYou should do some trial and error to find good defaults for the following other hyperparameters: `learning rate`, `learning rate warmup`, other AdamW hyperparameters ($\\beta_1, \\beta_2, \\epsilon$), and `weight decay`. You can find some typical choices of such hyperparameters in Kingma and Ba [2015].\n\n**Putting it together** Now you can put everything together by getting a trained BPE tokenizer, tokenizing the training dataset, and running this in the training loop that you wrote. **Important note:** If your implementation is correct and efficient, the above hyperparameters should result in a roughly 30-40 minute runtime on 1 H100 GPU. If you have runtimes that are much longer, please check and make sure your dataloading, checkpointing, or validation loss code is not bottlenecking your runtimes and that your implementation is properly batched.\n\n**Tips and tricks for debugging model architectures** We highly recommend getting comfortable with your IDE's built-in debugger (e.g., VSCode/PyCharm), which will save you time compared to debugging with print statements. If you use a text editor, you can use something more like `pdb`. A few other good practices when debugging model architectures are:\n\n- A common first step when developing any neural net architecture is to overfit to a single minibatch. If your implementation is correct, you should be able to quickly drive the training loss to near-zero.\n- Set debug breakpoints in various model components, and inspect the shapes of intermediate tensors to make sure they match your expectations.\n- Monitor the norms of activations, model weights, and gradients to make sure they are not exploding or vanishing.\n\n> **Problem (`learning_rate`): Tune the learning rate (3 points) (4 H100 hrs)**\n>\n> The learning rate is one of the most important hyperparameters to tune. Taking the base model you've trained, answer the following questions:\n>\n> (a) Perform a hyperparameter sweep over the learning rates and report the final losses (or note divergence if the optimizer diverges).\n>\n> **Deliverable**: Learning curves associated with multiple learning rates. Explain your hyperparameter search strategy.\n>\n> **Deliverable**: A model with validation loss (per-token) on TinyStories of at most 1.45\n>\n> > **Low-Resource/Downscaling Tip: Train for few steps on CPU or Apple Silicon**\n> >\n> > If you are running on `cpu` or `mps`, you should instead reduce the total tokens processed count to 40,000,000, which will be sufficient to produce reasonably fluent text. You may also increase the target validation loss from 1.45 to 2.00.\n> >\n> > Running our solution code with a tuned learning rate on an M3 Max chip and 36 GB of RAM, we use batch size $\\times$ total step count $\\times$ context length $= 32 \\times 5000 \\times 256 = 40{,}960{,}000$ tokens, which takes 1 hour and 22 minutes on `cpu` and 36 minutes on `mps`. At step 5000, we achieve a validation loss of 1.80.\n> >\n> > Some additional tips:\n> >\n> > - When using $X$ training steps, we suggest adjusting the cosine learning rate decay schedule to terminate its decay (i.e., reach the minimum learning rate) at precisely step $X$.\n> > - When using `mps`, avoid setting\n> >\n> >   ```python\n> >   torch.set_float32_matmul_precision('high')\n> >   ```\n> >\n> >   as you might with `cuda` devices. This setting is CUDA-oriented and can lead to unstable behavior on some MPS/PyTorch versions.\n> > - You can speed up training by JIT-compiling your model with `torch.compile`. Specifically:\n> >   - On `cpu`, compile your model with\n> >\n> >     ```python\n> >     model = torch.compile(model)\n> >     ```\n> >\n> >   - On `mps`, you can somewhat optimize the backward pass using\n> >\n> >     ```python\n> >     model = torch.compile(model, backend=\"aot_eager\")\n> >     ```\n> >\n> >     Compilation with Inductor is not supported on `mps` as of `torch` version 2.6.0.\n>\n> (b) Folk wisdom is that the best learning rate is \"at the edge of stability.\" Investigate how the point at which learning rates diverge is related to your best learning rate.\n>\n> **Deliverable**: Learning curves of increasing learning rate which include at least one divergent run and an analysis of how this relates to convergence rates.\n\nNow let's vary the batch size and see what happens to training. Batch sizes are important -- they let us get higher efficiency from our GPUs by doing larger matrix multiplies, but is it true that we always want batch sizes to be large? Let's run some experiments to find out.\n\n> **Problem (`batch_size_experiment`): Batch size variations (1 point) (2 H100 hrs)**\n>\n> Vary your batch size all the way from 1 to the GPU memory limit. Try at least a few batch sizes in between, including typical sizes like 64 and 128.\n>\n> **Deliverable**: Learning curves for runs with different batch sizes. The learning rates should be optimized again if necessary.\n>\n> **Deliverable**: A few sentences discussing your findings on batch sizes and their impacts on training.\n\nWith your decoder in hand, we can now generate text! We will generate from the model and see how good it is. As a reference, you should get outputs that look at least as good as the example below.\n\n> **Example (`ts_generate_example`): Sample output from a TinyStories language model**\n>\n> Once upon a time, there was a pretty girl named Lily. She loved to eat gum, especially the big black one. One day, Lily's mom asked her to help cook dinner. Lily was so excited! She loved to help her mom. Lily's mom made a big pot of soup for dinner. Lily was so happy and said, \"Thank you, Mommy! I love you.\" She helped her mom pour the soup into a big bowl. After dinner, Lily's mom made some yummy soup. Lily loved it! She said, \"Thank you, Mommy! This soup is so yummy!\" Her mom smiled and said, \"I'm glad you like it, Lily.\" They finished cooking and continued to cook together. The end.\n\n> **Low-Resource/Downscaling Tip: Generate text on CPU or Apple Silicon**\n>\n> If instead you used the low-resource configuration with 40M tokens processed, you should see generations that still resemble English but are not as fluent as above. For example, our sample output from a TinyStories language model trained on 40M tokens is below:\n>\n> Once upon a time, there was a little girl named Sue. Sue had a tooth that she loved very much. It was his best head. One day, Sue went for a walk and met a ladybug! They became good friends and played on the path together.\n>\n> \"Hey, Polly! Let's go out!\" said Tim. Sue looked at the sky and saw that it was difficult to find a way to dance shining. She smiled and agreed to help the talking!\"\n>\n> As Sue watched the sky moved, what it was. She\n\nHere is the precise problem statement and what we ask for:\n\n> **Problem (`generate`): Generate text (1 point)**\n>\n> Using your decoder and your trained checkpoint, report the text generated by your model. You may need to manipulate decoder parameters (temperature, top-p, etc.) to get fluent outputs.\n>\n> **Deliverable**: Text dump of at least 256 tokens of text (or until the first `<|endoftext|>` token), and a brief comment on the fluency of this output and at least two factors which affect how good or bad this output is.\n\n### 7.3 Ablations and architecture modification\n\nThe best way to understand the Transformer is to actually modify it and see how it behaves. We will now do a few simple ablations and modifications.\n\n**Ablation 1: layer normalization** It is often said that layer normalization is important for the stability of Transformer training. But perhaps we want to live dangerously. Let's remove RMSNorm from each of our Transformer blocks and see what happens.\n\n> **Problem (`layer_norm_ablation`): Remove RMSNorm and train (1 point) (1 H100 hr)**\n>\n> Remove all of the RMSNorms from your Transformer and train. What happens at the previous optimal learning rate? Can you get stability by using a lower learning rate?\n>\n> **Deliverable**: A learning curve for when you remove RMSNorms and train, as well as a learning curve for the best learning rate.\n>\n> **Deliverable**: A few sentence commentary on the impact of RMSNorm.\n\nLet's now investigate another layer normalization choice that seems arbitrary at first glance. *Pre-norm* Transformer blocks are defined as\n\n$$z = x + \\text{MultiHeadSelfAttention}(\\text{RMSNorm}(x))$$\n\n$$y = z + \\text{FFN}(\\text{RMSNorm}(z)).$$\n\nThis is one of the few 'consensus' modifications to the original Transformer architecture, which used a *post-norm* approach (here with RMSNorm rather than LayerNorm) as\n\n$$z = \\text{RMSNorm}(x + \\text{MultiHeadSelfAttention}(x))$$\n\n$$y = \\text{RMSNorm}(z + \\text{FFN}(z)).$$\n\nLet's revert back to the *post-norm* approach and see what happens.\n\n> **Problem (`pre_norm_ablation`): Implement post-norm and train (1 point) (1 H100 hr)**\n>\n> Modify your pre-norm Transformer implementation into a post-norm one. Train with the post-norm model and see what happens.\n>\n> **Deliverable**: A learning curve for a post-norm transformer, compared to the pre-norm one.\n\nWe see that layer normalization has a major impact on the behavior of the transformer, and that even the position of the layer normalization is important.\n\n**Ablation 2: position embeddings** We will next investigate the impact of the position embeddings on the performance of the model. Specifically, we will compare our base model (with RoPE) with not including position embeddings at all (NoPE). It turns out that decoder-only transformers, i.e., those with a causal mask as we have implemented, can in theory infer relative or absolute position information without being provided with position embeddings explicitly [Tsai et al., 2019; Kazemnejad et al., 2023]. We will now test empirically how NoPE performs compared to RoPE.\n\n> **Problem (`no_pos_emb`): Implement NoPE (1 point) (1 H100 hr)**\n>\n> Modify your Transformer implementation with RoPE to remove the position embedding information entirely, and see what happens.\n>\n> **Deliverable**: A learning curve comparing the performance of RoPE and NoPE.\n\n**Ablation 3: SwiGLU vs. SiLU** Next, we will follow Shazeer [2020] and test the importance of gating in the feed-forward network, by comparing the performance of SwiGLU feed-forward networks versus feed-forward networks using SiLU activations but no gated linear unit (GLU):\n\n$$\\text{FFN}_{\\text{SiLU}}(x) = \\text{SiLU}(xW_1^{\\top})W_2^{\\top}.\\tag{25}$$\n\nRecall that in our SwiGLU implementation, we set the dimensionality of the inner feed-forward layer to be roughly $d_{\\text{ff}} = \\frac{8}{3} d_{\\text{model}}$ (while ensuring that $d_{\\text{ff}} \\bmod 64 = 0$, to make use of GPU tensor cores). In your $\\text{FFN}_{\\text{SiLU}}$ implementation you should set $d_{\\text{ff}} = 4 \\times d_{\\text{model}}$, to approximately match the parameter count of the SwiGLU feed-forward network (which has three instead of two weight matrices).\n\n> **Problem (`swiglu_ablation`): SwiGLU vs. SiLU (1 point) (1 H100 hr)**\n>\n> **Deliverable**: A learning curve comparing the performance of SwiGLU and SiLU feed-forward networks, with approximately matched parameter counts.\n>\n> **Deliverable**: A few sentences discussing your findings.\n\n> **Low-Resource/Downscaling Tip: Online students with limited GPU resources should test modifications on TinyStories**\n>\n> In the remainder of the assignment, we will move to a larger-scale, noisier web dataset (OpenWebText), experimenting with architecture modifications and (optionally) making a submission to the course leaderboard.\n>\n> It takes a long time to train an LM to fluency on OpenWebText, so we suggest online students with limited GPU access continue testing modifications on TinyStories (using validation loss as a metric to evaluate performance).\n\n### 7.4 Running on OpenWebText\n\nWe will now move to a more standard pretraining dataset created from a webcrawl. A small sample of OpenWebText [Gokaslan et al., 2019] is also provided as a single text file: see section 1 for how to access this file.\n\nHere is an example from OpenWebText. Note how the text is much more realistic, complex, and varied. You may want to look through the training dataset to get a sense of what training data looks like for a webscraped corpus.\n\n> **Example (`owt_example`): One example from OWT**\n>\n> Baseball Prospectus director of technology Harry Pavlidis took a risk when he hired Jonathan Judge. Pavlidis knew that, as Alan Schwarz wrote in The Numbers Game, \"no corner of American culture is more precisely counted, more passionately quantified, than performances of baseball players.\" With a few clicks here and there, you can findout that Noah Syndergaard's fastball revolves more than 2,100 times per minute on its way to the plate, that Nelson Cruz had the game's highest average exit velocity among qualified hitters in 2016 and myriad other tidbits that seem ripped from a video game or science fiction novel. The rising ocean of data has empowered an increasingly important actor in baseball's culture: the analytical hobbyist.\n>\n> That empowerment comes with added scrutiny -- on the measurements, but also on the people and publications behind them. With Baseball Prospectus, Pavlidis knew all about the backlash that accompanies quantitative imperfection. He also knew the site's catching metrics needed to be reworked, and that it would take a learned mind -- someone who could tackle complex statistical modeling problems -- to complete the job.\n>\n> \"He freaks us out.\" Harry Pavlidis\n>\n> Pavlidis had a hunch that Judge \"got it\" based on the latter's writing and their interaction at a site-sponsored ballpark event. Soon thereafter, the two talked over drinks. Pavlidis' intuition was validated. Judge was a fit for the position -- better yet, he was a willing fit. \"I spoke to a lot of people,\" Pavlidis said, \"he was the only one brave enough to take it on.\" [...]\n\n**Note:** You may have to re-tune your hyperparameters such as learning rate or batch size for this experiment.\n\n> **Problem (`main_experiment`): Experiment on OWT (2 points) (3 H100 hrs)**\n>\n> Train your language model on OpenWebText with the same model architecture and total training iterations as TinyStories. How well does this model do?\n>\n> **Deliverable**: A learning curve of your language model on OpenWebText. Describe the difference in losses from TinyStories -- how should we interpret these losses?\n>\n> **Deliverable**: Generated text from OpenWebText LM, in the same format as the TinyStories outputs. How is the fluency of this text? Why is the output quality worse even though we have the same model and compute budget as TinyStories?\n\n### 7.5 Your own modification + leaderboard\n\nCongratulations on getting to this point. You're almost done! You will now try to improve upon the Transformer architecture, and see how your hyperparameters and architecture stack up against other students in the class.\n\n**Rules for the leaderboard** There are no restrictions other than the following:\n\n**Runtime** Your submission can run for at most 1.5 hours on an H100. You can enforce this by setting `--time=01:30:00` in your slurm submission script.\n\n**Data** You may only use the OpenWebText training dataset that we provide.\n\nOtherwise, you are free to do whatever your heart desires.\n\nIf you are looking for some ideas on what to implement, you can checkout some of these resources:\n\n- State-of-the-art open-source LLM families, such as Llama 3 [Grattafiori et al., 2024] or Qwen 2.5 [Yang et al., 2024].\n- The NanoGPT speedrun repository ([https://github.com/KellerJordan/modded-nanogpt](https://github.com/KellerJordan/modded-nanogpt)), where community members post many interesting modifications for \"speedrunning\" small-scale language model pretraining. For example, a common modification that dates back to the original Transformer paper is to tie the weights of the input and output embeddings together (see Vaswani et al. [2017] (Section 3.4) and Chowdhery et al. [2022] (Section 2)). If you do try weight tying, you may have to decrease the standard deviation of the embedding/LM head init.\n\nYou will want to test these on either a small subset of OpenWebText or on TinyStories before trying the full 1.5-hour run.\n\nAs a caveat, we do note that some of the modifications you may find working well in this leaderboard may not generalize to larger-scale pretraining. We will explore this idea further in the scaling laws unit of the course.\n\n> **Problem (`leaderboard`): Leaderboard (6 points) (10 H100 hrs)**\n>\n> You will train a model under the leaderboard rules above with the goal of minimizing the validation loss of your language model within 1.5 H100-hour.\n>\n> **Deliverable**: The final validation loss that was recorded, an associated learning curve that clearly shows a wallclock-time x-axis that is less than 1.5 hours and a description of what you did. We expect a leaderboard submission to beat at least the naive baseline of a 5.0 loss. Submit to the leaderboard here: [https://github.com/stanford-cs336/assignment1-basics-leaderboard](https://github.com/stanford-cs336/assignment1-basics-leaderboard).\n\n---\n\n## References\n\nRonen Eldan and Yuanzhi Li. TinyStories: How small can language models be and still speak coherent English?, 2023. arXiv:2305.07759.\n\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. OpenWebText corpus. [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus), 2019.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In *Proc. of ACL*, 2016.\n\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords, 2019. arXiv:1909.03341.\n\nPhilip Gage. A new algorithm for data compression. *C Users Journal*, 12(2):23--38, February 1994. ISSN 0898-9788.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Proc. of NeurIPS*, 2017.\n\nToan Q. Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. In *Proc. of IWSWLT*, 2019.\n\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the Transformer architecture. In *Proc. of ICML*, 2020.\n\nJimmy Lei Ba, Jamie Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. arXiv:1607.06450.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. arXiv:2302.13971.\n\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. In *Proc. of NeurIPS*, 2019.\n\nAaron Grattafiori et al. The Llama 3 Herd of Models, 2024. arXiv:2407.21783.\n\nAn Yang et al. Qwen2.5 Technical Report, 2024. arXiv:2412.15115.\n\nDan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs), 2016.\n\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning, 2017.\n\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language Modeling with Gated Convolutional Networks, 2017.\n\nNoam Shazeer. GLU Variants Improve Transformer, 2020.\n\nAakanksha Chowdhery et al. PaLM: Scaling Language Modeling with Pathways, 2022. arXiv:2204.02311.\n\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding, 2021.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization, 2015.\n\nIlya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, 2019.\n\nTom B. Brown et al. Language Models are Few-Shot Learners. In *Proc. of NeurIPS*, 2020.\n\nJared Kaplan et al. Scaling Laws for Neural Language Models, 2020. arXiv:2001.08361.\n\nJordan Hoffmann et al. Training Compute-Optimal Large Language Models, 2022. arXiv:2203.15556.\n\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer Dissection: A Unified Understanding for Transformer's Attention via the Lens of Coreference Resolution, 2019.\n\nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan, Payel Das, and Siva Reddy. The Impact of Positional Encoding on Length Generalization in Transformers, 2023.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The Curious Case of Neural Text Degeneration, 2020."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"can_save":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["553e6b8d-0723-4efe-85c8-358d8d8e8fcf"]},"553e6b8d-0723-4efe-85c8-358d8d8e8fcf":{"id":"553e6b8d-0723-4efe-85c8-358d8d8e8fcf","message":{"id":"553e6b8d-0723-4efe-85c8-358d8d8e8fcf","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1771969920.4364064,"update_time":null,"content":{"content_type":"text","parts":["## High-level verdict\n\nThe document is *mostly* coherent and internally consistent, and the core mathematical definitions of Transformer components (RMSNorm, attention, RoPE, causal masking, cross-entropy, perplexity, cosine LR schedule, etc.) are broadly correct.\n\nHowever, there are a **few significant correctness/consistency issues** that I would fix before giving this to students\u2014especially around:\n\n1. **AdamW weight decay semantics** (your pseudocode is *not* equivalent to Algorithm 1 in AdamW as commonly implemented).\n2. **Resource/memory accounting sections** being inconsistent with the **SwiGLU (3-matrix)** FFN you ask them to implement.\n3. A **parameter-count claim** in the TinyStories hyperparameters that appears wrong under the baseline architecture as written.\n4. The tokenizer\u2019s **streaming / `encode_iterable` correctness** claims: as written, exact equivalence under arbitrary chunking is not actually guaranteed without additional constraints/logic.\n\nI\u2019ll go section-by-section and call out (a) factual accuracy checks, (b) math/algorithm correctness, (c) semantics/regex/symbol usage, and (d) suggested concrete edits.\n\n---\n\n## 1) Factual accuracy checks (things you state as facts)\n\n### Unicode version / counts\n- You state: \u201cUnicode 16.0 (released in September 2024)\u2026 154,998 characters\u2026\u201d\n  - **Correct**: Unicode 16.0 totals **154,998** characters. ([unicode.org](https://www.unicode.org/versions/Unicode16.0.0/?utm_source=openai))  \n  - Release timing: Unicode 16.0 was planned/released in **September 2024** (Unicode\u2019s beta page references Sept 10, 2024). ([unicode.org](https://www.unicode.org/versions/beta-16.0.0.html?utm_source=openai))\n- You also state \u201cacross 168 scripts.\u201d I did not find an authoritative Unicode Consortium page in the quick checks that states the *total script count* in exactly that phrasing, but it\u2019s a commonly cited figure for Unicode 16.0-era script counts (and by Unicode 17.0 it\u2019s often cited as 172 scripts). If you want this to be maximally defensible, consider:\n  - Either remove the exact number (\u201cacross many scripts\u201d), or\n  - Cite a specific source that enumerates the script count for Unicode 16.0.\n\n### UTF-8 share of webpages\n- You state: \u201cUTF-8 \u2026 (more than 98% of all webpages).\u201d\n  - **Correct** and even slightly conservative: W3Techs reports **98.9%** (Feb 2026) for sites with known encoding. ([w3techs.com](https://w3techs.com/technologies/details/en-utf8?utm_source=openai))\n\n### TinyStories dataset sizes\n- You claim TinyStories validation has ~22k documents and training ~2.12M.\n  - This matches the Hugging Face dataset viewer (\u201ctrain \u00b7 2.12M rows\u201d, \u201cvalidation (22k rows)\u201d). ([huggingface.co](https://huggingface.co/datasets/roneneldan/TinyStories/viewer/default/train))\n\n### GPT\u20112 XL configuration values\n- In the accounting problem you use: vocab_size 50,257; context_length 1,024; layers 48; d_model 1,600; heads 25; d_ff 6,400.\n  - Those are the standard GPT\u20112 XL hyperparameters (HF config files show n_ctx=1024, n_embd=1600, n_layer=48, n_head=25, vocab_size=50257). ([huggingface.co](https://huggingface.co/wjn1996/hugnlp-hugchat-gpt2-xl/blob/063394a9c4d771872d72a4e537b75dc0552bf92b/config.json?utm_source=openai))\n\n### A100 FP32 peak FLOPs\n- You state: \u201cA100 \u2026 19.5 teraFLOP/s for float32 operations.\u201d\n  - NVIDIA\u2019s spec page lists FP32 = **19.5 TFLOPS**. ([nvidia.com](https://www.nvidia.com/en-us/data-center/a100/?utm_source=openai))\n\n---\n\n## 2) Major mathematical/algorithmic issues to fix\n\n### Issue A \u2014 **AdamW pseudocode applies weight decay to the *post-update* parameters**\nIn \u00a74.3 you say:\n\n> \u03b8 \u2190 \u03b8 \u2212 \u03b1_t m/(\u221av + \u03b5)  \n> \u03b8 \u2190 \u03b8 \u2212 \u03b1 \u03bb \u03b8\n\nThis is **not equivalent** to the common AdamW update (and not equivalent to the single-line \u201cdecoupled weight decay\u201d form used in many references / PyTorch), which conceptually applies weight decay proportional to the **current parameters** (the pre-update \u03b8), not the already-updated \u03b8.\n\n- If you apply it sequentially as written, you get:  \n  \\[\n  \\theta \\leftarrow (\\theta_{\\text{old}} - \\Delta_{\\text{adam}})\\;-\\;\\alpha\\lambda(\\theta_{\\text{old}} - \\Delta_{\\text{adam}})\n  = (1-\\alpha\\lambda)\\theta_{\\text{old}} - (1-\\alpha\\lambda)\\Delta_{\\text{adam}}\n  \\]\n- The more standard AdamW decoupled form is:  \n  \\[\n  \\theta \\leftarrow (1-\\alpha\\lambda)\\theta_{\\text{old}} - \\Delta_{\\text{adam}}\n  \\]\nThese differ by a multiplicative factor on the Adam step.\n\n**Recommended fix (text + pseudocode):**\nWrite the update in one of these equivalent forms:\n\n**Form 1 (multiplicative decay, then Adam update):**\n1. \\(\\theta \\leftarrow (1-\\alpha\\lambda)\\theta\\)\n2. \\(\\theta \\leftarrow \\theta - \\alpha_t \\frac{m}{\\sqrt{v}+\\epsilon}\\)\n\n**Form 2 (single line):**\n\\[\n\\theta \\leftarrow \\theta - \\alpha_t \\frac{m}{\\sqrt{v}+\\epsilon} - \\alpha \\lambda \\theta\n\\]\n\u2026but be explicit that the \\(\\theta\\) on the RHS is the **pre-step** value.\n\nAlso: you say \u201cimplement AdamW as described in algorithm 1 of Loshchilov and Hutter [2019]\u201d\u2014that pushes students toward matching the reference implementation, so this mismatch is worth fixing.\n\n---\n\n### Issue B \u2014 **Accounting sections assume a 2-matrix FFN, but you require SwiGLU (3 matrices)**\n\nThere are multiple places where the document\u2019s accounting assumes a classic 2-layer MLP:\n\n- In `adamwAccounting` you say \u201cAssume d_ff = 4 \u00d7 d_model\u201d and when listing activation components you include:\n  - \u201cPosition-wise feed-forward: W1 matrix multiply, SiLU, W2 matrix multiply\u201d\n  - This corresponds to **SiLU FFN**, not **SwiGLU FFN**, and it omits the **W3** projection and the gating multiply.\n\nBut earlier in \u00a73.5.2 you very clearly define and require **SwiGLU**:\n\n\\[\n(\\text{SiLU}(xW_1^\\top)\\odot xW_3^\\top)W_2^\\top\n\\]\n\n**Why this matters:**\n- **Parameter counts** change by ~50% for FFN if you keep the same d_ff but go from 2 matrices \u2192 3 matrices.\n- **Activation memory** changes (you now need to store gate/up activations for backprop).\n- **FLOPs** change (extra matmul + elementwise multiply).\n\n**Recommended fixes: pick one of these strategies and make it consistent:**\n\n1) **If you want accounting to match the model students build**, then:\n- Use **SwiGLU** in the activation accounting list:  \n  \u201cW1 matmul, W3 matmul, SiLU, elementwise multiply, W2 matmul\u201d.\n- Decide whether `d_ff` is:\n  - the **SwiGLU inner dimension** (common), or\n  - the **equivalent 4\u00d7 dimension** used for 2-matrix MLPs.  \n  Then define it consistently across the handout.\n\n2) **If you want to simplify accounting by assuming a classic 2-matrix FFN**, then:\n- Say explicitly in the accounting problems:  \n  \u201cFor accounting only, assume a standard 2-layer FFN with d_ff = 4 d_model,\u201d  \n  and warn students the numbers won\u2019t match their SwiGLU implementation exactly.\n\nRight now it reads like the accounting is *about the exact architecture you built*, but it isn\u2019t.\n\n---\n\n### Issue C \u2014 \u201cGPT\u20112 XL\u201d parameter/FLOP accounting will **not match GPT\u20112 XL** if using your SwiGLU block\nYour architecture differs from GPT\u20112 in at least:\n- RMSNorm vs LayerNorm (minor for parameter count)\n- RoPE vs learned positional embeddings (parameter count differs)\n- SwiGLU vs GELU MLP (big difference)\n- Bias-less linears (GPT\u20112 uses biases)\n\nSo: \u201cConsider GPT\u20112 XL \u2026 configuration \u2026 How many parameters would *our model* have?\u201d is fine if you mean \u201cGPT\u20112 XL-shaped widths/depths\u201d, but it\u2019s misleading to call it GPT\u20112 XL without clarifying \u201cnot the same architecture.\u201d\n\n**Recommended fix:**\nRename to something like:\n- \u201cGPT\u20112 XL *dimensions*\u201d or \u201cGPT\u20112 XL-shaped (L=48, d_model=1600, \u2026) model\u201d.\n\n---\n\n### Issue D \u2014 TinyStories \u201c~17M non-embedding parameters\u201d appears incorrect as written\nIn \u00a77.2 you state:\n\n> 4 layers, 16 heads\u2026 Together, this will give about 17M non-embedding parameters\u2026\n\nBut with your baseline as described earlier (separate token embedding and LM head; SwiGLU FFN with `d_ff=1344`), rough counts look like:\n\nPer layer (d_model=512, d_ff=1344):\n- Attention weights: Wq, Wk, Wv, Wo each 512\u00d7512 \u21d2 4\u00d7262,144 \u2248 1.05M\n- SwiGLU weights: W1 1344\u00d7512 \u2248 0.69M; W3 same \u2248 0.69M; W2 512\u00d71344 \u2248 0.69M \u21d2 ~2.06M\n- Total per layer \u2248 3.11M (norm gains are tiny)\n\nFor 4 layers: ~12.4M **non-embedding** parameters.\n\nEmbeddings (vocab=10k):\n- Token embedding: 10,000\u00d7512 \u2248 5.12M\n- LM head: 512\u00d710,000 \u2248 5.12M (if untied)\n\nSo:\n- **Non-embedding** \u2248 12.4M\n- **Total with untied embeddings** \u2248 22.6M\n- **Total with tied embeddings** \u2248 17.5M (because you count embeddings once)\n\nYour \u201c17M non-embedding\u201d looks like it might accidentally be \u201c~17M total with weight tying,\u201d but weight tying is not stated as part of the baseline there (it\u2019s mentioned later as an optional leaderboard mod).\n\n**Recommended fix:**\nReplace that sentence with something like:\n\n- \u201cThis is ~12\u201313M non-embedding parameters; total is ~22\u201323M if you keep the input embedding and LM head untied (or ~17\u201318M if you tie them).\u201d\n\n---\n\n## 3) Tokenizer: semantics + regex + correctness issues\n\n### Issue E \u2014 `encode_iterable` streaming correctness is overstated / underspecified\nYou correctly warn earlier:\n\n> Because the pattern includes an optional leading space (` ?`), this pre-tokenization is context-sensitive at chunk boundaries\u2026\n\nBut then in \u00a72.6 you say:\n\n> To efficiently tokenize a large file\u2026 break it up into manageable chunks\u2026 make sure that a token doesn't cross chunk boundaries, else we'll get a different tokenization than tokenizing the entire sequence in-memory.\n\nThis is true, but **not sufficient** for this regex-based pretokenizer:\n- Tokens *can* effectively cross naive chunk boundaries in the sense that whether a space attaches to the following token depends on context.\n- Also, the regex has alternatives like `\\s+` that can swallow arbitrarily long whitespace spans; splitting lines/chunks changes grouping, and thus can change BPE merges (since merges are learned within a pre-token).\n\n**Recommended fix options (choose one and state it explicitly):**\n\n1) **Constrain what `encode_iterable` promises**\n   - Say: \"`encode_iterable` is memory-efficient for iterables of *document strings* (or strings that are already safe chunk boundaries, e.g., split on `<|endoftext|>`), not arbitrary chunking.\"\n   - And recommend users pass an iterable where each element is a full document or document chunk.\n\n2) **Require exact equivalence to `encode(''.join(iterable))`**\n   - Then you must instruct students to implement buffering logic:\n     - Keep a rolling buffer,\n     - Only emit tokens for regex matches that are guaranteed complete,\n     - Carry over a suffix that might affect the next chunk (e.g., trailing spaces / partial word / partial special token).\n   - This is doable, but nontrivial; without guidance many students will \u201ctokenize each line independently\u201d and silently get different results.\n\nGiven the educational goals, I\u2019d recommend option (1): require streaming over *safe boundaries* (e.g., `<|endoftext|>`-delimited docs), because it aligns with your earlier chunking advice in training.\n\n---\n\n### Regex PAT correctness\nThe GPT\u20112-style pretokenizer pattern you give:\n\n```python\nPAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n```\n\n- This is a known variant of the GPT\u20112 regex (often written as `'s|'t|'re|...| ?\\p{L}+|...`). ([zenn.dev](https://zenn.dev/syoyo/articles/c33b082d458283?utm_source=openai))\n- Your explanation about optional leading space and chunk-boundary sensitivity is correct.\n\n**Small semantic note:** This is the GPT\u20112-style regex; it is not the newer `cl100k_base` / GPT\u20114-style pattern (which has some different pieces like `\\p{N}{1,3}` etc.). If that distinction matters for students, you might explicitly say \u201cGPT\u20112 pretokenizer regex\u201d.\n\n---\n\n### BPE training example: notation is slightly misleading (but fixable)\nIn `bpe_example`, you describe pair counts like:\n\n> {`b\"lo\"`: 7, `b\"ow\"`: 7, \u2026}\n\nBut earlier you define that you represent a pre-token as `tuple[bytes]` of single-byte symbols. Then the adjacent pair is **not** `b\"lo\"`, it\u2019s `(b\"l\", b\"o\")`, and the *merged token* becomes `b\"lo\"`.\n\nStudents often get tripped up here.\n\n**Recommended micro-edit:**\nChange to something like:\n\n> We count adjacent symbol pairs, e.g. `(b\"l\", b\"o\") \u2192 7`, `(b\"o\", b\"w\") \u2192 7`, \u2026\n\n\u2026and then say the merge creates the new token `b\"lo\"`.\n\n---\n\n### Type annotations: `tuple[bytes]` is wrong for variable-length tuples\nYou use (multiple times) something like:\n\n> `dict[tuple[bytes], int]`, e.g. `{(b\"l\", b\"o\", b\"w\"): 5 ...}`\n\nIn Python typing, `tuple[bytes]` means a **1-tuple** of bytes, not a variable-length tuple. The correct type is:\n\n- `tuple[bytes, ...]`\n\nSame for lists of pairs etc. (You get those right elsewhere.)\n\n---\n\n### Special token splitting via regex: suggest a safer one-liner\nYou say:\n\n> `re.split` with `\"|\".join(special_tokens)` \u2026 with careful use of `re.escape`\n\nGood\u2014but many students will forget and get subtle bugs.\n\nConsider giving the exact safe snippet:\n\n```python\npattern = \"|\".join(map(re.escape, special_tokens))\nparts = re.split(pattern, text)\n```\n\nAnd if they need to *keep* delimiters (for encoding special tokens), they can use capturing parentheses:\n\n```python\npattern = \"(\" + \"|\".join(map(re.escape, special_tokens)) + \")\"\nparts = re.split(pattern, text)  # keeps specials in the output list\n```\n\n---\n\n## 4) Transformer math / symbols / explanations: mostly correct, with a few clarity fixes\n\n### RMSNorm (\u00a73.5.1)\n- Formula is correct.\n- Upcasting to float32 is good advice.\n\n**Optional improvement:** mention that `g` (gain) should be in the module\u2019s parameter dtype, but multiplication should occur in float32 before downcast, to avoid precision loss in bf16/fp16.\n\n---\n\n### RoPE (\u00a73.5.3)\nYour RoPE description is mathematically consistent:\n- Angle schedule \\(\\theta_{i,k} = i / \\Theta^{(2k-2)/d}\\) with k starting at 1 matches the common 0-index form.  \n- You correctly note `d_k` must be even.\n\n**Two implementation-clarity notes:**\n1) If students do multi-head attention with `d_k = d_model/num_heads`, you should explicitly remind them that **per-head `d_k`** must be even (not just `d_model`).\n2) Token positions shape: you say token_positions is `(..., seq_len)`. That\u2019s good, but it\u2019s unusual for token positions to have all batch dims; often it\u2019s just `(seq_len,)`. Maybe explicitly allow both via broadcasting.\n\n---\n\n### Softmax + attention masking (\u00a73.5.4)\nThe numerically stable \u201csubtract max\u201d explanation is good.\n\n**One missing practical detail:** for fp16/bf16 attention logits, it\u2019s common to compute softmax in float32. You do this for RMSNorm but don\u2019t mention it here. If you expect students to train in bf16/fp16, this omission can cause training instability.\n\n**Suggested addition:** \u201cCompute the attention logits and softmax in float32 (cast back afterward), especially when using bf16/fp16.\u201d\n\nMask semantics (True=keep, False=block) are clearly stated\u2014good.\n\n---\n\n### Multi-head attention equations (\u00a73.5.5)\nYour Eq. 12 defines MultiHead as concat(heads), and Eq. 14 applies the output projection \\(W_O\\). This is fine.\n\n**But** some students may read Eq. 12 as \u201cMultiHead does not include the output projection,\u201d which differs from the Vaswani definition. If you want to avoid confusion, define:\n\n\\[\n\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h) W_O^\\top\n\\]\n\nand then self-attention is just plugging in \\(Q=xW_Q^\\top\\), etc.\n\nNot a correctness error, just clarity.\n\n---\n\n## 5) Training math / loss / schedule: mostly correct, a few ambiguity fixes\n\n### Cross-entropy\nYour definition is correct.\n\n**Potential ambiguity to fix:** the problem statement says:\n> \u201cHandle any additional batch dimensions and return the average across the batch.\u201d\n\nBut for LM training, you usually average across **(batch \u00d7 sequence)** positions, not just batch. You probably mean \u201caverage across all non-vocab dimensions.\u201d\n\n**Suggested rewording:**\n> \u201c\u2026return the mean over all leading dimensions (e.g., batch and sequence positions).\u201d\n\nAlso recommend float32 for logsumexp.\n\n---\n\n### Learning rate schedule indexing\nYou define:\n- warmup if \\(t < T_w\\)\n- cosine if \\(T_w \\le t \\le T_{\\text{end}}\\)\n\nBut you never define whether **t starts at 0 or 1**. In AdamW pseudocode you start t at 1. In LR schedules it\u2019s common to start at 0.\n\nThis matters for tests and for whether the first step uses lr=0.\n\n**Suggested fix:** explicitly state:\n- \u201cLet t be the **0-indexed** optimization step (t = 0,1,2,\u2026)\u201d  \nor\n- \u201cLet t be **1-indexed** \u2026\u201d\n\n\u2026and ensure the tests align.\n\n---\n\n## 6) Smaller coherency / editorial issues (worth fixing)\n\n- Several problems list \u201c(unspecified points)\u201d; likely placeholders:\n  - `cross_entropy`\n  - `learning_rate_schedule`\n- \u201cPyTorch.org/docs\u2026\u201d capitalization is odd but harmless.\n- \u201cWe recommend you to read them!\u201d \u2192 \u201cWe recommend you read them.\u201d\n- Some terminology flips between \u201cpre-token\u201d, \u201cpretoken\u201d, \u201cpre-tokenize\u201d (fine, but pick one).\n- In \u00a72.5: \u201cwe never want to merge across document boundaries\u201d \u2014 correct for your setup, but note that this is a *design choice* (not a universal truth about BPE).\n\n---\n\n## 7) Summary of recommended concrete changes (quick checklist)\n\nIf you want the smallest patch that fixes the important issues:\n\n1) **Fix AdamW pseudocode** so weight decay is applied to the pre-update parameters (or multiplicative decay form).\n2) **Make accounting consistent with SwiGLU** (add W3 + gating, and clarify what d_ff means).\n3) **Fix/clarify the \u201c17M non-embedding params\u201d** claim in TinyStories baseline.\n4) **Clarify `encode_iterable` guarantees**: either restrict input to `<|endoftext|>`-aligned chunks/docs, or provide real buffering guidance for exact equivalence.\n5) Replace `tuple[bytes]` with `tuple[bytes, ...]` in typing examples.\n6) In BPE example, represent counted pairs as `(b\"l\", b\"o\")` etc., reserving `b\"lo\"` for merged tokens.\n\n---\n\nIf you want, I can also produce:\n- A revised AdamW algorithm box (drop-in replacement),\n- A corrected parameter-count estimate for the TinyStories baseline (with/without weight tying),\n- A \u201cstreaming tokenizer\u201d specification that is actually achievable and testable (and doesn\u2019t accidentally require full buffering)."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"can_save":true,"model_slug":"gpt-5.2-pro","default_model_slug":"gpt-5.2-pro"},"recipient":"all","channel":null},"parent":"2539351f-e0f1-415d-9b1d-68b388628e59","children":[]}}}