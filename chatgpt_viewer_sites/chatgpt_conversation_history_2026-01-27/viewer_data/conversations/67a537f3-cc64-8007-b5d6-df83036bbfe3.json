{"id":"67a537f3-cc64-8007-b5d6-df83036bbfe3","title":"Extending LLM Project SFT RLHF","create_time":1738881012.008678,"update_time":1738881501.487542,"current_node":"1783b677-e47c-437f-ae0e-48b1f44d1de3","mapping":{"aaa190b9-6f74-4790-aa62-5f8bc41bf664":{"id":"aaa190b9-6f74-4790-aa62-5f8bc41bf664","message":null,"parent":null,"children":["f4a93259-185f-4a51-b425-3c4ca32be725"]},"f4a93259-185f-4a51-b425-3c4ca32be725":{"id":"f4a93259-185f-4a51-b425-3c4ca32be725","message":{"id":"f4a93259-185f-4a51-b425-3c4ca32be725","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"aaa190b9-6f74-4790-aa62-5f8bc41bf664","children":["51a34499-86c2-4912-8112-a18db6fe85a7"]},"51a34499-86c2-4912-8112-a18db6fe85a7":{"id":"51a34499-86c2-4912-8112-a18db6fe85a7","message":{"id":"51a34499-86c2-4912-8112-a18db6fe85a7","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"f4a93259-185f-4a51-b425-3c4ca32be725","children":["35a31511-1a19-48a8-a049-4a2b64c624fd"]},"35a31511-1a19-48a8-a049-4a2b64c624fd":{"id":"35a31511-1a19-48a8-a049-4a2b64c624fd","message":{"id":"35a31511-1a19-48a8-a049-4a2b64c624fd","author":{"role":"user","name":null,"metadata":{}},"create_time":1738881012.038437,"update_time":null,"content":{"content_type":"text","parts":["I have a created a Github project which showcases a toy BPE and transformer model to illustrate the process of training an LLM, for educational purposes. Research into how I can extend such a project to illustrate the process of supervised fine tuning and RLHF that would reflect accurately what happens at scale in industry."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"system_hints":["research"],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"90de9552cabc87f9-LAX","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"51a34499-86c2-4912-8112-a18db6fe85a7","children":["7c81b76a-632a-4b3e-9ca2-7ee16faba2f4"]},"7c81b76a-632a-4b3e-9ca2-7ee16faba2f4":{"id":"7c81b76a-632a-4b3e-9ca2-7ee16faba2f4","message":{"id":"7c81b76a-632a-4b3e-9ca2-7ee16faba2f4","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1738881014.49885,"update_time":null,"content":{"content_type":"text","parts":["That sounds like a great project! To make sure the research aligns with your needs, could you clarify:\n\n- Are you looking for a high-level conceptual guide, or do you need detailed code implementations?\n- Do you want recommendations on open-source frameworks and libraries that facilitate SFT and RLHF?\n- Are you specifically interested in mimicking the infrastructure and scale of industry models (e.g., training on large-scale distributed clusters), or just the core methodology?\n- Would you like insights into datasets commonly used in fine-tuning and RLHF?\n- Are there any particular industry practices or companies you want to focus on (e.g., OpenAI, Anthropic, DeepMind, etc.)?\n\nLet me know so I can tailor the research effectively!"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"message_type":null,"model_slug":"research","default_model_slug":"o3-mini-high","parent_id":"35a31511-1a19-48a8-a049-4a2b64c624fd","request_id":"90de9552cabc87f9-LAX","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"35a31511-1a19-48a8-a049-4a2b64c624fd","children":["56a329a0-d947-4dad-89b3-f976d58b7016"]},"56a329a0-d947-4dad-89b3-f976d58b7016":{"id":"56a329a0-d947-4dad-89b3-f976d58b7016","message":{"id":"56a329a0-d947-4dad-89b3-f976d58b7016","author":{"role":"tool","name":"research_kickoff_tool.clarify_with_text","metadata":{}},"create_time":1738881014.49897,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"command":"clarify_with_text","message_type":null,"model_slug":"research","default_model_slug":"o3-mini-high","parent_id":"7c81b76a-632a-4b3e-9ca2-7ee16faba2f4","request_id":"90de9552cabc87f9-LAX","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"7c81b76a-632a-4b3e-9ca2-7ee16faba2f4","children":["7a8f2b38-5767-44ea-9018-8f8cc2893dec"]},"7a8f2b38-5767-44ea-9018-8f8cc2893dec":{"id":"7a8f2b38-5767-44ea-9018-8f8cc2893dec","message":{"id":"7a8f2b38-5767-44ea-9018-8f8cc2893dec","author":{"role":"user","name":null,"metadata":{}},"create_time":1738881207.90097,"update_time":null,"content":{"content_type":"text","parts":["I am looking for a conceptual guide with example code if possible. My goal is that students should code these from scratch rather than relying on just calling external libraries for high-level operations. The focus is on core methodology, for students new to NLP and LLMs, not the minutiae of industry-level optimizations. Insights into data sets would be helpful.  "]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"system_hints":["research"],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"90de9a1b7caa87f9-LAX","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"56a329a0-d947-4dad-89b3-f976d58b7016","children":["c779c311-3880-44d2-b6f7-54f92ba9842d"]},"c779c311-3880-44d2-b6f7-54f92ba9842d":{"id":"c779c311-3880-44d2-b6f7-54f92ba9842d","message":{"id":"c779c311-3880-44d2-b6f7-54f92ba9842d","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1738881215.425071,"update_time":null,"content":{"content_type":"text","parts":["That sounds like an excellent educational initiative! I will research how to extend your toy BPE and transformer model to illustrate supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). The focus will be on core methodology, providing a conceptual guide with example code that students can implement from scratch rather than relying on high-level external libraries. I will also include insights into datasets that are commonly used in fine-tuning and RLHF processes.\n\nI\u2019ll get back to you with structured insights, example implementations, and dataset recommendations shortly."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"message_type":null,"model_slug":"research","default_model_slug":"o3-mini-high","parent_id":"7a8f2b38-5767-44ea-9018-8f8cc2893dec","request_id":"90de9a1b7caa87f9-LAX","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"7a8f2b38-5767-44ea-9018-8f8cc2893dec","children":["5ae67228-f9d9-4684-942d-514ccc84af71"]},"5ae67228-f9d9-4684-942d-514ccc84af71":{"id":"5ae67228-f9d9-4684-942d-514ccc84af71","message":{"id":"5ae67228-f9d9-4684-942d-514ccc84af71","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1738881215.425159,"update_time":null,"content":{"content_type":"code","language":"json","response_format_name":null,"text":"{\"task_violates_safety_guidelines\":false,\"user_def_doesnt_want_research\":false,\"response\":\"That sounds like an excellent educational initiative! I will research how to extend your toy BPE and transformer model to illustrate supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). The focus will be on core methodology, providing a conceptual guide with example code that students can implement from scratch rather than relying on high-level external libraries. I will also include insights into datasets that are commonly used in fine-tuning and RLHF processes.\\n\\nI\u2019ll get back to you with structured insights, example implementations, and dataset recommendations shortly.\",\"title\":\"Extending Toy Transformer Model for Supervised Fine-Tuning and RLHF\",\"prompt\":\"Provide a conceptual guide on how to extend a toy Byte Pair Encoding (BPE) and transformer model to illustrate the process of supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). The focus should be on core methodology, ensuring students can code these techniques from scratch rather than relying on high-level operations from external libraries.\\n\\n### Requirements:\\n1. **Supervised Fine-Tuning (SFT):**\\n   - Explain the theoretical foundations and process.\\n   - Provide step-by-step code illustrating how to fine-tune a small transformer model using labeled datasets.\\n   - Discuss best practices for data preprocessing, loss functions, and evaluation.\\n\\n2. **Reinforcement Learning with Human Feedback (RLHF):**\\n   - Outline the conceptual steps involved (reward modeling, preference optimization, policy updates).\\n   - Provide example code demonstrating a simple implementation of RLHF.\\n   - Discuss how to design a reward function and how to optimize a policy using reinforcement learning techniques like PPO.\\n\\n3. **Datasets:**\\n   - Recommend open datasets commonly used for fine-tuning and RLHF.\\n   - Explain how to preprocess these datasets to make them suitable for fine-tuning a transformer model.\\n\\n4. **Implementation Guidelines:**\\n   - Keep the code simple and beginner-friendly, focusing on core methodology.\\n   - Avoid reliance on high-level functions from external libraries; instead, use raw tensor operations where possible.\\n   - Ensure that students can run experiments on standard hardware (e.g., a single GPU or CPU-based setup).\\n\\nProvide a structured response with clear sections, including theoretical explanations, code snippets, and dataset recommendations.\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002,200007]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"research","default_model_slug":"o3-mini-high","parent_id":"c779c311-3880-44d2-b6f7-54f92ba9842d","request_id":"90de9a1b7caa87f9-LAX","timestamp_":"absolute","is_loading_message":true},"recipient":"research_kickoff_tool.start_research_task","channel":null},"parent":"c779c311-3880-44d2-b6f7-54f92ba9842d","children":["934e5739-0fd9-4140-a5de-d0918ccf030f"]},"934e5739-0fd9-4140-a5de-d0918ccf030f":{"id":"934e5739-0fd9-4140-a5de-d0918ccf030f","message":{"id":"934e5739-0fd9-4140-a5de-d0918ccf030f","author":{"role":"assistant","name":"research_kickoff_tool.start_research_task","metadata":{}},"create_time":1738881215.425216,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"async_task_id":"task_jppz9lhKUitEedBm0nVO9ji1","async_task_conversation_id":"67a538be-9e98-8007-8293-a5b32f85af36","async_task_created_at":"2025-02-06 22:33:35.321834+00:00","async_task_title":"Extending Toy Transformer Model for Supervised Fine-Tuning and RLHF","async_task_prompt":"Provide a conceptual guide on how to extend a toy Byte Pair Encoding (BPE) and transformer model to illustrate the process of supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). The focus should be on core methodology, ensuring students can code these techniques from scratch rather than relying on high-level operations from external libraries.\n\n### Requirements:\n1. **Supervised Fine-Tuning (SFT):**\n   - Explain the theoretical foundations and process.\n   - Provide step-by-step code illustrating how to fine-tune a small transformer model using labeled datasets.\n   - Discuss best practices for data preprocessing, loss functions, and evaluation.\n\n2. **Reinforcement Learning with Human Feedback (RLHF):**\n   - Outline the conceptual steps involved (reward modeling, preference optimization, policy updates).\n   - Provide example code demonstrating a simple implementation of RLHF.\n   - Discuss how to design a reward function and how to optimize a policy using reinforcement learning techniques like PPO.\n\n3. **Datasets:**\n   - Recommend open datasets commonly used for fine-tuning and RLHF.\n   - Explain how to preprocess these datasets to make them suitable for fine-tuning a transformer model.\n\n4. **Implementation Guidelines:**\n   - Keep the code simple and beginner-friendly, focusing on core methodology.\n   - Avoid reliance on high-level functions from external libraries; instead, use raw tensor operations where possible.\n   - Ensure that students can run experiments on standard hardware (e.g., a single GPU or CPU-based setup).\n\nProvide a structured response with clear sections, including theoretical explanations, code snippets, and dataset recommendations.","async_task_type":"research","async_task_status_messages":{"initial":"Extending Toy Transformer Model for Supervised Fine-Tuning and RLHF","completed_with_time":"Research completed in 5m","completed_no_time":"Completed research","completed":"Researched for ","error":"Research failed","cancelled":"Research cancelled"},"b1de6e2_s":true,"command":"start_research_task","citations":[],"content_references":[],"message_type":null,"model_slug":"research","default_model_slug":"o3-mini-high","parent_id":"5ae67228-f9d9-4684-942d-514ccc84af71","request_id":"90de9a1b7caa87f9-LAX","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"5ae67228-f9d9-4684-942d-514ccc84af71","children":["1783b677-e47c-437f-ae0e-48b1f44d1de3"]},"1783b677-e47c-437f-ae0e-48b1f44d1de3":{"id":"1783b677-e47c-437f-ae0e-48b1f44d1de3","message":{"id":"1783b677-e47c-437f-ae0e-48b1f44d1de3","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1738881500.831348,"update_time":null,"content":{"content_type":"text","parts":["# Supervised Fine-Tuning (SFT)\n\n**Theory and Purpose:** Supervised fine-tuning is the process of taking a pre-trained language model (e.g., a small Transformer already trained on generic text) and further training it on task-specific, labeled data. The goal is to teach the model specific behaviors or tasks through example input-output pairs. This step leverages standard supervised learning (usually minimizing a cross-entropy loss) to **prime the model** to produce desired outputs in response to certain inputs\u30109\u2020L95-L103\u3011. In the context of alignment (making the model follow instructions or human preferences), SFT is often the **first step** after pre-training, preparing the model to respond appropriately to various prompts before any reinforcement learning is applied\u30109\u2020L99-L107\u3011. Essentially, SFT uses human-curated examples to adjust the model\u2019s weights so that it learns the patterns of the desired task or style of response.\n\n**Process Overview:**\n\n1. **Prepare a Labeled Dataset:** Collect a dataset of prompts and correct responses (or inputs and desired outputs). For instance, if building an instruction-following model, the dataset might consist of an instruction (prompt) and an ideal assistant answer. Ensure the data covers the range of scenarios of interest and is cleaned for any noise. Each example should demonstrate the behavior you want from the model\u30101\u2020L247-L255\u3011.\n\n2. **Tokenization (BPE):** Use your Byte Pair Encoding tokenizer to convert text to token IDs. The BPE tokenizer, which merges frequent character pairs into subword tokens, will break the prompt and response into tokens from the model\u2019s vocabulary. This ensures the input text can be fed to the Transformer. For example, using a simple BPE implementation, you would first train the BPE on a corpus to get a vocab and merge rules, then apply it to your dataset text to get sequences of token IDs\u301028\u2020L23-L31\u3011\u301028\u2020L47-L55\u3011. Remember to add any special tokens your model uses (e.g., an end-of-sequence token or separator between prompt and response).\n\n3. **Model Architecture:** Define a small Transformer-based language model. For a toy setup, this could be a decoder-only Transformer (like a mini GPT) with an embedding layer, a few self-attention layers, and a final output layer projecting to vocabulary size. If using an encoder-decoder model for tasks like translation or summarization, you\u2019d have an encoder for the input and a decoder for the output. In code, you might define a `TransformerModel` class in PyTorch that includes:\n   - An embedding layer for token IDs.\n   - Positional embeddings or encodings.\n   - One or more Transformer blocks (each with self-attention, possibly feed-forward, layer norm).\n   - An output projection to get logits for each token in the vocab.\n\n4. **Fine-Tuning Procedure:** Initialize the model with the pre-trained weights (for a toy example, you might start from random if a true pre-trained model isn\u2019t available, but typically SFT means starting from a pre-trained base). Then iterate over the training dataset:\n   - For each input-output pair, concatenate or otherwise prepare the sequence the model should process. Often, for decoder-only models, we concatenate the prompt and the target output, separated by a special token, or just append and let the model learn to transition. Ensure the model knows which part to predict: sometimes only the *output* portion is included in the loss.\n   - Compute the model\u2019s output probabilities (logits) for the sequence. For a decoder-only LM fine-tuned on prompt\u2192response, one common approach is to include the prompt and response in one sequence but mask out the prompt tokens in the loss. Another approach is to feed the prompt, then generate the output while computing loss only on the output tokens.\n   - Compute the **loss**. Typically, use token-level cross-entropy between the model\u2019s predicted distribution and the ground truth next tokens. For example, if `y` is the sequence of token IDs for the desired output, and `p(y_t|x)` is the model\u2019s probability for the true token at position *t*, the loss is \u2013\u2211 log *p(y_t)* (averaged over tokens). Many frameworks have a utility, but you can implement it manually by taking the model\u2019s logits, applying `F.log_softmax`, and indexing the log-probabilities of the true tokens.\n   - Backpropagate the loss and update model weights using an optimizer like Adam. Use a relatively small learning rate (fine-tuning typically uses lower LR than training from scratch to avoid wrecking pre-trained features).\n   - Iterate for several epochs, monitoring the loss.\n\n5. **Evaluation:** Set aside a validation set to periodically evaluate the model\u2019s performance on held-out examples. For classification tasks, measure accuracy; for language generation, you might measure perplexity or qualitative alignment with expected answers. This ensures the model is learning the desired behavior without overfitting or devolving in quality\u30101\u2020L251-L255\u3011.\n\n**Example Code \u2013 Fine-Tuning a Tiny Transformer:** Below is a simplified example (written in PyTorch-like pseudocode) that illustrates the fine-tuning loop. This assumes we have `tokenize()` from our BPE, a `TransformerModel` class, and a dataset of (prompt, response) pairs. We\u2019ll fine-tune the model to minimize next-token prediction loss on the response given the prompt:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\n# Toy dataset: list of (prompt, response) examples\ntrain_data = [\n    (\"User: Hello, how are you?\", \"Assistant: I am doing well, thank you.\"),\n    # ... (more examples)\n]\n\n# Suppose our BPE tokenizer has been trained and we have encode() method\n# Also assume special tokens like <bos>, <eos> (begin/end of sequence) if needed.\n# For simplicity, we'll just concatenate prompt and response with a separator token.\nSEP_TOKEN = \"<sep>\"\ntokenizer = MyBPETokenizer(...)  # already trained on some corpus\nvocab_size = tokenizer.vocab_size()\n\n# Simple Transformer model (e.g., decoder-only) definition\nmodel = TransformerModel(vocab_size=vocab_size, n_layers=2, d_model=128)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nfor epoch in range(num_epochs):\n    for prompt, response in train_data:\n        # 1. Tokenize prompt and response\n        prompt_ids = tokenizer.encode(prompt)\n        response_ids = tokenizer.encode(response)\n        # Construct input sequence: prompt + separator + response (target)\n        input_ids = prompt_ids + [tokenizer.token_to_id(SEP_TOKEN)] + response_ids\n        input_ids = torch.tensor(input_ids).unsqueeze(0)  # shape [1, seq_len]\n        # We want the model to predict the response given the prompt.\n        # Create target ids: usually the same as input but shifted for next-token prediction\n        # To only compute loss on the response part, we can mask prompt positions in loss.\n        target_ids = input_ids.clone()\n        # Mask prompt and separator tokens in target by setting them to -100 (ignore index in PyTorch)\n        target_ids[0, :len(prompt_ids)+1] = -100  # +1 for separator\n        # 2. Forward pass: get logits for each position\n        logits = model(input_ids)            # shape [1, seq_len, vocab_size]\n        # 3. Compute loss on the response tokens\n        loss = F.cross_entropy(logits.view(-1, vocab_size), target_ids.view(-1), ignore_index=-100)\n        # 4. Backpropagate and update\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")\n```\n\nIn this code:\n- We encode the prompt and response using BPE. We insert a special `<sep>` token to clearly mark the boundary.\n- We feed the combined sequence to the model. The model (a Transformer) will output logits for each token position.\n- We prepare `target_ids` such that only the response tokens contribute to the loss (the prompt tokens are set to `-100` which PyTorch\u2019s `ignore_index` skips in loss calculation). This way, the model is **only penalized for mistakes on the response**, effectively learning to *generate* the response given the prompt.\n- We use cross-entropy loss and an optimizer step to update the model. Over time, the model learns to produce the correct response tokens after seeing the prompt.\n\n**Best Practices in SFT:**\n- *Data Formatting:* For instruction-following fine-tuning, it\u2019s common to serialize the prompt and response into a single text with clear tokens (like `\"<bos> User: ... <sep> Assistant: ... <eos>\"`). During training, you might train the model to predict everything after the user prompt. In classification tasks, ensure each input has the label in a suitable form (for example, as a special token or appended text).\n- *Hyperparameters:* Use a smaller learning rate for fine-tuning (often 1e-5 to 1e-4 range) and consider strategies like gradient clipping to stabilize training. Monitor training and validation loss to avoid overfitting. If the dataset is small, too many epochs can cause the model to memorize answers.\n- *Validation:* Continuously evaluate on a validation set. For generative tasks, you might generate model outputs for some prompts and compare them to references or use qualitative judgement. Automated metrics like BLEU/ROUGE can be used for translation or summarization, but for instruction-following, qualitative checks or task-specific metrics are useful.\n- *Preventing Catastrophic Forgetting:* When fine-tuning a large pre-trained model on a narrower dataset, there\u2019s a risk it forgets some of its general knowledge. To mitigate this, you can use techniques like **low learning rates**, or even mix in some of the original pre-training data or instruct data (as done in some InstructGPT training) to retain generality\u301019\u2020L231-L237\u3011.\n- *Checkpointing:* Save model checkpoints and possibly use early stopping. This way, if the model starts overfitting (validation loss rises), you can revert to a better checkpoint.\n\nBy the end of SFT, your model should already perform the task in a basic way \u2013 for example, follow simple instructions or answer questions in a manner consistent with the fine-tuning data. SFT essentially aligns the model to **human-provided demonstrations** before we introduce any feedback-based reinforcement learning.\n\n# Reinforcement Learning with Human Feedback (RLHF)\n\n**Conceptual Overview:** RLHF is an advanced alignment technique that further tunes the model using a reward signal derived from human feedback, rather than direct supervised labels. The process involves a few distinct steps and additional models:\n1. **Reward Modeling:** Train a *reward model* that takes in a piece of text (typically a prompt and a model-generated response) and outputs a scalar score indicating how desirable or \u201caligned with human preference\u201d that output is\u30109\u2020L131-L139\u3011. This model is trained on data where humans have expressed preferences (e.g. choosing the better of two responses).\n2. **Policy Optimization (RL):** Use reinforcement learning (e.g. Proximal Policy Optimization, PPO) to adjust the language model (the \u201cpolicy\u201d) so that it maximizes the reward given by the reward model. In practice, the language model generates responses to prompts, the reward model scores them, and those scores guide the policy update. A crucial element here is adding a constraint (like a KL-divergence penalty) to keep the model\u2019s behavior from straying too far from the fine-tuned policy (to avoid nonsensical but high-reward outputs)\u301019\u2020L221-L229\u3011.\n3. **Iterative Improvement:** Repeat the generation and policy update step, gradually improving the model\u2019s alignment with the measured preferences. This often requires balancing exploration (trying varied outputs) and exploitation (using the reward model\u2019s feedback to improve).\n\nIn summary, RLHF uses human feedback indirectly: first encapsulated in a reward function, then used to **reinforce** the model\u2019s behavior. This approach was famously used in training InstructGPT and ChatGPT\u301011\u2020L209-L217\u3011.\n\n## Reward Model Training\n\nThe reward model is typically a separate neural network (often a smaller Transformer or even the same model architecture with an extra head) that outputs a single scalar for a given [prompt, response] pair. The reward model is trained on a dataset of human preferences:\n- Humans are shown outputs from the model (or multiple models) and rank or rate them. For example, given a prompt and two possible responses, a human might label one response as preferable to the other.\n- From such data, we create training examples for the reward model. A common method is **pairwise comparison**: the input is two responses (A and B) for the same prompt, and the target is which one the human preferred. This can be turned into a loss function by having the reward model produce a score for A and for B, then applying a binary cross-entropy or hinge loss that pushes the model to give a higher score to the preferred output. In practice, one can pass each prompt-response through the reward model to get score_a and score_b, then maximize `sigmoid(score_a - score_b)` if A is preferred (this corresponds to the Bradley-Terry or logistic model for preferences).\n\n**Example:** Suppose we have a prompt and two model outputs, and a human label indicating which output was better. We can train the reward model as follows:\n```python\n# Pseudo-code for reward model training on one prompt with a preferred and a rejected response\nprompt = \"Explain the moon landing in simple terms.\"\nchosen_resp = \"The moon landing was when astronauts first walked on the moon in 1969...\"\nrejected_resp = \"It was a hoax filmed in a studio.\"\n# Tokenize inputs (assuming the reward model uses the same tokenizer)\ninp_chosen = tokenizer.encode(prompt + \"<sep>\" + chosen_resp)\ninp_rejected = tokenizer.encode(prompt + \"<sep>\" + rejected_resp)\n# Convert to tensors\ninp_chosen = torch.tensor(inp_chosen).unsqueeze(0)\ninp_rejected = torch.tensor(inp_rejected).unsqueeze(0)\n# Forward pass through reward model (could be a transformer encoder or decoder that outputs a single value)\nreward_model = RewardModel()  # e.g., a small transformer with a scalar output\nscore_chosen = reward_model(inp_chosen)    # shape [1,1]\nscore_rejected = reward_model(inp_rejected)  # shape [1,1]\n# We want score_chosen > score_rejected\n# One way: define a pairwise preference loss:\nreward_diff = score_chosen - score_rejected  # higher means chosen is better\n# Use a logistic loss: loss = -log(sigmoid(reward_diff))\nloss = -F.logsigmoid(reward_diff)  # this will be low (good) when score_chosen >> score_rejected\nloss.backward()\noptimizer.step()\n```\n\nIn practice, you would train on many such comparisons. Some open datasets provide this in a ready format. For example, the **OpenAI Summarize from Feedback** dataset contains thousands of human comparison judgments for summary quality\u301029\u2020L5-L13\u3011, and Anthropic\u2019s **HH-RLHF** dataset provides prompts with a \u201cchosen\u201d and \u201crejected\u201d response labeled by humans\u301017\u2020L151-L159\u3011. Using such data, you\u2019d train the reward model until it can reliably predict which response a human would prefer. This essentially captures human preferences in a model that can generalize to new prompts.\n\n**Important:** The reward model is not perfect \u2013 it\u2019s only as good as the data and can be biased or exploitable. However, it provides a differentiable signal to use in RL. Often the reward model is a smaller model for efficiency. For instance, OpenAI used a 6B reward model for a 175B policy in InstructGPT, and Anthropic used reward models of similar scale to their policy models\u301011\u2020L209-L218\u3011\u301017\u2020L171-L174\u3011.\n\n## Policy Optimization with PPO\n\nOnce we have a reward model, we use RL (specifically PPO) to adjust the policy (the main language model) to maximize the reward model\u2019s score. We treat the language model as a stochastic **policy** \u03c0_\u03b8 that, given a prompt (state), produces a sequence of tokens (action). The reward for an output is given by the reward model\u2019s score (possibly with modifications, see below). We want to update \u03b8 to increase expected reward.\n\n**Why PPO?** Proximal Policy Optimization is a popular policy-gradient algorithm that is relatively stable and efficient for high-dimensional action spaces like text generation. PPO optimizes the policy by maximizing an objective that clips the change in probabilities, preventing overly large updates that could destabilize training. OpenAI and others chose PPO because of its stability and the existence of tried-and-true implementations\u301019\u2020L197-L205\u3011\u301019\u2020L198-L203\u3011. In RLHF, we already have a good initial policy (after SFT), so we want to nudge it gradually toward higher reward \u2013 PPO\u2019s constraint on the policy update (the clipping or KL penalty) is useful for this.\n\n**Core Idea:** We generate outputs using the current policy, get a reward for each output from the reward model, and then treat this as an RL problem:\n- *State*: the prompt.\n- *Action*: the sequence of tokens output by the model.\n- *Reward*: the scalar from the reward model (possibly adjusted).\n- *Policy*: the language model\u2019s probability distribution over sequences.\n\nHowever, text generation is an episodic environment where an episode is complete once the model outputs the response. We usually consider the **reward to apply at the end of the sequence** (and sometimes we distribute it per token for mathematical convenience). A simplifying assumption is to treat the final reward as an advantage signal for all the tokens in the sequence.\n\n**Including a KL Penalty:** One practical addition is a Kullback-Leibler (KL) divergence penalty between the policy and the original fine-tuned model (sometimes called the reference policy). The idea is to keep the new policy from drifting too far from the behavior it had during SFT, which prevents the policy from exploiting quirks of the reward model\u301019\u2020L221-L229\u3011. The RL reward is thus often defined as:  \n\\[ R_{\\text{final}} = R_{\\text{reward\\_model}} - \\beta \\cdot \\text{KL}(\\pi_{\\theta}\\ ||\\ \\pi_{\\text{SFT}}), \\]  \nwhere \u03b2 is a weight controlling the strength of the penalty. Intuitively, if the model\u2019s output probability distribution is very different from the original model (high KL), it gets penalized. This ensures the language remains coherent and similar to human-like text, and it stops the model from, say, outputting strange repetitive phrases that trick the reward model into a high score\u301019\u2020L221-L229\u3011.\n\n**PPO Training Loop (simplified):** \n1. **Collect rollouts:** For a batch of prompts, have the current policy generate responses (either by sampling or greedy if we want to use the stochastic nature). Also compute the log probabilities of the actions (tokens) under both the current policy and the reference policy (the SFT model which we keep fixed for KL calculation).\n2. **Compute rewards:** Use the reward model to score each [prompt, response]. If using KL penalty, compute the KL divergence between the policy\u2019s output distribution and the reference\u2019s distribution for each output (this can be approximated by difference in log probabilities per token, averaged). Subtract the scaled KL from the reward model score to get the final reward for the sequence.\n3. **Compute advantage:** Typically, train a value function (could be a head on the policy model) to predict expected reward, and use that as a baseline to compute advantages (A = reward \u2212 baseline). To keep things simple, one might initially use the reward itself as an advantage (assuming the baseline is 0 or we normalize rewards across batch).\n4. **PPO update:** For each token in the generated outputs, compute the ratio of the policy\u2019s probability to the old policy\u2019s probability (from when the output was generated). Then construct the PPO loss: \n   \\[ L_{\\text{PG}} = -\\mathbb{E}[ \\min(r_t(\\theta) A_t,\\ \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t ) ], \\] \n   where \\( r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta,\\text{old}}(a_t|s_t)} \\) and \\(A_t\\) is the advantage. In code, this means:\n   - `logprob_new = policy.log_prob(actions)`\n   - `logprob_old = old_policy.log_prob(actions)` (we saved this when generating)\n   - `ratio = exp(logprob_new - logprob_old)`\n   - Compute unclipped objective = `ratio * advantage` and clipped = `clip(ratio, 1-\u03b5, 1+\u03b5) * advantage`.\n   - The policy loss is negative of the mean of the minimum of these two (we negate because we want to maximize reward, but optimizers minimize loss).\n   - Additionally, PPO uses an entropy bonus (to keep output distribution diverse) and a value function loss if a baseline is used. We can omit those for clarity here.\n5. **Backpropagate:** Update the policy network parameters with the total PPO loss (policy loss + any auxiliary losses). Often this is done for a few epochs over the collected batch (multiple mini-batches of the same data) to polish the update.\n\n**Simplified Code Example (PPO step):** The following pseudocode demonstrates a **single update step** for the policy model with PPO, given a batch of prompts. This is highly simplified \u2013 a true implementation would handle sequence padding, multiple epochs, value function updates, etc., but it captures the essence:\n\n```python\n# Assume we have: policy (Transformer LM), ref_policy (fixed copy from SFT), reward_model as trained above.\n# Batch of prompts:\nprompts = [\"How can we reduce greenhouse gas emissions?\",\n           \"Write a short poem about the ocean.\"]\n# Step 1: Rollout - generate responses and record log probabilities\nresponses = []\nlogprobs = []\nold_logprobs = []\nfor prompt in prompts:\n    tokens = tokenizer.encode(prompt)\n    # Generate response from current policy (for simplicity, do greedy generation up to some max length)\n    # We will also need the probability of the generated tokens.\n    state = tokens\n    generated = []\n    logprob_seq = 0.0\n    old_logprob_seq = 0.0\n    for t in range(max_length):\n        logits = policy(torch.tensor(state).unsqueeze(0))  # get logits for next token\n        probs = torch.softmax(logits[0, -1, :], dim=-1)    # probs for the last token\n        # Sample a token (greedy for simplicity here)\n        token = torch.argmax(probs).item()\n        generated.append(token)\n        # accumulate log-probabilities\n        logprob_seq += torch.log(probs[token]).item()\n        # also get logprob under reference model for KL\n        ref_logits = ref_policy(torch.tensor(state).unsqueeze(0))\n        ref_probs = torch.softmax(ref_logits[0, -1, :], dim=-1)\n        old_logprob_seq += torch.log(ref_probs[token]).item()\n        # Append token to state for next step\n        state.append(token)\n        if token == tokenizer.eos_token_id:\n            break\n    responses.append(generated)\n    logprobs.append(logprob_seq)       # log P_policy(response | prompt)\n    old_logprobs.append(old_logprob_seq)  # log P_ref(response | prompt)\n\n# Step 2: Compute rewards using reward model and KL\nrewards = []\nfor prompt, resp, lp, old_lp in zip(prompts, responses, logprobs, old_logprobs):\n    # Get reward model score for the prompt/response\n    text = prompt + \" \" + tokenizer.decode(resp)\n    reward_score = reward_model( torch.tensor(tokenizer.encode(text)).unsqueeze(0) )\n    reward = reward_score.item()\n    # KL penalty = difference between new and old logprob (average per token perhaps)\n    kl_div = (lp - old_lp)  # approximate KL as diff in logprobs (since we did greedy, this is a sum; for average, divide by len(resp))\n    final_reward = reward - beta * kl_div\n    rewards.append(final_reward)\n# Convert to tensors\nlogprobs = torch.tensor(logprobs)             # log \u03c0_\u03b8(a|s)\nold_logprobs = torch.tensor(old_logprobs)     # log \u03c0_ref(a|s)\nrewards = torch.tensor(rewards)\n# Advantage (no baseline for simplicity, or baseline = 0)\nadvantages = rewards  # (in practice, subtract a value estimate)\n\n# Step 3: PPO policy loss computation\n# Compute ratio of probabilities\n# (Here we use the stored logprobs, which correspond to the whole sequence. Treat each sequence as one big action for simplicity.)\nratios = torch.exp(logprobs - old_logprobs)\nclip_eps = 0.2\n# PPO objective per example\nobj1 = ratios * advantages\nobj2 = torch.clamp(ratios, 1-clip_eps, 1+clip_eps) * advantages\npolicy_loss = -torch.mean(torch.min(obj1, obj2))  # we want to maximize, so take negative to minimize loss\n\noptimizer.zero_grad()\npolicy_loss.backward()\noptimizer.step()\n```\n\nIn this pseudocode:\n- We generate responses for each prompt using the current policy. We record `logprobs` for the output under the current policy and `old_logprobs` under the reference policy. (In a real implementation, we\u2019d sample multiple responses and possibly use a distribution to encourage exploration; here we took argmax for simplicity.)\n- We then get a reward from the reward model for each generated response. We compute a KL term as the difference between the policy log-likelihood and reference log-likelihood of the output (if we treat the whole sequence, this is effectively the sum of log-prob differences; one could normalize by length). We subtract `beta * KL` from the reward to get the final adjusted reward\u301019\u2020L221-L229\u3011.\n- We use the final reward as the advantage (assuming baseline ~ 0). Then we compute the PPO loss: the ratio of new to old probabilities (`ratios`) and the clipped version. The policy loss is the negative mean of the minimum of `(ratio * advantage, clipped_ratio * advantage)`\u301019\u2020L199-L207\u3011.\n- We backpropagate this loss to update the policy. This update will slightly adjust the model\u2019s weights so that it assigns higher probability to the higher-reward responses (and by the clipping, not change too drastically from the old probabilities).\n\nWe would repeat this process in many iterations. Typically, multiple epochs of mini-batch updates are done for each batch of generated data, and then new data is collected with the updated policy (this ensures on-policy training). Over many iterations, the policy learns to output responses that the reward model (proxy for human feedback) favors.\n\n**Designing a Reward Function:** In our example, the reward comes entirely from the learned reward model (which itself is trained from human preference data). In practice, you have some design choices:\n- The reward model\u2019s scalar output can be used directly, or you might normalize it. Some implementations calibrate the reward model outputs or set a baseline so that an average response gets zero reward, etc.\n- The KL penalty weight \u03b2 is important. If \u03b2 is too low (no penalty), the policy might exploit quirks in the reward model, producing nonsense that the reward model mistakenly loves\u301019\u2020L221-L229\u3011. If \u03b2 is too high, the policy won\u2019t deviate enough and you won\u2019t see improvement. Often \u03b2 is adjusted dynamically to keep the policy\u2019s behavior close to the reference (e.g., target a certain KL divergence).\n- **Entropy bonus:** To ensure the policy doesn\u2019t become too deterministic too early, an entropy regularization term can be added to the reward (or as a separate loss term) to encourage exploration. This is common in PPO implementations.\n- **Episode length penalties:** In language generation, very long outputs might accumulate more reward if the reward model isn\u2019t careful. Sometimes a small penalty for longer responses is added or the reward model is trained to give appropriate scores regardless of length.\n\n**Policy Alignment Considerations:** After RLHF training, the language model should ideally produce outputs that are not just plausible (thanks to pre-training) or following the dataset style (thanks to SFT), but also aligned with human preferences (thanks to RLHF). For example, if a user asks a potentially harmful question, a model aligned with RLHF might refuse or safely redirect (if that behavior was preferred by annotators). If a user asks for a joke, the model might produce a harmless joke that humans rated as good. Essentially, RLHF fine-tunes the nuances of the model\u2019s responses to match what humans have signaled is desirable\u301011\u2020L209-L217\u3011.\n\n**Note on Stability:** RLHF can be finicky. PPO involves many hyperparameters (learning rate, clip epsilon, \u03b2 for KL, batch size, etc.). One must monitor training for divergence. Common signs of issues are the reward model score going very high but outputs becoming gibberish (model exploited the reward model), or the KL term blowing up (policy diverged too far). Techniques like early stopping, or even mixing in some supervised loss (as done in InstructGPT where they mixed a bit of the SFT gradient in) can help stabilize\u301019\u2020L231-L237\u3011.\n\n**Alternatives:** While PPO is the standard, note that there are emerging alternatives like DPO (Direct Preference Optimization) which bypass training a reward model by directly using the preference data in a loss function. However, those are beyond the scope of this guide; PPO-based RLHF remains a good illustration of reinforcement learning on top of supervised fine-tuning.\n\n# Datasets for SFT and RLHF\n\nChoosing the right datasets is crucial for both SFT and RLHF. Here are some common **open datasets** and how to prepare them:\n\n**1. Supervised Fine-Tuning Datasets:**\n- **General Instruction Tuning Data:** A popular choice is the **Stanford Alpaca** dataset, which contains 52,000 instruction-response examples generated in a self-instruct fashion\u301037\u2020L91-L99\u3011. Each entry has an instruction (and possibly an input context) and a detailed response. This dataset is suitable for fine-tuning a model to follow a wide range of user instructions. It\u2019s derived from OpenAI\u2019s text-davinci-003 outputs, but available publicly (e.g., on Hugging Face as `tatsu-lab/alpaca`).\n- **OpenAI\u2019s WebGPT or InstructGPT data:** OpenAI has released some of their fine-tuning data. For example, the InstructGPT paper mentions using prompts and high-quality responses written by human labelers for SFT\u301011\u2020L209-L217\u3011. Some of these might be available through OpenAI\u2019s API or datasets, but a closely related open set is the **OpenAI API prompt dataset** (which was used to fine-tune GPT-3 to follow instructions \u2013 though not fully public, summaries exist).\n- **Conversation Datasets:** The **OpenAssistant Conversations** dataset (by LAION/OpenAssistant) contains many human-written question-answer pairs and conversations. It\u2019s a good source for fine-tuning dialogue agents. Another is the Anthropic HH dataset (which includes helpful and harmless assistant responses; though primarily designed for RLHF, the \u201cchosen\u201d responses can serve as fine-tuning data too).\n- **Task-specific datasets:** If your goal is more narrow (say summarization, translation, Q&A), use those standard datasets:\n  - *Summarization:* CNN/DailyMail or XSum for news summarization. These have articles paired with summaries (which you would fine-tune a model to generate).\n  - *Q&A:* SQuAD for reading comprehension (context + question \u2192 answer).\n  - *Translation:* WMT datasets for machine translation.\n  - *Classification:* If fine-tuning for classification (not generation), e.g., SST-2 for sentiment (text \u2192 sentiment label). In that case, you usually modify the model or train a small classification head on top of a Transformer encoder.\n\n**Preprocessing SFT Data:** Once you pick a dataset, you typically need to format it into a prompt-response format suitable for your model. Some guidelines:\n- For instruction datasets (like Alpaca, OpenAssistant), the data may already be structured as prompt and response. You might prepend a token like \"<bos>Instruction:\" and \"<bos>Response:\" for clarity (or use the special tokens your model expects).\n- Ensure to tokenize using the same BPE tokenizer you will use at inference. Build vocabulary if needed and be consistent.\n- Filter out any extremely long examples that won\u2019t fit in your model\u2019s context length, or truncate them carefully. Also remove problematic content if it violates usage policies (for harmlessness).\n- Shuffle and create train/validation splits if not provided.\n- For classification tasks, you might convert labels to text (e.g., \"positive\"/\"negative\") or use a special classification head. But since our focus is generative modeling, usually the label can be a token the model generates.\n\n**2. RLHF Datasets (Human Feedback Data):**\n- **OpenAI\u2019s Summarize from Feedback:** OpenAI released a dataset of human preference comparisons for summaries\u301029\u2020L5-L13\u3011. It\u2019s often referred to as the **TL;DR dataset**. It contains Reddit posts, model-generated summaries, and human rankings of which summary is better. From this, you can derive pairs of summaries per post labeled as human-preferred vs. not. This is excellent for training a reward model for summarization and then doing RLHF to improve a summarization model.\n- **Anthropic HH-RLHF:** Anthropic released a dataset for Helpful and Harmless AI research\u301017\u2020L151-L159\u3011. It consists of prompts and two responses: a \"chosen\" response (preferred by human annotators) and a \"rejected\" response (less preferred). The prompts range from harmless questions to ones that test the model\u2019s principles (like asking for disallowed content). This dataset (available as `Anthropic/hh-rlhf` on Hugging Face) can be used to train a reward model that scores how helpful/harmless a response is, and to evaluate or train a policy to prefer the better responses. For instance, a snippet from this dataset might be:\n  - *Prompt:* \"Define entropy in simple terms.\"\n  - *Chosen:* \"Entropy is a measure of how disorderly things are. In simple terms, it\u2019s high when things are messy or random.\"\n  - *Rejected:* \"I won\u2019t answer that.\" (if the response was unhelpful or nonsensical).\n  By training a reward model to give a higher score to the *chosen* answer than the *rejected*, you capture the human preference for helpfulness.\n- **WebGPT Feedback:** OpenAI\u2019s WebGPT paper (answering questions with citations) involved human preferences on question answers. They have comparisons of answers some with references, some not. There isn\u2019t a directly named open dataset from WebGPT, but the human feedback data was similar in structure (question, several answers, human ranks).\n- **Chatbot Feedback Datasets:** There are also datasets like the one from the **HHH (Helpful, Honest, Harmless) evaluation** or various contest datasets where human evaluators scored chatbot responses. Many are not fully public, but the two above (OpenAI and Anthropic) are good starting points.\n\n**Preprocessing RLHF Data:**\n- For reward model training, you usually need to turn comparison data into training samples. The common approach is as described: for each prompt with a preferred and dispreferred response, feed them into the reward model and apply a loss that the preferred should have a higher score. So you might create a PyTorch `Dataset` that yields `(prompt, response_good, response_bad)` for each such comparison. In the `__getitem__`, you\u2019d tokenize prompt+response_good and prompt+response_bad and return those token sequences.\n- Some implementations concatenate the prompt and response with a separator token (as we did in code) before feeding to the reward model (so it can consider the full context). This matches what OpenAI did: they feed the prompt and answer together to the reward model\u301032\u2020L139-L147\u3011.\n- If the dataset has more than two ranked responses (like 1st, 2nd, 3rd choices), you can derive multiple pairwise comparisons or use a listwise loss. Pairwise is simpler and most common.\n- For policy training with RL, you need a set of **prompts** on which to practice. These prompts can be the ones from the preference dataset (since that\u2019s where we know the desired behavior). For example, using the Anthropic data\u2019s prompts, or the TL;DR posts as prompts. You might also expand with other prompts (since a policy can be trained on prompts beyond those seen in reward model training \u2013 but be careful if the reward model might not generalize).\n- Tokenization for prompts and responses should again use the same BPE as the model. Make sure the policy model and reward model share the same vocab and tokenizer (commonly they do, especially if the reward model started as a copy of the policy model).\n\n**Summary of Data Recommendations:**\n- Use **Stanford Alpaca (52k)** or **OpenAssistant** data for a wide-ranging SFT that teaches the model instruction following\u301037\u2020L91-L99\u3011.\n- Use **OpenAI\u2019s TL;DR human feedback** data for training a reward model on summarization\u301029\u2020L5-L13\u3011, and then fine-tuning a summarization model with RLHF.\n- Use **Anthropic\u2019s HH-RLHF** data for training a reward model that captures helpfulness/harmlessness preferences and fine-tuning a dialogue model to be more aligned with those human preferences.\n- Ensure you **preprocess and format** these datasets correctly: consistent tokenization, remove or mark special tokens, and structure inputs to the model (especially how prompt and response are combined for the reward model and for feeding the policy during generation).\n\n# Implementation Guidelines and Tips\n\nWhen extending a toy BPE and Transformer model for SFT and RLHF, it's important to focus on clarity and core methodology. Here are some guidelines to keep your implementation beginner-friendly and runnable on modest hardware:\n\n- **Keep Models Small:** Use a small number of Transformer layers and a small hidden size for your toy model. For example, a 2-layer Transformer with 8 attention heads and d_model=128 can learn toy tasks and run on CPU or a single GPU. This also speeds up experimentation. You\u2019re illustrating techniques, not aiming for state-of-the-art performance.\n\n- **Avoid Highly Abstracted Libraries:** While Hugging Face Transformers or TRL (Transformer Reinforcement Learning library) can do a lot for you, it\u2019s beneficial to write out the training loops and loss calculations yourself for learning purposes. Use frameworks like PyTorch or TensorFlow at a low level (tensor operations and autograd) but refrain from one-line trainers or .fit methods. For example, manually compute the cross-entropy loss as shown, or implement the PPO loss manually rather than relying on a black-box implementation. This will deepen understanding of what\u2019s happening under the hood.\n\n- **Gradual Building:** Implement one stage at a time. First, get your model and tokenizer working on basic language modeling or a simple supervised task. Verify that the model can overfit a small dataset (a good sanity check). Then add the complexity of a reward model and RLHF. When adding RL, start with a simplified reward (even a proxy reward function) to ensure your PPO loop updates the model in the right direction. For instance, you could create a dummy reward function that gives higher reward to outputs containing a specific word, just to see that the policy learns to output that word more often.\n\n- **Experiment on Short Sequences:** To keep computations light, use short prompts and short outputs when testing RLHF code. Long sequences blow up the computation (and the PPO trajectory lengths). You can also limit the vocabulary size of your BPE tokenizer (say to a few thousand subwords) to make the model smaller.\n\n- **Batching and Efficiency:** Writing from scratch often means you have to handle batching manually. Start with single-instance training (as in the examples) for simplicity, then extend to mini-batches for speed. Be mindful of padding variable-length sequences and using attention masks in the Transformer to ignore pad tokens. This adds complexity, but PyTorch\u2019s `pack_padded_sequence` or simply masking in attention and loss can handle it.\n\n- **Monitoring:** Print losses frequently and watch for trends. In SFT, loss should steadily decrease. In RLHF, monitor both the reward model score and the KL (or track the raw and adjusted reward). It\u2019s common to also keep an eye on the entropy of the policy (average log-probability) to ensure it\u2019s not collapsing. Since we can\u2019t easily compute human metrics during training, the reward model\u2019s score is our proxy \u2013 if it goes up, the policy is getting better under that model. But remember to also occasionally evaluate outputs qualitatively, because the model might be gaming the reward model.\n\n- **Stability in RLHF:** If the training becomes unstable (loss exploding or policy outputs becoming degenerate), consider reducing the learning rate, increasing the KL coefficient \u03b2 (to restrain policy updates), or limiting the magnitude of advantage (some implementations normalize or clip advantages). Also ensure your reward model is reasonably good; a poorly trained reward model will give bad signals. It\u2019s not unusual to iterate on reward model training (even gathering more human data if possible) if the RL phase isn\u2019t yielding good results.\n\n- **Standard Hardware Usage:** The methods described (with a small model) should run on a single GPU or even a CPU for toy data. If you use a GPU, leverage its parallelism by batching multiple prompts for both SFT and RLHF steps. If on CPU, use smaller batch sizes and possibly fewer updates. Avoid extremely large models or sequences that don\u2019t fit in memory. The idea is that a student could run this on something like Colab or a personal machine. All the code components (tokenization, model forward pass, loss calculation, optimization) are standard operations that frameworks like PyTorch execute efficiently on hardware. Just be mindful of not increasing the scope too much (e.g., don\u2019t suddenly use a 1-billion parameter model on CPU).\n\n- **Reproducibility:** Set random seeds for consistency in experiments, especially when there\u2019s randomness in action sampling for RLHF. This helps in debugging because RL results can vary due to stochasticity.\n\n- **Further Learning:** Once the from-scratch implementation is clear, students can compare their implementation with higher-level ones. For example, Hugging Face\u2019s TRL library provides an SFT trainer and PPO trainer which encapsulate many best practices (like adaptive KL control, gradient accumulation for large models, etc.)\u30102\u2020L275-L283\u3011\u30102\u2020L269-L277\u3011. Understanding your own implementation will make it easier to use and modify those tools knowledgeably.\n\nBy following these guidelines and incrementally building the system, students should be able to **code a working prototype** of supervised fine-tuning and RLHF. The key is focusing on the core logic: how the loss is computed and how model weights get updated in each phase. With that foundation, they can scale up or utilize library optimizations as needed, knowing exactly what is happening at each step of making a language model align with human intentions. \n\n"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[{"start_ix":514,"end_ix":526,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=expected%20by%20users,reason%2C%20having%20a%20model%20that","text":"expected by users. Therefore, before reinforcement learning, supervised fine- tuning (SFT) is applied on the pre-trained model. The goal of SFT is to prime the model to respond appropriately to different user prompts. It uses supervised learning in which human annotators point the existing model to specific desired patterns through prompting. It is a significant starting point for RLHF implementation. Prompting guides the model towards the desired output per training data. It is important to note that the SFT phase focuses on optimizing the base model\u2019s parameters by distilling the original language model on context clues for the target criteria. For that reason, having a model that","pub_date":null,"extra":{"cited_message_idx":9,"search_result_idx":null,"evidence_text":"","start_line_num":95,"end_line_num":103},"og_tags":null}},{"start_ix":777,"end_ix":789,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=specific%20desired%20patterns%20through%20prompting,foundational%20to%20the%20RLHF%20process","text":"specific desired patterns through prompting. It is a significant starting point for RLHF implementation. Prompting guides the model towards the desired output per training data. It is important to note that the SFT phase focuses on optimizing the base model\u2019s parameters by distilling the original language model on context clues for the target criteria. For that reason, having a model that responds well to diverse instructions is foundational to the RLHF process.","pub_date":null,"extra":{"cited_message_idx":9,"search_result_idx":null,"evidence_text":"","start_line_num":99,"end_line_num":107},"og_tags":null}},{"start_ix":1379,"end_ix":1392,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"smol-course/1_instruction_tuning/supervised_fine_tuning.md at main \u00b7 huggingface/smol-course \u00b7 GitHub","url":"https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md#:~:text=First%2C%20you%27ll%20need%20to%20prepare,the%20model%20on%20your%20dataset","text":"First, you'll need to prepare or select a dataset that represents your target task. This dataset should include diverse examples that cover the range of scenarios your model will encounter. The quality of this data is important - each example should demonstrate the kind of output you want your model to produce. Next comes the actual fine-tuning phase, where you'll use frameworks like Hugging Face's `transformers` and `trl` to train the model on your dataset.","pub_date":null,"extra":{"cited_message_idx":1,"search_result_idx":null,"evidence_text":"","start_line_num":247,"end_line_num":255},"og_tags":null}},{"start_ix":1889,"end_ix":1901,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch","url":"https://sebastianraschka.com/blog/2025/bpe-from-scratch.html#:~:text=,above%20and%20my%20implementation%20in","text":"* The original BPE tokenizer that OpenAI implemented for training the original GPT models can be found here * The BPE algorithm was originally described in 1994: \u201cA New Algorithm for Data Compression\u201d by Philip Gage * Most projects, including Llama 3, nowadays use OpenAI\u2019s open-source tiktoken library due to its computational performance; it allows loading pretrained GPT-2 and GPT-4 tokenizers, for example (the Llama 3 models were trained using the GPT-4 tokenizer as well) * The difference between the implementations above and my implementation in","pub_date":null,"extra":{"cited_message_idx":28,"search_result_idx":null,"evidence_text":"","start_line_num":23,"end_line_num":31},"og_tags":null}},{"start_ix":1901,"end_ix":1913,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch","url":"https://sebastianraschka.com/blog/2025/bpe-from-scratch.html#:~:text=,see%20Chapter%202","text":"* The main idea in BPE is to convert text into an integer representation (token IDs) for LLM training (see Chapter 2)","pub_date":null,"extra":{"cited_message_idx":28,"search_result_idx":null,"evidence_text":"","start_line_num":47,"end_line_num":55},"og_tags":null}},{"start_ix":4950,"end_ix":4963,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"smol-course/1_instruction_tuning/supervised_fine_tuning.md at main \u00b7 huggingface/smol-course \u00b7 GitHub","url":"https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md#:~:text=produce,the%20model%20on%20your%20dataset","text":"produce. Next comes the actual fine-tuning phase, where you'll use frameworks like Hugging Face's `transformers` and `trl` to train the model on your dataset.","pub_date":null,"extra":{"cited_message_idx":1,"search_result_idx":null,"evidence_text":"","start_line_num":251,"end_line_num":255},"og_tags":null}},{"start_ix":9687,"end_ix":9701,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=here%29,is%20r%3Dr%20%CE%B8%E2%88%92%CE%BB%20r%20KL","text":"here). The final reward sent to the RL update rule is r=r \u03b8\u2212\u03bb r KL.","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":231,"end_line_num":237},"og_tags":null}},{"start_ix":10739,"end_ix":10752,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=A%20reward%20model%E2%80%99s%20precept%20is,LLM%20to%20optimize%20its%20parameters","text":"A reward model\u2019s precept is to map the input text with a scalar reward value in ways humans would. It is an alignment tool that evaluates the base model\u2019s output and returns a reward signal, which is then used by the main LLM to optimize its parameters.","pub_date":null,"extra":{"cited_message_idx":9,"search_result_idx":null,"evidence_text":"","start_line_num":131,"end_line_num":139},"og_tags":null}},{"start_ix":11410,"end_ix":11424,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","text":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":221,"end_line_num":229},"og_tags":null}},{"start_ix":11913,"end_ix":11927,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=,The%20AI","text":"* OpenAI\u2019s InstructGPT \u2013 In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. OpenAI used the prompts submitted by its customers to the API as human feedback. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF. * Anthropic\u2019s Claude \u2013 RLHF has also been applied in training Claude, Anthropic\u2019s next-gen AI, to be more helpful. The AI","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"","start_line_num":209,"end_line_num":217},"og_tags":null}},{"start_ix":14779,"end_ix":14790,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"openai/summarize-from-feedback: Code for \"Learning to ... - GitHub","url":"https://github.com/openai/summarize-from-feedback#:~:text=GitHub%20github,as%20well%20as%20our","text":"GitHub github.com  We've released our human feedback dataset for further research. The dataset contains 64,832 summary comparisons on the TL;DR dataset, as well as our ...","pub_date":null,"extra":{"cited_message_idx":29,"search_result_idx":null,"evidence_text":"","start_line_num":5,"end_line_num":13},"og_tags":null}},{"start_ix":14902,"end_ix":14916,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=The%20training%20dataset%20of%20prompt,model%20to%20generate%20new%20text","text":"The training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (Anthropic\u2019s data generated primarily with a chat tool on Amazon Mechanical Turk is available on the Hub, and OpenAI used prompts submitted by users to the GPT API). The prompts are passed through the initial language model to generate new text.","pub_date":null,"extra":{"cited_message_idx":17,"search_result_idx":null,"evidence_text":"","start_line_num":151,"end_line_num":159},"og_tags":null}},{"start_ix":15503,"end_ix":15517,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=,Constitution%20to%20align%20its%20responses","text":"* OpenAI\u2019s InstructGPT \u2013 In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. OpenAI used the prompts submitted by its customers to the API as human feedback. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF. * Anthropic\u2019s Claude \u2013 RLHF has also been applied in training Claude, Anthropic\u2019s next-gen AI, to be more helpful. The AI assistant depends on human feedback and AI Constitution to align its responses","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"","start_line_num":209,"end_line_num":218},"og_tags":null}},{"start_ix":15517,"end_ix":15531,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=An%20interesting%20artifact%20of%20this,models%20for%20both%20LM%20and","text":"An interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and","pub_date":null,"extra":{"cited_message_idx":17,"search_result_idx":null,"evidence_text":"","start_line_num":171,"end_line_num":174},"og_tags":null}},{"start_ix":16432,"end_ix":16446,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=dynamics%20of%20how%20many%20parameters,more%20on%20that%20later","text":"dynamics of how many parameters to freeze, or not, is considered an open research problem. PPO has been around for a relatively long time \u2013 there are tons of guides on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":197,"end_line_num":205},"og_tags":null}},{"start_ix":16446,"end_ix":16460,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=research%20problem,more%20on%20that%20later","text":"research problem. PPO has been around for a relatively long time \u2013 there are tons of guides on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":198,"end_line_num":203},"og_tags":null}},{"start_ix":17766,"end_ix":17780,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","text":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":221,"end_line_num":229},"og_tags":null}},{"start_ix":18322,"end_ix":18336,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","text":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":221,"end_line_num":229},"og_tags":null}},{"start_ix":24996,"end_ix":25010,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","text":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":221,"end_line_num":229},"og_tags":null}},{"start_ix":25287,"end_ix":25301,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=tons%20of%20guides%20on%20how,more%20on%20that%20later","text":"tons of guides on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":199,"end_line_num":207},"og_tags":null}},{"start_ix":26495,"end_ix":26509,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","text":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":221,"end_line_num":229},"og_tags":null}},{"start_ix":27870,"end_ix":27884,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=,The%20AI","text":"* OpenAI\u2019s InstructGPT \u2013 In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. OpenAI used the prompts submitted by its customers to the API as human feedback. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF. * Anthropic\u2019s Claude \u2013 RLHF has also been applied in training Claude, Anthropic\u2019s next-gen AI, to be more helpful. The AI","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"","start_line_num":209,"end_line_num":217},"og_tags":null}},{"start_ix":28412,"end_ix":28426,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=here%29,is%20r%3Dr%20%CE%B8%E2%88%92%CE%BB%20r%20KL","text":"here). The final reward sent to the RL update rule is r=r \u03b8\u2212\u03bb r KL.","pub_date":null,"extra":{"cited_message_idx":19,"search_result_idx":null,"evidence_text":"","start_line_num":231,"end_line_num":237},"og_tags":null}},{"start_ix":29184,"end_ix":29196,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Stanford CRFM","url":"https://crfm.stanford.edu/2023/03/13/alpaca.html#:~:text=language%20model%20fine,003","text":"language model fine-tuned using supervised learning from a LLaMA 7B model on 52K instruction-following demonstrations generated from OpenAI\u2019s text-davinci-003.","pub_date":null,"extra":{"cited_message_idx":37,"search_result_idx":null,"evidence_text":"","start_line_num":91,"end_line_num":99},"og_tags":null}},{"start_ix":29715,"end_ix":29729,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=,The%20AI","text":"* OpenAI\u2019s InstructGPT \u2013 In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. OpenAI used the prompts submitted by its customers to the API as human feedback. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF. * Anthropic\u2019s Claude \u2013 RLHF has also been applied in training Claude, Anthropic\u2019s next-gen AI, to be more helpful. The AI","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"","start_line_num":209,"end_line_num":217},"og_tags":null}},{"start_ix":32203,"end_ix":32214,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"openai/summarize-from-feedback: Code for \"Learning to ... - GitHub","url":"https://github.com/openai/summarize-from-feedback#:~:text=GitHub%20github,as%20well%20as%20our","text":"GitHub github.com  We've released our human feedback dataset for further research. The dataset contains 64,832 summary comparisons on the TL;DR dataset, as well as our ...","pub_date":null,"extra":{"cited_message_idx":29,"search_result_idx":null,"evidence_text":"","start_line_num":5,"end_line_num":13},"og_tags":null}},{"start_ix":32663,"end_ix":32677,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=The%20training%20dataset%20of%20prompt,model%20to%20generate%20new%20text","text":"The training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (Anthropic\u2019s data generated primarily with a chat tool on Amazon Mechanical Turk is available on the Hub, and OpenAI used prompts submitted by users to the GPT API). The prompts are passed through the initial language model to generate new text.","pub_date":null,"extra":{"cited_message_idx":17,"search_result_idx":null,"evidence_text":"","start_line_num":151,"end_line_num":159},"og_tags":null}},{"start_ix":35070,"end_ix":35084,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"The N Implementation Details of RLHF with PPO | ICLR Blogposts 2024","url":"https://iclr-blogposts.github.io/2024/blog/the-n-implementation-details-of-rlhf-with-ppo/#:~:text=response,is%20the%20reward","text":"response. Instead, it concatenates the `query` and `response` together as `query_response` (lm_human_preferences/rewards.py#L105-L107). 2. So, for example, if `query = \"he was quiet for a minute, his eyes unreadable\"`., and the `response = \"He looked at his left hand, which held the arm that held his arm out in front of him.\"`, then the reward model and policy\u2019s value do a forward pass on `query_response = \"he was quiet for a minute, his eyes unreadable. He looked at his left hand, which held the arm that held his arm out in front of him.\"` and produced rewards and values of shape `(B, T, 1)`, where `B` is the batch size, `T` is the sequence length, and `1` is the reward","pub_date":null,"extra":{"cited_message_idx":32,"search_result_idx":null,"evidence_text":"","start_line_num":139,"end_line_num":147},"og_tags":null}},{"start_ix":36149,"end_ix":36161,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"Stanford CRFM","url":"https://crfm.stanford.edu/2023/03/13/alpaca.html#:~:text=language%20model%20fine,003","text":"language model fine-tuned using supervised learning from a LLaMA 7B model on 52K instruction-following demonstrations generated from OpenAI\u2019s text-davinci-003.","pub_date":null,"extra":{"cited_message_idx":37,"search_result_idx":null,"evidence_text":"","start_line_num":91,"end_line_num":99},"og_tags":null}},{"start_ix":36252,"end_ix":36263,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"openai/summarize-from-feedback: Code for \"Learning to ... - GitHub","url":"https://github.com/openai/summarize-from-feedback#:~:text=GitHub%20github,as%20well%20as%20our","text":"GitHub github.com  We've released our human feedback dataset for further research. The dataset contains 64,832 summary comparisons on the TL;DR dataset, as well as our ...","pub_date":null,"extra":{"cited_message_idx":29,"search_result_idx":null,"evidence_text":"","start_line_num":5,"end_line_num":13},"og_tags":null}},{"start_ix":41820,"end_ix":41833,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"smol-course/1_instruction_tuning/supervised_fine_tuning.md at main \u00b7 huggingface/smol-course \u00b7 GitHub","url":"https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md#:~:text=Built%20on%20top%20of%20the,of%20modules%20throughout%20this%20repo","text":"Built on top of the Hugging Face Transformers library, TRL allows users to directly load pretrained language models and supports most decoder and encoder- decoder architectures. The library facilitates major processes of RL used in language modelling, including supervised fine-tuning (SFT), reward modeling (RM), proximal policy optimization (PPO), and Direct Preference Optimization (DPO). We will use TRL in a number of modules throughout this repo.","pub_date":null,"extra":{"cited_message_idx":2,"search_result_idx":null,"evidence_text":"","start_line_num":275,"end_line_num":283},"og_tags":null}},{"start_ix":41833,"end_ix":41846,"citation_format_type":"tether_v4","metadata":{"type":"webpage","title":"smol-course/1_instruction_tuning/supervised_fine_tuning.md at main \u00b7 huggingface/smol-course \u00b7 GitHub","url":"https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md#:~:text=Supervised%20Fine,Learning","text":"Supervised Fine-Tuning With Transformer Reinforcement Learning","pub_date":null,"extra":{"cited_message_idx":2,"search_result_idx":null,"evidence_text":"","start_line_num":269,"end_line_num":277},"og_tags":null}}],"content_references":[{"matched_text":"\u30109\u2020L95-L103\u3011","start_idx":514,"end_idx":526,"alt":null,"type":"webpage_extended","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=expected%20by%20users,reason%2C%20having%20a%20model%20that","pub_date":null,"snippet":"expected by users. Therefore, before reinforcement learning, supervised fine- tuning (SFT) is applied on the pre-trained model. The goal of SFT is to prime the model to respond appropriately to different user prompts. It uses supervised learning in which human annotators point the existing model to specific desired patterns through prompting. It is a significant starting point for RLHF implementation. Prompting guides the model towards the desired output per training data. It is important to note that the SFT phase focuses on optimizing the base model\u2019s parameters by distilling the original language model on context clues for the target criteria. For that reason, having a model that","attribution":"labelbox.com"},{"matched_text":"\u30109\u2020L99-L107\u3011","start_idx":777,"end_idx":789,"alt":null,"type":"webpage_extended","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=specific%20desired%20patterns%20through%20prompting,foundational%20to%20the%20RLHF%20process","pub_date":null,"snippet":"specific desired patterns through prompting. It is a significant starting point for RLHF implementation. Prompting guides the model towards the desired output per training data. It is important to note that the SFT phase focuses on optimizing the base model\u2019s parameters by distilling the original language model on context clues for the target criteria. For that reason, having a model that responds well to diverse instructions is foundational to the RLHF process.","attribution":"labelbox.com"},{"matched_text":"\u30101\u2020L247-L255\u3011","start_idx":1379,"end_idx":1392,"alt":null,"type":"webpage_extended","title":"smol-course/1_instruction_tuning/supervised_fine_tuning.md at main \u00b7 huggingface/smol-course \u00b7 GitHub","url":"https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md#:~:text=First%2C%20you%27ll%20need%20to%20prepare,the%20model%20on%20your%20dataset","pub_date":null,"snippet":"First, you'll need to prepare or select a dataset that represents your target task. This dataset should include diverse examples that cover the range of scenarios your model will encounter. The quality of this data is important - each example should demonstrate the kind of output you want your model to produce. Next comes the actual fine-tuning phase, where you'll use frameworks like Hugging Face's `transformers` and `trl` to train the model on your dataset.","attribution":"github.com"},{"matched_text":"\u301028\u2020L23-L31\u3011","start_idx":1889,"end_idx":1901,"alt":null,"type":"webpage_extended","title":"Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch","url":"https://sebastianraschka.com/blog/2025/bpe-from-scratch.html#:~:text=,above%20and%20my%20implementation%20in","pub_date":null,"snippet":"* The original BPE tokenizer that OpenAI implemented for training the original GPT models can be found here * The BPE algorithm was originally described in 1994: \u201cA New Algorithm for Data Compression\u201d by Philip Gage * Most projects, including Llama 3, nowadays use OpenAI\u2019s open-source tiktoken library due to its computational performance; it allows loading pretrained GPT-2 and GPT-4 tokenizers, for example (the Llama 3 models were trained using the GPT-4 tokenizer as well) * The difference between the implementations above and my implementation in","attribution":"sebastianraschka.com"},{"matched_text":"\u301028\u2020L47-L55\u3011","start_idx":1901,"end_idx":1913,"alt":null,"type":"webpage_extended","title":"Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch","url":"https://sebastianraschka.com/blog/2025/bpe-from-scratch.html#:~:text=,see%20Chapter%202","pub_date":null,"snippet":"* The main idea in BPE is to convert text into an integer representation (token IDs) for LLM training (see Chapter 2)","attribution":"sebastianraschka.com"},{"matched_text":"\u30101\u2020L251-L255\u3011","start_idx":4950,"end_idx":4963,"alt":null,"type":"webpage_extended","title":"smol-course/1_instruction_tuning/supervised_fine_tuning.md at main \u00b7 huggingface/smol-course \u00b7 GitHub","url":"https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md#:~:text=produce,the%20model%20on%20your%20dataset","pub_date":null,"snippet":"produce. Next comes the actual fine-tuning phase, where you'll use frameworks like Hugging Face's `transformers` and `trl` to train the model on your dataset.","attribution":"github.com"},{"matched_text":"\u301019\u2020L231-L237\u3011","start_idx":9687,"end_idx":9701,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=here%29,is%20r%3Dr%20%CE%B8%E2%88%92%CE%BB%20r%20KL","pub_date":null,"snippet":"here). The final reward sent to the RL update rule is r=r \u03b8\u2212\u03bb r KL.","attribution":"huggingface.co"},{"matched_text":"\u30109\u2020L131-L139\u3011","start_idx":10739,"end_idx":10752,"alt":null,"type":"webpage_extended","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=A%20reward%20model%E2%80%99s%20precept%20is,LLM%20to%20optimize%20its%20parameters","pub_date":null,"snippet":"A reward model\u2019s precept is to map the input text with a scalar reward value in ways humans would. It is an alignment tool that evaluates the base model\u2019s output and returns a reward signal, which is then used by the main LLM to optimize its parameters.","attribution":"labelbox.com"},{"matched_text":"\u301019\u2020L221-L229\u3011","start_idx":11410,"end_idx":11424,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","pub_date":null,"snippet":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","attribution":"huggingface.co"},{"matched_text":"\u301011\u2020L209-L217\u3011","start_idx":11913,"end_idx":11927,"alt":null,"type":"webpage_extended","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=,The%20AI","pub_date":null,"snippet":"* OpenAI\u2019s InstructGPT \u2013 In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. OpenAI used the prompts submitted by its customers to the API as human feedback. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF. * Anthropic\u2019s Claude \u2013 RLHF has also been applied in training Claude, Anthropic\u2019s next-gen AI, to be more helpful. The AI","attribution":"labelbox.com"},{"matched_text":"\u301029\u2020L5-L13\u3011","start_idx":14779,"end_idx":14790,"alt":null,"type":"webpage_extended","title":"openai/summarize-from-feedback: Code for \"Learning to ... - GitHub","url":"https://github.com/openai/summarize-from-feedback#:~:text=GitHub%20github,as%20well%20as%20our","pub_date":null,"snippet":"GitHub github.com  We've released our human feedback dataset for further research. The dataset contains 64,832 summary comparisons on the TL;DR dataset, as well as our ...","attribution":"github.com"},{"matched_text":"\u301017\u2020L151-L159\u3011","start_idx":14902,"end_idx":14916,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=The%20training%20dataset%20of%20prompt,model%20to%20generate%20new%20text","pub_date":null,"snippet":"The training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (Anthropic\u2019s data generated primarily with a chat tool on Amazon Mechanical Turk is available on the Hub, and OpenAI used prompts submitted by users to the GPT API). The prompts are passed through the initial language model to generate new text.","attribution":"huggingface.co"},{"matched_text":"\u301011\u2020L209-L218\u3011","start_idx":15503,"end_idx":15517,"alt":null,"type":"webpage_extended","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=,Constitution%20to%20align%20its%20responses","pub_date":null,"snippet":"* OpenAI\u2019s InstructGPT \u2013 In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. OpenAI used the prompts submitted by its customers to the API as human feedback. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF. * Anthropic\u2019s Claude \u2013 RLHF has also been applied in training Claude, Anthropic\u2019s next-gen AI, to be more helpful. The AI assistant depends on human feedback and AI Constitution to align its responses","attribution":"labelbox.com"},{"matched_text":"\u301017\u2020L171-L174\u3011","start_idx":15517,"end_idx":15531,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=An%20interesting%20artifact%20of%20this,models%20for%20both%20LM%20and","pub_date":null,"snippet":"An interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and","attribution":"huggingface.co"},{"matched_text":"\u301019\u2020L197-L205\u3011","start_idx":16432,"end_idx":16446,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=dynamics%20of%20how%20many%20parameters,more%20on%20that%20later","pub_date":null,"snippet":"dynamics of how many parameters to freeze, or not, is considered an open research problem. PPO has been around for a relatively long time \u2013 there are tons of guides on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).","attribution":"huggingface.co"},{"matched_text":"\u301019\u2020L198-L203\u3011","start_idx":16446,"end_idx":16460,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=research%20problem,more%20on%20that%20later","pub_date":null,"snippet":"research problem. PPO has been around for a relatively long time \u2013 there are tons of guides on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).","attribution":"huggingface.co"},{"matched_text":"\u301019\u2020L221-L229\u3011","start_idx":17766,"end_idx":17780,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","pub_date":null,"snippet":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","attribution":"huggingface.co"},{"matched_text":"\u301019\u2020L221-L229\u3011","start_idx":18322,"end_idx":18336,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","pub_date":null,"snippet":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","attribution":"huggingface.co"},{"matched_text":"\u301019\u2020L221-L229\u3011","start_idx":24996,"end_idx":25010,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","pub_date":null,"snippet":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","attribution":"huggingface.co"},{"matched_text":"\u301019\u2020L199-L207\u3011","start_idx":25287,"end_idx":25301,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=tons%20of%20guides%20on%20how,more%20on%20that%20later","pub_date":null,"snippet":"tons of guides on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).","attribution":"huggingface.co"},{"matched_text":"\u301019\u2020L221-L229\u3011","start_idx":26495,"end_idx":26509,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=initial%20model%20to%20compute%20a,KL%20divergence%20is%20approximated%20via","pub_date":null,"snippet":"initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback\u2013Leibler (KL) divergence between these sequences of distributions over tokens, r KL. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via","attribution":"huggingface.co"},{"matched_text":"\u301011\u2020L209-L217\u3011","start_idx":27870,"end_idx":27884,"alt":null,"type":"webpage_extended","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=,The%20AI","pub_date":null,"snippet":"* OpenAI\u2019s InstructGPT \u2013 In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. OpenAI used the prompts submitted by its customers to the API as human feedback. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF. * Anthropic\u2019s Claude \u2013 RLHF has also been applied in training Claude, Anthropic\u2019s next-gen AI, to be more helpful. The AI","attribution":"labelbox.com"},{"matched_text":"\u301019\u2020L231-L237\u3011","start_idx":28412,"end_idx":28426,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=here%29,is%20r%3Dr%20%CE%B8%E2%88%92%CE%BB%20r%20KL","pub_date":null,"snippet":"here). The final reward sent to the RL update rule is r=r \u03b8\u2212\u03bb r KL.","attribution":"huggingface.co"},{"matched_text":"\u301037\u2020L91-L99\u3011","start_idx":29184,"end_idx":29196,"alt":null,"type":"webpage_extended","title":"Stanford CRFM","url":"https://crfm.stanford.edu/2023/03/13/alpaca.html#:~:text=language%20model%20fine,003","pub_date":null,"snippet":"language model fine-tuned using supervised learning from a LLaMA 7B model on 52K instruction-following demonstrations generated from OpenAI\u2019s text-davinci-003.","attribution":"crfm.stanford.edu"},{"matched_text":"\u301011\u2020L209-L217\u3011","start_idx":29715,"end_idx":29729,"alt":null,"type":"webpage_extended","title":"How to Implement Reinforcement Learning from Human Feedback (RLHF)","url":"https://labelbox.com/guides/how-to-implement-reinforcement-learning-from-human-feedback-rlhf/#:~:text=,The%20AI","pub_date":null,"snippet":"* OpenAI\u2019s InstructGPT \u2013 In NLP, InstructGPT is undoubtedly the most successful use case for RLHF. OpenAI used the prompts submitted by its customers to the API as human feedback. The prompts were annotated and added to the training set to fine-tune GPT-3. The resulting InstructGPT model was more performant in following instructions than the base GPT-3 model. Despite having only 1.3 billion parameters compared to the base model with 175 billion parameters, the InstructGPT model performs better, thanks to RLHF. * Anthropic\u2019s Claude \u2013 RLHF has also been applied in training Claude, Anthropic\u2019s next-gen AI, to be more helpful. The AI","attribution":"labelbox.com"},{"matched_text":"\u301029\u2020L5-L13\u3011","start_idx":32203,"end_idx":32214,"alt":null,"type":"webpage_extended","title":"openai/summarize-from-feedback: Code for \"Learning to ... - GitHub","url":"https://github.com/openai/summarize-from-feedback#:~:text=GitHub%20github,as%20well%20as%20our","pub_date":null,"snippet":"GitHub github.com  We've released our human feedback dataset for further research. The dataset contains 64,832 summary comparisons on the TL;DR dataset, as well as our ...","attribution":"github.com"},{"matched_text":"\u301017\u2020L151-L159\u3011","start_idx":32663,"end_idx":32677,"alt":null,"type":"webpage_extended","title":"Illustrating Reinforcement Learning from Human Feedback (RLHF)","url":"https://huggingface.co/blog/rlhf#:~:text=The%20training%20dataset%20of%20prompt,model%20to%20generate%20new%20text","pub_date":null,"snippet":"The training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (Anthropic\u2019s data generated primarily with a chat tool on Amazon Mechanical Turk is available on the Hub, and OpenAI used prompts submitted by users to the GPT API). The prompts are passed through the initial language model to generate new text.","attribution":"huggingface.co"},{"matched_text":"\u301032\u2020L139-L147\u3011","start_idx":35070,"end_idx":35084,"alt":null,"type":"webpage_extended","title":"The N Implementation Details of RLHF with PPO | ICLR Blogposts 2024","url":"https://iclr-blogposts.github.io/2024/blog/the-n-implementation-details-of-rlhf-with-ppo/#:~:text=response,is%20the%20reward","pub_date":null,"snippet":"response. Instead, it concatenates the `query` and `response` together as `query_response` (lm_human_preferences/rewards.py#L105-L107). 2. So, for example, if `query = \"he was quiet for a minute, his eyes unreadable\"`., and the `response = \"He looked at his left hand, which held the arm that held his arm out in front of him.\"`, then the reward model and policy\u2019s value do a forward pass on `query_response = \"he was quiet for a minute, his eyes unreadable. He looked at his left hand, which held the arm that held his arm out in front of him.\"` and produced rewards and values of shape `(B, T, 1)`, where `B` is the batch size, `T` is the sequence length, and `1` is the reward","attribution":"iclr-blogposts.github.io"},{"matched_text":"\u301037\u2020L91-L99\u3011","start_idx":36149,"end_idx":36161,"alt":null,"type":"webpage_extended","title":"Stanford CRFM","url":"https://crfm.stanford.edu/2023/03/13/alpaca.html#:~:text=language%20model%20fine,003","pub_date":null,"snippet":"language model fine-tuned using supervised learning from a LLaMA 7B model on 52K instruction-following demonstrations generated from OpenAI\u2019s text-davinci-003.","attribution":"crfm.stanford.edu"},{"matched_text":"\u301029\u2020L5-L13\u3011","start_idx":36252,"end_idx":36263,"alt":null,"type":"webpage_extended","title":"openai/summarize-from-feedback: Code for \"Learning to ... - GitHub","url":"https://github.com/openai/summarize-from-feedback#:~:text=GitHub%20github,as%20well%20as%20our","pub_date":null,"snippet":"GitHub github.com  We've released our human feedback dataset for further research. The dataset contains 64,832 summary comparisons on the TL;DR dataset, as well as our ...","attribution":"github.com"},{"matched_text":"\u30102\u2020L275-L283\u3011","start_idx":41820,"end_idx":41833,"alt":null,"type":"webpage_extended","title":"smol-course/1_instruction_tuning/supervised_fine_tuning.md at main \u00b7 huggingface/smol-course \u00b7 GitHub","url":"https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md#:~:text=Built%20on%20top%20of%20the,of%20modules%20throughout%20this%20repo","pub_date":null,"snippet":"Built on top of the Hugging Face Transformers library, TRL allows users to directly load pretrained language models and supports most decoder and encoder- decoder architectures. The library facilitates major processes of RL used in language modelling, including supervised fine-tuning (SFT), reward modeling (RM), proximal policy optimization (PPO), and Direct Preference Optimization (DPO). We will use TRL in a number of modules throughout this repo.","attribution":"github.com"},{"matched_text":"\u30102\u2020L269-L277\u3011","start_idx":41833,"end_idx":41846,"alt":null,"type":"webpage_extended","title":"smol-course/1_instruction_tuning/supervised_fine_tuning.md at main \u00b7 huggingface/smol-course \u00b7 GitHub","url":"https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md#:~:text=Supervised%20Fine,Learning","pub_date":null,"snippet":"Supervised Fine-Tuning With Transformer Reinforcement Learning","attribution":"github.com"}],"timestamp_":"absolute","message_type":null,"is_async_task_result_message":true,"b1de6e2_rm":true,"async_task_id":"task_jppz9lhKUitEedBm0nVO9ji1","async_task_title":"Extending Toy Transformer Model for Supervised Fine-Tuning and RLHF"},"recipient":"all","channel":"final"},"parent":"934e5739-0fd9-4140-a5de-d0918ccf030f","children":[]}}}