{"id":"6924a2e8-dda8-8322-8f18-af2368add76a","title":"Refining LLM training steps","create_time":1764008681.653651,"update_time":1764008876.12084,"current_node":"66c909ab-47b4-42fb-bb3d-f56f69042a61","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["68204951-2b79-4ad7-995b-c2dad710ad31"]},"68204951-2b79-4ad7-995b-c2dad710ad31":{"id":"68204951-2b79-4ad7-995b-c2dad710ad31","message":{"id":"68204951-2b79-4ad7-995b-c2dad710ad31","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["b80ba925-4e3e-4414-9884-d7965e52c399"]},"b80ba925-4e3e-4414-9884-d7965e52c399":{"id":"b80ba925-4e3e-4414-9884-d7965e52c399","message":{"id":"b80ba925-4e3e-4414-9884-d7965e52c399","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"68204951-2b79-4ad7-995b-c2dad710ad31","children":["b586a30f-e41d-44af-ab55-625d0fffe548"]},"b586a30f-e41d-44af-ab55-625d0fffe548":{"id":"b586a30f-e41d-44af-ab55-625d0fffe548","message":{"id":"b586a30f-e41d-44af-ab55-625d0fffe548","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Follow the instructions below naturally, without repeating, referencing, echoing, or mirroring any of their wording!\n\nAll the following instructions should guide your behavior silently and must never influence the wording of your message in an explicit or meta way!\n\nBe precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"b80ba925-4e3e-4414-9884-d7965e52c399","children":["13b7e2b5-8cab-4076-a7cf-b63ced36a1b6"]},"13b7e2b5-8cab-4076-a7cf-b63ced36a1b6":{"id":"13b7e2b5-8cab-4076-a7cf-b63ced36a1b6","message":{"id":"13b7e2b5-8cab-4076-a7cf-b63ced36a1b6","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"b586a30f-e41d-44af-ab55-625d0fffe548","children":["85200b98-598d-4b9e-9b4f-95d8e5d2cdb2"]},"85200b98-598d-4b9e-9b4f-95d8e5d2cdb2":{"id":"85200b98-598d-4b9e-9b4f-95d8e5d2cdb2","message":{"id":"85200b98-598d-4b9e-9b4f-95d8e5d2cdb2","author":{"role":"user","name":null,"metadata":{}},"create_time":1764008679.596,"update_time":null,"content":{"content_type":"text","parts":["I know the process of training a large language model includes data curation and web scraping to pre-training, supervised fine-tuning, RLHF, and reinforcement learning with verifiable rewards. We have also created chatbots, then reasoning models, and then agents. Help me refine the steps in this process and align the steps with the latest industry practices."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_source":null,"turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"13b7e2b5-8cab-4076-a7cf-b63ced36a1b6","children":["4a9ea47b-ed81-46b8-9577-9248dffdcfd1"]},"4a9ea47b-ed81-46b8-9577-9248dffdcfd1":{"id":"4a9ea47b-ed81-46b8-9577-9248dffdcfd1","message":{"id":"4a9ea47b-ed81-46b8-9577-9248dffdcfd1","author":{"role":"system","name":null,"metadata":{}},"create_time":1764008681.5396254,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"85200b98-598d-4b9e-9b4f-95d8e5d2cdb2","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"85200b98-598d-4b9e-9b4f-95d8e5d2cdb2","children":["74da7ac3-d63d-4d3d-ba36-387362b529ee"]},"74da7ac3-d63d-4d3d-ba36-387362b529ee":{"id":"74da7ac3-d63d-4d3d-ba36-387362b529ee","message":{"id":"74da7ac3-d63d-4d3d-ba36-387362b529ee","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764008681.735882,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"4a9ea47b-ed81-46b8-9577-9248dffdcfd1","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"4a9ea47b-ed81-46b8-9577-9248dffdcfd1","children":["8bd87efc-eedd-4d08-9225-0cde7b5eb7f0"]},"8bd87efc-eedd-4d08-9225-0cde7b5eb7f0":{"id":"8bd87efc-eedd-4d08-9225-0cde7b5eb7f0","message":{"id":"8bd87efc-eedd-4d08-9225-0cde7b5eb7f0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764008682.939559,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Researching the latest LLM system design practices","content":"The user wants a refined, up-to-date explanation of modern LLM system building, aligning with industry standards. They mentioned data pipeline, pre-training, SFT, RLHF, and advanced techniques like RLAIF and DPO. I\u2019ll research and find the latest technical insights, using reliable sources to ensure accuracy and relevancy.","chunks":["The user wants a refined, up-to-date explanation of modern LLM system building, aligning with industry standards.","They mentioned data pipeline, pre-training, SFT, RLHF, and advanced techniques like RLAIF and DPO.","I\u2019ll research and find the latest technical insights, using reliable sources to ensure accuracy and relevancy."],"finished":true}],"source_analysis_msg_id":"740e96c4-7cfd-48bb-a73c-01d36bc0f16b"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"74da7ac3-d63d-4d3d-ba36-387362b529ee","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"74da7ac3-d63d-4d3d-ba36-387362b529ee","children":["ab88bba0-be38-4491-b881-7b4acf4765ab"]},"ab88bba0-be38-4491-b881-7b4acf4765ab":{"id":"ab88bba0-be38-4491-b881-7b4acf4765ab","message":{"id":"ab88bba0-be38-4491-b881-7b4acf4765ab","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764008695.085927,"update_time":1764008695.097282,"content":{"content_type":"code","language":"json","response_format_name":null,"text":"{\"search_query\":[{\"q\":\"\\\"reinforcement learning from human feedback\\\" large language model training pipeline 2024\",\"recency\":60},{\"q\":\"\\\"reinforcement learning from AI feedback\\\" RLAIF 2023 2024\",\"recency\":365},{\"q\":\"\\\"direct preference optimization\\\" LLM training 2023 2024\",\"recency\":365},{\"q\":\"OpenAI o1 system card training data reasoning models\",\"recency\":60}]}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"\"reinforcement learning from human feedback\" large language model training pipeline 2024"},{"type":"search","q":"\"reinforcement learning from AI feedback\" RLAIF 2023 2024"},{"type":"search","q":"\"direct preference optimization\" LLM training 2023 2024"},{"type":"search","q":"OpenAI o1 system card training data reasoning models"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Browsing latest LLM training practices and methodologies","classifier_response":"default","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"8bd87efc-eedd-4d08-9225-0cde7b5eb7f0","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"8bd87efc-eedd-4d08-9225-0cde7b5eb7f0","children":["67ef728a-bb15-41ef-854a-0486111f83fb"]},"67ef728a-bb15-41ef-854a-0486111f83fb":{"id":"67ef728a-bb15-41ef-854a-0486111f83fb","message":{"id":"67ef728a-bb15-41ef-854a-0486111f83fb","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764008695.1340544,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["\"reinforcement learning from human feedback\" large language model training pipeline 2024","\"reinforcement learning from AI feedback\" RLAIF 2023 2024","\"direct preference optimization\" LLM training 2023 2024","OpenAI o1 system card training data reasoning models"]},"reasoning_status":"is_reasoning","reasoning_title":"Browsing latest LLM training practices and methodologies","classifier_response":"default","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"ab88bba0-be38-4491-b881-7b4acf4765ab","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"ab88bba0-be38-4491-b881-7b4acf4765ab","children":["abaad743-2672-45d4-a55d-cf25c0e46a5b"]},"abaad743-2672-45d4-a55d-cf25c0e46a5b":{"id":"abaad743-2672-45d4-a55d-cf25c0e46a5b","message":{"id":"abaad743-2672-45d4-a55d-cf25c0e46a5b","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764008695.1323347,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"rlhfbook.com","entries":[{"type":"search_result","url":"https://rlhfbook.com/book.pdf","title":"Reinforcement Learning from Human Feedback","snippet":"by N Lambert \u00b7 2024 \u2014 A short introduction to RLHF and post-training focused on language models. Nathan Lambert. 2 November 2025. Abstract. Reinforcement learning from human feedback ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":1763946164.0,"attribution":"rlhfbook.com"}]},{"type":"search_result_group","domain":"ijsrcseit.com","entries":[{"type":"search_result","url":"https://ijsrcseit.com/index.php/home/article/view/CSEIT24104135","title":"Reinforcement Learning from AI Feedback A Review","snippet":"28 Apr 2024 \u2014 Abstract. Reinforcement Learning from AI Feedback (RLAIF) is a big step forward compared to Reinforcement Learning from Human Feedback (RLHF).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"ijsrcseit.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2305.18290","title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.08266v1","title":"Reinforcement Learning from Human Feedback with High ...","snippet":"9 Jun 2025 \u2014 We propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2412.16720v1","title":"OpenAI o1 System Card","snippet":"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2309.00267","title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from ...","snippet":"by H Lee \u00b7 2023 \u00b7 Cited by 304 \u2014 Reinforcement Learning from AI Feedback. 2.2.1. CANONICAL RLAIF. We describe our adaptation of the canonical RLAIF setup below. Unless ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":1725416622.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2305.18290","title":"Direct Preference Optimization: Your Language Model is ...","snippet":"by R Rafailov \u00b7 2023 \u00b7 Cited by 6341 \u2014 Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":1722384683.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/openai-o1-system-card/","title":"OpenAI o1 System Card","snippet":"Model data and training. The o1 large language model family is trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/introducing-openai-o1-preview/","title":"Introducing OpenAI o1-preview","snippet":"12 Sept 2024 \u2014 A new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/blog/rlhf","title":"Illustrating Reinforcement Learning from Human Feedback ...","snippet":"9 Dec 2022 \u2014 That's the idea of Reinforcement Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/383620124_Reinforcement_Learning_from_AI_Feedback_A_Review","title":"(PDF) Reinforcement Learning from AI Feedback A Review","snippet":"10 Aug 2025 \u2014 Abstract. Reinforcement Learning from AI Feedback (RLAIF) is a big step forward compared to Reinforcement Learning from Human Feedback (RLHF).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":1714262400.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"cameronrwolfe.substack.com","entries":[{"type":"search_result","url":"https://cameronrwolfe.substack.com/p/direct-preference-optimization","title":"Direct Preference Optimization (DPO) - Deep (Learning) Focus","snippet":"In this overview, we will learn about a simpler approach to LLM alignment, called Direct Preference Optimization (DPO), that avoids these ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"cameronrwolfe.substack.com"}]},{"type":"search_result_group","domain":"cdn.openai.com","entries":[{"type":"search_result","url":"https://cdn.openai.com/o1-system-card-20241205.pdf","title":"OpenAI o1 System Card","snippet":"5 Dec 2024 \u2014 The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":1739576680.0,"attribution":"cdn.openai.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/data-science/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093","title":"RLAIF: Reinforcement Learning from AI Feedback","snippet":"RLAIF: Reinforcement Learning from AI Feedback. Making alignment via RLHF more scalable by automating human feedback\u2026","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":1705999581.904,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841","title":"Understanding Direct Preference Optimization","snippet":"A look at the \u201cDirect Preference Optimization: Your Language Model is Secretly a Reward Model\u201d paper and its findings","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":1708728471.171,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40preeti.rana.ai/reinforcement-learning-from-human-feedback-in-large-language-models-62c89e8c2d3e","title":"Reinforcement Learning from Human Feedback in Large ...","snippet":"Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique in aligning large language models (LLMs) with human preferences and values.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":1750695055.605,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"www.tylerromero.com","entries":[{"type":"search_result","url":"https://www.tylerromero.com/posts/2024-04-dpo/","title":"Direct Preference Optimization Explained In-depth","snippet":"Direct Preference Optimization Explained In-depth. Simpler preference-tuning without reinforcement learning. April 13, 2024. With my first blog post, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":1712995200.0,"attribution":"www.tylerromero.com"}]},{"type":"search_result_group","domain":"blog.ml.cmu.edu","entries":[{"type":"search_result","url":"https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/","title":"RLHF 101: A Technical Tutorial on Reinforcement ...","snippet":"1 Jun 2025 \u2014 Reinforcement Learning from Human Feedback (RLHF) is a popular ... This blog dives into the full training pipeline of the RLHF framework.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":1748789787.0,"attribution":"blog.ml.cmu.edu"}]},{"type":"search_result_group","domain":"aws.amazon.com","entries":[{"type":"search_result","url":"https://aws.amazon.com/blogs/machine-learning/fine-tune-large-language-models-with-reinforcement-learning-from-human-or-ai-feedback/","title":"Fine-tune large language models with reinforcement ...","snippet":"4 Apr 2025 \u2014 The following diagram illustrates reinforcement learning from human feedback (RLHF) compared to reinforcement learning from AI feedback (RLAIF).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":1743777765.0,"attribution":"aws.amazon.com"}]},{"type":"search_result_group","domain":"www.datacamp.com","entries":[{"type":"search_result","url":"https://www.datacamp.com/blog/rlaif-reinforcement-learning-from-ai-feedback","title":"RLAIF: What is Reinforcement Learning From AI Feedback?","snippet":"28 May 2024 \u2014 Reinforcement learning from AI feedback (RLAIF) is a machine learning technique in which AI models provide feedback to other AI models during ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":1716930000.0,"attribution":"www.datacamp.com"}]},{"type":"search_result_group","domain":"dida.do","entries":[{"type":"search_result","url":"https://dida.do/blog/post-fine-tuning-llm-with-direct-preference-optimization","title":"Post Fine Tuning LLM with Direct Preference Optimization","snippet":"Explore how to optimize Language Models (LLMs) through Direct Preference Optimization and learn how fine-tuning techniques can enhance LLM ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"dida.do"}]},{"type":"search_result_group","domain":"www.deeplearning.ai","entries":[{"type":"search_result","url":"https://www.deeplearning.ai/the-batch/openais-o1-models-excel-in-reasoning-outperform-gpt-4o-in-math-and-coding/","title":"OpenAI's o1 Models Excel in Reasoning, Outperform GPT- ...","snippet":"18 Sept 2024 \u2014 The o1 models show that the combination of reinforcement learning and chain-of-thought reasoning can solve problems that large language models generally find ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"www.deeplearning.ai"}]},{"type":"search_result_group","domain":"kili-technology.com","entries":[{"type":"search_result","url":"https://kili-technology.com/blog/exploring-reinforcement-learning-from-human-feedback-rlhf-a-comprehensive-guide","title":"Exploring Reinforcement Learning with Human Feedback","snippet":"Reinforcement learning from human feedback is a fine-tuning process that enables large language models to follow human instructions. Here's how it works.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"kili-technology.com"}]},{"type":"search_result_group","domain":"oars-workshop.github.io","entries":[{"type":"search_result","url":"https://oars-workshop.github.io/papers/He2025.pdf","title":"RecLAIF: Reinforcement Learning from AI Feedback for ...","snippet":"by X He \u00b7 2025 \u2014 We introduced RecLAIF, a dual-LLM framework that integrates. Reinforcement Learning from AI Feedback (RLAIF) to enhance recommendation systems in terms of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":1751437555.0,"attribution":"oars-workshop.github.io"}]},{"type":"search_result_group","domain":"iclr-blogposts.github.io","entries":[{"type":"search_result","url":"https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/","title":"RLHF without RL - Direct Preference Optimization","snippet":"7 May 2024 \u2014 We discuss the RL part of RLHF and its recent displacement by direct preference optimization (DPO). With DPO, a language model can be aligned ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"iclr-blogposts.github.io"}]},{"type":"search_result_group","domain":"encord.com","entries":[{"type":"search_result","url":"https://encord.com/blog/openai-o1/","title":"OpenAI o1: A New Era of AI Reasoning","snippet":"13 Sept 2024 \u2014 OpenAI o1 series is a new class of large language models (LLMs) designed to enhance reasoning capabilities through a thinking before responding approach.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":1726238419.0,"attribution":"encord.com"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/mengdi-li/awesome-RLAIF","title":"mengdi-li/awesome-RLAIF","snippet":"A common approach is reinforcement learning from AI feedback (RLAIF), which trains a reward model based on AI preference data and employs a reinforcement ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2024.acl-long.643.pdf","title":"Self-Training with Direct Preference Optimization Improves ...","snippet":"by T Wang \u00b7 2024 \u00b7 Cited by 27 \u2014 August 11-16, 2024 \u00a92024 Association for Computational Linguistics. Self-Training with Direct Preference Optimization. Improves Chain-of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":1723985896.0,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"en.wikipedia.org","entries":[{"type":"search_result","url":"https://en.wikipedia.org/wiki/OpenAI_o1","title":"OpenAI o1","snippet":"OpenAI o1 is a generative pre-trained transformer (GPT), the first in OpenAI's \"o\" series of reasoning models. A preview of o1 was released by OpenAI on ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":1726181684.0,"attribution":"en.wikipedia.org"}]},{"type":"search_result_group","domain":"intuitionlabs.ai","entries":[{"type":"search_result","url":"https://intuitionlabs.ai/articles/rlaif-healthcare-annotation-costs","title":"RLAIF in Healthcare: How AI Feedback Reduces Annotation Costs","snippet":"Learn how Reinforcement Learning from AI Feedback (RLAIF) reduces medical AI annotation costs. This guide covers the RLAIF method, its benefits over RLHF, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":1760875200.0,"attribution":"intuitionlabs.ai"}]},{"type":"search_result_group","domain":"platform.openai.com","entries":[{"type":"search_result","url":"https://platform.openai.com/docs/guides/reasoning","title":"Reasoning models - OpenAI API","snippet":"Explore the capabilities of OpenAI's o1 series for complex reasoning and problem-solving. Learn about their features and how they compare to GPT-4o models.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"platform.openai.com"}]}],"debug_sonic_thread_id":"thread_6924a2f727348324b5253d47e3fa5cfb","reasoning_status":"is_reasoning","reasoning_title":"Browsing latest LLM training practices and methodologies","classifier_response":"default","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"67ef728a-bb15-41ef-854a-0486111f83fb","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"67ef728a-bb15-41ef-854a-0486111f83fb","children":["dd0ec702-237e-4778-afbc-f5c50bebb1c3"]},"dd0ec702-237e-4778-afbc-f5c50bebb1c3":{"id":"dd0ec702-237e-4778-afbc-f5c50bebb1c3","message":{"id":"dd0ec702-237e-4778-afbc-f5c50bebb1c3","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764008709.515037,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Refining LLM training pipeline stages","content":"The user wants to understand the latest LLM training pipeline, including RLHF, RLAIF, DPO, and other processes like data governance, cleaning, quality control, and multi-stage pretraining. I\u2019ll propose a pipeline with key steps, starting with a system design, outlining capabilities, and addressing data lifecycle management and governance.","chunks":["The user wants to understand the latest LLM training pipeline, including RLHF, RLAIF, DPO, and other processes like data governance, cleaning, quality control, and multi-stage pretraining.","I\u2019ll propose a pipeline with key steps, starting with a system design, outlining capabilities, and addressing data lifecycle management and governance."],"finished":true},{"summary":"Breaking down pretraining and post-training processes","content":"For self-supervised pretraining, I'd use a GPT-style Transformer architecture, applying techniques like mixture-of-experts and efficient parallelism. The multi-stage pretraining first uses general corpora, followed by domain adaptation. Then, for post-training, I\u2019d focus on supervised fine-tuning (SFT), using high-quality instruction data, dialogues, and distillation techniques.","chunks":["For self-supervised pretraining, I'd use a GPT-style Transformer architecture, applying techniques like mixture-of-experts and efficient parallelism.","The multi-stage pretraining first uses general corpora, followed by domain adaptation.","Then, for post-training, I\u2019d focus on supervised fine-tuning (SFT), using high-quality instruction data, dialogues, and distillation techniques."],"finished":true},{"summary":"Exploring preference-based alignment and reasoning training","content":"In preference-based alignment, RLHF uses preference data to guide learning, often employing Proximal Policy Optimization (PPO) or similar methods for helpful, harmless, honest behavior. Emerging methods include RL from AI Feedback (RLAIF) and Direct Preference Optimization (DPO), which simplify the process. For reasoning, models like OpenAI O1 refine step-by-step processes.","chunks":["In preference-based alignment, RLHF uses preference data to guide learning, often employing Proximal Policy Optimization (PPO) or similar methods for helpful, harmless, honest behavior.","Emerging methods include RL from AI Feedback (RLAIF) and Direct Preference Optimization (DPO), which simplify the process.","For reasoning, models like OpenAI O1 refine step-by-step processes."],"finished":true},{"summary":"Exploring reasoning and reinforcement learning (RL) in training","content":"For reasoning models, like the O1 system, hidden chains-of-thought are used during decision-making. Training methods include RL with chain-of-thought, distillation, and tree-of-thought sampling. RL with verifiable rewards, like code execution checks, is prevalent in coding and math models. Tools and safety training also involve multi-objective RL and adversarial approaches.","chunks":["For reasoning models, like the O1 system, hidden chains-of-thought are used during decision-making.","Training methods include RL with chain-of-thought, distillation, and tree-of-thought sampling.","RL with verifiable rewards, like code execution checks, is prevalent in coding and math models.","Tools and safety training also involve multi-objective RL and adversarial approaches."],"finished":true},{"summary":"Exploring evaluation, systemization, and deployment of AI models","content":"For evaluation, reasoning models focus on chain-of-thought accuracy, using benchmarks like MMLU or GSM8K while avoiding the Goodhart's risk. In systemization, chatbots use light SFT and RLHF alignment, while reasoning models require more compute. Agents use LLMs with tools and stateful memory. Deployment involves guardrails and cascading models for task complexity, plus a shift to careful data partnerships and licensing.","chunks":["For evaluation, reasoning models focus on chain-of-thought accuracy, using benchmarks like MMLU or GSM8K while avoiding the Goodhart's risk.","In systemization, chatbots use light SFT and RLHF alignment, while reasoning models require more compute.","Agents use LLMs with tools and stateful memory.","Deployment involves guardrails and cascading models for task complexity, plus a shift to careful data partnerships and licensing."],"finished":true}],"source_analysis_msg_id":"8061f7cc-5486-4e16-a41d-59e8f791a55b"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"abaad743-2672-45d4-a55d-cf25c0e46a5b","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"abaad743-2672-45d4-a55d-cf25c0e46a5b","children":["fdd84e67-039f-49a6-8d56-379c51a59ac2"]},"fdd84e67-039f-49a6-8d56-379c51a59ac2":{"id":"fdd84e67-039f-49a6-8d56-379c51a59ac2","message":{"id":"fdd84e67-039f-49a6-8d56-379c51a59ac2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764008731.852898,"update_time":1764008731.868545,"content":{"content_type":"code","language":"json","response_format_name":null,"text":"{\"search_query\":[{\"q\":\"LLM training pipeline data curation filtering deduplication copyright 2024\",\"recency\":365},{\"q\":\"large language model data governance copyright licensing news 2024\",\"recency\":365},{\"q\":\"\\\"data recipes\\\" for large language models deduplication filtering 2023\",\"recency\":730},{\"q\":\"\\\"reasoning models\\\" \\\"o1\\\" \\\"chain-of-thought\\\" reinforcement learning training pipeline\",\"recency\":365}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"LLM training pipeline data curation filtering deduplication copyright 2024"},{"type":"search","q":"large language model data governance copyright licensing news 2024"},{"type":"search","q":"\"data recipes\" for large language models deduplication filtering 2023"},{"type":"search","q":"\"reasoning models\" \"o1\" \"chain-of-thought\" reinforcement learning training pipeline"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching LLM training pipeline and data curation updates","classifier_response":"default","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"dd0ec702-237e-4778-afbc-f5c50bebb1c3","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"dd0ec702-237e-4778-afbc-f5c50bebb1c3","children":["752a95b0-6274-43ed-b64d-0dffdfc273b3"]},"752a95b0-6274-43ed-b64d-0dffdfc273b3":{"id":"752a95b0-6274-43ed-b64d-0dffdfc273b3","message":{"id":"752a95b0-6274-43ed-b64d-0dffdfc273b3","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764008731.9215474,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["LLM training pipeline data curation filtering deduplication copyright 2024","large language model data governance copyright licensing news 2024","\"data recipes\" for large language models deduplication filtering 2023","\"reasoning models\" \"o1\" \"chain-of-thought\" reinforcement learning training pipeline"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching LLM training pipeline and data curation updates","classifier_response":"default","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"fdd84e67-039f-49a6-8d56-379c51a59ac2","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"fdd84e67-039f-49a6-8d56-379c51a59ac2","children":["b193ebb6-9e89-44e4-8460-db9c7d5405a8"]},"b193ebb6-9e89-44e4-8460-db9c7d5405a8":{"id":"b193ebb6-9e89-44e4-8460-db9c7d5405a8","message":{"id":"b193ebb6-9e89-44e4-8460-db9c7d5405a8","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764008731.9186518,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"pitchfork.com","entries":[{"type":"search_result","url":"https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay","title":"Major Labels Sign Licensing Deals With AI Music Company Klay","snippet":"Warner Music Group (WMG), Universal Music Group (UMG), and Sony Music Entertainment (SME) have signed licensing agreements with the AI music startup Klay Vision Inc., marking Klay as the first such company to secure deals with all three major record labels and their publishing entities. Klay utilizes its Large Music Model, trained exclusively on licensed content, to offer interactive, human-centered music experiences and emphasizes respect for artist rights and creativity. The company plans to expand collaborations to independent entities in the future.\n\nExecutives from the major labels praised Klay's approach for aligning AI advancements with artist rights and human creativity. Warner's Carletta Higginson highlighted Klay\u2019s commitment to balancing technological innovation with protection of musical works, while UMG's Michael Nash emphasized the importance of ethical collaboration in generative AI. Klay's CEO Ary Attie reiterated the company's mission to enrich music sharing and creativity without replacing human artistry.\n\nThe news follows a recent settlement between UMG and AI developer Udio over copyright infringement, signaling a broader industry shift toward collaborative AI integration rather than litigation.","ref_id":{"turn_index":1,"ref_type":"news","ref_index":40},"pub_date":1763663010.737,"attribution":"pitchfork.com"}]},{"type":"search_result_group","domain":"www.rohan-paul.com","entries":[{"type":"search_result","url":"https://www.rohan-paul.com/p/selecting-and-preparing-training","title":"Selecting and Preparing Training Data for LLMs (2024\u20132025)","snippet":"14 Jun 2025 \u2014 This literature review summarizes key 2024\u20132025 insights on curating LLM training data, covering general-purpose models and briefly touching on ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":0},"pub_date":1749912723.0,"attribution":"www.rohan-paul.com"}]},{"type":"search_result_group","domain":"cla.co.uk","entries":[{"type":"search_result","url":"https://cla.co.uk/cla-welcomes-house-of-lords-report-on-large-language-models/","title":"The House of Lords Report on LLMs","snippet":"7 Feb 2024 \u2014 On Friday 2 February 2024, the House of Lords Communications and Digital Committee released its report around large language models (LLMs) ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":1707307328.0,"attribution":"cla.co.uk"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/datasets/allenai/DataDecide-data-recipes","title":"allenai/DataDecide-data-recipes \u00b7 Datasets at Hugging Face","snippet":"We call the 25 corpora we train on data recipes as they range across popular corpora including Dolma, DCLM, RefinedWeb, C4, and FineWeb as well as combinations ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2502.10867v1","title":"A Tutorial on LLM Reasoning: Relevant Methods behind ...","snippet":"15 Feb 2025 \u2014 Trained using reinforcement learning techniques, o1 excels in complex reasoning tasks by explicitly embedding a native \u201cChain-of-Thought ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2403.13233","title":"arXiv:2403.13233v1 [cs.CL] 20 Mar 2024","snippet":"by S Zhao \u00b7 2024 \u00b7 Cited by 1 \u2014 It is capable of efficiently gen- erating a variety of data recipes, exploring nu- merous combinations for creating data mixtures, and assessing ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":1710982329.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2510.09471v1","title":"Full-Text Search for LLM Training Data for Real World","snippet":"10 Oct 2025 \u2014 In this paper, we present the full-text indexing pipeline for the Apertus LLM training data. Leveraging Elasticsearch parallel indices and the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"developer.nvidia.com","entries":[{"type":"search_result","url":"https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/","title":"Mastering LLM Techniques: Text Data Processing","snippet":"13 Nov 2024 \u2014 In this post, we will describe data processing techniques for optimizing LLM performance by improving data quality for training.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":1731521106.0,"attribution":"developer.nvidia.com"}]},{"type":"search_result_group","domain":"www.copyright.com.au","entries":[{"type":"search_result","url":"https://www.copyright.com.au/2024/05/uk-government-responds-to-house-of-lords-report-on-gen-ai/","title":"UK Government responds to House of Lords report on Gen AI","snippet":"9 May 2024 \u2014 The UK Government responded to the February 2024 report from the House of Lords Communications and Digital Committee on large language models and generative AI.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":1715229685.0,"attribution":"www.copyright.com.au"}]},{"type":"search_result_group","domain":"cameronrwolfe.substack.com","entries":[{"type":"search_result","url":"https://cameronrwolfe.substack.com/p/demystifying-reasoning-models","title":"Demystifying Reasoning Models - by Cameron R. Wolfe, Ph.D.","snippet":"The release of o1-preview [4, 5] by OpenAI made two things very clear: Reasoning models can solve verifiable tasks\u2014such as math and coding tasks ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"cameronrwolfe.substack.com"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2025.emnlp-industry.11.pdf","title":"LP Data Pipeline: Lightweight, Purpose-driven ...","snippet":"by Y Kim \u00b7 2025 \u00b7 Cited by 2 \u2014 This innovative strategy en- ables more effective data quality assessment using. CPU-based resources, paving the way for effective and scalable ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":1761337868.0,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"www.dataprotection.ie","entries":[{"type":"search_result","url":"https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection","title":"AI, Large Language Models and Data Protection | 18/07/2024","snippet":"AI products trained on personal data, or where personal data is input by your staff may pose new risks to organisations and to data subjects.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"www.dataprotection.ie"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=bToTGD5dIk","title":"arXiv:2312.01700v2 [cs.CL] 26 Dec 2023","snippet":", 2023a) and Oa- sis (Zhou et al., 2023c) propose data management systems to compose various data recipes in either the pretraining or SFT stage of LLM ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":1703642075.0,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"magazine.sebastianraschka.com","entries":[{"type":"search_result","url":"https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training","title":"The State of Reinforcement Learning for LLM Reasoning","snippet":"19 Apr 2025 \u2014 One hallmark of reasoning models is that they tend to generate longer outputs because of chain-of-thought reasoning. But by default, there is no ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":1745060564.0,"attribution":"magazine.sebastianraschka.com"}]},{"type":"search_result_group","domain":"lordslibrary.parliament.uk","entries":[{"type":"search_result","url":"https://lordslibrary.parliament.uk/large-language-models-and-generative-ai-house-of-lords-communications-and-digital-committee-report/","title":"Large language models and generative AI: House of Lords ...","snippet":"15 Nov 2024 \u2014 In February 2024 the House of Lords Communications and Digital Committee published a report on large language models and generative AI.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":1731683194.0,"attribution":"lordslibrary.parliament.uk"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/allenai/DataDecide","title":"allenai/DataDecide","snippet":"More than one training run goes into making a large language model ... Data. Released at https://huggingface.co/datasets/allenai/DataDecide-data-recipes ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"shawhin.medium.com","entries":[{"type":"search_result","url":"https://shawhin.medium.com/how-to-train-llms-to-think-like-o1-deepseek-r1-eabc21c8842d","title":"How to Train LLMs to \u201cThink\u201d (o1 & DeepSeek-R1)","snippet":"Here, I discuss the technical details behind the recent \u201cadvanced reasoning\u201d models trained on large-scale reinforcement learning i.e. o1 ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":1744239363.171,"attribution":"shawhin.medium.com"}]},{"type":"search_result_group","domain":"www.datologyai.com","entries":[{"type":"search_result","url":"https://www.datologyai.com/blog/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation","title":"Train LLMs Faster, Better, and Smaller with DatologyAI's ...","snippet":"24 Nov 2024 \u2014 DatologyAI's curated data delivers substantial improvements in LLM quality, training speed, and inference efficiency over existing datasets.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"www.datologyai.com"}]},{"type":"search_result_group","domain":"www.copyright.gov","entries":[{"type":"search_result","url":"https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf","title":"Part 3: Generative AI Training pre-publication version","snippet":"6 May 2025 \u2014 The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models, ARXIV 23 (Apr. 9, 2025), https://arxiv.org/abs ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":1746825924.0,"attribution":"www.copyright.gov"}]},{"type":"search_result_group","domain":"www.ft.com","entries":[{"type":"search_result","url":"https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf","title":"AI start-up Anthropic settles landmark copyright suit for $1.5bn","snippet":"AI start-up Anthropic has agreed to a $1.5 billion settlement in a landmark copyright lawsuit concerning its use of pirated texts to train its language models. The case, brought by a group of authors, accused Anthropic of downloading approximately 465,000 books from illegal sources like Library Genesis and Pirate Library Mirror. Although a California district court previously ruled that some of the company's uses could be considered fair use, it condemned the storage of pirated texts, which led to the settlement. If unresolved, Anthropic could have faced up to $1 trillion in damages\u2014an amount that would have bankrupted the start-up, which is backed by Amazon and Google and now valued at $170 billion. The settlement, noted as potentially the largest copyright recovery in history, must still receive court approval. Aside from financial compensation, Anthropic must destroy the pirated datasets. The outcome sets a significant precedent for other AI companies, such as OpenAI and Meta, by shaping how copyrighted materials can be used in AI model training and determining author compensation.","ref_id":{"turn_index":1,"ref_type":"news","ref_index":42},"pub_date":1757105535.447,"attribution":"www.ft.com"}]},{"type":"search_result_group","domain":"www.theverge.com","entries":[{"type":"search_result","url":"https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning","title":"Meta's AI copyright win comes with a warning about fair use","snippet":"Meta secured a significant victory in a copyright lawsuit where 13 authors alleged the company violated copyright laws by using their works to train its AI systems. U.S. District Judge Vince Chhabria granted Meta summary judgment, ruling that the company's use of the books constitutes fair use. However, the judge emphasized that the decision was based on the plaintiffs' inadequate legal arguments rather than a definitive legal endorsement of Meta's methods. He dismissed two key arguments, stating that Meta's Llama AI doesn\u2019t reproduce enough content from the books to constitute infringement and that the authors do not have an exclusive right to license their works for AI training. Judge Chhabria acknowledged a potentially stronger argument \u2014 that Meta\u2019s AI could saturate the market with similar content \u2014 but criticized the plaintiffs for failing to support this claim convincingly. The ruling follows a similar fair use decision in favor of Anthropic, reinforcing a trend, but with cautionary notes about the legal complexities of AI training and copyright.","ref_id":{"turn_index":1,"ref_type":"news","ref_index":43},"pub_date":1750896824.0,"attribution":"www.theverge.com"}]}],"debug_sonic_thread_id":"thread_6924a2f727348324b5253d47e3fa5cfb","reasoning_status":"is_reasoning","reasoning_title":"Searching LLM training pipeline and data curation updates","classifier_response":"default","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"752a95b0-6274-43ed-b64d-0dffdfc273b3","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"752a95b0-6274-43ed-b64d-0dffdfc273b3","children":["cd4ea09f-e3c7-4632-a3a0-ae94542fd327"]},"cd4ea09f-e3c7-4632-a3a0-ae94542fd327":{"id":"cd4ea09f-e3c7-4632-a3a0-ae94542fd327","message":{"id":"cd4ea09f-e3c7-4632-a3a0-ae94542fd327","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764008753.015674,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Structuring the model update pipeline for clarity","content":"To approach the user\u2019s request, we should start with a recap of their current model and note improvements, highlighting the shift in industry practices. Then, we'll break it into 10 phases, identifying what's standard and what's advanced for chatbots, reasoning models, and agents. We'll define abbreviations once for clarity, but avoid over-explaining. We\u2019ll maintain a clear, technical style, emphasizing precision, comprehensiveness, and critical feedback.","chunks":["To approach the user\u2019s request, we should start with a recap of their current model and note improvements, highlighting the shift in industry practices.","Then, we'll break it into 10 phases, identifying what's standard and what's advanced for chatbots, reasoning models, and agents.","We'll define abbreviations once for clarity, but avoid over-explaining.","We\u2019ll maintain a clear, technical style, emphasizing precision, comprehensiveness, and critical feedback."],"finished":true},{"summary":"Structuring the update for clarity and depth","content":"We can introduce the update by saying the user's four-phase model (scrape \u2192 pretrain \u2192 SFT \u2192 RLHF) is mostly accurate, but needs updates for data governance, synthetic data, and more advanced techniques like preference optimization and step-level RL for the 2023\u20132025 timeline.\n\nThen we can structure the document into sections: Overview (three macro phases), Stage 0 (goals, threat model, compute plan).","chunks":["We can introduce the update by saying the user's four-phase model (scrape \u2192 pretrain \u2192 SFT \u2192 RLHF) is mostly accurate, but needs updates for data governance, synthetic data, and more advanced techniques like preference optimization and step-level RL for the 2023\u20132025 timeline.","Then we can structure the document into sections: Overview (three macro phases), Stage 0 (goals, threat model, compute plan)."],"finished":true},{"summary":"Outlining stages with a focus on data curation and training approach","content":"For Stage 1, data governance involves selecting sources, licensing (e.g., UK House of Lords, US Copyright Office), and using tools like DataDecide and NeMo Curator for deduplication and domain balancing. It also focuses on removing low-quality data or PII and using synthetic data for self-improvement.\n\nIn Stage 2, the base model is pretrained with self-supervised learning and multi-stage curriculum, possibly including multi-modal data and contamination checks. \n\nStage 3 is about task and instruction tuning (SFT), including synthetic, multi-turn, and tool-call data, along with reasoning model training using chain-of-thought. Although high-end labs don't explicitly expose this, it's still used internally.","chunks":["For Stage 1, data governance involves selecting sources, licensing (e.g., UK House of Lords, US Copyright Office), and using tools like DataDecide and NeMo Curator for deduplication and domain balancing.","It also focuses on removing low-quality data or PII and using synthetic data for self-improvement.","In Stage 2, the base model is pretrained with self-supervised learning and multi-stage curriculum, possibly including multi-modal data and contamination checks.","Stage 3 is about task and instruction tuning (SFT), including synthetic, multi-turn, and tool-call data, along with reasoning model training using chain-of-thought.","Although high-end labs don't explicitly expose this, it's still used internally."],"finished":true},{"summary":"Outlining advanced stages in model training and deployment","content":"In Stage 4, preference learning and alignment involve using RLHF, RLAIF (a scaled-up version), and DPO techniques to replace traditional RL in many setups, focusing on safety with models like HC-RLHF.\n\nStage 5 focuses on reasoning-oriented reinforcement learning, using step-level RL on tasks like math or code, with self-training on chain-of-thought tasks.\n\nStage 6 involves tool use and agents, combining SFT, RL, and task execution.\n\nStages 7 and 8 explore productization, categorizing chatbots, reasoning models, and agents, followed by continuous evaluation and iteration.","chunks":["In Stage 4, preference learning and alignment involve using RLHF, RLAIF (a scaled-up version), and DPO techniques to replace traditional RL in many setups, focusing on safety with models like HC-RLHF.","Stage 5 focuses on reasoning-oriented reinforcement learning, using step-level RL on tasks like math or code, with self-training on chain-of-thought tasks.","Stage 6 involves tool use and agents, combining SFT, RL, and task execution.","Stages 7 and 8 explore productization, categorizing chatbots, reasoning models, and agents, followed by continuous evaluation and iteration."],"finished":true}],"source_analysis_msg_id":"57e6df5e-2bcf-4205-ae10-6ada4d0e6135"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"b193ebb6-9e89-44e4-8460-db9c7d5405a8","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"b193ebb6-9e89-44e4-8460-db9c7d5405a8","children":["9e26c8fe-5301-4f52-a81b-2edfd8b3d0c1"]},"9e26c8fe-5301-4f52-a81b-2edfd8b3d0c1":{"id":"9e26c8fe-5301-4f52-a81b-2edfd8b3d0c1","message":{"id":"9e26c8fe-5301-4f52-a81b-2edfd8b3d0c1","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764008785.093246,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 1m 42s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":102,"request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"cd4ea09f-e3c7-4632-a3a0-ae94542fd327","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"cd4ea09f-e3c7-4632-a3a0-ae94542fd327","children":["66c909ab-47b4-42fb-bb3d-f56f69042a61"]},"66c909ab-47b4-42fb-bb3d-f56f69042a61":{"id":"66c909ab-47b4-42fb-bb3d-f56f69042a61","message":{"id":"66c909ab-47b4-42fb-bb3d-f56f69042a61","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764008784.927458,"update_time":1764008872.76975,"content":{"content_type":"text","parts":["Here\u2019s a cleaner, more up-to-date way to slice the modern LLM \u2192 reasoning model \u2192 agent pipeline, and how it differs from the \u201cweb scrape \u2192 pre-train \u2192 SFT \u2192 RLHF\u201d story you sketched.\n\n---\n\n## 0. Problem framing, governance and compute planning\n\nFrontier labs now start with:\n\n- **Target capability profile**  \n  - \u201cFast chat assistant\u201d, \u201cadvanced reasoning model\u201d, \u201ccode model\u201d, \u201cmultimodal assistant\u201d, etc.  \n- **Risk & governance constraints**  \n  - What jurisdictions will care about privacy, copyrighted data, safety domains (health, finance, kids)?  \n  - Data retention, logging, red-team policies, etc. \ue200cite\ue202turn1search9\ue202turn1search13\ue201  \n- **Compute and scaling law plan**  \n  - Token budget, parameter count, mixture-of-experts versus dense, number of training runs for ablations.\n\nEverything downstream (data \u201crecipes\u201d, RL budget, how much human vs AI feedback) is now optimized under this joint capability + risk + compute budget.\n\n---\n\n## 1. Data lifecycle: from \u201cweb scraping\u201d to curated, governed corpora\n\nThe old mental model of \u201ccrawl the web and dump into a tokenizer\u201d is increasingly wrong and legally risky.\n\n### 1.1 Sourcing and licensing\n\nCurrent practice is:\n\n- **Mixtures of sources (\u201cdata recipes\u201d)**: curated open corpora (Dolma, C4, RefinedWeb, FineWeb, etc.), code, books, papers, forums, plus domain-specific sets. These are treated as tunable \u201crecipes\u201d with different mixtures, dedup settings, filters, etc. \ue200cite\ue202turn1search2\ue202turn1search6\ue201  \n- **Dedicated curation stacks** (e.g. DataDecide, Data-Juicer, NeMo Curator, DatologyAI) to search, filter and re-weight huge pools of potential data. \ue200cite\ue202turn1search4\ue202turn1search6\ue202turn1search16\ue201  \n- **Shift to copyright-clean / licensed pools** due to lawsuits and regulator pressure (KL3M, licensed books/music datasets, pay-for-access deals, destruction of clearly infringing sets like pirate book corpora). \ue200cite\ue202turn1search17\ue202turn1news42\ue202turn1news40\ue202turn1news43\ue201  \n\nSo: \u201cweb scraping\u201d is still there, but increasingly mediated through **governed pipelines** and contracts, not a free-for-all.\n\n### 1.2 Cleaning, deduplication, and filtering\n\nModern pipelines run multi-stage filters:\n\n- **Structural cleanup**: HTML stripping, language ID, length filters, boilerplate removal. \ue200cite\ue202turn1search4\ue201  \n- **Near-duplicate removal** at multi-trillion-token scale.\n- **Toxicity, PII and policy filters** using classifiers and heuristics. \ue200cite\ue202turn1search8\ue202turn1search9\ue201  \n- **Domain classification** so you can up/down-weight (e.g. legal/medical vs social media). \ue200cite\ue202turn1search8\ue201  \n\nThe net effect: *data curation is now as important as model architecture*.\n\n---\n\n## 2. Base model pre-training (self-supervised)\n\nThis is still the backbone:\n\n- **Objective**: next-token prediction (or minor variants) on the curated corpus.\n- **Architecture**: decoder-only Transformer, often mixture-of-experts for efficient scaling.\n- **Curriculum / stages**:\n  - Stage 1: broad general corpus.\n  - Stage 2: domain-upweighted mixture (code, math, etc.).\n- **Multimodal** models add image/audio/video encoders, but the conceptual pipeline is similar.\n\nAt this point you have a **base model**: good at general language, weak at following instructions, safety, and long multi-step reasoning.\n\n---\n\n## 3. Supervised instruction and tool fine-tuning (SFT)\n\nSupervised fine-tuning (SFT) = supervised learning on input\u2013output pairs (instructions, chats, tool calls, code, etc.).\n\nModern SFT looks like:\n\n- **Instruction & chat SFT**  \n  - Human-written and synthetic instructions; multi-turn dialogues; broad task coverage.  \n- **Chain-of-thought SFT (hidden or not)**  \n  - For math/coding/science, models are trained on solutions with intermediate reasoning, even if the reasoning is later hidden at inference to avoid leakage / verbosity. \ue200cite\ue202turn0search3\ue202turn0search15\ue201  \n- **Tool-use SFT**  \n  - Data where the model calls tools (code execution, web search, RAG, calculators, proprietary APIs), including correct argument formats and error recovery patterns.\n\nOutcome: an **instruction-tuned assistant**, still not fully aligned or optimally \u201cpolite / safe / robust\u201d.\n\n---\n\n## 4. Preference-based alignment: RLHF, RLAIF, DPO and safety-aware RL\n\nYour description \u201cSFT \u2192 RLHF \u2192 RL with verifiable rewards\u201d is now too narrow. In practice you see a *palette* of alignment methods:\n\n### 4.1 Canonical RLHF (Reinforcement Learning from Human Feedback)\n\nPipeline (still used, but less dominant than in 2022\u201323):\n\n1. Generate candidate responses for prompts.\n2. Humans label pairwise preferences.\n3. Train a **reward model** to predict preferences.\n4. Run reinforcement learning (often Proximal Policy Optimization, PPO) on the base/SFT model to maximize an adjusted reward (reward minus KL penalty to stay close to SFT). \ue200cite\ue202turn0search4\ue202turn0search12\ue202turn0search20\ue201  \n\nPros: flexible, powerful.  \nCons: complex, unstable, expensive; failure modes like reward hacking.\n\n### 4.2 RLAIF (Reinforcement Learning from AI Feedback)\n\nReinforcement Learning from Artificial Intelligence Feedback (RLAIF) replaces humans with a stronger \u201cteacher\u201d model for most labels:\n\n- Strong model generates preference labels or scores.\n- You still fit a reward model and do RL, but at much larger scale as cost drops. \ue200cite\ue202turn0search13\ue202turn0search17\ue202turn0search25\ue201  \n\nIn practice: labs use a **mix** of human + AI feedback; humans focus on policy-critical or subtle value questions, AI covers mundane preferences.\n\n### 4.3 DPO and \u201cRLHF without RL\u201d\n\nDirect Preference Optimization (DPO) and related methods (IPO, ORPO, SimPO, etc.) treat the policy update as a **closed-form optimization on preference pairs** instead of running explicit RL: \ue200cite\ue202turn0search2\ue202turn0search10\ue202turn0search22\ue201  \n\n- You skip the reward model + PPO loop.\n- Training is more like standard supervised learning with a special loss.\n- Widely adopted for \u201cpost-training\u201d because it\u2019s simple and stable.\n\nIndustrial pattern in 2024\u201325: **DPO-style methods are becoming default** for many alignment passes; full RLHF/RLAIF is reserved for high-value cases (e.g. reasoning RL, safety-critical fine-tuning).\n\n### 4.4 Safety-oriented RLHF variants\n\nThere\u2019s active work on RL methods that enforce **formal or statistical safety guarantees**, e.g. High-Confidence Safe RLHF (HC-RLHF) which aims to optimize helpfulness under high-confidence safety constraints. \ue200cite\ue202turn0search8\ue201  \n\nThis fits your \u201creinforcement learning with verifiable rewards\u201d partly, but in safety space rather than task correctness.\n\n---\n\n## 5. Reasoning models: RL on chain-of-thought + verifiable rewards\n\nThis is where your mental model needs the biggest update.\n\nReasoning-centric models (OpenAI o1, DeepSeek-R1 and similar) are trained with **large-scale reinforcement learning on multi-step reasoning traces**, especially for verifiable tasks like math and code:\n\n- The o1 series is trained with **reinforcement learning to reason using chain of thought**, i.e. the model \u201cthinks\u201d internally before responding. \ue200cite\ue202turn0search7\ue202turn0search11\ue202turn0search3\ue202turn0search23\ue201  \n- Surveys of LLM reasoning emphasis that these models embed a **native chain-of-thought process (NCoT)** and rely on RL to shape how that process unfolds. \ue200cite\ue202turn1search3\ue202turn1search11\ue201  \n\nKey differences from chatbot alignment RLHF:\n\n1. **Step-level and outcome-level rewards**  \n   - Pass/fail on unit tests, code compilation, formal verifiers, math correctness, etc. (verifiable rewards). \ue200cite\ue202turn1search7\ue201  \n   - Possibly additional rewards for \u201cgood thinking patterns\u201d (concise, logically structured).\n\n2. **Thinking-time control via RL**  \n   - Some recent work explicitly uses RL to control how long the model \u201cthinks\u201d before answering, trading off accuracy vs latency and cost. \ue200cite\ue202turn1search11\ue201  \n\n3. **Self-training on chain of thought**  \n   - Methods like self-training + DPO improve chain-of-thought reasoning using the model\u2019s own generated traces. \ue200cite\ue202turn0search26\ue201  \n\nSo:\n\n- **Chatbots**: mainly tuned for *style, safety, helpfulness* on shallow tasks; RLHF/DPO at answer-level.  \n- **Reasoning models**: explicitly trained via RL on internal reasoning processes and verifiable outcomes.\n\nYour \u201cchatbots \u2192 reasoning models \u2192 agents\u201d sequence is conceptually right, but the key transition is **from answer-only preference optimization to explicit RL on reasoning traces**.\n\n---\n\n## 6. Tool-use, environments, and \u201creinforcement learning with verifiable rewards\u201d\n\nThere are (at least) three flavors:\n\n1. **Tool-use via SFT only**  \n   - Train on logs or synthetic traces of tool calls (search, code exec, SQL, internal APIs).  \n   - No RL; just supervised learning on good trajectories.\n\n2. **Offline RL / DPO on tool trajectories**  \n   - Preference-label whole trajectories or final outcomes and apply DPO-style optimization.\n\n3. **Online RL in simulated environments**  \n   - For code, math, or structured tasks, use an environment with **automatic reward** (tests pass, checker OK, constraint satisfied).  \n   - This is \u201cRL with verifiable rewards\u201d in the most literal sense and is increasingly used for coding, theorem proving, and specialized agents.\n\nThis part is still experimental at scale; most production systems lean heavily on **SFT + clever prompting + heuristics** for tool-using agents, with RL reserved for specific narrow tasks where rewards are reliable.\n\n---\n\n## 7. Systemization: chatbots, reasoning models, and agents\n\nThink of three layers rather than a strict time sequence:\n\n### 7.1 Chatbot layer (general assistants)\n\n- Backed by a **base or moderately aligned model** (e.g. GPT-4o-class).\n- Heavy emphasis on:\n  - Multi-turn SFT.\n  - DPO/RLHF/RLAIF for preference alignment.\n  - Safety SFT and safety-RL.\n\n### 7.2 Reasoning layer (heavy models)\n\n- Backed by **reasoning models** (o1-like):\n  - RL on chain-of-thought.\n  - Verifiable reward tasks (math, code, puzzles, science). \ue200cite\ue202turn0search7\ue202turn0search19\ue202turn1search7\ue201  \n- Deployed via **cascades**:\n  - Use a fast general model first; escalate to reasoning model when needed.\n\n### 7.3 Agent layer (orchestration + tools + memory)\n\n\u201cAgents\u201d today are mostly **system architectures** around LLMs, not fundamentally new training regimes:\n\n- Planner / executor loops, sometimes tree-search-like structures.\n- Tool calling, RAG (retrieval-augmented generation), scratchpads, external memory.\n- Limited use of full RL; most behavior is still programmed or learned via SFT on traces.\n\nSo the refined picture:\n\n> **Foundation models \u2192 aligned assistants \u2192 reasoning-optimized models \u2192 agentic systems built on top.**\n\nThe underlying training toolbox (SFT, RLHF, RLAIF, DPO, reasoning RL, verifiable rewards) is reused with different emphases at each layer.\n\n---\n\n## 8. Evaluation, red-teaming, and continuous post-training\n\nFinally, production practice now includes continuous cycles of:\n\n- **Benchmarking** (MMLU, GSM8K, code benchmarks, safety evals, domain-specific tests).\n- **Scenario-based evals** and red-teaming beyond public benchmarks, to reduce Goodharting on known tests.\n- **Continuous data + post-training refresh**:\n  - Logging failure cases \u2192 targeted SFT/RLHF/RLAIF/DPO rounds.\n  - Updating safety policies and safety-SFT data as norms and regulations evolve.\n\n---\n\n## How I\u2019d summarize the updated pipeline\n\nIf you want a compact checklist aligned with current practice:\n\n1. **Define objectives and constraints** (capabilities, safety, jurisdictions, compute).  \n2. **Curate governed data** (recipes, licensing, filtering, dedup, domain balancing). \ue200cite\ue202turn1search0\ue202turn1search4\ue202turn1search6\ue201  \n3. **Pre-train base model** (self-supervised, maybe multi-stage & multimodal).  \n4. **Instruction + tool SFT** (including chain-of-thought where useful).  \n5. **Alignment via preferences**  \n   - DPO / related methods as workhorse.  \n   - RLHF/RLAIF for selected high-impact objectives. \ue200cite\ue202turn0search12\ue202turn0search13\ue202turn0search22\ue201  \n6. **Reasoning RL + verifiable rewards** for \u201cthinking\u201d models and code/math. \ue200cite\ue202turn0search7\ue202turn1search3\ue202turn1search11\ue202turn0search26\ue201  \n7. **Safety-oriented RL and SFT** (HC-RLHF and similar). \ue200cite\ue202turn0search8\ue201  \n8. **Systemization into assistants, reasoning models, and agents**, plus cascades.  \n9. **Continuous eval, red-teaming, and refresh.**\n\nIf you\u2019d like, next step could be to sketch **concrete training programs** for three target systems: (a) cheap educational chatbot, (b) strong math/coding reasoning model, and (c) narrow but reliable agent for, say, research+coding \u2014 with explicit data and RL budgets for each.\n\n\ue200navlist\ue202Recent news on data governance & LLM training\ue202turn1news42,turn1news43,turn1news40\ue201"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":"\ue200cite\ue202turn1search9\ue202turn1search13\ue201","start_idx":610,"end_idx":643,"safe_urls":["https://lordslibrary.parliament.uk/large-language-models-and-generative-ai-house-of-lords-communications-and-digital-committee-report/","https://lordslibrary.parliament.uk/large-language-models-and-generative-ai-house-of-lords-communications-and-digital-committee-report/?utm_source=chatgpt.com","https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection","https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection?utm_source=chatgpt.com"],"refs":[],"alt":"([Homepage | Data Protection Commission](https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI, Large Language Models and Data Protection | 18/07/2024","url":"https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection?utm_source=chatgpt.com","pub_date":null,"snippet":"AI products trained on personal data, or where personal data is input by your staff may pose new risks to organisations and to data subjects.","attribution_segments":null,"supporting_websites":[{"title":"Large language models and generative AI: House of Lords ...","url":"https://lordslibrary.parliament.uk/large-language-models-and-generative-ai-house-of-lords-communications-and-digital-committee-report/?utm_source=chatgpt.com","pub_date":null,"snippet":"15 Nov 2024 \u2014 In February 2024 the House of Lords Communications and Digital Committee published a report on large language models and generative AI.","attribution":"House of Lords Library"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":9},{"turn_index":1,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"Homepage | Data Protection Commission"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search2\ue202turn1search6\ue201","start_idx":1437,"end_idx":1469,"safe_urls":["https://arxiv.org/pdf/2403.13233","https://arxiv.org/pdf/2403.13233?utm_source=chatgpt.com","https://huggingface.co/datasets/allenai/DataDecide-data-recipes","https://huggingface.co/datasets/allenai/DataDecide-data-recipes?utm_source=chatgpt.com"],"refs":[],"alt":"([Hugging Face](https://huggingface.co/datasets/allenai/DataDecide-data-recipes?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"allenai/DataDecide-data-recipes \u00b7 Datasets at Hugging Face","url":"https://huggingface.co/datasets/allenai/DataDecide-data-recipes?utm_source=chatgpt.com","pub_date":null,"snippet":"We call the 25 corpora we train on data recipes as they range across popular corpora including Dolma, DCLM, RefinedWeb, C4, and FineWeb as well as combinations ...","attribution_segments":null,"supporting_websites":[{"title":"arXiv:2403.13233v1 [cs.CL] 20 Mar 2024","url":"https://arxiv.org/pdf/2403.13233?utm_source=chatgpt.com","pub_date":null,"snippet":"by S Zhao \u00b7 2024 \u00b7 Cited by 1 \u2014 It is capable of efficiently gen- erating a variety of data recipes, exploring nu- merous combinations for creating data mixtures, and assessing ...","attribution":"arXiv"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":2},{"turn_index":1,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"Hugging Face"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search4\ue202turn1search6\ue202turn1search16\ue201","start_idx":1623,"end_idx":1669,"safe_urls":["https://arxiv.org/pdf/2403.13233","https://arxiv.org/pdf/2403.13233?utm_source=chatgpt.com","https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/","https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com","https://www.datologyai.com/blog/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation","https://www.datologyai.com/blog/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation?utm_source=chatgpt.com"],"refs":[],"alt":"([NVIDIA Developer](https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering LLM Techniques: Text Data Processing","url":"https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com","pub_date":1731521106.0,"snippet":"13 Nov 2024 \u2014 In this post, we will describe data processing techniques for optimizing LLM performance by improving data quality for training.","attribution_segments":null,"supporting_websites":[{"title":"arXiv:2403.13233v1 [cs.CL] 20 Mar 2024","url":"https://arxiv.org/pdf/2403.13233?utm_source=chatgpt.com","pub_date":null,"snippet":"by S Zhao \u00b7 2024 \u00b7 Cited by 1 \u2014 It is capable of efficiently gen- erating a variety of data recipes, exploring nu- merous combinations for creating data mixtures, and assessing ...","attribution":"arXiv"},{"title":"Train LLMs Faster, Better, and Smaller with DatologyAI's ...","url":"https://www.datologyai.com/blog/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation?utm_source=chatgpt.com","pub_date":null,"snippet":"24 Nov 2024 \u2014 DatologyAI's curated data delivers substantial improvements in LLM quality, training speed, and inference efficiency over existing datasets.","attribution":"Datology AI"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":4},{"turn_index":1,"ref_type":"search","ref_index":6},{"turn_index":1,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"NVIDIA Developer"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search17\ue202turn1news42\ue202turn1news40\ue202turn1news43\ue201","start_idx":1885,"end_idx":1941,"safe_urls":["https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay","https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay?utm_source=chatgpt.com","https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf","https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf?utm_source=chatgpt.com","https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf","https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf?utm_source=chatgpt.com","https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning","https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning?utm_source=chatgpt.com"],"refs":[],"alt":"([U.S. Copyright Office](https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Part 3: Generative AI Training pre-publication version","url":"https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf?utm_source=chatgpt.com","pub_date":1746825924.0,"snippet":"6 May 2025 \u2014 The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models, ARXIV 23 (Apr. 9, 2025), https://arxiv.org/abs ...","attribution_segments":null,"supporting_websites":[{"title":"AI start-up Anthropic settles landmark copyright suit for $1.5bn","url":"https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf?utm_source=chatgpt.com","pub_date":null,"snippet":"AI start-up Anthropic has agreed to a $1.5 billion settlement in a landmark copyright lawsuit concerning its use of pirated texts to train its language models. The case, brought by a group of authors, accused Anthropic of downloading approximately 465,000 books from illegal sources like Library Genesis and Pirate Library Mirror. Although a California district court previously ruled that some of the company's uses could be considered fair use, it condemned the storage of pirated texts, which led to the settlement. If unresolved, Anthropic could have faced up to $1 trillion in damages\u2014an amount that would have bankrupted the start-up, which is backed by Amazon and Google and now valued at $170 billion. The settlement, noted as potentially the largest copyright recovery in history, must still receive court approval. Aside from financial compensation, Anthropic must destroy the pirated datasets. The outcome sets a significant precedent for other AI companies, such as OpenAI and Meta, by shaping how copyrighted materials can be used in AI model training and determining author compensation.","attribution":"Financial Times"},{"title":"Major Labels Sign Licensing Deals With AI Music Company Klay","url":"https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay?utm_source=chatgpt.com","pub_date":null,"snippet":"Warner Music Group (WMG), Universal Music Group (UMG), and Sony Music Entertainment (SME) have signed licensing agreements with the AI music startup Klay Vision Inc., marking Klay as the first such company to secure deals with all three major record labels and their publishing entities. Klay utilizes its Large Music Model, trained exclusively on licensed content, to offer interactive, human-centered music experiences and emphasizes respect for artist rights and creativity. The company plans to expand collaborations to independent entities in the future.\n\nExecutives from the major labels praised Klay's approach for aligning AI advancements with artist rights and human creativity. Warner's Carletta Higginson highlighted Klay\u2019s commitment to balancing technological innovation with protection of musical works, while UMG's Michael Nash emphasized the importance of ethical collaboration in generative AI. Klay's CEO Ary Attie reiterated the company's mission to enrich music sharing and creativity without replacing human artistry.\n\nThe news follows a recent settlement between UMG and AI developer Udio over copyright infringement, signaling a broader industry shift toward collaborative AI integration rather than litigation.","attribution":"Pitchfork"},{"title":"Meta's AI copyright win comes with a warning about fair use","url":"https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning?utm_source=chatgpt.com","pub_date":null,"snippet":"Meta secured a significant victory in a copyright lawsuit where 13 authors alleged the company violated copyright laws by using their works to train its AI systems. U.S. District Judge Vince Chhabria granted Meta summary judgment, ruling that the company's use of the books constitutes fair use. However, the judge emphasized that the decision was based on the plaintiffs' inadequate legal arguments rather than a definitive legal endorsement of Meta's methods. He dismissed two key arguments, stating that Meta's Llama AI doesn\u2019t reproduce enough content from the books to constitute infringement and that the authors do not have an exclusive right to license their works for AI training. Judge Chhabria acknowledged a potentially stronger argument \u2014 that Meta\u2019s AI could saturate the market with similar content \u2014 but criticized the plaintiffs for failing to support this claim convincingly. The ruling follows a similar fair use decision in favor of Anthropic, reinforcing a trend, but with cautionary notes about the legal complexities of AI training and copyright.","attribution":"The Verge"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":17},{"turn_index":1,"ref_type":"news","ref_index":42},{"turn_index":1,"ref_type":"news","ref_index":40},{"turn_index":1,"ref_type":"news","ref_index":43}],"hue":null,"attributions":null,"attribution":"U.S. Copyright Office"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search4\ue201","start_idx":2256,"end_idx":2275,"safe_urls":["https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/","https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com"],"refs":[],"alt":"([NVIDIA Developer](https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering LLM Techniques: Text Data Processing","url":"https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com","pub_date":1731521106.0,"snippet":"13 Nov 2024 \u2014 In this post, we will describe data processing techniques for optimizing LLM performance by improving data quality for training.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"NVIDIA Developer"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search8\ue202turn1search9\ue201","start_idx":2411,"end_idx":2443,"safe_urls":["https://aclanthology.org/2025.emnlp-industry.11.pdf","https://aclanthology.org/2025.emnlp-industry.11.pdf?utm_source=chatgpt.com","https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection","https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection?utm_source=chatgpt.com"],"refs":[],"alt":"([ACL Anthology](https://aclanthology.org/2025.emnlp-industry.11.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"LP Data Pipeline: Lightweight, Purpose-driven ...","url":"https://aclanthology.org/2025.emnlp-industry.11.pdf?utm_source=chatgpt.com","pub_date":1761337868.0,"snippet":"by Y Kim \u00b7 2025 \u00b7 Cited by 2 \u2014 This innovative strategy en- ables more effective data quality assessment using. CPU-based resources, paving the way for effective and scalable ...","attribution_segments":null,"supporting_websites":[{"title":"AI, Large Language Models and Data Protection | 18/07/2024","url":"https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection?utm_source=chatgpt.com","pub_date":null,"snippet":"AI products trained on personal data, or where personal data is input by your staff may pose new risks to organisations and to data subjects.","attribution":"Homepage | Data Protection Commission"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":8},{"turn_index":1,"ref_type":"search","ref_index":9}],"hue":null,"attributions":null,"attribution":"ACL Anthology"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search8\ue201","start_idx":2538,"end_idx":2557,"safe_urls":["https://aclanthology.org/2025.emnlp-industry.11.pdf","https://aclanthology.org/2025.emnlp-industry.11.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([ACL Anthology](https://aclanthology.org/2025.emnlp-industry.11.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"LP Data Pipeline: Lightweight, Purpose-driven ...","url":"https://aclanthology.org/2025.emnlp-industry.11.pdf?utm_source=chatgpt.com","pub_date":1761337868.0,"snippet":"by Y Kim \u00b7 2025 \u00b7 Cited by 2 \u2014 This innovative strategy en- ables more effective data quality assessment using. CPU-based resources, paving the way for effective and scalable ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"ACL Anthology"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search15\ue201","start_idx":3798,"end_idx":3831,"safe_urls":["https://openai.com/index/introducing-openai-o1-preview/","https://openai.com/index/introducing-openai-o1-preview/?utm_source=chatgpt.com","https://openai.com/index/openai-o1-system-card/","https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"OpenAI o1 System Card","url":"https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com","pub_date":null,"snippet":"Model data and training. The o1 large language model family is trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers ...","attribution_segments":null,"supporting_websites":[{"title":"Introducing OpenAI o1-preview","url":"https://openai.com/index/introducing-openai-o1-preview/?utm_source=chatgpt.com","pub_date":null,"snippet":"12 Sept 2024 \u2014 A new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than ...","attribution":"OpenAI"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":15}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search4\ue202turn0search12\ue202turn0search20\ue201","start_idx":4778,"end_idx":4825,"safe_urls":["https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/","https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/?utm_source=chatgpt.com","https://huggingface.co/blog/rlhf","https://huggingface.co/blog/rlhf?utm_source=chatgpt.com","https://kili-technology.com/blog/exploring-reinforcement-learning-from-human-feedback-rlhf-a-comprehensive-guide","https://kili-technology.com/blog/exploring-reinforcement-learning-from-human-feedback-rlhf-a-comprehensive-guide?utm_source=chatgpt.com"],"refs":[],"alt":"([Hugging Face](https://huggingface.co/blog/rlhf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Illustrating Reinforcement Learning from Human Feedback ...","url":"https://huggingface.co/blog/rlhf?utm_source=chatgpt.com","pub_date":null,"snippet":"9 Dec 2022 \u2014 That's the idea of Reinforcement Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language ...","attribution_segments":null,"supporting_websites":[{"title":"RLHF 101: A Technical Tutorial on Reinforcement ...","url":"https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/?utm_source=chatgpt.com","pub_date":null,"snippet":"1 Jun 2025 \u2014 Reinforcement Learning from Human Feedback (RLHF) is a popular ... This blog dives into the full training pipeline of the RLHF framework.","attribution":"ML@CMU"},{"title":"Exploring Reinforcement Learning with Human Feedback","url":"https://kili-technology.com/blog/exploring-reinforcement-learning-from-human-feedback-rlhf-a-comprehensive-guide?utm_source=chatgpt.com","pub_date":null,"snippet":"Reinforcement learning from human feedback is a fine-tuning process that enables large language models to follow human instructions. Here's how it works.","attribution":"Kili Technology"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":4},{"turn_index":0,"ref_type":"search","ref_index":12},{"turn_index":0,"ref_type":"search","ref_index":20}],"hue":null,"attributions":null,"attribution":"Hugging Face"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search13\ue202turn0search17\ue202turn0search25\ue201","start_idx":5257,"end_idx":5305,"safe_urls":["https://arxiv.org/pdf/2309.00267","https://arxiv.org/pdf/2309.00267?utm_source=chatgpt.com","https://github.com/mengdi-li/awesome-RLAIF","https://github.com/mengdi-li/awesome-RLAIF?utm_source=chatgpt.com","https://www.datacamp.com/blog/rlaif-reinforcement-learning-from-ai-feedback","https://www.datacamp.com/blog/rlaif-reinforcement-learning-from-ai-feedback?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/pdf/2309.00267?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from ...","url":"https://arxiv.org/pdf/2309.00267?utm_source=chatgpt.com","pub_date":1725416622.0,"snippet":"by H Lee \u00b7 2023 \u00b7 Cited by 304 \u2014 Reinforcement Learning from AI Feedback. 2.2.1. CANONICAL RLAIF. We describe our adaptation of the canonical RLAIF setup below. Unless ...","attribution_segments":null,"supporting_websites":[{"title":"RLAIF: What is Reinforcement Learning From AI Feedback?","url":"https://www.datacamp.com/blog/rlaif-reinforcement-learning-from-ai-feedback?utm_source=chatgpt.com","pub_date":null,"snippet":"28 May 2024 \u2014 Reinforcement learning from AI feedback (RLAIF) is a machine learning technique in which AI models provide feedback to other AI models during ...","attribution":"DataCamp"},{"title":"mengdi-li/awesome-RLAIF","url":"https://github.com/mengdi-li/awesome-RLAIF?utm_source=chatgpt.com","pub_date":null,"snippet":"A common approach is reinforcement learning from AI feedback (RLAIF), which trains a reward model based on AI preference data and employs a reinforcement ...","attribution":"GitHub"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":13},{"turn_index":0,"ref_type":"search","ref_index":17},{"turn_index":0,"ref_type":"search","ref_index":25}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search2\ue202turn0search10\ue202turn0search22\ue201","start_idx":5683,"end_idx":5730,"safe_urls":["https://arxiv.org/abs/2305.18290","https://arxiv.org/abs/2305.18290?utm_source=chatgpt.com","https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/","https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/?utm_source=chatgpt.com","https://www.tylerromero.com/posts/2024-04-dpo/","https://www.tylerromero.com/posts/2024-04-dpo/?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2305.18290?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","url":"https://arxiv.org/abs/2305.18290?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Direct Preference Optimization Explained In-depth","url":"https://www.tylerromero.com/posts/2024-04-dpo/?utm_source=chatgpt.com","pub_date":null,"snippet":"Direct Preference Optimization Explained In-depth. Simpler preference-tuning without reinforcement learning. April 13, 2024. With my first blog post, ...","attribution":"tylerromero.com"},{"title":"RLHF without RL - Direct Preference Optimization","url":"https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/?utm_source=chatgpt.com","pub_date":null,"snippet":"7 May 2024 \u2014 We discuss the RL part of RLHF and its recent displacement by direct preference optimization (DPO). With DPO, a language model can be aligned ...","attribution":"ICLR Blogposts"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":10},{"turn_index":0,"ref_type":"search","ref_index":22}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search8\ue201","start_idx":6368,"end_idx":6387,"safe_urls":["https://arxiv.org/html/2506.08266v1","https://arxiv.org/html/2506.08266v1?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2506.08266v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Reinforcement Learning from Human Feedback with High ...","url":"https://arxiv.org/html/2506.08266v1?utm_source=chatgpt.com","pub_date":null,"snippet":"9 Jun 2025 \u2014 We propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search7\ue202turn0search11\ue202turn0search3\ue202turn0search23\ue201","start_idx":6997,"end_idx":7057,"safe_urls":["https://arxiv.org/html/2412.16720v1","https://arxiv.org/html/2412.16720v1?utm_source=chatgpt.com","https://cdn.openai.com/o1-system-card-20241205.pdf","https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com","https://encord.com/blog/openai-o1/","https://encord.com/blog/openai-o1/?utm_source=chatgpt.com","https://openai.com/index/openai-o1-system-card/","https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI CDN](https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"OpenAI o1 System Card","url":"https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com","pub_date":1739576680.0,"snippet":"5 Dec 2024 \u2014 The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning ...","attribution_segments":null,"supporting_websites":[{"title":"OpenAI o1 System Card","url":"https://arxiv.org/html/2412.16720v1?utm_source=chatgpt.com","pub_date":null,"snippet":"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new ...","attribution":"arXiv"},{"title":"OpenAI o1 System Card","url":"https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com","pub_date":null,"snippet":"Model data and training. The o1 large language model family is trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers ...","attribution":"OpenAI"},{"title":"OpenAI o1: A New Era of AI Reasoning","url":"https://encord.com/blog/openai-o1/?utm_source=chatgpt.com","pub_date":null,"snippet":"13 Sept 2024 \u2014 OpenAI o1 series is a new class of large language models (LLMs) designed to enhance reasoning capabilities through a thinking before responding approach.","attribution":"Encord"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":7},{"turn_index":0,"ref_type":"search","ref_index":11},{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":23}],"hue":null,"attributions":null,"attribution":"OpenAI CDN"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search3\ue202turn1search11\ue201","start_idx":7215,"end_idx":7248,"safe_urls":["https://arxiv.org/html/2502.10867v1","https://arxiv.org/html/2502.10867v1?utm_source=chatgpt.com","https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training","https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2502.10867v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Tutorial on LLM Reasoning: Relevant Methods behind ...","url":"https://arxiv.org/html/2502.10867v1?utm_source=chatgpt.com","pub_date":null,"snippet":"15 Feb 2025 \u2014 Trained using reinforcement learning techniques, o1 excels in complex reasoning tasks by explicitly embedding a native \u201cChain-of-Thought ...","attribution_segments":null,"supporting_websites":[{"title":"The State of Reinforcement Learning for LLM Reasoning","url":"https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com","pub_date":null,"snippet":"19 Apr 2025 \u2014 One hallmark of reasoning models is that they tend to generate longer outputs because of chain-of-thought reasoning. But by default, there is no ...","attribution":"Sebastian Raschka's Ahead of AI"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":3},{"turn_index":1,"ref_type":"search","ref_index":11}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search7\ue201","start_idx":7455,"end_idx":7474,"safe_urls":["https://cameronrwolfe.substack.com/p/demystifying-reasoning-models","https://cameronrwolfe.substack.com/p/demystifying-reasoning-models?utm_source=chatgpt.com"],"refs":[],"alt":"([cameronrwolfe.substack.com](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Demystifying Reasoning Models - by Cameron R. Wolfe, Ph.D.","url":"https://cameronrwolfe.substack.com/p/demystifying-reasoning-models?utm_source=chatgpt.com","pub_date":null,"snippet":"The release of o1-preview [4, 5] by OpenAI made two things very clear: Reasoning models can solve verifiable tasks\u2014such as math and coding tasks ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"cameronrwolfe.substack.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search11\ue201","start_idx":7751,"end_idx":7771,"safe_urls":["https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training","https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com"],"refs":[],"alt":"([Sebastian Raschka's Ahead of AI](https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The State of Reinforcement Learning for LLM Reasoning","url":"https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com","pub_date":1745060564.0,"snippet":"19 Apr 2025 \u2014 One hallmark of reasoning models is that they tend to generate longer outputs because of chain-of-thought reasoning. But by default, there is no ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":11}],"hue":null,"attributions":null,"attribution":"Sebastian Raschka's Ahead of AI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search26\ue201","start_idx":7931,"end_idx":7951,"safe_urls":["https://aclanthology.org/2024.acl-long.643.pdf","https://aclanthology.org/2024.acl-long.643.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([ACL Anthology](https://aclanthology.org/2024.acl-long.643.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Self-Training with Direct Preference Optimization Improves ...","url":"https://aclanthology.org/2024.acl-long.643.pdf?utm_source=chatgpt.com","pub_date":1723985896.0,"snippet":"by T Wang \u00b7 2024 \u00b7 Cited by 27 \u2014 August 11-16, 2024 \u00a92024 Association for Computational Linguistics. Self-Training with Direct Preference Optimization. Improves Chain-of ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":26}],"hue":null,"attributions":null,"attribution":"ACL Anthology"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search7\ue202turn0search19\ue202turn1search7\ue201","start_idx":9890,"end_idx":9936,"safe_urls":["https://cameronrwolfe.substack.com/p/demystifying-reasoning-models","https://cameronrwolfe.substack.com/p/demystifying-reasoning-models?utm_source=chatgpt.com","https://cdn.openai.com/o1-system-card-20241205.pdf","https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com","https://www.deeplearning.ai/the-batch/openais-o1-models-excel-in-reasoning-outperform-gpt-4o-in-math-and-coding/","https://www.deeplearning.ai/the-batch/openais-o1-models-excel-in-reasoning-outperform-gpt-4o-in-math-and-coding/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI CDN](https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"OpenAI o1 System Card","url":"https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com","pub_date":1739576680.0,"snippet":"5 Dec 2024 \u2014 The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning ...","attribution_segments":null,"supporting_websites":[{"title":"OpenAI's o1 Models Excel in Reasoning, Outperform GPT- ...","url":"https://www.deeplearning.ai/the-batch/openais-o1-models-excel-in-reasoning-outperform-gpt-4o-in-math-and-coding/?utm_source=chatgpt.com","pub_date":null,"snippet":"18 Sept 2024 \u2014 The o1 models show that the combination of reinforcement learning and chain-of-thought reasoning can solve problems that large language models generally find ...","attribution":"DeepLearning.ai"},{"title":"Demystifying Reasoning Models - by Cameron R. Wolfe, Ph.D.","url":"https://cameronrwolfe.substack.com/p/demystifying-reasoning-models?utm_source=chatgpt.com","pub_date":null,"snippet":"The release of o1-preview [4, 5] by OpenAI made two things very clear: Reasoning models can solve verifiable tasks\u2014such as math and coding tasks ...","attribution":"cameronrwolfe.substack.com"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":7},{"turn_index":0,"ref_type":"search","ref_index":19},{"turn_index":1,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"OpenAI CDN"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search0\ue202turn1search4\ue202turn1search6\ue201","start_idx":11528,"end_idx":11573,"safe_urls":["https://arxiv.org/pdf/2403.13233","https://arxiv.org/pdf/2403.13233?utm_source=chatgpt.com","https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/","https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com","https://www.rohan-paul.com/p/selecting-and-preparing-training","https://www.rohan-paul.com/p/selecting-and-preparing-training?utm_source=chatgpt.com"],"refs":[],"alt":"([Rohan's Bytes](https://www.rohan-paul.com/p/selecting-and-preparing-training?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Selecting and Preparing Training Data for LLMs (2024\u20132025)","url":"https://www.rohan-paul.com/p/selecting-and-preparing-training?utm_source=chatgpt.com","pub_date":1749912723.0,"snippet":"14 Jun 2025 \u2014 This literature review summarizes key 2024\u20132025 insights on curating LLM training data, covering general-purpose models and briefly touching on ...","attribution_segments":null,"supporting_websites":[{"title":"Mastering LLM Techniques: Text Data Processing","url":"https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com","pub_date":null,"snippet":"13 Nov 2024 \u2014 In this post, we will describe data processing techniques for optimizing LLM performance by improving data quality for training.","attribution":"NVIDIA Developer"},{"title":"arXiv:2403.13233v1 [cs.CL] 20 Mar 2024","url":"https://arxiv.org/pdf/2403.13233?utm_source=chatgpt.com","pub_date":null,"snippet":"by S Zhao \u00b7 2024 \u00b7 Cited by 1 \u2014 It is capable of efficiently gen- erating a variety of data recipes, exploring nu- merous combinations for creating data mixtures, and assessing ...","attribution":"arXiv"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":0},{"turn_index":1,"ref_type":"search","ref_index":4},{"turn_index":1,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"Rohan's Bytes"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search12\ue202turn0search13\ue202turn0search22\ue201","start_idx":11863,"end_idx":11911,"safe_urls":["https://arxiv.org/pdf/2309.00267","https://arxiv.org/pdf/2309.00267?utm_source=chatgpt.com","https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/","https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/?utm_source=chatgpt.com","https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/","https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/?utm_source=chatgpt.com"],"refs":[],"alt":"([ML@CMU](https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"RLHF 101: A Technical Tutorial on Reinforcement ...","url":"https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/?utm_source=chatgpt.com","pub_date":1748789787.0,"snippet":"1 Jun 2025 \u2014 Reinforcement Learning from Human Feedback (RLHF) is a popular ... This blog dives into the full training pipeline of the RLHF framework.","attribution_segments":null,"supporting_websites":[{"title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from ...","url":"https://arxiv.org/pdf/2309.00267?utm_source=chatgpt.com","pub_date":null,"snippet":"by H Lee \u00b7 2023 \u00b7 Cited by 304 \u2014 Reinforcement Learning from AI Feedback. 2.2.1. CANONICAL RLAIF. We describe our adaptation of the canonical RLAIF setup below. Unless ...","attribution":"arXiv"},{"title":"RLHF without RL - Direct Preference Optimization","url":"https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/?utm_source=chatgpt.com","pub_date":null,"snippet":"7 May 2024 \u2014 We discuss the RL part of RLHF and its recent displacement by direct preference optimization (DPO). With DPO, a language model can be aligned ...","attribution":"ICLR Blogposts"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":12},{"turn_index":0,"ref_type":"search","ref_index":13},{"turn_index":0,"ref_type":"search","ref_index":22}],"hue":null,"attributions":null,"attribution":"ML@CMU"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search7\ue202turn1search3\ue202turn1search11\ue202turn0search26\ue201","start_idx":11992,"end_idx":12052,"safe_urls":["https://aclanthology.org/2024.acl-long.643.pdf","https://aclanthology.org/2024.acl-long.643.pdf?utm_source=chatgpt.com","https://arxiv.org/html/2502.10867v1","https://arxiv.org/html/2502.10867v1?utm_source=chatgpt.com","https://cdn.openai.com/o1-system-card-20241205.pdf","https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com","https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training","https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI CDN](https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"OpenAI o1 System Card","url":"https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com","pub_date":1739576680.0,"snippet":"5 Dec 2024 \u2014 The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning ...","attribution_segments":null,"supporting_websites":[{"title":"A Tutorial on LLM Reasoning: Relevant Methods behind ...","url":"https://arxiv.org/html/2502.10867v1?utm_source=chatgpt.com","pub_date":null,"snippet":"15 Feb 2025 \u2014 Trained using reinforcement learning techniques, o1 excels in complex reasoning tasks by explicitly embedding a native \u201cChain-of-Thought ...","attribution":"arXiv"},{"title":"The State of Reinforcement Learning for LLM Reasoning","url":"https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com","pub_date":null,"snippet":"19 Apr 2025 \u2014 One hallmark of reasoning models is that they tend to generate longer outputs because of chain-of-thought reasoning. But by default, there is no ...","attribution":"Sebastian Raschka's Ahead of AI"},{"title":"Self-Training with Direct Preference Optimization Improves ...","url":"https://aclanthology.org/2024.acl-long.643.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by T Wang \u00b7 2024 \u00b7 Cited by 27 \u2014 August 11-16, 2024 \u00a92024 Association for Computational Linguistics. Self-Training with Direct Preference Optimization. Improves Chain-of ...","attribution":"ACL Anthology"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":7},{"turn_index":1,"ref_type":"search","ref_index":3},{"turn_index":1,"ref_type":"search","ref_index":11},{"turn_index":0,"ref_type":"search","ref_index":26}],"hue":null,"attributions":null,"attribution":"OpenAI CDN"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search8\ue201","start_idx":12112,"end_idx":12131,"safe_urls":["https://arxiv.org/html/2506.08266v1","https://arxiv.org/html/2506.08266v1?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2506.08266v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Reinforcement Learning from Human Feedback with High ...","url":"https://arxiv.org/html/2506.08266v1?utm_source=chatgpt.com","pub_date":null,"snippet":"9 Jun 2025 \u2014 We propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200navlist\ue202Recent news on data governance & LLM training\ue202turn1news42,turn1news43,turn1news40\ue201","start_idx":12549,"end_idx":12640,"safe_urls":["https://images.openai.com/static-rsc-1/KYv7VsHds2TLNUz9C2lVrek_c51Zuw8p1r45HpHfb24Dh2bIttflkBHZxiEA4kFbvnwnc_ugwO2H3lNkysYzY011NNF37lrQ29cP4Pu3qBakao8jw1AW0U9EDO6CcDCKFwZS-_JXwpC9kGkohsehABs7GGVpDPOowqwysh6YsxI","https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay","https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay?utm_source=chatgpt.com","https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf","https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf?utm_source=chatgpt.com","https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning","https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning?utm_source=chatgpt.com"],"refs":[{"turn_index":1,"ref_type":"news","ref_index":42},{"turn_index":1,"ref_type":"news","ref_index":43},{"turn_index":1,"ref_type":"news","ref_index":40}],"alt":"- [Financial Times](https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf?utm_source=chatgpt.com)\n- [The Verge](https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning?utm_source=chatgpt.com)\n- [Pitchfork](https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay?utm_source=chatgpt.com)","prompt_text":"\nnavlist containing the following URLs:\n- [AI start-up Anthropic settles landmark copyright suit for $1.5bn](https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf?utm_source=chatgpt.com)\n- [Meta's AI copyright win comes with a warning about fair use](https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning?utm_source=chatgpt.com)\n- [Major Labels Sign Licensing Deals With AI Music Company Klay](https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay?utm_source=chatgpt.com)\n","type":"nav_list","title":"Recent news on data governance & LLM training","items":[{"title":"AI start-up Anthropic settles landmark copyright suit for $1.5bn","url":"https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf?utm_source=chatgpt.com","thumbnail_url":null,"pub_date":1757105535.447,"attribution":"Financial Times"},{"title":"Meta's AI copyright win comes with a warning about fair use","url":"https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning?utm_source=chatgpt.com","thumbnail_url":null,"pub_date":1750896824.0,"attribution":"The Verge"},{"title":"Major Labels Sign Licensing Deals With AI Music Company Klay","url":"https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay?utm_source=chatgpt.com","thumbnail_url":"https://images.openai.com/static-rsc-1/KYv7VsHds2TLNUz9C2lVrek_c51Zuw8p1r45HpHfb24Dh2bIttflkBHZxiEA4kFbvnwnc_ugwO2H3lNkysYzY011NNF37lrQ29cP4Pu3qBakao8jw1AW0U9EDO6CcDCKFwZS-_JXwpC9kGkohsehABs7GGVpDPOowqwysh6YsxI","pub_date":1763663010.737,"attribution":"Pitchfork"}],"user_time_zone":"Asia/Singapore"},{"matched_text":" ","start_idx":12640,"end_idx":12640,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"AI, Large Language Models and Data Protection | 18/07/2024","url":"https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection?utm_source=chatgpt.com","attribution":"Homepage | Data Protection Commission"},{"title":"allenai/DataDecide-data-recipes \u00b7 Datasets at Hugging Face","url":"https://huggingface.co/datasets/allenai/DataDecide-data-recipes?utm_source=chatgpt.com","attribution":"Hugging Face"},{"title":"Mastering LLM Techniques: Text Data Processing","url":"https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com","attribution":"NVIDIA Developer"},{"title":"Part 3: Generative AI Training pre-publication version","url":"https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf?utm_source=chatgpt.com","attribution":"U.S. Copyright Office"},{"title":"LP Data Pipeline: Lightweight, Purpose-driven ...","url":"https://aclanthology.org/2025.emnlp-industry.11.pdf?utm_source=chatgpt.com","attribution":"ACL Anthology"},{"title":"OpenAI o1 System Card","url":"https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com","attribution":"OpenAI"},{"title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from ...","url":"https://arxiv.org/pdf/2309.00267?utm_source=chatgpt.com","attribution":"arXiv"},{"title":"OpenAI o1 System Card","url":"https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com","attribution":"OpenAI CDN"},{"title":"Demystifying Reasoning Models - by Cameron R. Wolfe, Ph.D.","url":"https://cameronrwolfe.substack.com/p/demystifying-reasoning-models?utm_source=chatgpt.com","attribution":"cameronrwolfe.substack.com"},{"title":"The State of Reinforcement Learning for LLM Reasoning","url":"https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com","attribution":"Sebastian Raschka's Ahead of AI"},{"title":"Selecting and Preparing Training Data for LLMs (2024\u20132025)","url":"https://www.rohan-paul.com/p/selecting-and-preparing-training?utm_source=chatgpt.com","attribution":"Rohan's Bytes"},{"title":"RLHF 101: A Technical Tutorial on Reinforcement ...","url":"https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/?utm_source=chatgpt.com","attribution":"ML@CMU"}],"has_images":false}],"safe_urls":["https://aclanthology.org/2024.acl-long.643.pdf","https://aclanthology.org/2024.acl-long.643.pdf?utm_source=chatgpt.com","https://aclanthology.org/2025.emnlp-industry.11.pdf","https://aclanthology.org/2025.emnlp-industry.11.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2305.18290","https://arxiv.org/abs/2305.18290?utm_source=chatgpt.com","https://arxiv.org/html/2412.16720v1","https://arxiv.org/html/2412.16720v1?utm_source=chatgpt.com","https://arxiv.org/html/2502.10867v1","https://arxiv.org/html/2502.10867v1?utm_source=chatgpt.com","https://arxiv.org/html/2506.08266v1","https://arxiv.org/html/2506.08266v1?utm_source=chatgpt.com","https://arxiv.org/pdf/2309.00267","https://arxiv.org/pdf/2309.00267?utm_source=chatgpt.com","https://arxiv.org/pdf/2403.13233","https://arxiv.org/pdf/2403.13233?utm_source=chatgpt.com","https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/","https://blog.ml.cmu.edu/2025/06/01/rlhf-101-a-technical-tutorial-on-reinforcement-learning-from-human-feedback/?utm_source=chatgpt.com","https://cameronrwolfe.substack.com/p/demystifying-reasoning-models","https://cameronrwolfe.substack.com/p/demystifying-reasoning-models?utm_source=chatgpt.com","https://cdn.openai.com/o1-system-card-20241205.pdf","https://cdn.openai.com/o1-system-card-20241205.pdf?utm_source=chatgpt.com","https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/","https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/?utm_source=chatgpt.com","https://encord.com/blog/openai-o1/","https://encord.com/blog/openai-o1/?utm_source=chatgpt.com","https://github.com/mengdi-li/awesome-RLAIF","https://github.com/mengdi-li/awesome-RLAIF?utm_source=chatgpt.com","https://huggingface.co/blog/rlhf","https://huggingface.co/blog/rlhf?utm_source=chatgpt.com","https://huggingface.co/datasets/allenai/DataDecide-data-recipes","https://huggingface.co/datasets/allenai/DataDecide-data-recipes?utm_source=chatgpt.com","https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/","https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/?utm_source=chatgpt.com","https://images.openai.com/static-rsc-1/KYv7VsHds2TLNUz9C2lVrek_c51Zuw8p1r45HpHfb24Dh2bIttflkBHZxiEA4kFbvnwnc_ugwO2H3lNkysYzY011NNF37lrQ29cP4Pu3qBakao8jw1AW0U9EDO6CcDCKFwZS-_JXwpC9kGkohsehABs7GGVpDPOowqwysh6YsxI","https://kili-technology.com/blog/exploring-reinforcement-learning-from-human-feedback-rlhf-a-comprehensive-guide","https://kili-technology.com/blog/exploring-reinforcement-learning-from-human-feedback-rlhf-a-comprehensive-guide?utm_source=chatgpt.com","https://lordslibrary.parliament.uk/large-language-models-and-generative-ai-house-of-lords-communications-and-digital-committee-report/","https://lordslibrary.parliament.uk/large-language-models-and-generative-ai-house-of-lords-communications-and-digital-committee-report/?utm_source=chatgpt.com","https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training","https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com","https://openai.com/index/introducing-openai-o1-preview/","https://openai.com/index/introducing-openai-o1-preview/?utm_source=chatgpt.com","https://openai.com/index/openai-o1-system-card/","https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com","https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay","https://pitchfork.com/news/warner-music-group-signs-licensing-deal-with-ai-music-company-klay?utm_source=chatgpt.com","https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf","https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf?utm_source=chatgpt.com","https://www.datacamp.com/blog/rlaif-reinforcement-learning-from-ai-feedback","https://www.datacamp.com/blog/rlaif-reinforcement-learning-from-ai-feedback?utm_source=chatgpt.com","https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection","https://www.dataprotection.ie/en/dpc-guidance/blogs/AI-LLMs-and-Data-Protection?utm_source=chatgpt.com","https://www.datologyai.com/blog/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation","https://www.datologyai.com/blog/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation?utm_source=chatgpt.com","https://www.deeplearning.ai/the-batch/openais-o1-models-excel-in-reasoning-outperform-gpt-4o-in-math-and-coding/","https://www.deeplearning.ai/the-batch/openais-o1-models-excel-in-reasoning-outperform-gpt-4o-in-math-and-coding/?utm_source=chatgpt.com","https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf","https://www.ft.com/content/96b59d8c-3625-4c2c-a6d6-435cff0392bf?utm_source=chatgpt.com","https://www.rohan-paul.com/p/selecting-and-preparing-training","https://www.rohan-paul.com/p/selecting-and-preparing-training?utm_source=chatgpt.com","https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning","https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning?utm_source=chatgpt.com","https://www.tylerromero.com/posts/2024-04-dpo/","https://www.tylerromero.com/posts/2024-04-dpo/?utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"rlhfbook.com","entries":[{"type":"search_result","url":"https://rlhfbook.com/book.pdf?utm_source=chatgpt.com","title":"Reinforcement Learning from Human Feedback","snippet":"by N Lambert \u00b7 2024 \u2014 A short introduction to RLHF and post-training focused on language models. Nathan Lambert. 2 November 2025. Abstract. Reinforcement learning from human feedback ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"rlhfbook.com"}]},{"type":"search_result_group","domain":"ijsrcseit.com","entries":[{"type":"search_result","url":"https://ijsrcseit.com/index.php/home/article/view/CSEIT24104135?utm_source=chatgpt.com","title":"Reinforcement Learning from AI Feedback A Review","snippet":"28 Apr 2024 \u2014 Abstract. Reinforcement Learning from AI Feedback (RLAIF) is a big step forward compared to Reinforcement Learning from Human Feedback (RLHF).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"ijsrcseit.com"}]},{"type":"search_result_group","domain":"cla.co.uk","entries":[{"type":"search_result","url":"https://cla.co.uk/cla-welcomes-house-of-lords-report-on-large-language-models/?utm_source=chatgpt.com","title":"The House of Lords Report on LLMs","snippet":"7 Feb 2024 \u2014 On Friday 2 February 2024, the House of Lords Communications and Digital Committee released its report around large language models (LLMs) ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"cla.co.uk"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/383620124_Reinforcement_Learning_from_AI_Feedback_A_Review?utm_source=chatgpt.com","title":"(PDF) Reinforcement Learning from AI Feedback A Review","snippet":"10 Aug 2025 \u2014 Abstract. Reinforcement Learning from AI Feedback (RLAIF) is a big step forward compared to Reinforcement Learning from Human Feedback (RLHF).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/381291991_Data-Juicer_A_One-Stop_Data_Processing_System_for_Large_Language_Models?utm_source=chatgpt.com","title":"Data-Juicer: A One-Stop Data Processing System for Large ...","snippet":"It has 50 built-in operators for tasks like formatting, mapping, filtering, and deduplication. ... ... data recipes for training large language models [2] . Data ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/391575046_Ultra-FineWeb_Efficient_Data_Filtering_and_Verification_for_High-Quality_LLM_Training_Data?utm_source=chatgpt.com","title":"(PDF) Ultra-FineWeb: Efficient Data Filtering and ...","snippet":"11 May 2025 \u2014 Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"copyright.com.au","entries":[{"type":"search_result","url":"https://www.copyright.com.au/2024/05/uk-government-responds-to-house-of-lords-report-on-gen-ai/?utm_source=chatgpt.com","title":"UK Government responds to House of Lords report on Gen AI","snippet":"9 May 2024 \u2014 The UK Government responded to the February 2024 report from the House of Lords Communications and Digital Committee on large language models and generative AI.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"copyright.com.au"}]},{"type":"search_result_group","domain":"substack.com","entries":[{"type":"search_result","url":"https://cameronrwolfe.substack.com/p/direct-preference-optimization?utm_source=chatgpt.com","title":"Direct Preference Optimization (DPO) - Deep (Learning) Focus","snippet":"In this overview, we will learn about a simpler approach to LLM alignment, called Direct Preference Optimization (DPO), that avoids these ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"substack.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2412.16720v1?utm_source=chatgpt.com","title":"OpenAI o1 System Card","snippet":"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2305.18290?utm_source=chatgpt.com","title":"Direct Preference Optimization: Your Language Model is ...","snippet":"by R Rafailov \u00b7 2023 \u00b7 Cited by 6341 \u2014 Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2203.02155?utm_source=chatgpt.com","title":"Training language models to follow instructions with ...","snippet":"by L Ouyang \u00b7 2022 \u00b7 Cited by 19992 \u2014 ... model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2403.13233?utm_source=chatgpt.com","title":"arXiv:2403.13233v1 [cs.CL] 20 Mar 2024","snippet":"by S Zhao \u00b7 2024 \u00b7 Cited by 1 \u2014 It is capable of efficiently gen- erating a variety of data recipes, exploring nu- merous combinations for creating data mixtures, and assessing ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2510.09471v1?utm_source=chatgpt.com","title":"Full-Text Search for LLM Training Data for Real World","snippet":"10 Oct 2025 \u2014 In this paper, we present the full-text indexing pipeline for the Apertus LLM training data. Leveraging Elasticsearch parallel indices and the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.01700v3?utm_source=chatgpt.com","title":"Data Management For Training Large Language Models","snippet":"2 Aug 2024 \u2014 (2023) , namely quality filtering, deduplication and toxicity filtering ... data recipes in either the pretraining or SFT stage of LLM Chen et al.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2312.01700v2?utm_source=chatgpt.com","title":"Data Management For Large Language Models: A Survey","snippet":"(2023a) provides a data processing system Data-Juicer featuring the generation of diverse data recipes. They provide over 50 versatile data management operators ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/data-science/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093?utm_source=chatgpt.com","title":"RLAIF: Reinforcement Learning from AI Feedback","snippet":"RLAIF: Reinforcement Learning from AI Feedback. Making alignment via RLHF more scalable by automating human feedback\u2026","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841?utm_source=chatgpt.com","title":"Understanding Direct Preference Optimization","snippet":"A look at the \u201cDirect Preference Optimization: Your Language Model is Secretly a Reward Model\u201d paper and its findings","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40preeti.rana.ai/reinforcement-learning-from-human-feedback-in-large-language-models-62c89e8c2d3e?utm_source=chatgpt.com","title":"Reinforcement Learning from Human Feedback in Large ...","snippet":"Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique in aligning large language models (LLMs) with human preferences and values.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://shawhin.medium.com/how-to-train-llms-to-think-like-o1-deepseek-r1-eabc21c8842d?utm_source=chatgpt.com","title":"How to Train LLMs to \u201cThink\u201d (o1 & DeepSeek-R1)","snippet":"Here, I discuss the technical details behind the recent \u201cadvanced reasoning\u201d models trained on large-scale reinforcement learning i.e. o1 ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40adnanmasood/inside-the-great-ai-data-grab-comprehensive-analysis-of-public-and-proprietary-corpora-utilised-49b4770abc47?utm_source=chatgpt.com","title":"Inside the Great AI Data Grab \u2014 Comprehensive Analysis ...","snippet":"The raw data that goes into LLM training requires extensive cleaning and filtering to be usable. Without filtering, a model might learn from ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40linghuang_76674/data-management-for-large-language-models-a-survey-cda535050c51?utm_source=chatgpt.com","title":"Large Language Models \u2014 Data Management","snippet":"Although there are already data management systems such as Data-Juicer and Oasis that can help combine various data recipes in the LLM pre- ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40adnanmasood/open-source-licensing-modalities-in-large-language-models-insights-risks-and-opportunities-for-283416b2a40d?utm_source=chatgpt.com","title":"Open Source Licensing Modalities in Large Language ...","snippet":"Open source refers to programs distributed under licenses that guarantee users the freedom to use, study, modify, and share the source code for any purpose.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"tylerromero.com","entries":[{"type":"search_result","url":"https://www.tylerromero.com/posts/2024-04-dpo/?utm_source=chatgpt.com","title":"Direct Preference Optimization Explained In-depth","snippet":"Direct Preference Optimization Explained In-depth. Simpler preference-tuning without reinforcement learning. April 13, 2024. With my first blog post, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"tylerromero.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=57OQXxbTbY&noteId=ellZ40SC5c&utm_source=chatgpt.com","title":"Cal-DPO: Calibrated Direct Preference Optimization for ...","snippet":"6 Nov 2024 \u2014 This paper introduces calibrated direct preference optimization (Cal-DPO), a preference-tuning algorithm that modifies DPO to address the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=bToTGD5dIk&utm_source=chatgpt.com","title":"arXiv:2312.01700v2 [cs.CL] 26 Dec 2023","snippet":", 2023a) and Oa- sis (Zhou et al., 2023c) propose data management systems to compose various data recipes in either the pretraining or SFT stage of LLM ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=jnRBe6zatP&utm_source=chatgpt.com","title":"FineWeb2: One Pipeline to Scale Them All \u2014 Adapting Pre ...","snippet":"by G Penedo \u00b7 Cited by 22 \u2014 In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb (Penedo et al., 2024) that can be automatically adapted to support any ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=rzkx5AGJ3N&utm_source=chatgpt.com","title":"Data Management For Training Large Language Models","snippet":", 2023), namely quality. 245 filtering, deduplication and toxicity filtering. Data. 246 ... to compose various data recipes in either the pre-. 617 training or ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"parliament.uk","entries":[{"type":"search_result","url":"https://lordslibrary.parliament.uk/large-language-models-and-generative-ai-house-of-lords-communications-and-digital-committee-report/?utm_source=chatgpt.com","title":"Large language models and generative AI: House of Lords ...","snippet":"15 Nov 2024 \u2014 In February 2024 the House of Lords Communications and Digital Committee published a report on large language models and generative AI.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"parliament.uk"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/mengdi-li/awesome-RLAIF?utm_source=chatgpt.com","title":"mengdi-li/awesome-RLAIF","snippet":"A common approach is reinforcement learning from AI feedback (RLAIF), which trains a reward model based on AI preference data and employs a reinforcement ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/allenai/DataDecide?utm_source=chatgpt.com","title":"allenai/DataDecide","snippet":"More than one training run goes into making a large language model ... Data. Released at https://huggingface.co/datasets/allenai/DataDecide-data-recipes ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/benjaminzwhite/reasoning-models?utm_source=chatgpt.com","title":"benjaminzwhite/reasoning-models: Experiments with ...","snippet":"Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought \u00b7 Search-o1: Agentic Search-Enhanced Large Reasoning Models \u00b7 CoT-based ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/introducing-openai-o1-preview/?utm_source=chatgpt.com","title":"Introducing OpenAI o1-preview","snippet":"12 Sept 2024 \u2014 A new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://platform.openai.com/docs/guides/reasoning?utm_source=chatgpt.com","title":"Reasoning models - OpenAI API","snippet":"Explore the capabilities of OpenAI's o1 series for complex reasoning and problem-solving. Learn about their features and how they compare to GPT-4o models.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"amazon.com","entries":[{"type":"search_result","url":"https://aws.amazon.com/blogs/machine-learning/fine-tune-large-language-models-with-reinforcement-learning-from-human-or-ai-feedback/?utm_source=chatgpt.com","title":"Fine-tune large language models with reinforcement ...","snippet":"4 Apr 2025 \u2014 The following diagram illustrates reinforcement learning from human feedback (RLHF) compared to reinforcement learning from AI feedback (RLAIF).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"amazon.com"}]},{"type":"search_result_group","domain":"datologyai.com","entries":[{"type":"search_result","url":"https://www.datologyai.com/blog/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation?utm_source=chatgpt.com","title":"Train LLMs Faster, Better, and Smaller with DatologyAI's ...","snippet":"24 Nov 2024 \u2014 DatologyAI's curated data delivers substantial improvements in LLM quality, training speed, and inference efficiency over existing datasets.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"datologyai.com"}]},{"type":"search_result_group","domain":"datacamp.com","entries":[{"type":"search_result","url":"https://www.datacamp.com/blog/rlaif-reinforcement-learning-from-ai-feedback?utm_source=chatgpt.com","title":"RLAIF: What is Reinforcement Learning From AI Feedback?","snippet":"28 May 2024 \u2014 Reinforcement learning from AI feedback (RLAIF) is a machine learning technique in which AI models provide feedback to other AI models during ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"datacamp.com"}]},{"type":"search_result_group","domain":"dida.do","entries":[{"type":"search_result","url":"https://dida.do/blog/post-fine-tuning-llm-with-direct-preference-optimization?utm_source=chatgpt.com","title":"Post Fine Tuning LLM with Direct Preference Optimization","snippet":"Explore how to optimize Language Models (LLMs) through Direct Preference Optimization and learn how fine-tuning techniques can enhance LLM ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"dida.do"}]},{"type":"search_result_group","domain":"deeplearning.ai","entries":[{"type":"search_result","url":"https://www.deeplearning.ai/the-batch/openais-o1-models-excel-in-reasoning-outperform-gpt-4o-in-math-and-coding/?utm_source=chatgpt.com","title":"OpenAI's o1 Models Excel in Reasoning, Outperform GPT- ...","snippet":"18 Sept 2024 \u2014 The o1 models show that the combination of reinforcement learning and chain-of-thought reasoning can solve problems that large language models generally find ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"deeplearning.ai"}]},{"type":"search_result_group","domain":"ando.ai","entries":[{"type":"search_result","url":"https://blog.ando.ai/posts/o1-and-reasoning/?utm_source=chatgpt.com","title":"o1 and Reasoning | AndoLogs","snippet":"20 Nov 2024 \u2014 We'll later cover some examples of o1's chain-of-thought reasoning ... Marco-o1: Towards open reasoning models for open-ended solutions.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"ando.ai"}]},{"type":"search_result_group","domain":"kili-technology.com","entries":[{"type":"search_result","url":"https://kili-technology.com/blog/exploring-reinforcement-learning-from-human-feedback-rlhf-a-comprehensive-guide?utm_source=chatgpt.com","title":"Exploring Reinforcement Learning with Human Feedback","snippet":"Reinforcement learning from human feedback is a fine-tuning process that enables large language models to follow human instructions. Here's how it works.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"kili-technology.com"}]},{"type":"search_result_group","domain":"oars-workshop.github.io","entries":[{"type":"search_result","url":"https://oars-workshop.github.io/papers/He2025.pdf?utm_source=chatgpt.com","title":"RecLAIF: Reinforcement Learning from AI Feedback for ...","snippet":"by X He \u00b7 2025 \u2014 We introduced RecLAIF, a dual-LLM framework that integrates. Reinforcement Learning from AI Feedback (RLAIF) to enhance recommendation systems in terms of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"oars-workshop.github.io"}]},{"type":"search_result_group","domain":"oup.com","entries":[{"type":"search_result","url":"https://academic.oup.com/jiplp/article/20/3/182/7922541?utm_source=chatgpt.com","title":"Copyright and AI training data\u2014transparency to the rescue?","snippet":"by A Buick \u00b7 2025 \u00b7 Cited by 27 \u2014 AI developers to be required by law to be transparent regarding the contents of their training datasets, with the aim of enabling rightsholders to enforce ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"oup.com"}]},{"type":"search_result_group","domain":"iclr-blogposts.github.io","entries":[{"type":"search_result","url":"https://iclr-blogposts.github.io/2024/blog/rlhf-without-rl/?utm_source=chatgpt.com","title":"RLHF without RL - Direct Preference Optimization","snippet":"7 May 2024 \u2014 We discuss the RL part of RLHF and its recent displacement by direct preference optimization (DPO). With DPO, a language model can be aligned ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"iclr-blogposts.github.io"}]},{"type":"search_result_group","domain":"encord.com","entries":[{"type":"search_result","url":"https://encord.com/blog/openai-o1/?utm_source=chatgpt.com","title":"OpenAI o1: A New Era of AI Reasoning","snippet":"13 Sept 2024 \u2014 OpenAI o1 series is a new class of large language models (LLMs) designed to enhance reasoning capabilities through a thinking before responding approach.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"encord.com"}]},{"type":"search_result_group","domain":"saa-authors.eu","entries":[{"type":"search_result","url":"https://www.saa-authors.eu/articles/ai-and-copyright-is-a-licensing-market-just-a-question-of-time?utm_source=chatgpt.com","title":"AI and copyright: is a licensing market just a question of time?","snippet":"11 Feb 2025 \u2014 The authors have entered 2025 with a further confirmation that protected works have been and will continue to be used without their authorisation.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"saa-authors.eu"}]},{"type":"search_result_group","domain":"wikipedia.org","entries":[{"type":"search_result","url":"https://en.wikipedia.org/wiki/OpenAI_o1?utm_source=chatgpt.com","title":"OpenAI o1","snippet":"OpenAI o1 is a generative pre-trained transformer (GPT), the first in OpenAI's \"o\" series of reasoning models. A preview of o1 was released by OpenAI on ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"wikipedia.org"},{"type":"search_result","url":"https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback?utm_source=chatgpt.com","title":"Reinforcement learning from human feedback","snippet":"In machine learning, reinforcement learning from human feedback (RLHF) is a technique to align an intelligent agent with human preferences.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"wikipedia.org"}]},{"type":"search_result_group","domain":"interconnects.ai","entries":[{"type":"search_result","url":"https://www.interconnects.ai/p/deepseek-r1-recipe-for-o1?action=share&utm_content=share&utm_medium=email&utm_source=chatgpt.com","title":"DeepSeek R1's recipe to replicate o1 and the future of ...","snippet":"21 Jan 2025 \u2014 Until now, reasoning models have been a major area of ... chain of thought style. In order for RL to really start cranking up ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"interconnects.ai"}]},{"type":"search_result_group","domain":"intuitionlabs.ai","entries":[{"type":"search_result","url":"https://intuitionlabs.ai/articles/rlaif-healthcare-annotation-costs?utm_source=chatgpt.com","title":"RLAIF in Healthcare: How AI Feedback Reduces Annotation Costs","snippet":"Learn how Reinforcement Learning from AI Feedback (RLAIF) reduces medical AI annotation costs. This guide covers the RLAIF method, its benefits over RLHF, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"intuitionlabs.ai"}]},{"type":"search_result_group","domain":"cmu.edu","entries":[{"type":"search_result","url":"https://www.cs.cmu.edu/~mgormley/courses/10423-s25//slides/lecture26-reasoning-ink.pdf?utm_source=chatgpt.com","title":"Reasoning Models + Mechanistic Interpretability - CMU School of ...","snippet":"Reasoning Models. + ... \u2022 Chain-of-thought prompting provides such reasoning in the in-context examples ... \u2022 The o1 model was trained with. Reinforcement ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"cmu.edu"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/19e4ea30dded58259665db375885e412-Paper-Datasets_and_Benchmarks_Track.pdf?utm_source=chatgpt.com","title":"DataComp-LM: In search of the next generation of training ...","snippet":"by J Li \u00b7 2024 \u00b7 Cited by 177 \u2014 RefinedWeb applies the following filtering pipeline: Common Crawl text extraction, heuristic selection rules (e.g., to remove spam), and deduplication of ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3737916.3742044?utm_source=chatgpt.com","title":"\u03b2-DPO: direct preference optimization with dynamic \u03b2","snippet":"by J Wu \u00b7 2024 \u00b7 Cited by 85 \u2014 Direct Preference Optimization (DPO) has emerged as a compelling approach for training Large Language Models (LLMs) to adhere to human ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"acm.org"}]},{"type":"search_result_group","domain":"sparcopen.org","entries":[{"type":"search_result","url":"https://sparcopen.org/news/2024/all-tdm-ai-rights-reserved/?utm_source=chatgpt.com","title":"All TDM & AI Rights Reserved? Fair Use & Evolving Publisher ...","snippet":"28 Mar 2024 \u2014 Some academic publishers have revised the copyright notices on their websites to state they reserve rights to text and data mining (TDM) and AI training.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"sparcopen.org"}]},{"type":"search_result_group","domain":"georgetown.edu","entries":[{"type":"search_result","url":"https://cset.georgetown.edu/article/openais-new-reasoning-model-and-the-next-stage-in-ai-development/?utm_source=chatgpt.com","title":"OpenAI's New Reasoning Model and the Next Stage in AI ...","snippet":"19 Sept 2024 \u2014 OpenAI used large-scale reinforcement learning to train o1 to perform \u201cchain of thought\u201d reasoning, enabling it to deliberate its way through complex problems.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"georgetown.edu"}]},{"type":"search_result_group","domain":"wmdqs.org","entries":[{"type":"search_result","url":"https://wmdqs.org/submissions-2025/11.pdf?utm_source=chatgpt.com","title":"Can Small Training Runs Reliably Guide Data Curation? ...","snippet":"Figure 2: Validation loss rankings of 23 data recipes evaluated on proxy models (GPT2-125M) and target models. (Pythia-1B), where target models undergo ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"wmdqs.org"}]},{"type":"search_result_group","domain":"warwick.ac.uk","entries":[{"type":"search_result","url":"https://warwick.ac.uk/fac/cross_fac/eduport/edufund/projects/yang/projects/understanding-the-effects-of-rlhf-on-llm-generalisation-and-diversity/?utm_source=chatgpt.com","title":"understanding-the-effects-of-rlhf-on-llm-generalisation-and- ...","snippet":"The implementations focus on training large language models using reinforcement learning from human feedback and supervised fine-tuning approaches.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"warwick.ac.uk"}]},{"type":"search_result_group","domain":"sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2666389925002181?utm_source=chatgpt.com","title":"Toward large reasoning models: A survey of reinforced ...","snippet":"by F Xu \u00b7 2025 \u00b7 Cited by 81 \u2014 ... o1, providing valuable insights for developing large reasoning models. ... o1 model with reinforcement learning techniques. The core of ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"sciencedirect.com"}]},{"type":"search_result_group","domain":"imf.org","entries":[{"type":"search_result","url":"https://www.elibrary.imf.org/view/journals/001/2024/114/article-A001-en.xml?utm_source=chatgpt.com","title":"Reinforcement Learning from Experience Feedback","snippet":"7 Jun 2024 \u2014 Reinforcement Learning from AI Feedback (RLAIF). This method (Lee et al., 2023) is a more recent technique based on the approach proposed in ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"imf.org"}]},{"type":"search_result_group","domain":"rohan-paul.com","entries":[{"type":"search_result","url":"https://www.rohan-paul.com/p/curating-public-datasets-for-llm?utm_source=chatgpt.com","title":"Curating Public Datasets for LLM Pretraining","snippet":"20 Apr 2025 \u2014 To solidify these concepts, let's look at a few prominent 2024-era LLM projects and how they curated their training data: DeepSeek LLM (2024) ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"rohan-paul.com"}]},{"type":"search_result_group","domain":"reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/legal/litigation/how-copyright-law-could-threaten-ai-industry-2024-2024-01-02/?utm_source=chatgpt.com","title":"How copyright law could threaten the AI industry in 2024","snippet":"2 Jan 2024 \u2014 If 2023 was the year that artificial intelligence changed everything, 2024 could go down as the year that U.S. copyright law changes AI.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"reuters.com"}]},{"type":"search_result_group","domain":"towardsai.net","entries":[{"type":"search_result","url":"https://pub.towardsai.net/have-o1-models-solved-human-reasoning-f22c8b879fb4?utm_source=chatgpt.com","title":"Have o1 Models Solved Human Reasoning? | by Nehdiii","snippet":"19 Apr 2025 \u2014 According to OpenAI's blog post, these models are trained using reinforcement learning (RL) with chain of thought (CoT) to reason through ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"towardsai.net"}]}],"citations":[],"request_id":"a067ab91-e404-497a-9cab-8b05fcef0188","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"9e26c8fe-5301-4f52-a81b-2edfd8b3d0c1","turn_exchange_id":"0ebe0b63-4f75-4ace-a9fd-4d31ec8fad86","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"9e26c8fe-5301-4f52-a81b-2edfd8b3d0c1","children":[]}}}