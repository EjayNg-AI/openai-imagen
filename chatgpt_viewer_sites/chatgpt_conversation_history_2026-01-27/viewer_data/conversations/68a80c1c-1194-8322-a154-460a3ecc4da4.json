{"id":"68a80c1c-1194-8322-a154-460a3ecc4da4","title":"Intelligence and information organization","create_time":1755843696.956481,"update_time":1755879083.541442,"current_node":"512e83e8-bca4-4e04-9748-9cb77bdc7433","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["5a4dd573-b461-4b7f-9c62-c15f4051b755"]},"5a4dd573-b461-4b7f-9c62-c15f4051b755":{"id":"5a4dd573-b461-4b7f-9c62-c15f4051b755","message":{"id":"5a4dd573-b461-4b7f-9c62-c15f4051b755","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["f387271a-ad80-42c1-979a-84ca539c1fdd"]},"f387271a-ad80-42c1-979a-84ca539c1fdd":{"id":"f387271a-ad80-42c1-979a-84ca539c1fdd","message":{"id":"f387271a-ad80-42c1-979a-84ca539c1fdd","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"5a4dd573-b461-4b7f-9c62-c15f4051b755","children":["d5d79e2d-dd50-4bea-8c71-00fd9f124c35"]},"d5d79e2d-dd50-4bea-8c71-00fd9f124c35":{"id":"d5d79e2d-dd50-4bea-8c71-00fd9f124c35","message":{"id":"d5d79e2d-dd50-4bea-8c71-00fd9f124c35","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"f387271a-ad80-42c1-979a-84ca539c1fdd","children":["856a8378-5f5c-49e8-9f9f-15503071c0e3"]},"856a8378-5f5c-49e8-9f9f-15503071c0e3":{"id":"856a8378-5f5c-49e8-9f9f-15503071c0e3","message":{"id":"856a8378-5f5c-49e8-9f9f-15503071c0e3","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"d5d79e2d-dd50-4bea-8c71-00fd9f124c35","children":["5caab59c-9286-47bd-b8d5-f810636e09fb"]},"5caab59c-9286-47bd-b8d5-f810636e09fb":{"id":"5caab59c-9286-47bd-b8d5-f810636e09fb","message":{"id":"5caab59c-9286-47bd-b8d5-f810636e09fb","author":{"role":"user","name":null,"metadata":{}},"create_time":1755843696.722,"update_time":null,"content":{"content_type":"text","parts":["Examine the hypothesis that intelligence is simply the ability to organize information. The word organization here refers to generalization, specialization, interpolation, extrapolation. elucidation, abstraction, concretization, recursion, simplification and quantization."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"9730455e6c4fe10a-IAD","message_source":null,"timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"856a8378-5f5c-49e8-9f9f-15503071c0e3","children":["909cc5be-fbaa-4861-a686-0c970087fea4"]},"909cc5be-fbaa-4861-a686-0c970087fea4":{"id":"909cc5be-fbaa-4861-a686-0c970087fea4","message":{"id":"909cc5be-fbaa-4861-a686-0c970087fea4","author":{"role":"system","name":null,"metadata":{}},"create_time":1755843696.7207863,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"9730455e6c4fe10a-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"5caab59c-9286-47bd-b8d5-f810636e09fb","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"5caab59c-9286-47bd-b8d5-f810636e09fb","children":["b7c9fc4d-4115-4356-adae-6512dbc8109d"]},"b7c9fc4d-4115-4356-adae-6512dbc8109d":{"id":"b7c9fc4d-4115-4356-adae-6512dbc8109d","message":{"id":"b7c9fc4d-4115-4356-adae-6512dbc8109d","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755843697.5237212,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"9730455e6c4fe10a-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"909cc5be-fbaa-4861-a686-0c970087fea4","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"909cc5be-fbaa-4861-a686-0c970087fea4","children":["62c9a417-8068-4b3e-8760-e9c54ba4df5f"]},"62c9a417-8068-4b3e-8760-e9c54ba4df5f":{"id":"62c9a417-8068-4b3e-8760-e9c54ba4df5f","message":{"id":"62c9a417-8068-4b3e-8760-e9c54ba4df5f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755843698.11301,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Analyzing intelligence as information organization","content":"The user is asking to examine the hypothesis that intelligence is the ability to organize information through various processes like generalization and abstraction. This calls for an analytical, objective critique, using concepts from cognitive science, information theory, and potentially algorithmic information theory. Given their academic background, a structured, professional tone focused on precision is preferred. I\u2019ll avoid updating facts and incorporate relevant theoretical frameworks instead.","chunks":[],"finished":true},{"summary":"Analyzing intelligence in terms of information organization","content":"I need to dissect the hypothesis of intelligence as organizing information, focusing on frameworks like category theory, information theory, and statistical learning theory. I'll map key terms (e.g., generalization, abstraction) to operations on representations. I'll explore axes like structure discovery, prediction, and action selection, examining both sufficiency/necessity and counterexamples, while addressing normative versus mechanistic views of intelligence.","chunks":[],"finished":true},{"summary":"Formalizing organization in information theory","content":"I could formalize information organization with category theory, using endofunctors to perform operations like recursion, abstraction, and quantization. I\u2019ll also critique the existing list, suggesting the inclusion of exploration, curiosity, counterfactual thinking, and others. I'll propose a \"Organization capacity\" measure based on transforming data into compressed, predictive models.","chunks":[],"finished":true},{"summary":"Refining organization with computational and information theory","content":"I could explore using concepts like Kolmogorov's structure function, predictive rate-distortion, and statistical complexity from computational mechanics. I\u2019ll discuss how generalization relates to group actions and explore specialized operations. I'll also caution on controversial ideas like integrated information theory and propose thought experiments involving datasets and tasks related to information-based principles.","chunks":[],"finished":true}],"source_analysis_msg_id":"8335a6e7-0b22-44cd-8b1e-2bdcc13f1665"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"9730455e6c4fe10a-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"b7c9fc4d-4115-4356-adae-6512dbc8109d","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"b7c9fc4d-4115-4356-adae-6512dbc8109d","children":["cfdce91b-648c-4edb-8490-57d5497bde9b"]},"cfdce91b-648c-4edb-8490-57d5497bde9b":{"id":"cfdce91b-648c-4edb-8490-57d5497bde9b","message":{"id":"cfdce91b-648c-4edb-8490-57d5497bde9b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755843757.93306,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 59s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":59,"request_id":"9730455e6c4fe10a-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"62c9a417-8068-4b3e-8760-e9c54ba4df5f","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"62c9a417-8068-4b3e-8760-e9c54ba4df5f","children":["51100113-b92c-4137-8ed9-2df110f2462f"]},"51100113-b92c-4137-8ed9-2df110f2462f":{"id":"51100113-b92c-4137-8ed9-2df110f2462f","message":{"id":"51100113-b92c-4137-8ed9-2df110f2462f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755843696.720231,"update_time":null,"content":{"content_type":"text","parts":["# Is intelligence \u201cjust\u201d the organization of information?\n\nYou\u2019ve given \u201corganization\u201d real content\u2014generalization, specialization, interpolation, extrapolation, elucidation, abstraction, concretization, recursion, simplification, quantization. That is already a useful operational sketch. Below I take a hard look at the thesis in four steps: formalizations that make \u201corganization\u201d precise; a mapping from your ten verbs to those formalisms; arguments for necessity vs. sufficiency; and falsifiable predictions/tests. I\u2019ll close with implications for AI design.\n\n---\n\n## 1) Making \u201corganization\u201d precise\n\nThere are at least four rigorous lenses under which your verbs become measurable operations rather than metaphors.\n\n### 1.1 Information-theoretic (compression \u21d2 prediction \u21d2 control)\nLet \\(X\\) be data, \\(Y\\) the targets/queries, \\(Z=f(X)\\) a representation. Organization is the choice of \\(f\\) that (i) compresses \\(X\\) and (ii) preserves what matters for \\(Y\\).\n\nA clean formalization is the (sequential) Information Bottleneck:\n\\[\n\\min_{p(z|x)} \\ I(X;Z)\\quad \\text{s.t.}\\quad I(Z;Y)\\ge \\kappa,\n\\]\nor its Lagrangian \\( \\min I(X;Z) - \\beta I(Z;Y)\\).\n\u201cGood organization\u201d yields small \\(I(X;Z)\\) (simplicity) and large \\(I(Z;Y)\\) (task relevance). Generalization ability is then tied to \\(Z\\) capturing invariants and discarding nuisance variation. For action, add a decision layer with a rational-inattention or rate\u2013distortion constraint:\n\\[\n\\min_{a(\\cdot)} \\mathbb{E}[\\ell(A,S)] \\quad \\text{s.t.}\\quad I(Z;X)\\le C.\n\\]\nThis explicitly models quantization and simplification as bounded information budgets.\n\n### 1.2 Algorithmic/MDL (structure function)\nLet \\(K(\\cdot)\\) be Kolmogorov complexity. Organization is finding a short program that (a) reproduces data regularities and (b) predicts held-out data with low regret. MDL formalizes elucidation as selecting hypotheses that minimize total description length of model + residuals. This directly operationalizes \u201csimplification\u201d without conflating it with underfitting.\n\n### 1.3 Statistical learning (bias, invariance, transfer)\nIn PAC-Bayes terms, organization picks a posterior \\(Q\\) over hypotheses balancing empirical fit and KL divergence to a prior \\(P\\) that encodes useful inductive biases (symmetries, sparsity, hierarchy). Interpolation/extrapolation reduce to different smoothness/causality assumptions; the former is within the support of \\(P\\), the latter requires the right invariances or causal structure.\n\n### 1.4 Category-/order-theoretic (adjunctions and coarse-grainings)\nLet \\(\\mathcal{S}\\) be a poset/lattice of properties or concepts. \u201cAbstraction\u201d and \u201cconcretization\u201d are a Galois connection \\( \\alpha \\dashv \\gamma : \\mathcal{C}\\rightleftarrows \\mathcal{A}\\) (abstract interpretation): \\(\\alpha\\) forgets detail while preserving correctness; \\(\\gamma\\) refines. Many of your verbs become endofunctors on a category of representations: recursion as fixed points of endofunctors (monads/comonads), generalization as closure operators (extensive, monotone, idempotent), specialization as interior operators, quantization as reflective subcategory (left adjoint to inclusion) or as a quotient/coarse-graining functor.\n\n---\n\n## 2) Mapping your ten verbs to the formalisms\n\n- **Generalization.** Identify invariants: in \\(\\mathrm{IB}\\), compress away nuisance; in learning theory, impose group equivariances; categorically, apply a closure operator capturing properties stable under permitted morphisms.\n- **Specialization.** Conditioning on context/task: \\(p(z|x,c)\\) or posterior restriction in PAC-Bayes; categorically, an interior operator (restricting to a subobject/fiber).\n- **Interpolation.** Extension within the observed regime: kernel regression on manifolds, sheaf-theoretic gluing when the local sections are compatible; measured by in-distribution error.\n- **Extrapolation.** Out-of-support extension requiring the *right* inductive bias (sparsity, causality, scaling laws). Often needs a causal or mechanistic representation; correlational compression alone underperforms.\n- **Elucidation.** Explanatory factorization: MDL/IB selects latent variables that are both compressive and predictive. Causally, elucidation is discovering a DAG/SCM whose interventions improve downstream control\u2014this is the \u201cwhy,\u201d not just the \u201cwhat.\u201d\n- **Abstraction.** Left adjoint/quotient/coarse-graining; information-theoretically a many-to-one map \\(X\\to Z\\) that maximizes \\(I(Z;Y)\\) per bit kept.\n- **Concretization.** Right adjoint/refinement: instantiate an abstract element; algorithmically, amortized inference/decoder.\n- **Recursion.** Fixed-point structure in programs and grammars; hierarchical models (hyperpriors) and iterative improvement; mathematically, least/greatest fixed points of an endofunctor (induction/coinduction).\n- **Simplification.** MDL/Occam; in practice, pruning/distillation/low-rank factorizations that keep \\(I(Z;Y)\\) high.\n- **Quantization.** Rate\u2013distortion optimal discretization (vector quantization, \\(k\\)-means as a Lloyd\u2013Max quantizer for Euclidean distortion); categorically, passage to a discrete reflective subcategory.\n\n---\n\n## 3) Is organization **necessary**? Almost certainly.\n\nAll intelligent systems we know\u2014brains, scientific communities, competent ML models\u2014win by reorganizing data into *structured, compact, task-relevant* internal states. Sufficient statistics, schemas, causal graphs, grammars, world models, and skills libraries are all forms of organization. Without this, you can\u2019t generalize, transfer, or plan economically.\n\n---\n\n## 4) Is organization **sufficient**? Not by itself.\n\nHere are hard edges where \u201corganization of information\u201d is *not* enough unless augmented.\n\n1. **Agency and preferences.** Intelligence in the wild selects and achieves goals under constraints. Mere organization lacks a utility functional and an actuation loop. Add decision theory (utility + costs) to make it complete.\n\n2. **Exploration and curiosity.** Useful organization often requires *creating* information via interventions (experiments). That\u2019s active learning under information budgets, not passive tidying.\n\n3. **Causality and counterfactuals.** Extrapolation outside the training distribution usually needs causal structure. Pure compression of correlations may fail to support do-calculus or robust OOD control.\n\n4. **Temporal credit assignment and planning.** Long-horizon tasks need search, hierarchy, and model-based control. Well-organized static representations still need planners and options.\n\n5. **Grounding and embodiment.** Symbol manipulation can be impeccably organized yet misaligned with the world without sensorimotor grounding or simulators ensuring semantic contact.\n\n6. **Social reasoning and norms.** Theory of mind, deception detection, and norm adherence are not reducible to compression unless the \u201ctask-relevant info\u201d includes latent social utilities and game-theoretic dynamics.\n\n**Verdict.** Organization is **necessary and foundational** but **insufficient** for intelligence that can act, adapt, and align in open worlds. A minimal recipe is:\n\\[\n\\textbf{Intelligence} \\approx \\underbrace{\\text{Organize}}_{\\text{represent/compress}} \\;+\\; \\underbrace{\\text{Model}}_{\\text{predict/causal}} \\;+\\; \\underbrace{\\text{Decide}}_{\\text{utility under constraints}} \\;+\\; \\underbrace{\\text{Explore}}_{\\text{close gaps}}.\n\\]\n\n---\n\n## 5) Axioms (succinct)\n\nLet \\((\\mathcal{X},\\mathcal{Q})\\) be data and queries/tasks; let \\(\\mathcal{R}\\) be representations.\n\n1. **Relevance:** An organization operator \\(O:\\mathcal{X}\\to\\mathcal{R}\\) is good iff for all \\(q\\in\\mathcal{Q}\\) there exists a fast \\(A_q\\) with small loss \\(L(A_q(O(X)), q)\\).\n2. **Parsimony:** Among such \\(O\\), prefer those minimizing \\(I(X;O(X))\\) (or MDL).\n3. **Compositionality:** \\(O\\) is closed under composition and admits fixed points (recursion).\n4. **Adjointness:** There exist \\(A\\dashv C\\) modeling abstraction/concretization with soundness: answering in the abstract transports soundly to the concrete via \\(C\\).\n5. **Robustness:** Answers are stable under admissible perturbations (group actions/coarse-grainings).\n\nThese five axioms recover your ten verbs as special cases or consequences.\n\n---\n\n## 6) What each verb *adds* (and where it fails)\n\n- **Generalization vs. specialization:** Dual moves; both are required. Over-generalization collapses distinctions; over-specialization overfits. The bias\u2013variance trade-off is precisely their tension.\n- **Interpolation vs. extrapolation:** Interpolation is mostly a geometry/smoothness question; extrapolation is a *model class* question (symmetries, sparsity, causal mechanisms). If you claim \u201corganization suffices,\u201d you must specify which invariances your organizer discovers.\n- **Elucidation:** Explanations that *reduce* future description length and *improve* intervention outcomes are the bar; post-hoc rationalizations do not count.\n- **Abstraction/concretization:** Think Galois connections; insist on soundness/precision bounds. Without a right adjoint, \u201cabstraction\u201d may be irreversible in harmful ways.\n- **Recursion:** Enables hierarchical skill reuse and algorithmic generalization; without it, you\u2019re stuck with surface statistics.\n- **Simplification:** MDL-driven pruning/distillation; but beware: simplification that reduces \\(I(Z;Y)\\) below task needs is not \u201cintelligent minimality,\u201d it\u2019s lossy confusion.\n- **Quantization:** Essential for action under budgets, but aggressive quantization can kill elucidation and extrapolation. Use rate\u2013distortion or IB to tune it.\n\n---\n\n## 7) Empirical predictions and tests\n\nTo move this from philosophy to science, test the \u201corganization hypothesis\u201d by building metrics and ablations that isolate each verb.\n\n1. **Organization capacity metric.** Train a model to produce \\(Z\\) under an IB/MDL objective. Evaluate *transfer risk* across a suite of tasks \\( \\mathcal{Q}\\) sharing latent structure. Prediction: higher compression at fixed relevance correlates with better few-shot transfer and robustness.\n\n2. **Abstraction/adjoitness test.** Construct an abstract domain \\(\\mathcal{A}\\) and concretization \\(\\gamma\\). Require soundness: solving in \\(\\mathcal{A}\\) then refining performs at least as well as solving directly in \\(\\mathcal{X}\\) at a given compute budget.\n\n3. **Extrapolation via causality.** Hold interpolation constant across models; vary only whether causal structure is learned/used. Prediction: only models that explicitly organize into causal factors succeed OOD.\n\n4. **Recursion competency.** Evaluate on algorithmic tasks with length generalization (e.g., Dyck languages, addition with carry, multi-step planning). Organization without recursion should fail on length extrapolation.\n\n5. **Quantization knob.** Sweep the bit-budget \\(C\\) in a rational-inattention/IB setup and plot utility vs. \\(C\\). Prediction: there is a sweet spot maximizing utility-per-bit; both under- and over-quantization harm performance differently across tasks.\n\nIf these predictions break, the hypothesis needs refinement (e.g., adding exploration/utility explicitly).\n\n---\n\n## 8) Design implications for AI systems\n\n- **Architectures:** Encourage *representational adjunctions*: explicit abstraction (coarse-graining modules) with learnable concretization/decoding; hierarchical monadic composition for recursion.\n- **Objectives:** Combine IB/MDL with auxiliary *causal* discovery losses and *control* objectives (regret, energy). Organization is the representational backbone; utility aligns it with goals.\n- **Training curricula:** Interleave passive compression with *active* data collection to force elucidation; include OOD splits requiring invariance discovery rather than curve-fitting.\n- **Tooling:** Measure \\(I(X;Z)\\), MDL, and causal identifiability proxies during training; not just task loss.\n\n---\n\n## 9) Bottom line\n\nYour \u201corganization\u201d cluster captures the *representational heart* of intelligence. It is **necessary** and can be made mathematically crisp (IB/MDL, PAC-Bayes, adjunctions, fixed points). But taken alone it is **insufficient** for general intelligence: you must add an explicit account of utility/agency, exploration, and causality. A productive synthesis is:\n\n- Use **organization** to build compact, compositional, task-relevant world models.\n- Use **elucidation** in the strong sense\u2014models that pay rent under intervention.\n- Couple both to **decision-making under constraints** and **active learning**.\n\nIf you want, I can sketch a minimal experimental framework (datasets, objectives, diagnostics) to quantify each of your ten operations and test their joint contribution to generalization and control."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[],"content_references":[],"request_id":"9730455e6c4fe10a-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"cfdce91b-648c-4edb-8490-57d5497bde9b","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"cfdce91b-648c-4edb-8490-57d5497bde9b","children":["7f3deab0-bf00-4192-ba47-92c2ab775cf3"]},"7f3deab0-bf00-4192-ba47-92c2ab775cf3":{"id":"7f3deab0-bf00-4192-ba47-92c2ab775cf3","message":{"id":"7f3deab0-bf00-4192-ba47-92c2ab775cf3","author":{"role":"user","name":null,"metadata":{}},"create_time":1755878508.202,"update_time":null,"content":{"content_type":"text","parts":["Use the ideas you have fleshed out as our starting point. Examine whether the scientific or AI community has expressed similar ideas in recent years or even performed research along one or more directions you have indicated."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"973397448d90ec6e-IAD","message_source":null,"timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"51100113-b92c-4137-8ed9-2df110f2462f","children":["917e287c-8924-4f3b-bea0-ec3fc56d430f"]},"917e287c-8924-4f3b-bea0-ec3fc56d430f":{"id":"917e287c-8924-4f3b-bea0-ec3fc56d430f","message":{"id":"917e287c-8924-4f3b-bea0-ec3fc56d430f","author":{"role":"system","name":null,"metadata":{}},"create_time":1755878508.627411,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"7f3deab0-bf00-4192-ba47-92c2ab775cf3","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"7f3deab0-bf00-4192-ba47-92c2ab775cf3","children":["22bdf46b-3197-4530-bec6-ac4da5d424b4"]},"22bdf46b-3197-4530-bec6-ac4da5d424b4":{"id":"22bdf46b-3197-4530-bec6-ac4da5d424b4","message":{"id":"22bdf46b-3197-4530-bec6-ac4da5d424b4","author":{"role":"system","name":null,"metadata":{}},"create_time":1755878508.9727414,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"917e287c-8924-4f3b-bea0-ec3fc56d430f","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"917e287c-8924-4f3b-bea0-ec3fc56d430f","children":["9739a6df-b4cd-4a15-88ed-735d5b74b0b1"]},"9739a6df-b4cd-4a15-88ed-735d5b74b0b1":{"id":"9739a6df-b4cd-4a15-88ed-735d5b74b0b1","message":{"id":"9739a6df-b4cd-4a15-88ed-735d5b74b0b1","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878509.488218,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring modern AI theories","content":"I\u2019m looking into topics like Algorithmic Information Theory, intelligence as compression, and the recent developments around transformers, like compression progress and curiosity. There's also curiosity about PAC-Bayes in deep learning, sparse autoencoders for interpretability, and emerging works in world models and self-supervised learning. We might need a deeper search for the latest relevant articles!","chunks":[],"finished":true},{"summary":"Structuring the response","content":"For this literature review, I think we won\u2019t need a product carousel, and a navlist wouldn\u2019t be suitable, since it\u2019s only for treating news sources specifically. I'll be sure to add citations inline after paragraphs, keeping the response concise yet comprehensive, and focus on the most important statements.","chunks":[],"finished":true},{"summary":"Conducting web searches","content":"I\u2019ll search for a series of queries, including topics like \"Information Bottleneck in deep learning,\" \"rate\u2013distortion reinforcement learning,\" \"Invariant Risk Minimization,\" and \"mechanistic interpretability in AI.\" These should cover new trends in reinforcement learning, program induction, and world models\u2014updated with the latest critiques and theories.","chunks":[],"finished":true},{"summary":"Looking into specific RL concepts","content":"I'll search for topics like decision-making under information constraints using soft actor-critic methods and maximum entropy RL. Additionally, I want to explore compositional generalization, neural module networks, and state abstraction in reinforcement learning (RL), focusing on recent articles from 2021 to 2023.","chunks":[],"finished":true}],"source_analysis_msg_id":"27dcc376-d700-40d6-bb3c-92d3da0a7a52"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"22bdf46b-3197-4530-bec6-ac4da5d424b4","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"22bdf46b-3197-4530-bec6-ac4da5d424b4","children":["2a4774f1-fb63-4795-a0e1-1c161c35900a"]},"2a4774f1-fb63-4795-a0e1-1c161c35900a":{"id":"2a4774f1-fb63-4795-a0e1-1c161c35900a","message":{"id":"2a4774f1-fb63-4795-a0e1-1c161c35900a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878508.710132,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Information Bottleneck deep learning survey 2023 2024 2025\"}, {\"q\": \"rational inattention reinforcement learning rate distortion information theoretic control review 2023 2024\"}, {\"q\": \"causal representation learning survey 2023 2024 2025 Sch\\u00f6lkopf Bengio review\"}, {\"q\": \"Invariant Risk Minimization recent developments 2023 2024 survey\"}, {\"q\": \"abstract interpretation machine learning Galois connection survey 2022 2023 2024\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Information Bottleneck deep learning survey 2023 2024 2025"},{"type":"search","q":"rational inattention reinforcement learning rate distortion information theoretic control review 2023 2024"},{"type":"search","q":"causal representation learning survey 2023 2024 2025 Sch\u00f6lkopf Bengio review"},{"type":"search","q":"Invariant Risk Minimization recent developments 2023 2024 survey"},{"type":"search","q":"abstract interpretation machine learning Galois connection survey 2022 2023 2024"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching recent research on information bottlenecks and more","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"9739a6df-b4cd-4a15-88ed-735d5b74b0b1","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"9739a6df-b4cd-4a15-88ed-735d5b74b0b1","children":["b9013016-8b70-4cdf-aef7-92bf324a8d01"]},"b9013016-8b70-4cdf-aef7-92bf324a8d01":{"id":"b9013016-8b70-4cdf-aef7-92bf324a8d01","message":{"id":"b9013016-8b70-4cdf-aef7-92bf324a8d01","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878567.0518215,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349","title":"A Survey on Information Bottleneck - ACM Digital Library","snippet":"by S Hu \u00b7 2024 \u00b7 Cited by 56 \u2014 This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3696412","title":"Causal representation learning through higher-level ...","snippet":"Nov 7, 2024 \u2014 We explore a perspective on this problem that is directed to learning the generative process with causality-related foundations.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.1145/3649309","title":"Accommodating Robustness by Abstract Interpretation","snippet":"by R Giacobazzi \u00b7 2024 \u00b7 Cited by 7 \u2014 2023. Domain precision in galois connection-less abstract interpretation. In Static Analysis Symposium, SAS 2023,Lecture Notes in Computer Science, Vol.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3524938.3524953","title":"Invariant risk minimization games | Proceedings of the 37th ...","snippet":"by K Ahuja \u00b7 2020 \u00b7 Cited by 330 \u2014 In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/figure/Rational-inattention-and-reinforcement-learning-A-Using-a-non-stationary-three-armed_fig4_350365666","title":"Rational inattention and reinforcement learning (A) Using a...","snippet":"With regard to rate-distortion theory, we previously found that tonic dopamine controls the allocation of cognitive resources (Bari et al., 2024;Bari ...","ref_id":null,"pub_date":null,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/392430428_Causal_Representation_Learning_A_Quick_Survey","title":"Causal Representation Learning: A Quick Survey","snippet":"Jun 7, 2025 \u2014 This survey presents recent advances in CRL in vision. Firstly, we introduce the basic concept of causal inference. Secondly, we analyze the CRL ...","ref_id":null,"pub_date":1721347200.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/378237626_A_Survey_on_Information_Bottleneck","title":"A Survey on Information Bottleneck | Request PDF","snippet":"This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, ...","ref_id":null,"pub_date":1707955200.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/368999264_Rational_Inattention_A_Review","title":"Rational Inattention: A Review | Request PDF","snippet":"Aug 5, 2025 \u2014 We review the recent literature on rational inattention, identify the main theoretical mechanisms, and explain how it helps us understand a ...","ref_id":null,"pub_date":1677628800.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2401.17541v3","title":"Towards Understanding Variants of Invariant Risk ...","snippet":"Responding to these limitations, there has been a surge in research focusing on methodologies like Invariant Risk Minimization (IRM) (Arjovsky et al., 2020) .","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.23151v1","title":"Abstractions of Sequences, Functions and Operators","snippet":"Jul 30, 2025 \u2014 We present theoretical and practical results on the order theory of lattices of functions, focusing on Galois connections that abstract (sets of) ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2102.11107","title":"[2102.11107] Towards Causal Representation Learning","snippet":"by B Sch\u00f6lkopf \u00b7 2021 \u00b7 Cited by 1752 \u2014 In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2404.18775v4","title":"Resource-rational reinforcement learning and ...","snippet":"Mar 19, 2025 \u2014 In this manuscript, we have proposed a new computational-level objective function for theoretical biology and theoretical neuroscience that ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2405.01389","title":"Invariant Risk Minimization Is A Total Variation Model","snippet":"by ZR Lai \u00b7 2024 \u00b7 Cited by 1 \u2014 Invariant risk minimization (IRM) is an arising ap- proach to generalize invariant features to different environments in machine learning. While most.","ref_id":null,"pub_date":1716163976.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2507.23151","title":"Abstractions of Sequences, Functions and Operators","snippet":"by L Rustenholz \u00b7 2025 \u2014 Abstract We present theoretical and practical results on the order theory of lattices of functions, focusing on. Galois connections that ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2502.14889","title":"narrowing information bottleneck theory","snippet":"by Z Zhu \u00b7 2025 \u2014 Published as a conference paper at ICLR 2025 interpretability of deep learning. We then introduce our Narrowing Bottleneck Theory, which is.","ref_id":null,"pub_date":1740358855.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2409.02772","title":"unifying causal representation learning with","snippet":"by D Yao \u00b7 2024 \u00b7 Cited by 12 \u2014 Overall, this paper clarifies the role of causal assumptions in the discovery of causal variables and shifts the focus to preserving data ...","ref_id":null,"pub_date":1741225196.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.bohrium.com","entries":[{"type":"search_result","url":"https://www.bohrium.com/paper-details/a-survey-on-information-bottleneck/977537932896240018-2429","title":"A Survey on Information Bottleneck","snippet":"This survey is for the remembrance of one of the creators of the information bottleneck theory, Prof. Naftali Tishby, passing away at the age of 68 on August, ...","ref_id":null,"pub_date":null,"attribution":"www.bohrium.com"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/","title":"Rational inattention in neural coding for economic choice","snippet":"by JM Fine \u00b7 2024 \u2014 We found, confirming predictions made by our extension of rational inattention theory, choice accuracy is positively correlated with recent reward rate.","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=9YqacugDER","title":"Towards Understanding Variants of Invariant Risk ...","snippet":"by K Yoshida \u00b7 Cited by 3 \u2014 Invariant Risk Minimization (IRM) emerges as a solution that aims to identify invariant features across different environments to enhance OOD ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=lpOC6s4BcM","title":"Invariance & Causal Representation Learning: Prospects ...","snippet":"by S Bing \u00b7 Cited by 3 \u2014 In this work, we explore this invariance principle as a candidate assumption to achieve identifiability of causal representations.","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"link.springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10115-024-02108-4","title":"Robustness verification of k-nearest neighbors by abstract ...","snippet":"by N Fassina \u00b7 2024 \u2014 In this work, we put forward an abstract interpretation-based framework for designing a sound approximate version of the kNN algorithm, which is ...","ref_id":null,"pub_date":null,"attribution":"link.springer.com"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0925231225007842","title":"Information bottleneck with input sampling for attribution","snippet":"by B Coelho \u00b7 2025 \u2014 In this paper, we propose IBISA, a model-agnostic attribution method that reaches state-of-the-art performance by optimizing sampling masks using the ...","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S1364661324000032","title":"Rationality, preferences, and emotions with biological ...","snippet":"by R Polan\u00eda \u00b7 2024 \u00b7 Cited by 16 \u2014 Both rational inattention and rate-distortion theory involve optimizing the allocation of scarce resources. In the case of rational inattention the resource ...","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"pubsonline.informs.org","entries":[{"type":"search_result","url":"https://pubsonline.informs.org/doi/abs/10.1287/trsc.2024.0562","title":"diffIRM: A Diffusion-Augmented Invariant Risk Minimization ...","snippet":"by Z Mo \u00b7 2025 \u00b7 Cited by 3 \u2014 In this study, we fill in this research gap and propose a diffusion-augmented invariant risk minimization (diffIRM) framework that combines ...","ref_id":null,"pub_date":null,"attribution":"pubsonline.informs.org"}]},{"type":"search_result_group","domain":"www.math.unipd.it","entries":[{"type":"search_result","url":"https://www.math.unipd.it/~ranzato/papers/kais24.pdf","title":"Robustness Verification of k-Nearest Neighbors by Abstract ...","snippet":"Abstract. We study the certification of stability properties, such as robustness and individ- ual fairness, of the k-Nearest Neighbor algorithm (kNN).","ref_id":null,"pub_date":1709722754.0,"attribution":"www.math.unipd.it"}]},{"type":"search_result_group","domain":"iclr.cc","entries":[{"type":"search_result","url":"https://iclr.cc/virtual/2024/poster/18081","title":"Information Bottleneck Analysis of Deep Neural Networks ...","snippet":"The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its ...","ref_id":null,"pub_date":1708560000.0,"attribution":"iclr.cc"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching recent research on information bottlenecks and more","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"2a4774f1-fb63-4795-a0e1-1c161c35900a","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"2a4774f1-fb63-4795-a0e1-1c161c35900a","children":["db418db6-e801-41e9-8d2c-67601a2b2a4b"]},"db418db6-e801-41e9-8d2c-67601a2b2a4b":{"id":"db418db6-e801-41e9-8d2c-67601a2b2a4b","message":{"id":"db418db6-e801-41e9-8d2c-67601a2b2a4b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878574.954548,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring IB and program induction topics","content":"I\u2019ll focus on recent surveys and analyses on Information Bottleneck (IB), with updates from 2024 and 2025 regarding attribution and narrowing bottleneck theory. I also want to dive into program induction and recursion, exploring models like \"Neural Programmer-Interpreters\" and \"Neural GPUs,\" plus transformer generalization in algorithmic recursion.","chunks":[],"finished":true}],"source_analysis_msg_id":"9ebd5f37-de13-4c1e-9ce9-8ab22be52450"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"b9013016-8b70-4cdf-aef7-92bf324a8d01","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"b9013016-8b70-4cdf-aef7-92bf324a8d01","children":["a4947dd5-89ad-41b2-b10b-47008ec4756e"]},"a4947dd5-89ad-41b2-b10b-47008ec4756e":{"id":"a4947dd5-89ad-41b2-b10b-47008ec4756e","message":{"id":"a4947dd5-89ad-41b2-b10b-47008ec4756e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878569.304484,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"length generalization transformers 2024 2025 survey recursion algorithmic tasks\"}, {\"q\": \"induction heads mechanistic interpretability 2023 2024 2025 sparse autoencoders monosemantic features Anthropic\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"length generalization transformers 2024 2025 survey recursion algorithmic tasks"},{"type":"search","q":"induction heads mechanistic interpretability 2023 2024 2025 sparse autoencoders monosemantic features Anthropic"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for transformer length generalization and induction models","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"db418db6-e801-41e9-8d2c-67601a2b2a4b","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"db418db6-e801-41e9-8d2c-67601a2b2a4b","children":["4011683f-011e-45df-ac82-3da2e2a1284a"]},"4011683f-011e-45df-ac82-3da2e2a1284a":{"id":"4011683f-011e-45df-ac82-3da2e2a1284a","message":{"id":"4011683f-011e-45df-ac82-3da2e2a1284a","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878576.0215538,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/2409.15647","title":"looped transformers for length generaliza","snippet":"by Y Fan \u00b7 2024 \u00b7 Cited by 19 \u2014 (2024a), they show algorithmic tasks that can be written as a RASP-L program can be easily learned by a Transformer in a length-generalizable ...","ref_id":null,"pub_date":1747099116.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2409.15647v5","title":"Looped Transformers for Length Generalization","snippet":"May 12, 2025 \u2014 In Zhou et al. (2024a) , they show algorithmic tasks that can be written as a RASP-L program can be easily learned by a Transformer in a length- ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.04878v1","title":"Sparse Autoencoders Do Not Find Canonical Units of ...","snippet":"SAEs for Mechanistic Interpretability. SAEs have been demonstrated to recover sparse, monosemantic, and interpretable features from language model activations ( ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2501.16496v1","title":"Open Problems in Mechanistic Interpretability","snippet":"Jan 27, 2025 \u2014 This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2309.08600","title":"Sparse Autoencoders Find Highly Interpretable Features in Language Models","snippet":"","ref_id":null,"pub_date":1694800615.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2502.01612","title":"Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges","snippet":"","ref_id":null,"pub_date":1738608322.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"transformer-circuits.pub","entries":[{"type":"search_result","url":"https://transformer-circuits.pub/2023/monosemantic-features","title":"Decomposing Language Models With Dictionary Learning","snippet":"Oct 4, 2023 \u2014 Sparse Autoencoders extract relatively monosemantic features. \u00b7 Sparse autoencoders produce interpretable features that are effectively invisible ...","ref_id":null,"pub_date":null,"attribution":"transformer-circuits.pub"},{"type":"search_result","url":"https://transformer-circuits.pub/2024/scaling-monosemanticity/","title":"Extracting Interpretable Features from Claude 3 Sonnet","snippet":"May 21, 2024 \u2014 Eight months ago, we demonstrated that sparse autoencoders could recover monosemantic features from a small one-layer transformer.","ref_id":null,"pub_date":null,"attribution":"transformer-circuits.pub"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=0sbIEkIutN","title":"Complete Length Generalization for Arithmetic Transformers","snippet":"by S Duan \u00b7 Cited by 16 \u2014 This paper studies length generalization of Transformer models on algorithmic tasks. It shows that the vanilla Transformer correctly learns the attention ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"intuitionlabs.ai","entries":[{"type":"search_result","url":"https://intuitionlabs.ai/articles/mechanistic-interpretability-ai-llms","title":"Understanding Mechanistic Interpretability in AI Models","snippet":"6 days ago \u2014 The primary goal of mechanistic interpretability is to understand neural networks at the algorithmic level. This means identifying the internal ...","ref_id":null,"pub_date":1755345600.0,"attribution":"intuitionlabs.ai"},{"type":"search_result","url":"https://intuitionlabs.ai/pdfs/understanding-mechanistic-interpretability-in-ai-models.pdf","title":"Understanding Mechanistic Interpretability in AI Models","snippet":"6 days ago \u2014 Sparse Autoencoders and Feature Factorization: A more recent technique, spearheaded by teams at DeepMind and Anthropic, is to automatically ...","ref_id":null,"pub_date":1755715200.0,"attribution":"intuitionlabs.ai"}]},{"type":"search_result_group","domain":"research.ibm.com","entries":[{"type":"search_result","url":"https://research.ibm.com/publications/recurrent-transformers-trade-off-parallelism-for-length-generalization-on-regular-languages","title":"Recurrent Transformers Trade-off Parallelism for Length ...","snippet":"by P Soulos \u00b7 2024 \u2014 This study underscores the critical role of layer-recurrence and chunk size in Recurrent Transformers, highlighting the trade-off between ...","ref_id":null,"pub_date":null,"attribution":"research.ibm.com"}]},{"type":"search_result_group","domain":"www.anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/research/engineering-challenges-interpretability","title":"The engineering challenges of scaling interpretability","snippet":"Jun 13, 2024 \u2014 Our Sparse Autoencoders\u2014the tools we use to investigate \u201cfeatures\u201d\u2014are trained on the activations of transformers, and those activations ...","ref_id":null,"pub_date":null,"attribution":"www.anthropic.com"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/384295632_Looped_Transformers_for_Length_Generalization","title":"Looped Transformers for Length Generalization","snippet":"We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.","ref_id":null,"pub_date":1727049600.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"beginswithai.com","entries":[{"type":"search_result","url":"https://beginswithai.com/anthropic-circuits-updates/","title":"Anthropic's Circuits Updates: All Releases (Updated May ...","snippet":"Analyzing attention heads and computational circuits in transformers; Improving interpretability with sparse autoencoders and monosemantic feature learning ...","ref_id":null,"pub_date":1716212662.0,"attribution":"beginswithai.com"}]},{"type":"search_result_group","domain":"machinelearning.apple.com","entries":[{"type":"search_result","url":"https://machinelearning.apple.com/research/transformers-learn","title":"What Algorithms can Transformers Learn? A Study in ...","snippet":"Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths.","ref_id":null,"pub_date":null,"attribution":"machinelearning.apple.com"}]},{"type":"search_result_group","domain":"direct.mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00663/120983/What-Formal-Languages-Can-Transformers-Express-A","title":"What Formal Languages Can Transformers Express? A ...","snippet":"by L Strobl \u00b7 2024 \u00b7 Cited by 82 \u2014 2024 . What algorithms can Transformers learn? A study in length generalization . In. Proceedings of the Twelfth International Conference on ...","ref_id":null,"pub_date":null,"attribution":"direct.mit.edu"}]},{"type":"search_result_group","domain":"www.pnas.org","entries":[{"type":"search_result","url":"https://www.pnas.org/doi/10.1073/pnas.2506316122","title":"Sparse autoencoders uncover biologically interpretable ...","snippet":"We leverage sparse autoencoders and transcoders from natural language processing as a way to extract biologically meaningful, interpretable features from both ...","ref_id":null,"pub_date":null,"attribution":"www.pnas.org"}]},{"type":"search_result_group","domain":"nips.cc","entries":[{"type":"search_result","url":"https://nips.cc/virtual/2024/papers.html","title":"NeurIPS 2024 Papers","snippet":"Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure ... Algorithmic Capabilities of Random Transformers ...","ref_id":null,"pub_date":null,"attribution":"nips.cc"}]},{"type":"search_result_group","domain":"edoc.sub.uni-hamburg.de","entries":[{"type":"search_result","url":"https://edoc.sub.uni-hamburg.de/informatik/volltexte/2025/301/pdf/thesis.pdf","title":"Length Generalization on Multi-Digit Integer Addition with ...","snippet":"by S Wermter \u00b7 2024 \u2014 2024 show using RASP that looped transformers can generalize to longer sequence lengths for binary ad- dition task, along with other algorithmic ...","ref_id":null,"pub_date":null,"attribution":"edoc.sub.uni-hamburg.de"}]},{"type":"search_result_group","domain":"ee.stanford.edu","entries":[{"type":"search_result","url":"https://ee.stanford.edu/event/03-07-2024/what-algorithms-can-transformers-learn-study-length-generalization","title":"What Algorithms can Transformers Learn? A Study in Length ...","snippet":"Mar 7, 2024 \u2014 We conjecture, informally, that: Transformers tend to length-generalize on a task if there exists a short RASP program that solves the task for ...","ref_id":null,"pub_date":null,"attribution":"ee.stanford.edu"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2025.findings-naacl.87.pdf","title":"Specialized Sparse Autoencoders for Interpreting Rare ...","snippet":"by A Muhamed \u00b7 2025 \u00b7 Cited by 3 \u2014 Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability.","ref_id":null,"pub_date":1745243616.0,"attribution":"aclanthology.org"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for transformer length generalization and induction models","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"a4947dd5-89ad-41b2-b10b-47008ec4756e","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"a4947dd5-89ad-41b2-b10b-47008ec4756e","children":["2dbaca24-e6c8-44ef-b52f-2d5074998352"]},"2dbaca24-e6c8-44ef-b52f-2d5074998352":{"id":"2dbaca24-e6c8-44ef-b52f-2d5074998352","message":{"id":"2dbaca24-e6c8-44ef-b52f-2d5074998352","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878580.297523,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Investigating world models and learning exploration","content":"I plan to explore topics like world models, with an emphasis on JEPA and DreamerV3. I\u2019ll also dive into active learning, exploration, and information gain, which are crucial concepts in improving model performance. Time to look up the latest work on these.","chunks":[],"finished":true}],"source_analysis_msg_id":"4c1ef265-4ec5-4352-9ffd-79afd0c279bc"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"4011683f-011e-45df-ac82-3da2e2a1284a","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"4011683f-011e-45df-ac82-3da2e2a1284a","children":["79cc5189-ec5d-4b3e-b1e5-ded069b8b8e2"]},"79cc5189-ec5d-4b3e-b1e5-ded069b8b8e2":{"id":"79cc5189-ec5d-4b3e-b1e5-ded069b8b8e2","message":{"id":"79cc5189-ec5d-4b3e-b1e5-ded069b8b8e2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878577.40774,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"JEPA I-JEPA V-JEPA LeCun 2023 2024 world model predictive architecture paper\"}, {\"q\": \"DreamerV3 world models 2023 2024 paper\"}, {\"q\": \"information-theoretic reinforcement learning rate distortion bounded rationality Ortega Braun Todorov 2020 2023 2024 review\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"JEPA I-JEPA V-JEPA LeCun 2023 2024 world model predictive architecture paper"},{"type":"search","q":"DreamerV3 world models 2023 2024 paper"},{"type":"search","q":"information-theoretic reinforcement learning rate distortion bounded rationality Ortega Braun Todorov 2020 2023 2024 review"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for world models, JEPA, and DreamerV3","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"2dbaca24-e6c8-44ef-b52f-2d5074998352","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"2dbaca24-e6c8-44ef-b52f-2d5074998352","children":["eca25238-79fc-4b41-8378-8a04d926f39c"]},"eca25238-79fc-4b41-8378-8a04d926f39c":{"id":"eca25238-79fc-4b41-8378-8a04d926f39c","message":{"id":"eca25238-79fc-4b41-8378-8a04d926f39c","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878581.8200033,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"ai.meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/","title":"V-JEPA: The next step toward advanced machine ...","snippet":"Feb 15, 2024 \u2014 V-JEPA is a non-generative model that learns by predicting missing or masked parts of a video in an abstract representation space.","ref_id":null,"pub_date":null,"attribution":"ai.meta.com"},{"type":"search_result","url":"https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/","title":"I-JEPA: The first AI model based on Yann LeCun's vision ...","snippet":"Jun 13, 2023 \u2014 This model, the Image Joint Embedding Predictive Architecture (I-JEPA), learns by creating an internal model of the outside world, which compares abstract ...","ref_id":null,"pub_date":null,"attribution":"ai.meta.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2301.04104","title":"Mastering Diverse Domains through World Models","snippet":"","ref_id":null,"pub_date":1673374336.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2301.08243","title":"Self-Supervised Learning from Images with a Joint ...","snippet":"by M Assran \u00b7 2023 \u00b7 Cited by 579 \u2014 We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2301.04104","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 837 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":null,"pub_date":1713402880.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2403.00504","title":"[2403.00504] Learning and Leveraging World Models in ...","snippet":"by Q Garrido \u00b7 2024 \u00b7 Cited by 40 \u2014 Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.12185v1","title":"Satisficing Exploration for Deep Reinforcement Learning","snippet":"Jul 16, 2024 \u2014 In this paper, we build upon a long-line of work in the cognitive-science literature that formalizes the limits of bounded decision-makers ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"direct.mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=consensus","title":"Bayesian Reinforcement Learning With Limited Cognitive ...","snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 proposals including information-theoretic bounded rationality (Gottwald & Braun ... Rate-distortion theory and computationally rational reinforcement learning.","ref_id":null,"pub_date":null,"attribution":"direct.mit.edu"},{"type":"search_result","url":"https://direct.mit.edu/neco/article/37/1/76/124827/Bounded-Rational-Decision-Networks-With-Belief","title":"Bounded Rational Decision Networks With Belief ...","snippet":"Dec 12, 2024 \u2014 One prominent approach that deals with utility-information trade-offs in complex decision networks is bounded rationality. Herbert Simon (1943, ...","ref_id":null,"pub_date":null,"attribution":"direct.mit.edu"}]},{"type":"search_result_group","domain":"malcolmlett.medium.com","entries":[{"type":"search_result","url":"https://malcolmlett.medium.com/critical-review-of-lecuns-introductory-jepa-paper-fabe5783134e","title":"Critical review of LeCun's Introductory JEPA paper | Medium","snippet":"The I-JEPA and V-JEPA work is already showing that JEPA enables the model to learn a more compact representation of the world than prior ...","ref_id":null,"pub_date":1743124146.118,"attribution":"malcolmlett.medium.com"}]},{"type":"search_result_group","domain":"danijar.com","entries":[{"type":"search_result","url":"https://danijar.com/project/dreamerv3/","title":"Mastering Diverse Control Tasks through World Models","snippet":"Dreamer is the first algorithm that collects diamonds in Minecraft from sparse rewards, without expert demonstrations or curricula, solving this challenge.","ref_id":null,"pub_date":null,"attribution":"danijar.com"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11045037/","title":"Bayesian Reinforcement Learning With Limited Cognitive ...","snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 The preceding section provides a cursory overview of how rate-distortion theory accommodates capacity-limited learning within a Bayesian decision-making ...","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"vitalab.github.io","entries":[{"type":"search_result","url":"https://vitalab.github.io/article/2023/01/19/DreamerV3.html","title":"DreamerV3: Mastering Diverse Domains through World Models","snippet":"Jan 19, 2023 \u2014 Model-based reinforcement learning is a type of RL where the agent will not only learn a policy, but also a world model which the agent can use to plan ahead.","ref_id":null,"pub_date":null,"attribution":"vitalab.github.io"}]},{"type":"search_result_group","domain":"www.pnas.org","entries":[{"type":"search_result","url":"https://www.pnas.org/doi/10.1073/pnas.2220593120","title":"Information-theoretic principles in incremental language ...","snippet":"by R Futrell \u00b7 2023 \u00b7 Cited by 22 \u2014 This theory, the rate\u2013distortion theory of control (RDC), uses the mathematical theory of lossy compression (5, 6) to impose informational constraints on the ...","ref_id":null,"pub_date":null,"attribution":"www.pnas.org"}]},{"type":"search_result_group","domain":"www.turingpost.com","entries":[{"type":"search_result","url":"https://www.turingpost.com/p/jepa","title":"What is Joint Embedding Predictive Architecture (JEPA)?","snippet":"Jun 12, 2024 \u2014 We discuss the Joint Embedding Predictive Architecture (JEPA), how it differs from transformers and provide you with list of models based on JEPA.","ref_id":null,"pub_date":null,"attribution":"www.turingpost.com"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41586-025-08744-2","title":"Mastering diverse control tasks through world models","snippet":"by D Hafner \u00b7 2025 \u00b7 Cited by 60 \u2014 All Dreamer and PPO agents in this paper were trained on a single Nvidia A100 GPU each. Dreamer uses the 200 million model size by default.","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"rohitbandaru.github.io","entries":[{"type":"search_result","url":"https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/","title":"Deep Dive into Yann LeCun's JEPA - Rohit Bandaru","snippet":"Jul 31, 2024 \u2014 In the JEPA paper, Yann LeCun proposes frameworks for models that can think. Learning to think may address the fundamental problems in current ...","ref_id":null,"pub_date":null,"attribution":"rohitbandaru.github.io"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/367019841_Mastering_Diverse_Domains_through_World_Models","title":"(PDF) Mastering Diverse Domains through World Models","snippet":"Jan 10, 2023 \u2014 Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long- ...","ref_id":null,"pub_date":1673308800.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/384771727_Bounded_Rational_Decision_Networks_with_Belief_Propagation","title":"(PDF) Bounded Rational Decision Networks With Belief ...","snippet":"Feb 5, 2025 \u2014 Here, we consider a decision-theoretic approach to study networks of bounded rational decision makers that are allowed to specialize and ...","ref_id":null,"pub_date":null,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"onlinelibrary.wiley.com","entries":[{"type":"search_result","url":"https://onlinelibrary.wiley.com/doi/10.1111/tops.12716","title":"An Information\u2010Theoretic Account of Availability Effects in ...","snippet":"by R Futrell \u00b7 2024 \u00b7 Cited by 6 \u2014 A model based on a combination of information theory and control theory predicts availability/accessibility effects in language production.","ref_id":null,"pub_date":null,"attribution":"onlinelibrary.wiley.com"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/danijar/dreamerv3","title":"danijar/dreamerv3: Mastering Diverse Domains through ...","snippet":"A reimplementation of DreamerV3, a scalable and general reinforcement learning algorithm that masters a wide range of applications with fixed hyperparameters.","ref_id":null,"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/04a80267ad46fc730011f8760f265054-Paper-Conference.pdf","title":"Connecting Joint-Embedding Predictive Architecture with ...","snippet":"by S Mo \u00b7 2024 \u00b7 Cited by 4 \u2014 In this paper, we aim to detail the theoretical underpinnings and empirical validations that substantiate the superiority of. C-JEPA over previous self- ...","ref_id":null,"pub_date":1730226656.0,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2307.07176","title":"Safe DreamerV3: Safe Reinforcement Learning with World ...","snippet":"We introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model.","ref_id":null,"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"dilipa.github.io","entries":[{"type":"search_result","url":"https://dilipa.github.io/papers/nips22_infocog_workshop_rdt_satisficing.pdf","title":"On Rate-Distortion Theory in Capacity-Limited Cognition ...","snippet":"by D Arumugam \u00b7 Cited by 5 \u2014 Rate-distortion theory and computationally rational reinforcement learning. ... An information-theoretic approach to curiosity-driven reinforcement learning.","ref_id":null,"pub_date":1666795674.0,"attribution":"dilipa.github.io"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for world models, JEPA, and DreamerV3","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"79cc5189-ec5d-4b3e-b1e5-ded069b8b8e2","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"79cc5189-ec5d-4b3e-b1e5-ded069b8b8e2","children":["da17cd87-0e39-4cbb-8e63-890916c82065"]},"da17cd87-0e39-4cbb-8e63-890916c82065":{"id":"da17cd87-0e39-4cbb-8e63-890916c82065","message":{"id":"da17cd87-0e39-4cbb-8e63-890916c82065","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878586.333497,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Looking into state abstraction in RL","content":"I\u2019ll be exploring state abstraction in reinforcement learning (RL) along with the concept of options in hierarchical RL. These areas focus on simplifying decision-making processes and improving efficiency in learning. Time to search for the most recent ideas and developments in these topics.","chunks":[],"finished":true}],"source_analysis_msg_id":"d62b2aa5-3fde-4ffc-95a9-312ac43999e0"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"eca25238-79fc-4b41-8378-8a04d926f39c","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"eca25238-79fc-4b41-8378-8a04d926f39c","children":["a7aac4e3-f646-44ee-9518-ddced8b619c9"]},"a7aac4e3-f646-44ee-9518-ddced8b619c9":{"id":"a7aac4e3-f646-44ee-9518-ddced8b619c9","message":{"id":"a7aac4e3-f646-44ee-9518-ddced8b619c9","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878583.643986,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"state abstraction reinforcement learning survey 2021 2022 2023 2024 Littman survey\"}, {\"q\": \"options framework hierarchical reinforcement learning 2023 2024 survey\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"state abstraction reinforcement learning survey 2021 2022 2023 2024 Littman survey"},{"type":"search","q":"options framework hierarchical reinforcement learning 2023 2024 survey"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for state abstraction and options in RL","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"da17cd87-0e39-4cbb-8e63-890916c82065","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"da17cd87-0e39-4cbb-8e63-890916c82065","children":["4212a05b-55a6-4a68-85c3-3f6e56655161"]},"4212a05b-55a6-4a68-85c3-3f6e56655161":{"id":"4212a05b-55a6-4a68-85c3-3f6e56655161","message":{"id":"4212a05b-55a6-4a68-85c3-3f6e56655161","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878588.0930688,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/372896237_Model-Based_Reinforcement_Learning_with_State_Abstraction_A_Survey?_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbCwic3ViUGFnZSI6bnVsbH19","title":"Model-Based Reinforcement Learning with State Abstraction","snippet":"Aug 7, 2025 \u2014 The survey finishes with Section VI, in which we discuss possible future research avenues for abstraction techniques. usage of abstractions in ...","ref_id":null,"pub_date":1691107200.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/352160708_Hierarchical_Reinforcement_Learning_A_Comprehensive_Survey","title":"Hierarchical Reinforcement Learning: A Comprehensive ...","snippet":"We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi- ...","ref_id":null,"pub_date":1622505600.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/380037591_Structure_in_Deep_Reinforcement_Learning_A_Survey_and_Open_Problems","title":"Structure in Deep Reinforcement Learning: A Survey and ...","snippet":"Apr 29, 2024 \u2014 2021, 2022a). Relational abstractions contribute to generalization by incorporating symbolic spaces. into ...","ref_id":null,"pub_date":1713657600.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0957417423019693","title":"Hierarchical reinforcement learning with unlimited option ...","snippet":"by Z Huang \u00b7 2024 \u00b7 Cited by 7 \u2014 In this paper, we provide a fresh perspective on option technology to comprehend different options based on knowledge representation.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2949855424000042","title":"A survey on multi-agent reinforcement learning and its ...","snippet":"by Z Ning \u00b7 2024 \u00b7 Cited by 86 \u2014 Abstract. Multi-agent reinforcement learning (MARL) has been a rapidly evolving field. This paper presents a comprehensive survey of MARL and its applications.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"www.fransoliehoek.net","entries":[{"type":"search_result","url":"https://www.fransoliehoek.net/docs/Starre22BNAICBenelearn.pdf","title":"Model-Based Reinforcement Learning with State Abstraction","snippet":"by RAN Starre \u00b7 Cited by 5 \u2014 Abstract. Model-based reinforcement learning methods are promising since they can increase sample e ciency while simultaneously improving generalizability.","ref_id":null,"pub_date":null,"attribution":"www.fransoliehoek.net"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2506.21872v1","title":"A Survey of Continual Reinforcement Learning","snippet":"Jun 26, 2025 \u2014 In this survey, we provide a comprehensive examination of CRL, focusing on its core concepts, challenges, and methodologies. Firstly, we conduct ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2401.02349v2","title":"A Survey Analyzing Generalization in Deep Reinforcement ...","snippet":"Oct 30, 2024 \u2014 Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2412.16395","title":"arXiv:2412.16395v1 [cs.AI] 20 Dec 2024","snippet":"by RK Nayyar \u00b7 2024 \u00b7 Cited by 3 \u2014 Abstraction is key to scaling up reinforcement learning (RL). However, autonomously learning abstract state and action.","ref_id":null,"pub_date":1735002815.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2401.02349v1","title":"A Survey Analyzing Generalization in Deep Reinforcement ...","snippet":"In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"scholar.google.com","entries":[{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=iRMZ2hoAAAAJ","title":"Michael Littman","snippet":"Reinforcement learning: A survey. LP Kaelbling, ML Littman, AW Moore. Journal ... Towards a unified theory of state abstraction for MDPs. L Li, TJ Walsh ...","ref_id":null,"pub_date":null,"attribution":"scholar.google.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=4JCVw8oMlf","title":"Effectively Learning Initiation Sets in Hierarchical ...","snippet":"by A Bagaria \u00b7 Cited by 8 \u2014 This paper proposes a method for learning an initiation set for options framework. After identifying challenges in learning initiation sets,","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"jair.org","entries":[{"type":"search_result","url":"https://jair.org/index.php/jair/article/download/15703/27028/38774","title":"Structure in Deep Reinforcement Learning: A Survey and ...","snippet":"by A Mohan \u00b7 2024 \u00b7 Cited by 36 \u2014 State abstractions are a standard choice for improving generalization performance by capturing shared dynamics across MDPs into abstract state spaces using.","ref_id":null,"pub_date":null,"attribution":"jair.org"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/e51ec47238fc3e82c269965c5533f9ef-Paper-Conference.pdf","title":"Hierarchical Programmatic Option Framework","snippet":"by YA Lin \u00b7 2024 \u00b7 Cited by 3 \u2014 Deep reinforcement learning aims to learn deep neural network policies to solve large-scale decision-making problems. However, approximating policies using.","ref_id":null,"pub_date":1730583269.0,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"repository.tudelft.nl","entries":[{"type":"search_result","url":"https://repository.tudelft.nl/file/File_fe3321ce-61ae-4b9d-ac3e-b5d2934bdc29","title":"Model-Based Reinforcement Learning with State Abstraction","snippet":"We provide a high- level view of the promising research in the field, covering a wide range of different types of state abstractions known from the literature.","ref_id":null,"pub_date":1698326160.0,"attribution":"repository.tudelft.nl"}]},{"type":"search_result_group","domain":"www.ifaamas.org","entries":[{"type":"search_result","url":"https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p1328.pdf","title":"Mixed-Initiative Bayesian Sub-Goal Optimization in ...","snippet":"by H Ma \u00b7 Cited by 9 \u2014 ABSTRACT. In hierarchical reinforcement learning, human expertise is often involved in defining sub-goals that decompose the final objective.","ref_id":null,"pub_date":1711481156.0,"attribution":"www.ifaamas.org"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41598-024-76719-w","title":"Exploring the limits of hierarchical world models in ...","snippet":"by R Schiewer \u00b7 2024 \u00b7 Cited by 3 \u2014 In this work we describe a novel HMBRL framework and evaluate it thoroughly. We construct hierarchical world models that simulate the environment at various ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1016/j.eswa.2023.121467","title":"Hierarchical reinforcement learning with unlimited option ...","snippet":"by Z Huang \u00b7 2024 \u00b7 Cited by 7 \u2014 In this paper, we provide a fresh perspective on option technology to comprehend different options based on knowledge representation. The ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"link.springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10462-022-10205-5","title":"Survey on reinforcement learning for language processing","snippet":"by V Uc-Cetina \u00b7 2023 \u00b7 Cited by 199 \u2014 This paper reviews the state of the art of RL methods for their possible use for different problems of NLP, focusing primarily on conversational systems.","ref_id":null,"pub_date":null,"attribution":"link.springer.com"}]},{"type":"search_result_group","domain":"www.ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2024/0272.pdf","title":"Hierarchical Reinforcement Learning for Point of Interest ...","snippet":"by Y Xiao \u00b7 Cited by 3 \u2014 This section provides foundational definitions to establish the context for our study on improving POI recommendation sys- tems using hierarchical reinforcement ...","ref_id":null,"pub_date":1721927573.0,"attribution":"www.ijcai.org"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for state abstraction and options in RL","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"a7aac4e3-f646-44ee-9518-ddced8b619c9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"a7aac4e3-f646-44ee-9518-ddced8b619c9","children":["89b2d480-29ad-4b73-a78f-fa62be565f73"]},"89b2d480-29ad-4b73-a78f-fa62be565f73":{"id":"89b2d480-29ad-4b73-a78f-fa62be565f73","message":{"id":"89b2d480-29ad-4b73-a78f-fa62be565f73","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878590.790907,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring invariances and group-equivariant networks","content":"I\u2019ll be diving into the concept of invariances and group-equivariant networks, which focus on improving generalization by using symmetries or structure in data. These ideas can potentially enhance how models generalize across different tasks or inputs. It\u2019s time to look into recent developments on this!","chunks":[],"finished":true}],"source_analysis_msg_id":"508ae0e3-e3a7-4124-963b-1d3afe2d6539"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"4212a05b-55a6-4a68-85c3-3f6e56655161","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"4212a05b-55a6-4a68-85c3-3f6e56655161","children":["8599bd73-a5b1-489c-a85b-66b44aacbb20"]},"8599bd73-a5b1-489c-a85b-66b44aacbb20":{"id":"8599bd73-a5b1-489c-a85b-66b44aacbb20","message":{"id":"8599bd73-a5b1-489c-a85b-66b44aacbb20","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878589.220624,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"equivariant neural networks survey 2023 2024 2025 group equivariant CNNs survey\"}, {\"q\": \"VICReg Barlow Twins invariance redundancy reduction self-supervised 2021 2024 survey\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"equivariant neural networks survey 2023 2024 2025 group equivariant CNNs survey"},{"type":"search","q":"VICReg Barlow Twins invariance redundancy reduction self-supervised 2021 2024 survey"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for surveys on equivariant networks and invariances","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"89b2d480-29ad-4b73-a78f-fa62be565f73","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"89b2d480-29ad-4b73-a78f-fa62be565f73","children":["c808eb8d-d7b2-452b-a911-06bd255f87f6"]},"c808eb8d-d7b2-452b-a911-06bd255f87f6":{"id":"c808eb8d-d7b2-452b-a911-06bd255f87f6","message":{"id":"c808eb8d-d7b2-452b-a911-06bd255f87f6","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878591.7574382,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41467-023-38468-8","title":"General framework for E(3)-equivariant neural network ...","snippet":"by X Gong \u00b7 2023 \u00b7 Cited by 136 \u2014 Here we propose an E(3)-equivariant deep-learning framework to represent density functional theory (DFT) Hamiltonian as a function of material structure.","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41598-025-08504-2","title":"Deep representation learning using layer-wise VICReg ...","snippet":"by J Datta \u00b7 2025 \u2014 Barlow twins: Self-supervised learning via redundancy reduction. In International conference on machine learning, 12310\u201312320 (PMLR, 2021).","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41598-025-01250-5","title":"Representing Born effective charges with equivariant ...","snippet":"by A Kutana \u00b7 2025 \u00b7 Cited by 2 \u2014 Here we explicitly encode such properties using an equivariant graph convolutional neural network. The network respects rotational symmetries of ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2103.03230","title":"Barlow Twins: Self-Supervised Learning via Redundancy Reduction","snippet":"","ref_id":null,"pub_date":1614884109.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2412.01926","title":"Higher-Order Redundancies in Self-Supervised ...","snippet":"by D Zollikofer \u00b7 2024 \u2014 Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pp. 12310\u201312320. PMLR ...","ref_id":null,"pub_date":1733795106.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2405.15429v5","title":"E(\ud835\udc5b) Equivariant Topological Neural Networks","snippet":"Graph neural networks excel at modeling pairwise interactions, but they cannot flexibly accommodate higher-order interactions and features.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/1602.07576","title":"[1602.07576] Group Equivariant Convolutional Networks","snippet":"by TS Cohen \u00b7 2016 \u00b7 Cited by 2660 \u2014 We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=n84bzMrGUD&noteId=sQG6abJbs8","title":"Clifford Group Equivariant Neural Networks","snippet":"Feb 20, 2024 \u2014 The paper develops a new kind of neural network model based on the Clifford group, which makes the neural network E(n)-equivariant.","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"link.springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10994-024-06708-7","title":"A survey on self-supervised methods for visual ...","snippet":"by T Uelwer \u00b7 2025 \u00b7 Cited by 2 \u2014 To implement this redundancy reduction principle, the Barlow Twins approach takes a batch of images X and creates two views {X^{(1)} = t^{(1)}(X)} ...","ref_id":null,"pub_date":null,"attribution":"link.springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s11704-025-41426-w","title":"A survey of geometric graph neural networks: data ...","snippet":"by J Han \u00b7 2025 \u00b7 Cited by 45 \u2014 Researchers proposed a variety of geometric GNNs equipped with invariant/equivariant properties to better characterize the geometry and topology of geometric ...","ref_id":null,"pub_date":null,"attribution":"link.springer.com"}]},{"type":"search_result_group","domain":"icml.cc","entries":[{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/45802","title":"GLGENN: A Novel Parameter-Light Equivariant Neural ...","snippet":"We propose, implement, and compare with competitors a new architecture of equivariant neural networks based on geometric (Clifford) algebras: Generalized ...","ref_id":null,"pub_date":1746662400.0,"attribution":"icml.cc"}]},{"type":"search_result_group","domain":"www.computer.org","entries":[{"type":"search_result","url":"https://www.computer.org/csdl/journal/tp/2024/12/10559458/1XR0ep31Wr6","title":"A Survey on Self-Supervised Learning","snippet":"by J Gui \u00b7 2024 \u00b7 Cited by 360 \u2014 This paper presents a review of diverse SSL methods, encompassing algorithmic aspects, application domains, three key trends, and open research questions.","ref_id":null,"pub_date":null,"attribution":"www.computer.org"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Geometrically-Equivariant-Graph-Neural-Networks%3A-A-Han-Rong/c4478d0c8279d19c78a3cc1d2d4f699679341a4d","title":"Geometrically Equivariant Graph Neural Networks: A Survey","snippet":"This work analyzes and classify existing methods into three groups regarding how the message passing and aggregation in GNNs are represented, and summarizes ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/349787221_Barlow_Twins_Self-Supervised_Learning_via_Redundancy_Reduction","title":"Barlow Twins: Self-Supervised Learning via Redundancy ...","snippet":"Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network ...","ref_id":null,"pub_date":1614816000.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0893608024009092","title":"Lie group convolution neural networks with scale-rotation ...","snippet":"by W Qiao \u00b7 2025 \u00b7 Cited by 1 \u2014 This study proposes a SIM(2) Lie group-CNN, which can simultaneously keep scale, rotation, and translation equivariance for image classification tasks.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC12058038/","title":"Contrastive-Equivariant Self-Supervised Learning ...","snippet":"by T Yerxa \u00b7 2024 \u00b7 Cited by 2 \u2014 Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pages 12310\u201312320. PMLR, 2021. [Google ...","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"www.mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/6/466","title":"MV\u2013MR: Multi-Views and Multi-Representations for Self- ...","snippet":"by V Kinakh \u00b7 2024 \u00b7 Cited by 4 \u2014 The VICReg [11] method extends the Barlow Twins [7] approach by imposing an additional constraint on the distance between the embeddings with and without ...","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.1007/s10462-023-10502-7","title":"Geometric deep learning and equivariant neural networks","snippet":"by JE Gerken \u00b7 2023 \u00b7 Cited by 108 \u2014 We survey the mathematical foundations of geometric deep learning, focusing on group equivariant and gauge equivariant neural networks.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.ecva.net","entries":[{"type":"search_result","url":"https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04236.pdf","title":"Decoupling Common and Unique Representations for ...","snippet":"by Y Wang \u00b7 Cited by 26 \u2014 In practice, this can be seen as a natural exten- sion of Barlow Twins [57], one self-supervised learning technique that conducts redundancy reduction on the ...","ref_id":null,"pub_date":1721395708.0,"attribution":"www.ecva.net"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/m1v9hg/r_barlow_twins_selfsupervised_learning_via/","title":"[R] Barlow Twins: Self-Supervised Learning via ...","snippet":"The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks.","ref_id":null,"pub_date":1615374897.226,"attribution":"www.reddit.com"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for surveys on equivariant networks and invariances","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"8599bd73-a5b1-489c-a85b-66b44aacbb20","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"8599bd73-a5b1-489c-a85b-66b44aacbb20","children":["0aacd4ed-4c01-45d1-b787-61d9e9aee9c9"]},"0aacd4ed-4c01-45d1-b787-61d9e9aee9c9":{"id":"0aacd4ed-4c01-45d1-b787-61d9e9aee9c9","message":{"id":"0aacd4ed-4c01-45d1-b787-61d9e9aee9c9","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878595.250974,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Investigating quantization in representation learning","content":"I\u2019ll look into quantization and rate-distortion in representation learning, particularly with models like VQ-VAE (Oord 2017) and concepts like info-distillation. I'm also interested in \"semantic compression\" and \"optimal quantization of representations.\" There\u2019s also the question of vector quantization for world models, like RQVAE. Let's see what\u2019s up-to-date on this!","chunks":[],"finished":true}],"source_analysis_msg_id":"fb23efb8-60b7-4017-b5fe-b7fb6e3afe14"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"c808eb8d-d7b2-452b-a911-06bd255f87f6","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"c808eb8d-d7b2-452b-a911-06bd255f87f6","children":["e146eb25-0561-430a-a5d9-c679a31827b8"]},"e146eb25-0561-430a-a5d9-c679a31827b8":{"id":"e146eb25-0561-430a-a5d9-c679a31827b8","message":{"id":"e146eb25-0561-430a-a5d9-c679a31827b8","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878592.746164,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"rate\\u2013distortion representation learning survey 2023 2024\"}, {\"q\": \"vector quantized autoencoders representation learning VQ-VAE survey 2023 2024 2025\"}, {\"q\": \"information bottleneck quantization deep learning rate distortion 2024 2025\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"rate\u2013distortion representation learning survey 2023 2024"},{"type":"search","q":"vector quantized autoencoders representation learning VQ-VAE survey 2023 2024 2025"},{"type":"search","q":"information bottleneck quantization deep learning rate distortion 2024 2025"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching recent papers on quantization and rate-distortion","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"0aacd4ed-4c01-45d1-b787-61d9e9aee9c9","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"0aacd4ed-4c01-45d1-b787-61d9e9aee9c9","children":["3073f913-65e4-47c5-9c10-51d33aca4467"]},"3073f913-65e4-47c5-9c10-51d33aca4467":{"id":"3073f913-65e4-47c5-9c10-51d33aca4467","message":{"id":"3073f913-65e4-47c5-9c10-51d33aca4467","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878596.6189175,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2405.14221v1","title":"Survey on Visual Signal Coding and Processing with ...","snippet":"May 23, 2024 \u2014 Some works attempt to perform hierarchical processing in high-dimensional representation space to achieve a better rate-distortion balance [97, ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.10547v1","title":"Quantize-then-Rectify: Efficient VQ-VAE Training","snippet":"Jul 14, 2025 \u2014 We present Quantize-then-Rectify (ReVQ), a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2407.12288","title":"Information-Theoretic Foundations for Machine Learning","snippet":"by HJ Jeon \u00b7 2024 \u00b7 Cited by 3 \u2014 the rate-distortion function for the deep neural network data generating process. ... Deep learning and the information bottleneck principle.","ref_id":null,"pub_date":1747959149.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.07997v1","title":"MGVQ: Could VQ-VAE Beat VAE? A Generalizable ...","snippet":"Jul 10, 2025 \u2014 Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models that compress continuous visual data into discrete tokens.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.03504v1","title":"Information-Bottleneck Driven Binary Neural Network for ...","snippet":"In this paper, we propose Binarized Change Detection (BiCD), the first binary neural network (BNN) designed specifically for change detection.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2401.03717v3","title":"Universal Time-Series Representation Learning: A Survey","snippet":"Aug 27, 2024 \u2014 This survey first presents a novel taxonomy based on three fundamental elements in designing state-of-the-art universal representation learning methods for ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/1099-4300/27/4/373","title":"Rate\u2013Distortion\u2013Perception Trade-Off in Information ...","snippet":"by X Niu \u00b7 2025 \u00b7 Cited by 2 \u2014 For decades, rate\u2013distortion (RD) theory has served as the de facto standard for evaluating real-world applications involving compression and communication.","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/3/187","title":"Theory and Application of the Information Bottleneck Method","snippet":"by J Lewandowsky \u00b7 2024 \u00b7 Cited by 5 \u2014 The fundamental idea of the information bottleneck method is to determine which information should be preserved under the invoked compression.","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/10/836","title":"Information Bottleneck Driven Deep Video Compression\u2014 ...","snippet":"by T Leiderman \u00b7 2024 \u00b7 Cited by 1 \u2014 This study, inspired by information bottleneck (IB) theory, introduces a novel approach that combines IB theory with wavelet transform.","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"}]},{"type":"search_result_group","domain":"journals.plos.org","entries":[{"type":"search_result","url":"https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1012952","title":"The geometry of efficient codes: How rate-distortion trade-offs ...","snippet":"by L D\u2019Amato \u00b7 2025 \u00b7 Cited by 1 \u2014 Here we investigate how rate-distortion trade-offs shape the latent representations of images in generative models, specifically Beta Variational Autoencoders ...","ref_id":null,"pub_date":null,"attribution":"journals.plos.org"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1016/j.cviu.2025.104392","title":"HVQ-VAE: : Variational auto-encoder with hyperbolic ...","snippet":"Aug 6, 2025 \u2014 This paper studies the geometric prior of hyperbolic spaces as a way to improve the learning capacity of VQ-VAE. That being said, working with ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"link.springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10994-024-06708-7","title":"A survey on self-supervised methods for visual ...","snippet":"by T Uelwer \u00b7 2025 \u00b7 Cited by 2 \u2014 Geiping et al. (2023) provide an overview of foundational techniques and practical tips for successfully training self-supervised methods.","ref_id":null,"pub_date":null,"attribution":"link.springer.com"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0031320325001608","title":"Attention-based vector quantized variational autoencoder ...","snippet":"by Q Yu \u00b7 2025 \u00b7 Cited by 1 \u2014 This paper introduces a new framework that uses a vector quantized variational autoencoder (VQVAE) enhanced by orthogonal subspace constraints (OSC) and ...","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2666827024000483","title":"A comprehensive study of auto-encoders for anomaly ...","snippet":"by AA Neloy \u00b7 2024 \u00b7 Cited by 27 \u2014 This study systematically reviews 11 Auto-Encoder architectures categorized into three groups, aiming to differentiate their reconstruction ability.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S235286482400018X","title":"Rate distortion optimization for adaptive gradient ...","snippet":"by G Chen \u00b7 2024 \u00b7 Cited by 6 \u2014 We propose an adaptive gradient quantization approach that enhances communication efficiency. Aiming to minimize the total communication costs.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"link.aps.org","entries":[{"type":"search_result","url":"https://link.aps.org/doi/10.1103/PhysRevLett.132.197201","title":"Machine-Learning Optimized Measurements of Chaotic ...","snippet":"by KA Murphy \u00b7 2024 \u00b7 Cited by 9 \u2014 We establish an equivalence between a perfect measurement and a variant of the information bottleneck. As a consequence, we can employ machine learning to ...","ref_id":null,"pub_date":1715299200.0,"attribution":"link.aps.org"}]},{"type":"search_result_group","domain":"academic.oup.com","entries":[{"type":"search_result","url":"https://academic.oup.com/bioinformatics/article/41/2/btaf039/7989445","title":"SSL-VQ: vector-quantized variational autoencoders for semi ...","snippet":"by S Namba \u00b7 2025 \u2014 This study presents a novel machine learning approach using multimodal vector-quantized variational autoencoders (VQ-VAEs) for predicting therapeutic target ...","ref_id":null,"pub_date":null,"attribution":"academic.oup.com"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC12068621/","title":"The geometry of efficient codes: How rate-distortion trade ...","snippet":"by L D\u2019Amato \u00b7 2025 \u00b7 Cited by 1 \u2014 Here we investigate how rate-distortion trade-offs shape the latent representations of images in generative models, specifically Beta Variational Autoencoders.","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=sfTsvy05MX","title":"LL-VQ-VAE: Learnable Lattice Vector-Quantization For ...","snippet":"by A Khalil \u00b7 Cited by 2 \u2014 In this paper we introduce vector quantization and demonstrate its effectiveness for learning discrete representations.","ref_id":null,"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=LnKDcqOfgy","title":"Rate/Distortion Constrained Model Quantization for Efficient...","snippet":"by A Conzelmann \u2014 In this paper, we combine techniques from quantization and compression to propose an efficient and highly effective post-training compression method for large ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/381322776_Deep_FAVIB_Deep_Learning-Based_Forward-Aware_Quantization_via_Information_Bottleneck_Method","title":"(PDF) Deep FAVIB: Deep Learning-Based Forward-Aware ...","snippet":"Jun 12, 2024 \u2014 Deep FAVIB: Deep Learning-Based Forward-Aware Quantization via Information Bottleneck Method. June 2024. DOI:10.1109/ICC51166.2024.10622994.","ref_id":null,"pub_date":1717200000.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/338081829_On_the_information_bottleneck_theory_of_deep_learning","title":"On the information bottleneck theory of deep learning","snippet":"The information bottleneck (IB) (Lewandowsky and Bauch 2024) originates from rate distortion theory and aims to compress the source variable X into a ...","ref_id":null,"pub_date":1576800000.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"pubs.aip.org","entries":[{"type":"search_result","url":"https://pubs.aip.org/pof/article/37/8/085146/3358152/Vector-quantization-variational-autoencoder-for","title":"Vector quantization variational autoencoder for turbulent ...","snippet":"Aug 8, 2025 \u2014 This study explores the use of vector-quantized variational autoencoder (VQ-VAE) to reconstruct flow images across a range of resolutions ( ...","ref_id":null,"pub_date":null,"attribution":"pubs.aip.org"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/rese1f/Awesome-VQVAE","title":"Awesome-VQVAE","snippet":"A collection of resources and papers on Vector Quantized Variational Autoencoder (VQ-VAE) and its application.","ref_id":null,"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://neurips.cc/virtual/2023/workshop/66535","title":"Information-Theoretic Principles in Cognitive Systems ...","snippet":"Fri 12:40 p.m. - 1:30 p.m.. A Work in Progress: Tighter Bounds on the Information Bottleneck for Deep Learning ( Poster ) > link ... Rate-Distortion-Perception- ...","ref_id":null,"pub_date":null,"attribution":"neurips.cc"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching recent papers on quantization and rate-distortion","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"e146eb25-0561-430a-a5d9-c679a31827b8","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"e146eb25-0561-430a-a5d9-c679a31827b8","children":["c4ebeb04-f8b2-4a71-b320-5f02b11468ee"]},"c4ebeb04-f8b2-4a71-b320-5f02b11468ee":{"id":"c4ebeb04-f8b2-4a71-b320-5f02b11468ee","message":{"id":"c4ebeb04-f8b2-4a71-b320-5f02b11468ee","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878602.807283,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring category theory in ML","content":"Next, I\u2019ll explore the tie between category theory and compositionality in machine learning. I\u2019m thinking about \"Applied category theory in ML,\" Fong-Spivak's \"Seven Sketches in Compositionality,\" and \"Backprop as functor.\" Also, categorical probabilistic programming and compositionality in ML with energy-based models. Time for a deeper dive!","chunks":[],"finished":true}],"source_analysis_msg_id":"c54f4d92-0fd4-44f8-843d-4e494c2f1e15"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"3073f913-65e4-47c5-9c10-51d33aca4467","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"3073f913-65e4-47c5-9c10-51d33aca4467","children":["ac0444d4-7f26-400a-9134-8f6abe92f04a"]},"ac0444d4-7f26-400a-9134-8f6abe92f04a":{"id":"ac0444d4-7f26-400a-9134-8f6abe92f04a","message":{"id":"ac0444d4-7f26-400a-9134-8f6abe92f04a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878597.807134,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"categorical machine learning survey 2023 2024 applied category theory machine learning Fong Spivak Coecke review\"}, {\"q\": \"abstract interpretation for neural networks survey 2023 2024 Galois connection machine learning robustness certification\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"categorical machine learning survey 2023 2024 applied category theory machine learning Fong Spivak Coecke review"},{"type":"search","q":"abstract interpretation for neural networks survey 2023 2024 Galois connection machine learning robustness certification"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for applied category theory in machine learning","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"c4ebeb04-f8b2-4a71-b320-5f02b11468ee","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"c4ebeb04-f8b2-4a71-b320-5f02b11468ee","children":["1bc567a5-bd4a-447e-8e40-2c483ebf0400"]},"1bc567a5-bd4a-447e-8e40-2c483ebf0400":{"id":"1bc567a5-bd4a-447e-8e40-2c483ebf0400","message":{"id":"1bc567a5-bd4a-447e-8e40-2c483ebf0400","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878606.7081535,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/2106.07032","title":"Category Theory in Machine Learning","snippet":"by D Shiebler \u00b7 2021 \u00b7 Cited by 68 \u2014 It is therefore unsurprising that there is a burgeoning interest in applying category theory to machine learning.","ref_id":null,"pub_date":1623721161.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2410.23903","title":"Neural Network Verification with PyRAT","snippet":"by A Lemesle \u00b7 2024 \u00b7 Cited by 8 \u2014 To this end, we present PyRAT, a tool based on abstract interpretation to verify the safety and the robustness of neural networks. In this paper ...","ref_id":null,"pub_date":1730422365.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2508.10018","title":"Categorical Homotopy Theory for Large Language Models","snippet":"by S Mahadevan \u00b7 2025 \u2014 We present a detailed overview of application of categorical homotopy to LLMs, from higher algebraic K-theory to model categories, building on ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"link.springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10115-024-02108-4","title":"Robustness verification of k-nearest neighbors by abstract ...","snippet":"by N Fassina \u00b7 2024 \u2014 We study the certification of stability properties, such as robustness and individual fairness, of the k-nearest neighbor algorithm (kNN).","ref_id":null,"pub_date":null,"attribution":"link.springer.com"},{"type":"search_result","url":"https://link.springer.com/content/pdf/10.1007/s10115-024-02108-4.pdf","title":"Robustness verification of k-nearest neighbors by abstract ...","snippet":"by N Fassina \u00b7 2024 \u2014 We study the certification of stability properties, such as robustness and individual fairness, of the k-nearest neighbor algorithm (kNN). Our ...","ref_id":null,"pub_date":1720681451.0,"attribution":"link.springer.com"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers?q=Category+theory","title":"Daily Papers","snippet":"A category theory framework for Bayesian learning\u200b\u200b Inspired by the foundational works by Spivak and Fong and Cruttwell et al., we introduce a categorical ...","ref_id":null,"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"www.math.unipd.it","entries":[{"type":"search_result","url":"https://www.math.unipd.it/~ranzato/papers/kais24.pdf","title":"Robustness Verification of k-Nearest Neighbors by Abstract ...","snippet":"Abstract. We study the certification of stability properties, such as robustness and individ- ual fairness, of the k-Nearest Neighbor algorithm (kNN).","ref_id":null,"pub_date":1709722754.0,"attribution":"www.math.unipd.it"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf?id=KXiQI6ggFc","title":"A Pattern Language for Machine Learning Tasks","snippet":"by B Rodatz \u00b7 Cited by 1 \u2014 Our mathematical approach draws broadly from the field of Applied Category Theory (Fong. & Spivak, 2019a), particularly in the use of string diagrams for the ...","ref_id":null,"pub_date":1732760926.0,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/69f5e599b8a17628ac0063afc0d35a4d3fc77e82.pdf","title":"A Pattern Language for Machine Learning Tasks","snippet":"field of Applied Category Theory (Fong & Spivak, 2019a), particularly in the use of string diagrams for the higher-algebraic data of concurrently and ...","ref_id":null,"pub_date":1746888182.0,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.1145/3649309","title":"Accommodating Robustness by Abstract Interpretation","snippet":"by R Giacobazzi \u00b7 2024 \u00b7 Cited by 7 \u2014 Robustness is a key and desirable property of any classifying system, in particular, to avoid the ever-rising threat of adversarial attacks.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3649309","title":"Accommodating Robustness by Abstract Interpretation","snippet":"Apr 27, 2024 \u2014 Abstract Robustness is a key and desirable property of any classifying system, in particular, to avoid the ever-rising threat of adversarial attacks.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/370995475_Computing_with_Categories_in_Machine_Learning","title":"Computing with Categories in Machine Learning","snippet":"In this survey, we provide an overview of category theory-derived machine learning from four mainstream perspectives: gradient-based learning, probability- ...","ref_id":null,"pub_date":null,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/326639135_AI2_Safety_and_Robustness_Certification_of_Neural_Networks_with_Abstract_Interpretation","title":"Safety and Robustness Certification of Neural Networks ...","snippet":"Formal verification methods, such as abstract interpretation [4, 7, 10], are commonly used to ensure that neural networks perform correctly in all potential ...","ref_id":null,"pub_date":1525132800.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.cs.utexas.edu","entries":[{"type":"search_result","url":"https://www.cs.utexas.edu/~swarat/pubs/ai2.pdf","title":"AI : Safety and Robustness Certification of Neural Networks ...","snippet":"by T Gehr \u00b7 Cited by 1222 \u2014 With abstract interpretation, a neural network computation is overapproxi- mated using an abstract domain. An abstract domain consists of logical formulas that ...","ref_id":null,"pub_date":1521744146.0,"attribution":"www.cs.utexas.edu"}]},{"type":"search_result_group","domain":"www.brunogavranovic.com","entries":[{"type":"search_result","url":"https://www.brunogavranovic.com/assets/FundamentalComponentsOfDeepLearning.pdf","title":"Fundamental Components of Deep Learning","snippet":"by B Gavranovic \u00b7 2024 \u00b7 Cited by 15 \u2014 This thesis develops a novel mathematical foundation for deep learning based on the language of category theory.","ref_id":null,"pub_date":1710290287.0,"attribution":"www.brunogavranovic.com"}]},{"type":"search_result_group","domain":"plato.stanford.edu","entries":[{"type":"search_result","url":"https://plato.stanford.edu/archives/fall2024/entries/category-theory/","title":"Category Theory - Stanford Encyclopedia of Philosophy","snippet":"Dec 6, 1996 \u2014 Category theory is both an interesting object of philosophical study, and a potentially powerful formal tool for philosophical investigations of ...","ref_id":null,"pub_date":null,"attribution":"plato.stanford.edu"}]},{"type":"search_result_group","domain":"amslaurea.unibo.it","entries":[{"type":"search_result","url":"https://amslaurea.unibo.it/id/eprint/32595/1/francesco_riccardo_crescenzi.pdf","title":"TOWARDS A CATEGORICAL FOUNDATION OF DEEP LEARNING","snippet":"The unprecedented pace of machine learning research has lead to incredible advances, but also poses hard challenges. At present, the field lacks strong ...","ref_id":null,"pub_date":1726121935.0,"attribution":"amslaurea.unibo.it"}]},{"type":"search_result_group","domain":"www.mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/2075-1680/14/3/204","title":"Category-Theoretical and Topos-Theoretical Frameworks ...","snippet":"by Y Jia \u00b7 2025 \u00b7 Cited by 1 \u2014 In this survey, we provide an overview of category theory-derived machine learning from four mainstream perspectives.","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"}]},{"type":"search_result_group","domain":"lambdaviking.com","entries":[{"type":"search_result","url":"https://lambdaviking.com/blog/2022/improved-adversarial-robustness/","title":"Improved Adversarial Robustness via Abstract Interpretation","snippet":"Apr 15, 2022 \u2014 This paper improves methods for certifying the adversarial robustness of neural networks using techniques from abstract interpretation.","ref_id":null,"pub_date":null,"attribution":"lambdaviking.com"}]},{"type":"search_result_group","domain":"jmlr.org","entries":[{"type":"search_result","url":"https://jmlr.org/tmlr/papers/","title":"Accepted papers","snippet":"Accepted TMLR papers can be awarded a number of certifications by the action editors or editors-in-chief of TMLR. The current list of awarded certificates is","ref_id":null,"pub_date":null,"attribution":"jmlr.org"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for applied category theory in machine learning","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"ac0444d4-7f26-400a-9134-8f6abe92f04a","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"ac0444d4-7f26-400a-9134-8f6abe92f04a","children":["e1cbd521-9cfb-4da7-bd5f-dd77c58aa1bd"]},"e1cbd521-9cfb-4da7-bd5f-dd77c58aa1bd":{"id":"e1cbd521-9cfb-4da7-bd5f-dd77c58aa1bd","message":{"id":"e1cbd521-9cfb-4da7-bd5f-dd77c58aa1bd","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878610.596739,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring MDL and deep learning","content":"Now, I\u2019m wondering about recent works that connect Minimum Description Length (MDL) with deep learning. I need to explore how MDL might link with modern techniques in deep learning and search for material on this topic. Let\u2019s see what\u2019s new in this area!","chunks":[],"finished":true}],"source_analysis_msg_id":"61ad8d4d-edfe-4c2b-8316-9a7baf272567"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"1bc567a5-bd4a-447e-8e40-2c483ebf0400","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"1bc567a5-bd4a-447e-8e40-2c483ebf0400","children":["7452aef0-3dca-436f-91dc-9d08120e8685"]},"7452aef0-3dca-436f-91dc-9d08120e8685":{"id":"7452aef0-3dca-436f-91dc-9d08120e8685","message":{"id":"7452aef0-3dca-436f-91dc-9d08120e8685","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878608.2004,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Minimum Description Length deep learning 2023 2024 survey MDL generalization\"}, {\"q\": \"PAC-Bayes deep learning new bounds 2023 2024 survey\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Minimum Description Length deep learning 2023 2024 survey MDL generalization"},{"type":"search","q":"PAC-Bayes deep learning new bounds 2023 2024 survey"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching recent works on MDL and deep learning","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"e1cbd521-9cfb-4da7-bd5f-dd77c58aa1bd","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"e1cbd521-9cfb-4da7-bd5f-dd77c58aa1bd","children":["b52ac45d-5f14-4ca2-b41c-6f2e7253303b"]},"b52ac45d-5f14-4ca2-b41c-6f2e7253303b":{"id":"b52ac45d-5f14-4ca2-b41c-6f2e7253303b","message":{"id":"b52ac45d-5f14-4ca2-b41c-6f2e7253303b","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878611.230586,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2402.03254","title":"Minimum Description Length and Generalization Guarantees for Representation Learning","snippet":"","ref_id":null,"pub_date":1707156748.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2110.11216","title":"User-friendly introduction to PAC-Bayes bounds","snippet":"by P Alquier \u00b7 2021 \u00b7 Cited by 171 \u2014 In statistical learning theory, there is a set of tools designed to understand the generalization ability of such procedures: PAC-Bayesian or PAC-Bayes bounds.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.13398v1","title":"A Minimum Description Length Approach to Regularization ...","snippet":"May 19, 2025 \u2014 The MDL [38, 33] approach to learning avoids overfitting by balancing data fit with model complexity. Maximizing data fit is common to standard ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2406.05469v1","title":"Bayesian vs. PAC-Bayesian Deep Neural Network ...","snippet":"Jun 8, 2024 \u2014 We argue that neither the sampling nor the weighting in a Bayes ensemble are particularly well-suited for increasing generalization performance.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=Ncb0MvVqRV","title":"Minimum Description Length and Generalization ...","snippet":"by M Sefidgaran \u00b7 Cited by 10 \u2014 In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=MP8bmxvWt6","title":"Improving Generalization of Complex Models under ...","snippet":"by X Zhang \u00b7 Cited by 2 \u2014 In this paper, we introduce a new PAC-Bayes training algorithm with improved performance and reduced reliance on prior tuning. This is achieved ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/2b567c9aa100a8edcd13607a91ed3022-Paper-Conference.pdf","title":"PAC-Bayes-Chernoff bounds for unbounded losses","snippet":"by I Casado \u00b7 2024 \u00b7 Cited by 10 \u2014 We introduce a new PAC-Bayes oracle bound for unbounded losses that extends. Cram\u00e9r-Chernoff bounds to the PAC-Bayesian setting. The proof technique relies.","ref_id":null,"pub_date":1730288175.0,"attribution":"proceedings.neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2023/file/054e9f9a286671ababa3213d6e59c1c2-Paper-Conference.pdf","title":"Minimum Description Length and Generalization ...","snippet":"by M Sefidgaran \u00b7 2023 \u00b7 Cited by 10 \u2014 This paper establishes a framework using Minimum Description Length (MDL) of labels or representations to derive upper bounds on generalization error in ...","ref_id":null,"pub_date":1704186535.0,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"www.jmlr.org","entries":[{"type":"search_result","url":"https://www.jmlr.org/papers/volume25/23-1360/23-1360.pdf","title":"More PAC-Bayes bounds: From bounded losses, to ...","snippet":"by B Rodr\u00edguez-G\u00e1lvez \u00b7 2024 \u00b7 Cited by 14 \u2014 This paper presents new PAC-Bayes bounds for bounded losses, general tail behaviors, and anytime validity, including a strengthened Catoni's bound and new ...","ref_id":null,"pub_date":1711050099.0,"attribution":"www.jmlr.org"}]},{"type":"search_result_group","domain":"www.ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2024/0506.pdf","title":"Meta-Learning via PAC-Bayesian with Data-Dependent Prior","snippet":"by S Liu \u00b7 Cited by 1 \u2014 We provide a new framework for PAC-Bayesian meta- learning incorporating data-dependent priors. Our PAC-. Bayesian generalization bound is defined as a function ...","ref_id":null,"pub_date":1721930752.0,"attribution":"www.ijcai.org"}]},{"type":"search_result_group","domain":"www.mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/5/354","title":"Implications of Minimum Description Length for Adversarial ...","snippet":"by K Tiwari \u00b7 2024 \u00b7 Cited by 3 \u2014 We treat the behavior of an attack model as a complex causal mechanism and quantify its algorithmic information using the minimum description length (MDL) ...","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"}]},{"type":"search_result_group","domain":"link.springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10994-024-06690-0","title":"Misclassification bounds for PAC-Bayesian sparse deep ...","snippet":"by TT Mai \u00b7 2025 \u00b7 Cited by 6 \u2014 (2024), however, the prediction error for classification using Bayesian sparse deep learning remains unexplored. This study aims to address this ...","ref_id":null,"pub_date":null,"attribution":"link.springer.com"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0031320323005873","title":"Minimum description length clustering to measure ...","snippet":"by L Mahon \u00b7 2024 \u00b7 Cited by 14 \u2014 A new technique for measuring the amount of meaningful complexity in an image. Such a measure should not respond to random, unstructured variation, eg white ...","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"journals.plos.org","entries":[{"type":"search_result","url":"https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1012383","title":"Understanding dual process cognition via the minimum ...","snippet":"by T Moskovitz \u00b7 2024 \u00b7 Cited by 4 \u2014 The key insight is that dual-process structure can enhance adaptive behavior by allowing an agent to minimize the description length of its own ...","ref_id":null,"pub_date":null,"attribution":"journals.plos.org"}]},{"type":"search_result_group","domain":"proceedings.mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v247/kuzborskij24a/kuzborskij24a.pdf","title":"Better-than-KL PAC-Bayes Bounds","snippet":"by I Kuzborskij \u00b7 2024 \u00b7 Cited by 6 \u2014 This paper challenges the tightness of KL-divergence-based PAC-Bayes bounds, showing it's possible to achieve a strictly tighter bound.","ref_id":null,"pub_date":1718115956.0,"attribution":"proceedings.mlr.press"}]},{"type":"search_result_group","domain":"www.worldscientific.com","entries":[{"type":"search_result","url":"https://www.worldscientific.com/doi/10.1142/S2661335219300018?srsltid=AfmBOoopxnXEgOoXvccthpf0_OcXTv2co8pXpw65TcB8Na0RJOD6wfua","title":"Minimum description length revisited","snippet":"The Minimum Description Length (MDL) Principle is a theory of inductive inference that can be applied to general problems in statistics, machine learning and ...","ref_id":null,"pub_date":null,"attribution":"www.worldscientific.com"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/362426070_Minimum_Description_Length_Recurrent_Neural_Networks","title":"Minimum Description Length Recurrent Neural Networks","snippet":"PDF | We train neural networks to optimize a Minimum Description Length score, that is, to balance between the complexity of the network and its.","ref_id":null,"pub_date":1658880000.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"icml.cc","entries":[{"type":"search_result","url":"https://icml.cc/virtual/2023/26163","title":"PAC-Bayesian Adversarially Robust Generalization ...","snippet":"Workshop: 2nd ICML Workshop on New Frontiers in Adversarial Machine Learning. PAC-Bayesian Adversarially Robust Generalization Bounds for Deep Neural Networks.","ref_id":null,"pub_date":null,"attribution":"icml.cc"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching recent works on MDL and deep learning","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"7452aef0-3dca-436f-91dc-9d08120e8685","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"7452aef0-3dca-436f-91dc-9d08120e8685","children":["70249099-c065-41a5-8fba-3001d58982f9"]},"70249099-c065-41a5-8fba-3001d58982f9":{"id":"70249099-c065-41a5-8fba-3001d58982f9","message":{"id":"70249099-c065-41a5-8fba-3001d58982f9","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878616.013113,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Looking into mechanistic interpretability and explainable AI","content":"I'm considering the mechanistic interpretability direction, which seems promising. Also looking into methods for \"elucidation\" in AI, such as explainable AI, concept activation vectors, and concept bottleneck models. Time for a deeper dive into these topics and see the latest approaches in this area!","chunks":[],"finished":true}],"source_analysis_msg_id":"6aa42761-a274-44a5-9bbb-f556ed5e9f36"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"b52ac45d-5f14-4ca2-b41c-6f2e7253303b","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"b52ac45d-5f14-4ca2-b41c-6f2e7253303b","children":["5cf1ec9c-c77a-469c-916d-ff87139380e1"]},"5cf1ec9c-c77a-469c-916d-ff87139380e1":{"id":"5cf1ec9c-c77a-469c-916d-ff87139380e1","message":{"id":"5cf1ec9c-c77a-469c-916d-ff87139380e1","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878612.376973,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Concept bottleneck models 2020 2023 2024 updates survey\"}, {\"q\": \"Self-explaining neural networks 2019 2024 survey elucidation explanation improving performance via explanations\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Concept bottleneck models 2020 2023 2024 updates survey"},{"type":"search","q":"Self-explaining neural networks 2019 2024 survey elucidation explanation improving performance via explanations"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching recent literature on explainable AI models","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"70249099-c065-41a5-8fba-3001d58982f9","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"70249099-c065-41a5-8fba-3001d58982f9","children":["835e95bf-3efc-44e0-a2f4-48f1f7188614"]},"835e95bf-3efc-44e0-a2f4-48f1f7188614":{"id":"835e95bf-3efc-44e0-a2f4-48f1f7188614","message":{"id":"835e95bf-3efc-44e0-a2f4-48f1f7188614","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878617.794329,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2505.16705v1","title":"An Analysis of Concept Bottleneck Models: Measuring ...","snippet":"May 22, 2025 \u2014 In this study, we present the first systematic study of noise in CBMs and show that even moderate corruption simultaneously impairs prediction ...See more","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.18067v1","title":"Self-Explaining Neural Networks for Business Process ...","snippet":"Mar 23, 2025 \u2014 In this work, we introduce, to the best of our knowledge, the first self-explaining neural network architecture for predictive process monitoring.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2508.14562v1","title":"Locality-aware Concept Bottleneck Model","snippet":"2 days ago \u2014 Concept bottleneck models (CBMs) are inherently interpretable models that make predictions based on human-understandable visual cues, ...See more","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2410.02331v2","title":"Self-eXplainable AI for Medical Image Analysis","snippet":"Nov 15, 2024 \u2014 This approach allows models to generate inherent explanations that are closely aligned with their internal decision-making processes, enhancing ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2401.14142","title":"Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Probabilistic Interpretations","snippet":"","ref_id":null,"pub_date":1706186797.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=a8wjeqTZ9C","title":"Concept Bottleneck Models under Label Noise","snippet":"by S Park \u2014 We analyze the impact of label noise on concept bottleneck models, revealing significant performance issues, and propose sharpness-aware minimization to ...See more","ref_id":null,"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=8nuzsfiQfS","title":"Explain Yourself, Briefly! Self-Explaining Neural Networks ...","snippet":"by S Bassan \u00b7 Cited by 4 \u2014 A method to generate concise and faithful sufficient explanations for predictions using self-explaining neural networks.","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"people.csail.mit.edu","entries":[{"type":"search_result","url":"https://people.csail.mit.edu/davidam/docs/SENN.pdf","title":"Self-Explaining Neural Networks - People | MIT CSAIL","snippet":"by D Alvarez-Melis \u00b7 Cited by 2 \u2014 We progressively generalize various aspects of the simple prediction models to yield a rich class of complex models that are self-explaining: i.e., they provide ...","ref_id":null,"pub_date":1518471881.0,"attribution":"people.csail.mit.edu"}]},{"type":"search_result_group","domain":"www.diva-portal.org","entries":[{"type":"search_result","url":"https://www.diva-portal.org/smash/get/diva2%3A1917781/FULLTEXT01.pdf","title":"Self-explaining Neural Networks in Reinforcement Learning","snippet":"by K Nowosadko \u00b7 2024 \u2014 Abstract. This thesis explores the integration of self-explaining neural networks (SENNs) into the field of reinforcement learning (RL), ...","ref_id":null,"pub_date":null,"attribution":"www.diva-portal.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/392466646_A_Comprehensive_Survey_on_the_Risks_and_Limitations_of_Concept-based_Models","title":"A Comprehensive Survey on the Risks and Limitations ...","snippet":"Jun 8, 2025 \u2014 PDF | Concept-based Models are a class of inherently explainable networks that improve upon standard Deep Neural Networks by providing a ...","ref_id":null,"pub_date":1748044800.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/355223733_Self-explaining_Neural_Network_with_Plausible_Explanations","title":"Self-explaining Neural Network with Plausible Explanations","snippet":"Oct 9, 2021 \u2014 Our framework is self-explaining; relevance scores are generated for each concept to predict and explain in an end-to-end joint training scheme.","ref_id":null,"pub_date":1633737600.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"icml.cc","entries":[{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/45301","title":"ICML Poster Editable Concept Bottleneck Models","snippet":"Jul 17, 2025 \u2014 This research propose editable concept bottleneck models. While we acknowledge the importance of evaluating societal impacts, our analysis ...See more","ref_id":null,"pub_date":null,"attribution":"icml.cc"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0925231224009755","title":"Interpretability of deep neural networks: A review ...","snippet":"by T Antamis \u00b7 2024 \u00b7 Cited by 19 \u2014 The following paper aims to provide a thorough overview of the methods that have so far been developed to explain deep neural networks.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/95178","title":"NeurIPS Poster Coarse-to-Fine Concept Bottleneck Models","snippet":"This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly ...See more","ref_id":null,"pub_date":1727308800.0,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"www.mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/2673-7426/5/3/36","title":"Self-Explaining Neural Networks for Food Recognition and ...","snippet":"by Z Revesai \u00b7 2025 \u2014 We developed a novel self-explaining neural architecture that integrates specialised attention mechanisms with temporal modules within a streamlined framework.","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/5c7894ac8788555f1cecf536f1e0fd35-Paper-Conference.pdf","title":"Stochastic Concept Bottleneck Models","snippet":"by M Vandenhirtz \u00b7 2024 \u00b7 Cited by 19 \u2014 Concept Bottleneck Models (CBMs) have emerged as a promising interpretable method whose final prediction is based on intermediate, human-understandable.See more","ref_id":null,"pub_date":1729153503.0,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"ojs.aaai.org","entries":[{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/35495/37650","title":"Explanation Bottleneck Models","snippet":"by S Yamaguchi \u00b7 2025 \u2014 Introduc- ing concept bottleneck models (CBMs, Koh et al. (2020)) is a promising approach to interpreting the output of deep models. In contrast to black-box ...See more","ref_id":null,"pub_date":null,"attribution":"ojs.aaai.org"}]},{"type":"search_result_group","domain":"openaccess.thecvf.com","entries":[{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Language_Guided_Concept_Bottleneck_Models_for_Interpretable_Continual_Learning_CVPR_2025_paper.pdf","title":"Language Guided Concept Bottleneck Models for ...","snippet":"by L Yu \u00b7 2025 \u00b7 Cited by 3 \u2014 Concept Bottleneck Models (CBMs) [22, 54] are a form of interpretable machine learning model that enhance trans- parency by organizing the learning process ...See more","ref_id":null,"pub_date":null,"attribution":"openaccess.thecvf.com"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.1145/3459637.3482306","title":"Towards Self-Explainable Graph Neural Network","snippet":"by E Dai \u00b7 2021 \u00b7 Cited by 113 \u2014 In this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2024.findings-eacl.67.pdf","title":"Explaining Language Model Predictions with High-Impact ...","snippet":"by R Zhao \u00b7 2024 \u00b7 Cited by 9 \u2014 Koh et al. (2020) provides a complete survey on concept bottleneck models and their interventions. Yeh et al. (2020) proposes an ...See more","ref_id":null,"pub_date":1709283058.0,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"direct.mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/coli/article/50/2/657/119158/Towards-Faithful-Model-Explanation-in-NLP-A-Survey","title":"Towards Faithful Model Explanation in NLP: A Survey","snippet":"by Q Lyu \u00b7 2024 \u00b7 Cited by 171 \u2014 Explainability allows us to find where a model succeeds or fails, and fix the weaknesses before they can be exploited by adversaries. For example, if a model ...","ref_id":null,"pub_date":null,"attribution":"direct.mit.edu"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching recent literature on explainable AI models","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"5cf1ec9c-c77a-469c-916d-ff87139380e1","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"5cf1ec9c-c77a-469c-916d-ff87139380e1","children":["dd8e2f2d-1302-4b72-a8f6-58f3ce724650"]},"dd8e2f2d-1302-4b72-a8f6-58f3ce724650":{"id":"dd8e2f2d-1302-4b72-a8f6-58f3ce724650","message":{"id":"dd8e2f2d-1302-4b72-a8f6-58f3ce724650","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878622.451628,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring curiosity and information creation","content":"I\u2019m thinking about exploration and curiosity as methods for creating information\u2014for instance, intrinsic motivation like information gain, empowerment, and surprise. Examples include VIME (2016), Empowerment (2005, 2010), and curiosity-driven approaches like Random Network Distillation (2018) and recent work in information-theoretic curiosity (2024). Time to look into those!","chunks":[],"finished":true}],"source_analysis_msg_id":"757f9439-1696-4a83-9135-00649e6178ee"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"835e95bf-3efc-44e0-a2f4-48f1f7188614","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"835e95bf-3efc-44e0-a2f4-48f1f7188614","children":["ccf7d934-d4b0-4391-a0c4-a3cd4fac539a"]},"ccf7d934-d4b0-4391-a0c4-a3cd4fac539a":{"id":"ccf7d934-d4b0-4391-a0c4-a3cd4fac539a","message":{"id":"ccf7d934-d4b0-4391-a0c4-a3cd4fac539a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878619.08586,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"intrinsic motivation information gain reinforcement learning 2023 2024 survey curiosity empowerment\"}, {\"q\": \"active learning for world models information gain 2024 2025 survey\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"intrinsic motivation information gain reinforcement learning 2023 2024 survey curiosity empowerment"},{"type":"search","q":"active learning for world models information gain 2024 2025 survey"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for recent work on intrinsic motivation and curiosity","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"dd8e2f2d-1302-4b72-a8f6-58f3ce724650","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"dd8e2f2d-1302-4b72-a8f6-58f3ce724650","children":["a2085a37-6460-4664-a6f3-46e10ebc2b0e"]},"a2085a37-6460-4664-a6f3-46e10ebc2b0e":{"id":"a2085a37-6460-4664-a6f3-46e10ebc2b0e","message":{"id":"a2085a37-6460-4664-a6f3-46e10ebc2b0e","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878623.3920295,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/2507.08210","title":"From Curiosity to Competence: How World Models Interact ...","snippet":"by F Mantiuk \u00b7 2025 \u00b7 Cited by 1 \u2014 We investigated how intrinsic motivation mechanisms\u2014 specifically curiosity (novelty and information gain) and competence (empowerment)\u2014guide ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.23631v2","title":"Intrinsically-Motivated Humans and Agents in Open-World ...","snippet":"May 27, 2025 \u2014 Hassas. An information-theoretic perspective on intrinsic motivation in reinforcement learning: a survey. Entropy, 25(2):327, 2023.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2502.11767","title":"A Survey of LLM-based Active Learning","snippet":"by Y Xia \u00b7 2025 \u00b7 Cited by 6 \u2014 Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.13200v1","title":"Learning To Explore With Predictive World Model Via Self- ...","snippet":"Feb 18, 2025 \u2014 Initially, intrinsic motivation used concepts of emotion, surprise, empowerment, entropy, and information gain to formulate intrinsic rewards.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"link.springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/article/10.1007/s41060-025-00777-0","title":"Large-scale simulation study of active learning models for ...","snippet":"by JJ Teijema \u00b7 2025 \u00b7 Cited by 9 \u2014 This study advocates for large-scale simulations as the gold standard for evaluating active learning models in systematic review screening.","ref_id":null,"pub_date":null,"attribution":"link.springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10994-023-06454-2","title":"Active learning for data streams: a survey | Machine Learning","snippet":"by D Cacciarelli \u00b7 2024 \u00b7 Cited by 80 \u2014 Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream.","ref_id":null,"pub_date":null,"attribution":"link.springer.com"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41467-024-49711-1","title":"Complex behavior from intrinsic motivation to occupy future ...","snippet":"by J Ram\u00edrez-Ruiz \u00b7 2024 \u00b7 Cited by 10 \u2014 However, intrinsic motivations, such as curiosity, push agents to visit new states by performing novel courses of action, which helps learning ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41586-025-08744-2","title":"Mastering diverse control tasks through world models","snippet":"by D Hafner \u00b7 2025 \u00b7 Cited by 60 \u2014 Here we present the third generation of Dreamer, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"spj.science.org","entries":[{"type":"search_result","url":"https://spj.science.org/doi/10.34133/icomputing.0058","title":"A Survey of Deep Active Learning for Foundation Models","snippet":"by T Wan \u00b7 2023 \u00b7 Cited by 20 \u2014 This survey endeavors to provide a comprehensive overview of DeepAL. Specifically, we first analyze and summarize various sample query strategies.","ref_id":null,"pub_date":null,"attribution":"spj.science.org"}]},{"type":"search_result_group","domain":"www.scribd.com","entries":[{"type":"search_result","url":"https://www.scribd.com/document/735738281/1908-06976v2","title":"A Survey On Intrinsic Motivation in Reinforcement Learning","snippet":"It surveys the role of intrinsic motivation in deep reinforcement learning to address these issues. Intrinsic motivation could help agents learn skills in a ...","ref_id":null,"pub_date":null,"attribution":"www.scribd.com"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/368443560_An_Information-Theoretic_Perspective_on_Intrinsic_Motivation_in_Reinforcement_Learning_A_Survey","title":"(PDF) An Information-Theoretic Perspective on Intrinsic ...","snippet":"Feb 7, 2023 \u2014 For several years now, IM has been widely used in RL, fostered by important results and. the emergence of deep learning.","ref_id":null,"pub_date":1675987200.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/390354185_Intrinsically-Motivated_Humans_and_Agents_in_Open-World_Exploration","title":"Intrinsically-Motivated Humans and Agents in Open-World ...","snippet":"Apr 3, 2025 \u2014 An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey. Article. Full-text available. Feb 2023 ...","ref_id":null,"pub_date":1743292800.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"hai.stanford.edu","entries":[{"type":"search_result","url":"https://hai.stanford.edu/ai-index/2025-ai-index-report","title":"The 2025 AI Index Report | Stanford HAI","snippet":"In 2024, U.S.-based institutions produced 40 notable AI models, significantly outpacing China's 15 and Europe's three. While the U.S. maintains its lead in ...","ref_id":null,"pub_date":null,"attribution":"hai.stanford.edu"}]},{"type":"search_result_group","domain":"ewrl.wordpress.com","entries":[{"type":"search_result","url":"https://ewrl.wordpress.com/wp-content/uploads/2023/09/georg-martius-intrinsic-motivation-in-reinforcement-learning.pdf","title":"Intrinsic Motivation in Reinforcement Learning","snippet":"\u27a2 Intrinsic motivations help us to formalize exploration strategies inductive bias to specify downstream task families. \u27a2 Model-based planning + predictive ...","ref_id":null,"pub_date":null,"attribution":"ewrl.wordpress.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=HkmymFPODz","title":"Deep Active Learning in the Open World","snippet":"by T Xie \u00b7 Cited by 4 \u2014 We introduce ALOE, a novel active learning algorithm for open-world environments designed to enhance model adaptation by incorporating new OOD classes.","ref_id":null,"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=vgXI1Ws0ma","title":"Towards Empowerment Gain through Causal Structure ...","snippet":"by H Cao \u00b7 Cited by 1 \u2014 Empowerment as an intrinsic motivation enhances the ability of agents to actively control their environments by maximizing the mutual information between future ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2666764925000104","title":"Unlocking the power of machine learning in big data","snippet":"by FM Surur \u00b7 2025 \u00b7 Cited by 8 \u2014 This study conducted a scoping survey to define the role of ML in BD by exploring its history and evolution.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC9954873/","title":"An Information-Theoretic Perspective on Intrinsic ...","snippet":"by A Aubret \u00b7 2023 \u00b7 Cited by 61 \u2014 Now that we clarified the notion of intrinsic motivation, we study how to integrate intrinsic motivation in the RL framework. An extensive overview of IM can be ...","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"rlj.cs.umass.edu","entries":[{"type":"search_result","url":"https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_77.pdf","title":"Surprise-Adaptive Intrinsic Motivation for Unsupervised ...","snippet":"by A Hugessen \u2014 Reinforcement learning. RL is a learning paradigm for sequential decision-making problems. In. RL, an agent acts in an environment from which it receives ...","ref_id":null,"pub_date":1719415620.0,"attribution":"rlj.cs.umass.edu"}]},{"type":"search_result_group","domain":"icml.cc","entries":[{"type":"search_result","url":"https://icml.cc/virtual/2025/papers.html","title":"ICML 2025 Papers","snippet":"IMLS Archives, Getting Started, Schedule, Tutorials, Main Conference, Invited Talks, Orals, Spotlight, Posters, Awards, Test of Time Award Papers, Workshops","ref_id":null,"pub_date":null,"attribution":"icml.cc"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for recent work on intrinsic motivation and curiosity","classifier_response":"default","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"ccf7d934-d4b0-4391-a0c4-a3cd4fac539a","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"ccf7d934-d4b0-4391-a0c4-a3cd4fac539a","children":["de81c5f6-386c-418b-bcb6-0e3a417dcaf4"]},"de81c5f6-386c-418b-bcb6-0e3a417dcaf4":{"id":"de81c5f6-386c-418b-bcb6-0e3a417dcaf4","message":{"id":"de81c5f6-386c-418b-bcb6-0e3a417dcaf4","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878637.884389,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring compression and simplification","content":"I\u2019m thinking about \"compression progress\" in AI\u2014Schmidhuber's work comes to mind, even though it's older. Hutter\u2019s \u201cUniversal AI as compression\u201d is another relevant idea. Also, there's the concept of elucidation and simplification through distillation or pruning. I could touch on the Lottery Ticket Hypothesis (2019) and newer advancements, like 2024\u2019s sparsity-oriented pruning for LLMs.","chunks":[],"finished":true},{"summary":"Mapping user operations to research streams","content":"To keep things concise, I'll link each of the user\u2019s operations to matching research topics:\n\n- Generalization: length generalization, world models, invariance, program induction.\n- Specialization: mixture-of-experts, sparsely activated networks.\n- Elucidation: mechanistic interpretation, concept bottlenecks.\n- Abstraction: abstract interpretation, hierarchical RL, category theory.\n- Recursion: looped transformers, recurrence models.\n- Simplification: distillation, pruning, \"lottery ticket\", quantization.\n- Quantization: VQ-VAE, IB-driven methods, rate-distortion quantization. \n- Grokking phenomenon, minimum norm.\n- Parsimony: Occam\u2019s razor.","chunks":[],"finished":true},{"summary":"Mapping user operations to research streams","content":"To stay concise, I'll map the user\u2019s operations to matching research topics:\n\n- Generalization: length generalization, world models, invariance, program induction.\n- Specialization: MoE gating, sparsely activated networks.\n- Elucidation: mechanistic interpretation, concept bottlenecks.\n- Abstraction: abstract interpretation, hierarchical RL, category theory.\n- Recursion: looped transformers, recurrence models.\n- Simplification: distillation, pruning, lottery ticket, quantization.\n- Quantization: VQ-VAE, IB-driven methods, rate-distortion.\n- Interpolation: Grokking phenomenon, minimum norm.\n- Occam\u2019s razor: parsimony.\n\nI\u2019ll also need to structure a literature mapping and identify existing gaps. Citations come next!","chunks":[],"finished":true}],"source_analysis_msg_id":"12cf748b-739b-442e-934a-b30d02b01fb1"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"a2085a37-6460-4664-a6f3-46e10ebc2b0e","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"a2085a37-6460-4664-a6f3-46e10ebc2b0e","children":["866d6f59-cc93-4f8e-bc92-42b9f69ad2d7"]},"866d6f59-cc93-4f8e-bc92-42b9f69ad2d7":{"id":"866d6f59-cc93-4f8e-bc92-42b9f69ad2d7","message":{"id":"866d6f59-cc93-4f8e-bc92-42b9f69ad2d7","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878739.031505,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 3m 49s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":229,"request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"de81c5f6-386c-418b-bcb6-0e3a417dcaf4","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"de81c5f6-386c-418b-bcb6-0e3a417dcaf4","children":["04d1f02c-c4c6-4dc9-ad82-fef7be0dbfcd"]},"04d1f02c-c4c6-4dc9-ad82-fef7be0dbfcd":{"id":"04d1f02c-c4c6-4dc9-ad82-fef7be0dbfcd","message":{"id":"04d1f02c-c4c6-4dc9-ad82-fef7be0dbfcd","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878695.282922,"update_time":null,"content":{"content_type":"text","parts":["Short answer: yes\u2014many active research programs explicitly cast \u201cintelligence\u201d as the organization of information, and they line up cleanly with the operators you listed. Below is a compact map from those operators to concrete, recent threads of work, with representative sources.\n\n# Where your operators show up in current research\n\n**Compression / abstraction (what to keep vs. discard).**  \n- The *Information Bottleneck* (IB) and *rate\u2013distortion* view treat learned representations as lossy compressors that retain task-relevant bits; this has been sharpened for deep nets and used to analyze training/generalization since ~2018, with fresh theoretical and empirical updates through 2024\u20132025. \ue200cite\ue202turn0search0\ue202turn0search25\ue202turn0search20\ue201  \n- Minimum Description Length (MDL) gives generalization guarantees by bounding the description length of labels/latents\u2014explicitly targeting representation compressibility (incl. IB-style encoders). \ue200cite\ue202turn7search0\ue202turn7search6\ue201\n\n**Decision under information constraints (specialization vs. generality as a resource trade-off).**  \n- Information-constrained/\u201cbounded-rational\u201d RL formalizes policy quality vs. channel capacity via rate\u2013distortion or rational inattention; this has matured into practical Bayesian/RL formulations and cognitive models in 2024\u20132025. \ue200cite\ue202turn2search8\ue202turn2search2\ue202turn2search5\ue201\n\n**Invariance/equivariance (controlled generalization & interpolation).**  \n- Causal representation learning and invariant risk principles aim to extract factors that remain stable across environments\u2014precisely to improve OOD generalization; the literature is active, with both advances and critiques in 2024\u20132025. \ue200cite\ue202turn0search2\ue202turn0search13\ue202turn0search17\ue201  \n- Group-equivariant and geometric networks hard-code symmetry structure (rotations, permutations, E(n), etc.), reducing sample complexity via organized feature sharing. \ue200cite\ue202turn4search14\ue202turn4search16\ue201\n\n**World models / prediction as structure discovery (abstraction, extrapolation).**  \n- *JEPA* (I-JEPA/V-JEPA) trains by predicting missing content in latent space\u2014learning compact, factorized representations without generative decoders. \ue200cite\ue202turn2search3\ue202turn2search0\ue201  \n- Model-based RL (e.g., *DreamerV3*) uses learned dynamics to plan\u2014an explicit \u201corganize-to-act\u201d pipeline\u2014now demonstrated at scale across diverse domains (Nature, 2025). \ue200cite\ue202turn2search13\ue201\n\n**Elucidation (making internal organization explicit).**  \n- Mechanistic interpretability with **sparse autoencoders** decomposes activations into monosemantic features; 2024\u20132025 work studies scaling, limits, and open problems. \ue200cite\ue202turn1academia20\ue202turn1search3\ue202turn1search15\ue201  \n- Concept bottleneck and self-explaining models re-organize predictors around human concepts for editability and diagnosis, with new robustness/continual-learning variants in 2024\u20132025. \ue200cite\ue202turn8search12\ue202turn8search0\ue201\n\n**Abstraction \u21c4 concretization (sound refinement).**  \n- *Abstract interpretation*\u2014the formal Galois-connection framework\u2014has been adapted from program analysis to certify neural robustness and, more broadly, reason about ML systems with provable approximations. Tooling and applications expanded in 2024. \ue200cite\ue202turn6search9\ue202turn6search1\ue202turn6search15\ue201  \n- Parallel efforts use category-theoretic formalisms as unifying languages for composition and learning, with updated surveys through 2025. \ue200cite\ue202turn6search0\ue202turn6search16\ue201\n\n**Recursion & algorithmic extrapolation.**  \n- Length-generalization and algorithmic-task studies show that when a task admits a short \u201cprogram\u201d (e.g., in RASP), Transformers (often with looping/recurrence) can learn iterative algorithms that extrapolate to much longer inputs (new results through mid-2025). \ue200cite\ue202turn1search10\ue202turn1search4\ue202turn1search12\ue201\n\n**Simplification & quantization (structured discretization).**  \n- Rate\u2013distortion lenses are now used for network quantization/compression and for analyzing *discrete* representation learning (VQ-VAE and successors), including 2025 methods and RD-perception trade-off studies. \ue200cite\ue202turn5search18\ue202turn5search1\ue202turn5search3\ue201\n\n**Specialization (organized division of labor).**  \n- Mixture-of-Experts (MoE) architectures explicitly route tokens to specialized sub-models, scaling capacity while constraining compute; multiple 2024\u20132025 surveys systematize designs and training stability. \ue200cite\ue202turn10search1\ue202turn10search2\ue202turn10search4\ue201\n\n**Information-seeking (active learning, curiosity).**  \n- Intrinsic-motivation and empowerment formalize exploration as information gain/control; recent work connects these with learned world models and systematically studies trade-offs. \ue200cite\ue202turn9search16\ue202turn9search2\ue202turn9search0\ue201\n\n# Bottom line\nYour \u201cintelligence = ability to organize information\u201d hypothesis is not only echoed but operationalized across today\u2019s theory and practice: compress what matters (IB/MDL), structure perception and action under capacity constraints (rate\u2013distortion/RL), factor the world into invariant/equivariant pieces (causal & geometric learning), build predictive abstractions (world models/JEPA), expose and edit the internal structure (interpretability/CBMs), and use recursion to extrapolate algorithmically. The field\u2019s open problem is less *whether* to view intelligence this way than how to *unify* these strands into a single quantitative objective and set of benchmarks that reward a model\u2019s *organizational repertoire* rather than its raw score on IID tasks. The citations above give you the most relevant, recent entry points into each strand."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":"\ue200cite\ue202turn0search0\ue202turn0search25\ue202turn0search20\ue201","start_idx":699,"end_idx":746,"safe_urls":["https://arxiv.org/pdf/2502.14889?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","https://iclr.cc/virtual/2024/poster/18081?utm_source=chatgpt.com"],"refs":[],"alt":"([ACM Digital Library](https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com), [ICLR](https://iclr.cc/virtual/2024/poster/18081?utm_source=chatgpt.com), [arXiv](https://arxiv.org/pdf/2502.14889?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Survey on Information Bottleneck - ACM Digital Library","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","pub_date":null,"snippet":"by S Hu \u00b7 2024 \u00b7 Cited by 56 \u2014 This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"ACM Digital Library"},{"title":"Information Bottleneck Analysis of Deep Neural Networks ...","url":"https://iclr.cc/virtual/2024/poster/18081?utm_source=chatgpt.com","pub_date":1708560000.0,"snippet":"The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":25}],"hue":null,"attributions":null,"attribution":"ICLR"},{"title":"narrowing information bottleneck theory","url":"https://arxiv.org/pdf/2502.14889?utm_source=chatgpt.com","pub_date":1740358855.0,"snippet":"by Z Zhu \u00b7 2025 \u2014 Published as a conference paper at ICLR 2025 interpretability of deep learning. We then introduce our Narrowing Bottleneck Theory, which is.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":20}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn7search0\ue202turn7search6\ue201","start_idx":948,"end_idx":980,"safe_urls":["https://arxiv.org/abs/2402.03254?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper_files/paper/2023/file/054e9f9a286671ababa3213d6e59c1c2-Paper-Conference.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2402.03254?utm_source=chatgpt.com), [NeurIPS Proceedings](https://proceedings.neurips.cc/paper_files/paper/2023/file/054e9f9a286671ababa3213d6e59c1c2-Paper-Conference.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Minimum Description Length and Generalization Guarantees for Representation Learning","url":"https://arxiv.org/abs/2402.03254?utm_source=chatgpt.com","pub_date":1707156748.0,"snippet":"","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":7,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Minimum Description Length and Generalization ...","url":"https://proceedings.neurips.cc/paper_files/paper/2023/file/054e9f9a286671ababa3213d6e59c1c2-Paper-Conference.pdf?utm_source=chatgpt.com","pub_date":1704186535.0,"snippet":"by M Sefidgaran \u00b7 2023 \u00b7 Cited by 10 \u2014 This paper establishes a framework using Minimum Description Length (MDL) of labels or representations to derive upper bounds on generalization error in ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":7,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"NeurIPS Proceedings"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search8\ue202turn2search2\ue202turn2search5\ue201","start_idx":1316,"end_idx":1361,"safe_urls":["https://direct.mit.edu/neco/article/37/1/76/124827/Bounded-Rational-Decision-Networks-With-Belief?utm_source=chatgpt.com","https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11045037/?utm_source=chatgpt.com"],"refs":[],"alt":"([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11045037/?utm_source=chatgpt.com), [MIT Press Direct](https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11045037/?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 The preceding section provides a cursory overview of how rate-distortion theory accommodates capacity-limited learning within a Bayesian decision-making ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"PMC"},{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 proposals including information-theoretic bounded rationality (Gottwald & Braun ... Rate-distortion theory and computationally rational reinforcement learning.","attribution_segments":null,"supporting_websites":[{"title":"Bounded Rational Decision Networks With Belief ...","url":"https://direct.mit.edu/neco/article/37/1/76/124827/Bounded-Rational-Decision-Networks-With-Belief?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 12, 2024 \u2014 One prominent approach that deals with utility-information trade-offs in complex decision networks is bounded rationality. Herbert Simon (1943, ...","attribution":"MIT Press Direct"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":2},{"turn_index":2,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"MIT Press Direct"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search2\ue202turn0search13\ue202turn0search17\ue201","start_idx":1677,"end_idx":1724,"safe_urls":["https://arxiv.org/pdf/2405.01389?utm_source=chatgpt.com","https://openreview.net/forum?id=lpOC6s4BcM&utm_source=chatgpt.com","https://www.researchgate.net/publication/392430428_Causal_Representation_Learning_A_Quick_Survey?utm_source=chatgpt.com"],"refs":[],"alt":"([ResearchGate](https://www.researchgate.net/publication/392430428_Causal_Representation_Learning_A_Quick_Survey?utm_source=chatgpt.com), [arXiv](https://arxiv.org/pdf/2405.01389?utm_source=chatgpt.com), [OpenReview](https://openreview.net/forum?id=lpOC6s4BcM&utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Causal Representation Learning: A Quick Survey","url":"https://www.researchgate.net/publication/392430428_Causal_Representation_Learning_A_Quick_Survey?utm_source=chatgpt.com","pub_date":1721347200.0,"snippet":"Jun 7, 2025 \u2014 This survey presents recent advances in CRL in vision. Firstly, we introduce the basic concept of causal inference. Secondly, we analyze the CRL ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"ResearchGate"},{"title":"Invariant Risk Minimization Is A Total Variation Model","url":"https://arxiv.org/pdf/2405.01389?utm_source=chatgpt.com","pub_date":1716163976.0,"snippet":"by ZR Lai \u00b7 2024 \u00b7 Cited by 1 \u2014 Invariant risk minimization (IRM) is an arising ap- proach to generalize invariant features to different environments in machine learning. While most.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Invariance & Causal Representation Learning: Prospects ...","url":"https://openreview.net/forum?id=lpOC6s4BcM&utm_source=chatgpt.com","pub_date":null,"snippet":"by S Bing \u00b7 Cited by 3 \u2014 In this work, we explore this invariance principle as a candidate assumption to achieve identifiability of causal representations.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":17}],"hue":null,"attributions":null,"attribution":"OpenReview"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search14\ue202turn4search16\ue201","start_idx":1896,"end_idx":1930,"safe_urls":["https://arxiv.org/abs/1602.07576?utm_source=chatgpt.com","https://dl.acm.org/doi/abs/10.1007/s10462-023-10502-7?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/1602.07576?utm_source=chatgpt.com), [ACM Digital Library](https://dl.acm.org/doi/abs/10.1007/s10462-023-10502-7?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"[1602.07576] Group Equivariant Convolutional Networks","url":"https://arxiv.org/abs/1602.07576?utm_source=chatgpt.com","pub_date":null,"snippet":"by TS Cohen \u00b7 2016 \u00b7 Cited by 2660 \u2014 We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Geometric deep learning and equivariant neural networks","url":"https://dl.acm.org/doi/abs/10.1007/s10462-023-10502-7?utm_source=chatgpt.com","pub_date":null,"snippet":"by JE Gerken \u00b7 2023 \u00b7 Cited by 108 \u2014 We survey the mathematical foundations of geometric deep learning, focusing on group equivariant and gauge equivariant neural networks.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"ACM Digital Library"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search3\ue202turn2search0\ue201","start_idx":2169,"end_idx":2201,"safe_urls":["https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com), [Meta AI](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Self-Supervised Learning from Images with a Joint ...","url":"https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","pub_date":null,"snippet":"by M Assran \u00b7 2023 \u00b7 Cited by 579 \u2014 We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"V-JEPA: The next step toward advanced machine ...","url":"https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=chatgpt.com","pub_date":null,"snippet":"Feb 15, 2024 \u2014 V-JEPA is a non-generative model that learns by predicting missing or masked parts of a video in an abstract representation space.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"Meta AI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search13\ue201","start_idx":2375,"end_idx":2395,"safe_urls":["https://www.nature.com/articles/s41586-025-08744-2?utm_source=chatgpt.com"],"refs":[],"alt":"([Nature](https://www.nature.com/articles/s41586-025-08744-2?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Mastering diverse control tasks through world models","url":"https://www.nature.com/articles/s41586-025-08744-2?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Hafner \u00b7 2025 \u00b7 Cited by 60 \u2014 All Dreamer and PPO agents in this paper were trained on a single Nvidia A100 GPU each. Dreamer uses the 200 million model size by default.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1academia20\ue202turn1search3\ue202turn1search15\ue201","start_idx":2626,"end_idx":2675,"safe_urls":["https://arxiv.org/abs/2309.08600?utm_source=chatgpt.com","https://arxiv.org/html/2501.16496v1?utm_source=chatgpt.com","https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2309.08600?utm_source=chatgpt.com), [Transformer Circuits](https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Sparse Autoencoders Find Highly Interpretable Features in Language Models","url":"https://arxiv.org/abs/2309.08600?utm_source=chatgpt.com","pub_date":1694800615.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Open Problems in Mechanistic Interpretability","url":"https://arxiv.org/html/2501.16496v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Jan 27, 2025 \u2014 This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.","attribution":"arXiv"}],"refs":[{"turn_index":1,"ref_type":"academia","ref_index":20},{"turn_index":1,"ref_type":"search","ref_index":15}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Extracting Interpretable Features from Claude 3 Sonnet","url":"https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com","pub_date":null,"snippet":"May 21, 2024 \u2014 Eight months ago, we demonstrated that sparse autoencoders could recover monosemantic features from a small one-layer transformer.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"Transformer Circuits"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn8search12\ue202turn8search0\ue201","start_idx":2864,"end_idx":2897,"safe_urls":["https://arxiv.org/html/2505.16705v1?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper_files/paper/2024/file/5c7894ac8788555f1cecf536f1e0fd35-Paper-Conference.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([NeurIPS Proceedings](https://proceedings.neurips.cc/paper_files/paper/2024/file/5c7894ac8788555f1cecf536f1e0fd35-Paper-Conference.pdf?utm_source=chatgpt.com), [arXiv](https://arxiv.org/html/2505.16705v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Stochastic Concept Bottleneck Models","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/5c7894ac8788555f1cecf536f1e0fd35-Paper-Conference.pdf?utm_source=chatgpt.com","pub_date":1729153503.0,"snippet":"by M Vandenhirtz \u00b7 2024 \u00b7 Cited by 19 \u2014 Concept Bottleneck Models (CBMs) have emerged as a promising interpretable method whose final prediction is based on intermediate, human-understandable.See more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":8,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"NeurIPS Proceedings"},{"title":"An Analysis of Concept Bottleneck Models: Measuring ...","url":"https://arxiv.org/html/2505.16705v1?utm_source=chatgpt.com","pub_date":null,"snippet":"May 22, 2025 \u2014 In this study, we present the first systematic study of noise in CBMs and show that even moderate corruption simultaneously impairs prediction ...See more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":8,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn6search9\ue202turn6search1\ue202turn6search15\ue201","start_idx":3205,"end_idx":3251,"safe_urls":["https://arxiv.org/pdf/2410.23903?utm_source=chatgpt.com","https://link.springer.com/content/pdf/10.1007/s10115-024-02108-4.pdf?utm_source=chatgpt.com","https://www.cs.utexas.edu/~swarat/pubs/ai2.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([Department of Computer Science](https://www.cs.utexas.edu/~swarat/pubs/ai2.pdf?utm_source=chatgpt.com), [arXiv](https://arxiv.org/pdf/2410.23903?utm_source=chatgpt.com), [SpringerLink](https://link.springer.com/content/pdf/10.1007/s10115-024-02108-4.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI : Safety and Robustness Certification of Neural Networks ...","url":"https://www.cs.utexas.edu/~swarat/pubs/ai2.pdf?utm_source=chatgpt.com","pub_date":1521744146.0,"snippet":"by T Gehr \u00b7 Cited by 1222 \u2014 With abstract interpretation, a neural network computation is overapproxi- mated using an abstract domain. An abstract domain consists of logical formulas that ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":6,"ref_type":"search","ref_index":9}],"hue":null,"attributions":null,"attribution":"Department of Computer Science"},{"title":"Neural Network Verification with PyRAT","url":"https://arxiv.org/pdf/2410.23903?utm_source=chatgpt.com","pub_date":1730422365.0,"snippet":"by A Lemesle \u00b7 2024 \u00b7 Cited by 8 \u2014 To this end, we present PyRAT, a tool based on abstract interpretation to verify the safety and the robustness of neural networks. In this paper ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":6,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Robustness verification of k-nearest neighbors by abstract ...","url":"https://link.springer.com/content/pdf/10.1007/s10115-024-02108-4.pdf?utm_source=chatgpt.com","pub_date":1720681451.0,"snippet":"by N Fassina \u00b7 2024 \u2014 We study the certification of stability properties, such as robustness and individual fairness, of the k-nearest neighbor algorithm (kNN). Our ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":6,"ref_type":"search","ref_index":15}],"hue":null,"attributions":null,"attribution":"SpringerLink"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn6search0\ue202turn6search16\ue201","start_idx":3394,"end_idx":3427,"safe_urls":["https://arxiv.org/pdf/2106.07032?utm_source=chatgpt.com","https://www.mdpi.com/2075-1680/14/3/204?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/pdf/2106.07032?utm_source=chatgpt.com), [MDPI](https://www.mdpi.com/2075-1680/14/3/204?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Category Theory in Machine Learning","url":"https://arxiv.org/pdf/2106.07032?utm_source=chatgpt.com","pub_date":1623721161.0,"snippet":"by D Shiebler \u00b7 2021 \u00b7 Cited by 68 \u2014 It is therefore unsurprising that there is a burgeoning interest in applying category theory to machine learning.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":6,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Category-Theoretical and Topos-Theoretical Frameworks ...","url":"https://www.mdpi.com/2075-1680/14/3/204?utm_source=chatgpt.com","pub_date":null,"snippet":"by Y Jia \u00b7 2025 \u00b7 Cited by 1 \u2014 In this survey, we provide an overview of category theory-derived machine learning from four mainstream perspectives.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":6,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"MDPI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search10\ue202turn1search4\ue202turn1search12\ue201","start_idx":3738,"end_idx":3785,"safe_urls":["https://arxiv.org/html/2409.15647v5?utm_source=chatgpt.com","https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00663/120983/What-Formal-Languages-Can-Transformers-Express-A?utm_source=chatgpt.com","https://machinelearning.apple.com/research/transformers-learn?utm_source=chatgpt.com"],"refs":[],"alt":"([Apple Machine Learning Research](https://machinelearning.apple.com/research/transformers-learn?utm_source=chatgpt.com), [arXiv](https://arxiv.org/html/2409.15647v5?utm_source=chatgpt.com), [MIT Press Direct](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00663/120983/What-Formal-Languages-Can-Transformers-Express-A?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"What Algorithms can Transformers Learn? A Study in ...","url":"https://machinelearning.apple.com/research/transformers-learn?utm_source=chatgpt.com","pub_date":null,"snippet":"Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":10}],"hue":null,"attributions":null,"attribution":"Apple Machine Learning Research"},{"title":"Looped Transformers for Length Generalization","url":"https://arxiv.org/html/2409.15647v5?utm_source=chatgpt.com","pub_date":null,"snippet":"May 12, 2025 \u2014 In Zhou et al. (2024a) , they show algorithmic tasks that can be written as a RASP-L program can be easily learned by a Transformer in a length- ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"What Formal Languages Can Transformers Express? A ...","url":"https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00663/120983/What-Formal-Languages-Can-Transformers-Express-A?utm_source=chatgpt.com","pub_date":null,"snippet":"by L Strobl \u00b7 2024 \u00b7 Cited by 82 \u2014 2024 . What algorithms can Transformers learn? A study in length generalization . In. Proceedings of the Twelfth International Conference on ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"MIT Press Direct"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn5search18\ue202turn5search1\ue202turn5search3\ue201","start_idx":4065,"end_idx":4111,"safe_urls":["https://arxiv.org/html/2507.10547v1?utm_source=chatgpt.com","https://openreview.net/forum?id=LnKDcqOfgy&utm_source=chatgpt.com","https://www.mdpi.com/1099-4300/27/4/373?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenReview](https://openreview.net/forum?id=LnKDcqOfgy&utm_source=chatgpt.com), [arXiv](https://arxiv.org/html/2507.10547v1?utm_source=chatgpt.com), [MDPI](https://www.mdpi.com/1099-4300/27/4/373?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Rate/Distortion Constrained Model Quantization for Efficient...","url":"https://openreview.net/forum?id=LnKDcqOfgy&utm_source=chatgpt.com","pub_date":null,"snippet":"by A Conzelmann \u2014 In this paper, we combine techniques from quantization and compression to propose an efficient and highly effective post-training compression method for large ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":5,"ref_type":"search","ref_index":18}],"hue":null,"attributions":null,"attribution":"OpenReview"},{"title":"Quantize-then-Rectify: Efficient VQ-VAE Training","url":"https://arxiv.org/html/2507.10547v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 14, 2025 \u2014 We present Quantize-then-Rectify (ReVQ), a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":5,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Rate\u2013Distortion\u2013Perception Trade-Off in Information ...","url":"https://www.mdpi.com/1099-4300/27/4/373?utm_source=chatgpt.com","pub_date":null,"snippet":"by X Niu \u00b7 2025 \u00b7 Cited by 2 \u2014 For decades, rate\u2013distortion (RD) theory has served as the de facto standard for evaluating real-world applications involving compression and communication.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":5,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"MDPI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn10search1\ue202turn10search2\ue202turn10search4\ue201","start_idx":4373,"end_idx":4421,"safe_urls":["https://arxiv.org/abs/2407.06204?utm_source=chatgpt.com","https://arxiv.org/html/2503.07137v1?utm_source=chatgpt.com","https://www.computer.org/csdl/journal/tk/2025/07/10937907/25n2xHILEpG?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2407.06204?utm_source=chatgpt.com), [IEEE Computer Society](https://www.computer.org/csdl/journal/tk/2025/07/10937907/25n2xHILEpG?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Survey on Mixture of Experts","url":"https://arxiv.org/abs/2407.06204?utm_source=chatgpt.com","pub_date":1719419673.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"A Comprehensive Survey of Mixture-of-Experts: Algorithms ...","url":"https://arxiv.org/html/2503.07137v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Mar 10, 2025 \u2014 In this paper, we first introduce the basic design of MoE, including gating functions, expert networks, routing mechanisms, training strategies, ...","attribution":"arXiv"}],"refs":[{"turn_index":10,"ref_type":"search","ref_index":1},{"turn_index":10,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"A Survey on Mixture of Experts in Large Language Models","url":"https://www.computer.org/csdl/journal/tk/2025/07/10937907/25n2xHILEpG?utm_source=chatgpt.com","pub_date":null,"snippet":"by W Cai \u00b7 2025 \u00b7 Cited by 55 \u2014 MoE leverages a collection of specialized models and a gating mechanism to dynamically select the appropriate \u201cexpert networks\u201d for processing a given input.See more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":10,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"IEEE Computer Society"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn9search16\ue202turn9search2\ue202turn9search0\ue201","start_idx":4661,"end_idx":4707,"safe_urls":["https://arxiv.org/pdf/2507.08210?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC9954873/?utm_source=chatgpt.com","https://www.nature.com/articles/s41467-024-49711-1?utm_source=chatgpt.com"],"refs":[],"alt":"([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9954873/?utm_source=chatgpt.com), [Nature](https://www.nature.com/articles/s41467-024-49711-1?utm_source=chatgpt.com), [arXiv](https://arxiv.org/pdf/2507.08210?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"An Information-Theoretic Perspective on Intrinsic ...","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC9954873/?utm_source=chatgpt.com","pub_date":null,"snippet":"by A Aubret \u00b7 2023 \u00b7 Cited by 61 \u2014 Now that we clarified the notion of intrinsic motivation, we study how to integrate intrinsic motivation in the RL framework. An extensive overview of IM can be ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":9,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"PMC"},{"title":"Complex behavior from intrinsic motivation to occupy future ...","url":"https://www.nature.com/articles/s41467-024-49711-1?utm_source=chatgpt.com","pub_date":null,"snippet":"by J Ram\u00edrez-Ruiz \u00b7 2024 \u00b7 Cited by 10 \u2014 However, intrinsic motivations, such as curiosity, push agents to visit new states by performing novel courses of action, which helps learning ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":9,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"Nature"},{"title":"From Curiosity to Competence: How World Models Interact ...","url":"https://arxiv.org/pdf/2507.08210?utm_source=chatgpt.com","pub_date":null,"snippet":"by F Mantiuk \u00b7 2025 \u00b7 Cited by 1 \u2014 We investigated how intrinsic motivation mechanisms\u2014 specifically curiosity (novelty and information gain) and competence (empowerment)\u2014guide ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":9,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":" ","start_idx":5564,"end_idx":5564,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"A Survey on Information Bottleneck - ACM Digital Library","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","attribution":"ACM Digital Library"},{"title":"Information Bottleneck Analysis of Deep Neural Networks ...","url":"https://iclr.cc/virtual/2024/poster/18081?utm_source=chatgpt.com","attribution":"ICLR"},{"title":"narrowing information bottleneck theory","url":"https://arxiv.org/pdf/2502.14889?utm_source=chatgpt.com","attribution":"arXiv"},{"title":"Minimum Description Length and Generalization ...","url":"https://proceedings.neurips.cc/paper_files/paper/2023/file/054e9f9a286671ababa3213d6e59c1c2-Paper-Conference.pdf?utm_source=chatgpt.com","attribution":"NeurIPS Proceedings"},{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11045037/?utm_source=chatgpt.com","attribution":"PMC"},{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","attribution":"MIT Press Direct"},{"title":"Causal Representation Learning: A Quick Survey","url":"https://www.researchgate.net/publication/392430428_Causal_Representation_Learning_A_Quick_Survey?utm_source=chatgpt.com","attribution":"ResearchGate"},{"title":"Invariance & Causal Representation Learning: Prospects ...","url":"https://openreview.net/forum?id=lpOC6s4BcM&utm_source=chatgpt.com","attribution":"OpenReview"},{"title":"V-JEPA: The next step toward advanced machine ...","url":"https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=chatgpt.com","attribution":"Meta AI"},{"title":"Mastering diverse control tasks through world models","url":"https://www.nature.com/articles/s41586-025-08744-2?utm_source=chatgpt.com","attribution":"Nature"},{"title":"Extracting Interpretable Features from Claude 3 Sonnet","url":"https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com","attribution":"Transformer Circuits"},{"title":"AI : Safety and Robustness Certification of Neural Networks ...","url":"https://www.cs.utexas.edu/~swarat/pubs/ai2.pdf?utm_source=chatgpt.com","attribution":"Department of Computer Science"},{"title":"Robustness verification of k-nearest neighbors by abstract ...","url":"https://link.springer.com/content/pdf/10.1007/s10115-024-02108-4.pdf?utm_source=chatgpt.com","attribution":"SpringerLink"},{"title":"Category-Theoretical and Topos-Theoretical Frameworks ...","url":"https://www.mdpi.com/2075-1680/14/3/204?utm_source=chatgpt.com","attribution":"MDPI"},{"title":"What Algorithms can Transformers Learn? A Study in ...","url":"https://machinelearning.apple.com/research/transformers-learn?utm_source=chatgpt.com","attribution":"Apple Machine Learning Research"},{"title":"A Survey on Mixture of Experts in Large Language Models","url":"https://www.computer.org/csdl/journal/tk/2025/07/10937907/25n2xHILEpG?utm_source=chatgpt.com","attribution":"IEEE Computer Society"}],"has_images":false}],"safe_urls":["https://arxiv.org/pdf/2502.14889?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","https://iclr.cc/virtual/2024/poster/18081?utm_source=chatgpt.com","https://arxiv.org/abs/2402.03254?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper_files/paper/2023/file/054e9f9a286671ababa3213d6e59c1c2-Paper-Conference.pdf?utm_source=chatgpt.com","https://direct.mit.edu/neco/article/37/1/76/124827/Bounded-Rational-Decision-Networks-With-Belief?utm_source=chatgpt.com","https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11045037/?utm_source=chatgpt.com","https://arxiv.org/pdf/2405.01389?utm_source=chatgpt.com","https://openreview.net/forum?id=lpOC6s4BcM&utm_source=chatgpt.com","https://www.researchgate.net/publication/392430428_Causal_Representation_Learning_A_Quick_Survey?utm_source=chatgpt.com","https://arxiv.org/abs/1602.07576?utm_source=chatgpt.com","https://dl.acm.org/doi/abs/10.1007/s10462-023-10502-7?utm_source=chatgpt.com","https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-025-08744-2?utm_source=chatgpt.com","https://arxiv.org/abs/2309.08600?utm_source=chatgpt.com","https://arxiv.org/html/2501.16496v1?utm_source=chatgpt.com","https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com","https://arxiv.org/html/2505.16705v1?utm_source=chatgpt.com","https://proceedings.neurips.cc/paper_files/paper/2024/file/5c7894ac8788555f1cecf536f1e0fd35-Paper-Conference.pdf?utm_source=chatgpt.com","https://arxiv.org/pdf/2410.23903?utm_source=chatgpt.com","https://link.springer.com/content/pdf/10.1007/s10115-024-02108-4.pdf?utm_source=chatgpt.com","https://www.cs.utexas.edu/~swarat/pubs/ai2.pdf?utm_source=chatgpt.com","https://arxiv.org/pdf/2106.07032?utm_source=chatgpt.com","https://www.mdpi.com/2075-1680/14/3/204?utm_source=chatgpt.com","https://arxiv.org/html/2409.15647v5?utm_source=chatgpt.com","https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00663/120983/What-Formal-Languages-Can-Transformers-Express-A?utm_source=chatgpt.com","https://machinelearning.apple.com/research/transformers-learn?utm_source=chatgpt.com","https://arxiv.org/html/2507.10547v1?utm_source=chatgpt.com","https://openreview.net/forum?id=LnKDcqOfgy&utm_source=chatgpt.com","https://www.mdpi.com/1099-4300/27/4/373?utm_source=chatgpt.com","https://arxiv.org/abs/2407.06204?utm_source=chatgpt.com","https://arxiv.org/html/2503.07137v1?utm_source=chatgpt.com","https://www.computer.org/csdl/journal/tk/2025/07/10937907/25n2xHILEpG?utm_source=chatgpt.com","https://arxiv.org/pdf/2507.08210?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC9954873/?utm_source=chatgpt.com","https://www.nature.com/articles/s41467-024-49711-1?utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2401.17541v3?utm_source=chatgpt.com","title":"Towards Understanding Variants of Invariant Risk ...","snippet":"Responding to these limitations, there has been a surge in research focusing on methodologies like Invariant Risk Minimization (IRM) (Arjovsky et al., 2020) .","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.23151v1?utm_source=chatgpt.com","title":"Abstractions of Sequences, Functions and Operators","snippet":"Jul 30, 2025 \u2014 We present theoretical and practical results on the order theory of lattices of functions, focusing on Galois connections that abstract (sets of) ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2102.11107?utm_source=chatgpt.com","title":"[2102.11107] Towards Causal Representation Learning","snippet":"by B Sch\u00f6lkopf \u00b7 2021 \u00b7 Cited by 1752 \u2014 In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2404.18775v4?utm_source=chatgpt.com","title":"Resource-rational reinforcement learning and ...","snippet":"Mar 19, 2025 \u2014 In this manuscript, we have proposed a new computational-level objective function for theoretical biology and theoretical neuroscience that ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2507.23151?utm_source=chatgpt.com","title":"Abstractions of Sequences, Functions and Operators","snippet":"by L Rustenholz \u00b7 2025 \u2014 Abstract We present theoretical and practical results on the order theory of lattices of functions, focusing on. Galois connections that ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2409.02772?utm_source=chatgpt.com","title":"unifying causal representation learning with","snippet":"by D Yao \u00b7 2024 \u00b7 Cited by 12 \u2014 Overall, this paper clarifies the role of causal assumptions in the discovery of causal variables and shifts the focus to preserving data ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.03504v1?utm_source=chatgpt.com","title":"Information-Bottleneck Driven Binary Neural Network for ...","snippet":"In this paper, we propose Binarized Change Detection (BiCD), the first binary neural network (BNN) designed specifically for change detection.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2409.15647?utm_source=chatgpt.com","title":"looped transformers for length generaliza","snippet":"by Y Fan \u00b7 2024 \u00b7 Cited by 19 \u2014 (2024a), they show algorithmic tasks that can be written as a RASP-L program can be easily learned by a Transformer in a length-generalizable ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.04878v1?utm_source=chatgpt.com","title":"Sparse Autoencoders Do Not Find Canonical Units of ...","snippet":"SAEs for Mechanistic Interpretability. SAEs have been demonstrated to recover sparse, monosemantic, and interpretable features from language model activations ( ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2501.16496v1?utm_source=chatgpt.com","title":"Open Problems in Mechanistic Interpretability","snippet":"Jan 27, 2025 \u2014 This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 840 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2301.04104?utm_source=chatgpt.com","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 837 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2403.00504?utm_source=chatgpt.com","title":"[2403.00504] Learning and Leveraging World Models in ...","snippet":"by Q Garrido \u00b7 2024 \u00b7 Cited by 40 \u2014 Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.12185v1?utm_source=chatgpt.com","title":"Satisficing Exploration for Deep Reinforcement Learning","snippet":"Jul 16, 2024 \u2014 In this paper, we build upon a long-line of work in the cognitive-science literature that formalizes the limits of bounded decision-makers ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.21872v1?utm_source=chatgpt.com","title":"A Survey of Continual Reinforcement Learning","snippet":"Jun 26, 2025 \u2014 In this survey, we provide a comprehensive examination of CRL, focusing on its core concepts, challenges, and methodologies. Firstly, we conduct ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2401.02349v2?utm_source=chatgpt.com","title":"A Survey Analyzing Generalization in Deep Reinforcement ...","snippet":"Oct 30, 2024 \u2014 Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2412.16395?utm_source=chatgpt.com","title":"arXiv:2412.16395v1 [cs.AI] 20 Dec 2024","snippet":"by RK Nayyar \u00b7 2024 \u00b7 Cited by 3 \u2014 Abstraction is key to scaling up reinforcement learning (RL). However, autonomously learning abstract state and action.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2401.02349v1?utm_source=chatgpt.com","title":"A Survey Analyzing Generalization in Deep Reinforcement ...","snippet":"In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2103.03230?utm_source=chatgpt.com","title":"Barlow Twins: Self-Supervised Learning via Redundancy ...","snippet":"by J Zbontar \u00b7 2021 \u00b7 Cited by 3133 \u2014 The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2412.01926?utm_source=chatgpt.com","title":"Higher-Order Redundancies in Self-Supervised ...","snippet":"by D Zollikofer \u00b7 2024 \u2014 Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pp. 12310\u201312320. PMLR ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2405.15429v5?utm_source=chatgpt.com","title":"E(\ud835\udc5b) Equivariant Topological Neural Networks","snippet":"Graph neural networks excel at modeling pairwise interactions, but they cannot flexibly accommodate higher-order interactions and features.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2405.14221v1?utm_source=chatgpt.com","title":"Survey on Visual Signal Coding and Processing with ...","snippet":"May 23, 2024 \u2014 Some works attempt to perform hierarchical processing in high-dimensional representation space to achieve a better rate-distortion balance [97, ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2407.12288?utm_source=chatgpt.com","title":"Information-Theoretic Foundations for Machine Learning","snippet":"by HJ Jeon \u00b7 2024 \u00b7 Cited by 3 \u2014 the rate-distortion function for the deep neural network data generating process. ... Deep learning and the information bottleneck principle.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.07997v1?utm_source=chatgpt.com","title":"MGVQ: Could VQ-VAE Beat VAE? A Generalizable ...","snippet":"Jul 10, 2025 \u2014 Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models that compress continuous visual data into discrete tokens.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2401.03717v3?utm_source=chatgpt.com","title":"Universal Time-Series Representation Learning: A Survey","snippet":"Aug 27, 2024 \u2014 This survey first presents a novel taxonomy based on three fundamental elements in designing state-of-the-art universal representation learning methods for ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2508.10018?utm_source=chatgpt.com","title":"Categorical Homotopy Theory for Large Language Models","snippet":"by S Mahadevan \u00b7 2025 \u2014 We present a detailed overview of application of categorical homotopy to LLMs, from higher algebraic K-theory to model categories, building on ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2110.11216?utm_source=chatgpt.com","title":"User-friendly introduction to PAC-Bayes bounds","snippet":"by P Alquier \u00b7 2021 \u00b7 Cited by 171 \u2014 In statistical learning theory, there is a set of tools designed to understand the generalization ability of such procedures: PAC-Bayesian or PAC-Bayes bounds.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.13398v1?utm_source=chatgpt.com","title":"A Minimum Description Length Approach to Regularization ...","snippet":"May 19, 2025 \u2014 The MDL [38, 33] approach to learning avoids overfitting by balancing data fit with model complexity. Maximizing data fit is common to standard ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2406.05469v1?utm_source=chatgpt.com","title":"Bayesian vs. PAC-Bayesian Deep Neural Network ...","snippet":"Jun 8, 2024 \u2014 We argue that neither the sampling nor the weighting in a Bayes ensemble are particularly well-suited for increasing generalization performance.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.18067v1?utm_source=chatgpt.com","title":"Self-Explaining Neural Networks for Business Process ...","snippet":"Mar 23, 2025 \u2014 In this work, we introduce, to the best of our knowledge, the first self-explaining neural network architecture for predictive process monitoring.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2508.14562v1?utm_source=chatgpt.com","title":"Locality-aware Concept Bottleneck Model","snippet":"2 days ago \u2014 Concept bottleneck models (CBMs) are inherently interpretable models that make predictions based on human-understandable visual cues, ...See more","ref_id":{"turn_index":8,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2410.02331v2?utm_source=chatgpt.com","title":"Self-eXplainable AI for Medical Image Analysis","snippet":"Nov 15, 2024 \u2014 This approach allows models to generate inherent explanations that are closely aligned with their internal decision-making processes, enhancing ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.23631v2?utm_source=chatgpt.com","title":"Intrinsically-Motivated Humans and Agents in Open-World ...","snippet":"May 27, 2025 \u2014 Hassas. An information-theoretic perspective on intrinsic motivation in reinforcement learning: a survey. Entropy, 25(2):327, 2023.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2502.11767?utm_source=chatgpt.com","title":"A Survey of LLM-based Active Learning","snippet":"by Y Xia \u00b7 2025 \u00b7 Cited by 6 \u2014 Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.13200v1?utm_source=chatgpt.com","title":"Learning To Explore With Predictive World Model Via Self- ...","snippet":"Feb 18, 2025 \u2014 Initially, intrinsic motivation used concepts of emotion, surprise, empowerment, entropy, and information gain to formulate intrinsic rewards.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2407.06204?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"by W Cai \u00b7 2024 \u00b7 Cited by 57 \u2014 The emergence of sparsely-gated. MoE [24], particularly within the integration of transformer- based large language models [25], has brought new ...","ref_id":{"turn_index":10,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.07137v1?utm_source=chatgpt.com","title":"A Comprehensive Survey of Mixture-of-Experts: Algorithms ...","snippet":"Mar 10, 2025 \u2014 In this paper, we first introduce the basic design of MoE, including gating functions, expert networks, routing mechanisms, training strategies, ...","ref_id":{"turn_index":10,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/figure/Rational-inattention-and-reinforcement-learning-A-Using-a-non-stationary-three-armed_fig4_350365666?utm_source=chatgpt.com","title":"Rational inattention and reinforcement learning (A) Using a...","snippet":"With regard to rate-distortion theory, we previously found that tonic dopamine controls the allocation of cognitive resources (Bari et al., 2024;Bari ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/378237626_A_Survey_on_Information_Bottleneck?utm_source=chatgpt.com","title":"A Survey on Information Bottleneck | Request PDF","snippet":"This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/368999264_Rational_Inattention_A_Review?utm_source=chatgpt.com","title":"Rational Inattention: A Review | Request PDF","snippet":"Aug 5, 2025 \u2014 We review the recent literature on rational inattention, identify the main theoretical mechanisms, and explain how it helps us understand a ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/392149062_Static_Analysis_by_Abstract_Interpretation_Against_Data_Leakage_in_Machine_Learning?utm_source=chatgpt.com","title":"Static Analysis by Abstract Interpretation Against Data ...","snippet":"Jul 14, 2025 \u2014 This paper focuses on improving the memory efficiency of abstract interpretation without sacrificing precision or time efficiency.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/340598532_An_Empirical_Study_of_Invariant_Risk_Minimization?_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbH19&utm_source=chatgpt.com","title":"(PDF) An Empirical Study of Invariant Risk Minimization","snippet":"Apr 10, 2020 \u2014 Invariant risk minimization (IRM; Arjovsky et al., 2019) is a recently proposed framework designed for learning predictors that are invariant to ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/384295632_Looped_Transformers_for_Length_Generalization?utm_source=chatgpt.com","title":"Looped Transformers for Length Generalization","snippet":"We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/367019841_Mastering_Diverse_Domains_through_World_Models?utm_source=chatgpt.com","title":"(PDF) Mastering Diverse Domains through World Models","snippet":"Jan 10, 2023 \u2014 Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long- ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/384771727_Bounded_Rational_Decision_Networks_with_Belief_Propagation?utm_source=chatgpt.com","title":"(PDF) Bounded Rational Decision Networks With Belief ...","snippet":"Feb 5, 2025 \u2014 Here, we consider a decision-theoretic approach to study networks of bounded rational decision makers that are allowed to specialize and ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/4765533_Probabilistic_choice_and_procedurally_bounded_rationality?utm_source=chatgpt.com","title":"Probabilistic choice and procedurally bounded rationality","snippet":"Aug 6, 2025 \u2014 The trade-off between utility and information lies at the heart of information-theoretic bounded rationality Braun, 2011, 2013;Genewein et al., ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/372896237_Model-Based_Reinforcement_Learning_with_State_Abstraction_A_Survey?_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbCwic3ViUGFnZSI6bnVsbH19&utm_source=chatgpt.com","title":"Model-Based Reinforcement Learning with State Abstraction","snippet":"Aug 7, 2025 \u2014 The survey finishes with Section VI, in which we discuss possible future research avenues for abstraction techniques. usage of abstractions in ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/352160708_Hierarchical_Reinforcement_Learning_A_Comprehensive_Survey?utm_source=chatgpt.com","title":"Hierarchical Reinforcement Learning: A Comprehensive ...","snippet":"We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi- ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/380037591_Structure_in_Deep_Reinforcement_Learning_A_Survey_and_Open_Problems?utm_source=chatgpt.com","title":"Structure in Deep Reinforcement Learning: A Survey and ...","snippet":"Apr 29, 2024 \u2014 2021, 2022a). Relational abstractions contribute to generalization by incorporating symbolic spaces. into ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/349787221_Barlow_Twins_Self-Supervised_Learning_via_Redundancy_Reduction?utm_source=chatgpt.com","title":"Barlow Twins: Self-Supervised Learning via Redundancy ...","snippet":"Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/381322776_Deep_FAVIB_Deep_Learning-Based_Forward-Aware_Quantization_via_Information_Bottleneck_Method?utm_source=chatgpt.com","title":"(PDF) Deep FAVIB: Deep Learning-Based Forward-Aware ...","snippet":"Jun 12, 2024 \u2014 Deep FAVIB: Deep Learning-Based Forward-Aware Quantization via Information Bottleneck Method. June 2024. DOI:10.1109/ICC51166.2024.10622994.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/338081829_On_the_information_bottleneck_theory_of_deep_learning?utm_source=chatgpt.com","title":"On the information bottleneck theory of deep learning","snippet":"The information bottleneck (IB) (Lewandowsky and Bauch 2024) originates from rate distortion theory and aims to compress the source variable X into a ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/370995475_Computing_with_Categories_in_Machine_Learning?utm_source=chatgpt.com","title":"Computing with Categories in Machine Learning","snippet":"In this survey, we provide an overview of category theory-derived machine learning from four mainstream perspectives: gradient-based learning, probability- ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/326639135_AI2_Safety_and_Robustness_Certification_of_Neural_Networks_with_Abstract_Interpretation?utm_source=chatgpt.com","title":"Safety and Robustness Certification of Neural Networks ...","snippet":"Formal verification methods, such as abstract interpretation [4, 7, 10], are commonly used to ensure that neural networks perform correctly in all potential ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/362426070_Minimum_Description_Length_Recurrent_Neural_Networks?utm_source=chatgpt.com","title":"Minimum Description Length Recurrent Neural Networks","snippet":"PDF | We train neural networks to optimize a Minimum Description Length score, that is, to balance between the complexity of the network and its.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/390405784_Minimum_Description_Length_of_a_Spectrum_Variational_Autoencoder_A_Theory?utm_source=chatgpt.com","title":"Minimum Description Length of a Spectrum Variational ...","snippet":"Apr 4, 2025 \u2014 In this paper, we argue that MDL is essential to deep learning and propose a further generalized principle: Understanding is the use of a small ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/392466646_A_Comprehensive_Survey_on_the_Risks_and_Limitations_of_Concept-based_Models?utm_source=chatgpt.com","title":"A Comprehensive Survey on the Risks and Limitations ...","snippet":"Jun 8, 2025 \u2014 PDF | Concept-based Models are a class of inherently explainable networks that improve upon standard Deep Neural Networks by providing a ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/355223733_Self-explaining_Neural_Network_with_Plausible_Explanations?utm_source=chatgpt.com","title":"Self-explaining Neural Network with Plausible Explanations","snippet":"Oct 9, 2021 \u2014 Our framework is self-explaining; relevance scores are generated for each concept to predict and explain in an end-to-end joint training scheme.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/368443560_An_Information-Theoretic_Perspective_on_Intrinsic_Motivation_in_Reinforcement_Learning_A_Survey?utm_source=chatgpt.com","title":"(PDF) An Information-Theoretic Perspective on Intrinsic ...","snippet":"Feb 7, 2023 \u2014 For several years now, IM has been widely used in RL, fostered by important results and. the emergence of deep learning.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/390354185_Intrinsically-Motivated_Humans_and_Agents_in_Open-World_Exploration?utm_source=chatgpt.com","title":"Intrinsically-Motivated Humans and Agents in Open-World ...","snippet":"Apr 3, 2025 \u2014 An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey. Article. Full-text available. Feb 2023 ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/391956541_A_Survey_of_Mixture_of_Experts_Models_Architectures_and_Applications_in_Business_and_Finance?utm_source=chatgpt.com","title":"A Survey of Mixture of Experts Models: Architectures and ...","snippet":"Jul 11, 2025 \u2014 We delve into the core concepts of MoE, including the gating network, expert networks, and routing mechanisms, and discuss how these components ...","ref_id":{"turn_index":10,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/390217128_A_Survey_on_Mixture_of_Experts_in_Large_Language_Models?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"Aug 6, 2025 \u2014 This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE.See more","ref_id":{"turn_index":10,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/393724282_Mixture_of_Experts_in_Large_Language_Models?utm_source=chatgpt.com","title":"(PDF) Mixture of Experts in Large Language Models","snippet":"Jul 18, 2025 \u2014 This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability ...See more","ref_id":{"turn_index":10,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/382111058_A_Survey_on_Mixture_of_Experts?utm_source=chatgpt.com","title":"(PDF) A Survey on Mixture of Experts","snippet":"Apr 10, 2025 \u2014 The illustration of various gating functions employed in MoE models, including (a) sparse MoE with top-1 gating [49], (b) BASE layers [87], (c) ...","ref_id":{"turn_index":10,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41598-024-76719-w?utm_source=chatgpt.com","title":"Exploring the limits of hierarchical world models in ...","snippet":"by R Schiewer \u00b7 2024 \u00b7 Cited by 3 \u2014 In this work we describe a novel HMBRL framework and evaluate it thoroughly. We construct hierarchical world models that simulate the environment at various ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41467-023-38468-8?utm_source=chatgpt.com","title":"General framework for E(3)-equivariant neural network ...","snippet":"by X Gong \u00b7 2023 \u00b7 Cited by 136 \u2014 Here we propose an E(3)-equivariant deep-learning framework to represent density functional theory (DFT) Hamiltonian as a function of material structure.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41598-025-08504-2?utm_source=chatgpt.com","title":"Deep representation learning using layer-wise VICReg ...","snippet":"by J Datta \u00b7 2025 \u2014 Barlow twins: Self-supervised learning via redundancy reduction. In International conference on machine learning, 12310\u201312320 (PMLR, 2021).","ref_id":{"turn_index":4,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41598-025-01250-5?utm_source=chatgpt.com","title":"Representing Born effective charges with equivariant ...","snippet":"by A Kutana \u00b7 2025 \u00b7 Cited by 2 \u2014 Here we explicitly encode such properties using an equivariant graph convolutional neural network. The network respects rotational symmetries of ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"nature.com"}]},{"type":"search_result_group","domain":"transformer-circuits.pub","entries":[{"type":"search_result","url":"https://transformer-circuits.pub/2023/monosemantic-features?utm_source=chatgpt.com","title":"Decomposing Language Models With Dictionary Learning","snippet":"Oct 4, 2023 \u2014 Sparse Autoencoders extract relatively monosemantic features. \u00b7 Sparse autoencoders produce interpretable features that are effectively invisible ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"transformer-circuits.pub"}]},{"type":"search_result_group","domain":"sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0925231225007842?utm_source=chatgpt.com","title":"Information bottleneck with input sampling for attribution","snippet":"by B Coelho \u00b7 2025 \u2014 In this paper, we propose IBISA, a model-agnostic attribution method that reaches state-of-the-art performance by optimizing sampling masks using the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S1364661324000032?utm_source=chatgpt.com","title":"Rationality, preferences, and emotions with biological ...","snippet":"by R Polan\u00eda \u00b7 2024 \u00b7 Cited by 16 \u2014 Both rational inattention and rate-distortion theory involve optimizing the allocation of scarce resources. In the case of rational inattention the resource ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0925231225008707?utm_source=chatgpt.com","title":"Achieving diverse compression with a single training","snippet":"by S Kudo \u00b7 2025 \u00b7 Cited by 2 \u2014 Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0957417423019693?utm_source=chatgpt.com","title":"Hierarchical reinforcement learning with unlimited option ...","snippet":"by Z Huang \u00b7 2024 \u00b7 Cited by 7 \u2014 In this paper, we provide a fresh perspective on option technology to comprehend different options based on knowledge representation.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2949855424000042?utm_source=chatgpt.com","title":"A survey on multi-agent reinforcement learning and its ...","snippet":"by Z Ning \u00b7 2024 \u00b7 Cited by 86 \u2014 Abstract. Multi-agent reinforcement learning (MARL) has been a rapidly evolving field. This paper presents a comprehensive survey of MARL and its applications.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0893608024009092?utm_source=chatgpt.com","title":"Lie group convolution neural networks with scale-rotation ...","snippet":"by W Qiao \u00b7 2025 \u00b7 Cited by 1 \u2014 This study proposes a SIM(2) Lie group-CNN, which can simultaneously keep scale, rotation, and translation equivariance for image classification tasks.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0031320325001608?utm_source=chatgpt.com","title":"Attention-based vector quantized variational autoencoder ...","snippet":"by Q Yu \u00b7 2025 \u00b7 Cited by 1 \u2014 This paper introduces a new framework that uses a vector quantized variational autoencoder (VQVAE) enhanced by orthogonal subspace constraints (OSC) and ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2666827024000483?utm_source=chatgpt.com","title":"A comprehensive study of auto-encoders for anomaly ...","snippet":"by AA Neloy \u00b7 2024 \u00b7 Cited by 27 \u2014 This study systematically reviews 11 Auto-Encoder architectures categorized into three groups, aiming to differentiate their reconstruction ability.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S235286482400018X?utm_source=chatgpt.com","title":"Rate distortion optimization for adaptive gradient ...","snippet":"by G Chen \u00b7 2024 \u00b7 Cited by 6 \u2014 We propose an adaptive gradient quantization approach that enhances communication efficiency. Aiming to minimize the total communication costs.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0031320323005873?utm_source=chatgpt.com","title":"Minimum description length clustering to measure ...","snippet":"by L Mahon \u00b7 2024 \u00b7 Cited by 14 \u2014 A new technique for measuring the amount of meaningful complexity in an image. Such a measure should not respond to random, unstructured variation, eg white ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0925231224009755?utm_source=chatgpt.com","title":"Interpretability of deep neural networks: A review ...","snippet":"by T Antamis \u00b7 2024 \u00b7 Cited by 19 \u2014 The following paper aims to provide a thorough overview of the methods that have so far been developed to explain deep neural networks.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2666764925000104?utm_source=chatgpt.com","title":"Unlocking the power of machine learning in big data","snippet":"by FM Surur \u00b7 2025 \u00b7 Cited by 8 \u2014 This study conducted a scoping survey to define the role of ML in BD by exploring its history and evolution.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"sciencedirect.com"}]},{"type":"search_result_group","domain":"springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10115-024-02108-4?utm_source=chatgpt.com","title":"Robustness verification of k-nearest neighbors by abstract ...","snippet":"by N Fassina \u00b7 2024 \u2014 In this work, we put forward an abstract interpretation-based framework for designing a sound approximate version of the kNN algorithm, which is ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10009-024-00774-x?utm_source=chatgpt.com","title":"Abstract domain adequacy | International Journal on ...","snippet":"by I Mastroeni \u00b7 2024 \u00b7 Cited by 2 \u2014 Abstract interpretation offers sound and decidable approximations for undecidable queries related to program behavior.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10462-022-10205-5?utm_source=chatgpt.com","title":"Survey on reinforcement learning for language processing","snippet":"by V Uc-Cetina \u00b7 2023 \u00b7 Cited by 199 \u2014 This paper reviews the state of the art of RL methods for their possible use for different problems of NLP, focusing primarily on conversational systems.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10994-024-06708-7?utm_source=chatgpt.com","title":"A survey on self-supervised methods for visual ...","snippet":"by T Uelwer \u00b7 2025 \u00b7 Cited by 2 \u2014 To implement this redundancy reduction principle, the Barlow Twins approach takes a batch of images X and creates two views {X^{(1)} = t^{(1)}(X)} ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s11704-025-41426-w?utm_source=chatgpt.com","title":"A survey of geometric graph neural networks: data ...","snippet":"by J Han \u00b7 2025 \u00b7 Cited by 45 \u2014 Researchers proposed a variety of geometric GNNs equipped with invariant/equivariant properties to better characterize the geometry and topology of geometric ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10994-024-06690-0?utm_source=chatgpt.com","title":"Misclassification bounds for PAC-Bayesian sparse deep ...","snippet":"by TT Mai \u00b7 2025 \u00b7 Cited by 6 \u2014 (2024), however, the prediction error for classification using Bayesian sparse deep learning remains unexplored. This study aims to address this ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s41060-025-00777-0?utm_source=chatgpt.com","title":"Large-scale simulation study of active learning models for ...","snippet":"by JJ Teijema \u00b7 2025 \u00b7 Cited by 9 \u2014 This study advocates for large-scale simulations as the gold standard for evaluating active learning models in systematic review screening.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10994-023-06454-2?utm_source=chatgpt.com","title":"Active learning for data streams: a survey | Machine Learning","snippet":"by D Cacciarelli \u00b7 2024 \u00b7 Cited by 80 \u2014 Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"springer.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=9YqacugDER&utm_source=chatgpt.com","title":"Towards Understanding Variants of Invariant Risk ...","snippet":"by K Yoshida \u00b7 Cited by 3 \u2014 Invariant Risk Minimization (IRM) emerges as a solution that aims to identify invariant features across different environments to enhance OOD ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=pWsfWDnJDa&utm_source=chatgpt.com","title":"Out-of-Distribution Optimality of Invariant Risk Minimization","snippet":"by S Toyota \u00b7 Cited by 1 \u2014 This paper examines out-of-distribution (OOD) generalization performance of models trained via Invariant Risk Minimization (IRM), and proves ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=huGECz8dPp&utm_source=chatgpt.com","title":"Information Bottleneck Analysis of Deep Neural Networks ...","snippet":"by I Butakov \u00b7 Cited by 13 \u2014 The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=0sbIEkIutN&utm_source=chatgpt.com","title":"Complete Length Generalization for Arithmetic Transformers","snippet":"by S Duan \u00b7 Cited by 16 \u2014 This paper studies length generalization of Transformer models on algorithmic tasks. It shows that the vanilla Transformer correctly learns the attention ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=9pe38WpsbX&utm_source=chatgpt.com","title":"MuDreamer: Learning Predictive World Models without ...","snippet":"by M Burchi \u00b7 Cited by 3 \u2014 In this paper, we present MuDreamer, a reinforcement learning agent that builds upon the DreamerV3 algorithm by learning a predictive world ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=4JCVw8oMlf&utm_source=chatgpt.com","title":"Effectively Learning Initiation Sets in Hierarchical ...","snippet":"by A Bagaria \u00b7 Cited by 8 \u2014 This paper proposes a method for learning an initiation set for options framework. After identifying challenges in learning initiation sets,","ref_id":{"turn_index":3,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=n84bzMrGUD&noteId=sQG6abJbs8&utm_source=chatgpt.com","title":"Clifford Group Equivariant Neural Networks","snippet":"Feb 20, 2024 \u2014 The paper develops a new kind of neural network model based on the Clifford group, which makes the neural network E(n)-equivariant.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=sfTsvy05MX&utm_source=chatgpt.com","title":"LL-VQ-VAE: Learnable Lattice Vector-Quantization For ...","snippet":"by A Khalil \u00b7 Cited by 2 \u2014 In this paper we introduce vector quantization and demonstrate its effectiveness for learning discrete representations.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=KXiQI6ggFc&utm_source=chatgpt.com","title":"A Pattern Language for Machine Learning Tasks","snippet":"by B Rodatz \u00b7 Cited by 1 \u2014 Our mathematical approach draws broadly from the field of Applied Category Theory (Fong. & Spivak, 2019a), particularly in the use of string diagrams for the ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/69f5e599b8a17628ac0063afc0d35a4d3fc77e82.pdf?utm_source=chatgpt.com","title":"A Pattern Language for Machine Learning Tasks","snippet":"field of Applied Category Theory (Fong & Spivak, 2019a), particularly in the use of string diagrams for the higher-algebraic data of concurrently and ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=Ncb0MvVqRV&utm_source=chatgpt.com","title":"Minimum Description Length and Generalization ...","snippet":"by M Sefidgaran \u00b7 Cited by 10 \u2014 In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=MP8bmxvWt6&utm_source=chatgpt.com","title":"Improving Generalization of Complex Models under ...","snippet":"by X Zhang \u00b7 Cited by 2 \u2014 In this paper, we introduce a new PAC-Bayes training algorithm with improved performance and reduced reliance on prior tuning. This is achieved ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=a8wjeqTZ9C&utm_source=chatgpt.com","title":"Concept Bottleneck Models under Label Noise","snippet":"by S Park \u2014 We analyze the impact of label noise on concept bottleneck models, revealing significant performance issues, and propose sharpness-aware minimization to ...See more","ref_id":{"turn_index":8,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=8nuzsfiQfS&utm_source=chatgpt.com","title":"Explain Yourself, Briefly! Self-Explaining Neural Networks ...","snippet":"by S Bassan \u00b7 Cited by 4 \u2014 A method to generate concise and faithful sufficient explanations for predictions using self-explaining neural networks.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=HkmymFPODz&utm_source=chatgpt.com","title":"Deep Active Learning in the Open World","snippet":"by T Xie \u00b7 Cited by 4 \u2014 We introduce ALOE, a novel active learning algorithm for open-world environments designed to enhance model adaptation by incorporating new OOD classes.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=vgXI1Ws0ma&utm_source=chatgpt.com","title":"Towards Empowerment Gain through Causal Structure ...","snippet":"by H Cao \u00b7 Cited by 1 \u2014 Empowerment as an intrinsic motivation enhances the ability of agents to actively control their environments by maximizing the mutual information between future ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"fransoliehoek.net","entries":[{"type":"search_result","url":"https://www.fransoliehoek.net/docs/Starre22BNAICBenelearn.pdf?utm_source=chatgpt.com","title":"Model-Based Reinforcement Learning with State Abstraction","snippet":"by RAN Starre \u00b7 Cited by 5 \u2014 Abstract. Model-based reinforcement learning methods are promising since they can increase sample e ciency while simultaneously improving generalizability.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"fransoliehoek.net"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/93459?utm_source=chatgpt.com","title":"From Causal to Concept-Based Representation Learning","snippet":"To build intelligent machine learning systems, modern representation learning attempts to recover latent generative factors from data, such as in causal ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/7972f3735e104a54715922aa416fde1b-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Towards Next-Level Robustness in Fine-tuning Zero-Shot ...","snippet":"by K Li \u00b7 2024 \u2014 A notable example is invariant risk minimization (IRM; Arjovsky et al., 2019), which aims to learn core-feature represen- tations from multi-domain data ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/04a80267ad46fc730011f8760f265054-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Connecting Joint-Embedding Predictive Architecture with ...","snippet":"by S Mo \u00b7 2024 \u00b7 Cited by 4 \u2014 In this paper, we aim to detail the theoretical underpinnings and empirical validations that substantiate the superiority of. C-JEPA over previous self- ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/e51ec47238fc3e82c269965c5533f9ef-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Hierarchical Programmatic Option Framework","snippet":"by YA Lin \u00b7 2024 \u00b7 Cited by 3 \u2014 Deep reinforcement learning aims to learn deep neural network policies to solve large-scale decision-making problems. However, approximating policies using.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://neurips.cc/virtual/2023/workshop/66535?utm_source=chatgpt.com","title":"Information-Theoretic Principles in Cognitive Systems ...","snippet":"Fri 12:40 p.m. - 1:30 p.m.. A Work in Progress: Tighter Bounds on the Information Bottleneck for Deep Learning ( Poster ) > link ... Rate-Distortion-Perception- ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/2b567c9aa100a8edcd13607a91ed3022-Paper-Conference.pdf?utm_source=chatgpt.com","title":"PAC-Bayes-Chernoff bounds for unbounded losses","snippet":"by I Casado \u00b7 2024 \u00b7 Cited by 10 \u2014 We introduce a new PAC-Bayes oracle bound for unbounded losses that extends. Cram\u00e9r-Chernoff bounds to the PAC-Bayesian setting. The proof technique relies.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/95178?utm_source=chatgpt.com","title":"NeurIPS Poster Coarse-to-Fine Concept Bottleneck Models","snippet":"This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly ...See more","ref_id":{"turn_index":8,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/neco/article/37/1/76/124827/Bounded-Rational-Decision-Networks-With-Belief?utm_source=chatgpt.com","title":"Bounded Rational Decision Networks With Belief ...","snippet":"Dec 12, 2024 \u2014 One prominent approach that deals with utility-information trade-offs in complex decision networks is bounded rationality. Herbert Simon (1943, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"mit.edu"},{"type":"search_result","url":"https://people.csail.mit.edu/davidam/docs/SENN.pdf?utm_source=chatgpt.com","title":"Self-Explaining Neural Networks - People | MIT CSAIL","snippet":"by D Alvarez-Melis \u00b7 Cited by 2 \u2014 We progressively generalize various aspects of the simple prediction models to yield a rich class of complex models that are self-explaining: i.e., they provide ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"mit.edu"},{"type":"search_result","url":"https://direct.mit.edu/coli/article/50/2/657/119158/Towards-Faithful-Model-Explanation-in-NLP-A-Survey?utm_source=chatgpt.com","title":"Towards Faithful Model Explanation in NLP: A Survey","snippet":"by Q Lyu \u00b7 2024 \u00b7 Cited by 171 \u2014 Explainability allows us to find where a model succeeds or fails, and fix the weaknesses before they can be exploited by adversaries. For example, if a model ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"mit.edu"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/danijar/dreamerv3?utm_source=chatgpt.com","title":"danijar/dreamerv3: Mastering Diverse Domains through ...","snippet":"A reimplementation of DreamerV3, a scalable and general reinforcement learning algorithm that masters a wide range of applications with fixed hyperparameters.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/rese1f/Awesome-VQVAE?utm_source=chatgpt.com","title":"Awesome-VQVAE","snippet":"A collection of resources and papers on Vector Quantized Variational Autoencoder (VQ-VAE) and its application.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/Zhen-Dong/Awesome-Quantization-Papers?utm_source=chatgpt.com","title":"Zhen-Dong/Awesome-Quantization-Papers","snippet":"This repo contains a comprehensive paper list of Model Quantization for efficient deep learning on AI conferences/journals/arXiv.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"A Survey on Mixture of Experts in. Large Language Models ... A chronological overview of several representative Mixture-of-Experts (MoE) models in recent years.","ref_id":{"turn_index":10,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/XueFuzhao/awesome-mixture-of-experts?utm_source=chatgpt.com","title":"XueFuzhao/awesome-mixture-of-experts","snippet":"This repo is a collection of AWESOME things about mixture-of-experts, including papers, code, etc. Feel free to star and fork.","ref_id":{"turn_index":10,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"google.com","entries":[{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=iRMZ2hoAAAAJ&utm_source=chatgpt.com","title":"Michael Littman","snippet":"Reinforcement learning: A survey. LP Kaelbling, ML Littman, AW Moore. Journal ... Towards a unified theory of state abstraction for MDPs. L Li, TJ Walsh ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"google.com"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2307.07176?utm_source=chatgpt.com","title":"Safe DreamerV3: Safe Reinforcement Learning with World ...","snippet":"We introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers?q=Category+theory&utm_source=chatgpt.com","title":"Daily Papers","snippet":"A category theory framework for Bayesian learning\u200b\u200b Inspired by the foundational works by Spivak and Fong and Cruttwell et al., we introduce a categorical ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"bohrium.com","entries":[{"type":"search_result","url":"https://www.bohrium.com/paper-details/a-survey-on-information-bottleneck/977537932896240018-2429?utm_source=chatgpt.com","title":"A Survey on Information Bottleneck","snippet":"This survey is for the remembrance of one of the creators of the information bottleneck theory, Prof. Naftali Tishby, passing away at the age of 68 on August, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"bohrium.com"}]},{"type":"search_result_group","domain":"intuitionlabs.ai","entries":[{"type":"search_result","url":"https://intuitionlabs.ai/articles/mechanistic-interpretability-ai-llms?utm_source=chatgpt.com","title":"Understanding Mechanistic Interpretability in AI Models","snippet":"6 days ago \u2014 The primary goal of mechanistic interpretability is to understand neural networks at the algorithmic level. This means identifying the internal ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"intuitionlabs.ai"},{"type":"search_result","url":"https://intuitionlabs.ai/pdfs/understanding-mechanistic-interpretability-in-ai-models.pdf?utm_source=chatgpt.com","title":"Understanding Mechanistic Interpretability in AI Models","snippet":"6 days ago \u2014 Sparse Autoencoders and Feature Factorization: A more recent technique, spearheaded by teams at DeepMind and Anthropic, is to automatically ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"intuitionlabs.ai"}]},{"type":"search_result_group","domain":"mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/6/466?utm_source=chatgpt.com","title":"MV\u2013MR: Multi-Views and Multi-Representations for Self- ...","snippet":"by V Kinakh \u00b7 2024 \u00b7 Cited by 4 \u2014 The VICReg [11] method extends the Barlow Twins [7] approach by imposing an additional constraint on the distance between the embeddings with and without ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/3/187?utm_source=chatgpt.com","title":"Theory and Application of the Information Bottleneck Method","snippet":"by J Lewandowsky \u00b7 2024 \u00b7 Cited by 5 \u2014 The fundamental idea of the information bottleneck method is to determine which information should be preserved under the invoked compression.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/10/836?utm_source=chatgpt.com","title":"Information Bottleneck Driven Deep Video Compression\u2014 ...","snippet":"by T Leiderman \u00b7 2024 \u00b7 Cited by 1 \u2014 This study, inspired by information bottleneck (IB) theory, introduces a novel approach that combines IB theory with wavelet transform.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/5/354?utm_source=chatgpt.com","title":"Implications of Minimum Description Length for Adversarial ...","snippet":"by K Tiwari \u00b7 2024 \u00b7 Cited by 3 \u2014 We treat the behavior of an attack model as a complex causal mechanism and quantify its algorithmic information using the minimum description length (MDL) ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/2673-7426/5/3/36?utm_source=chatgpt.com","title":"Self-Explaining Neural Networks for Food Recognition and ...","snippet":"by Z Revesai \u00b7 2025 \u2014 We developed a novel self-explaining neural architecture that integrates specialised attention mechanisms with temporal modules within a streamlined framework.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"mdpi.com"}]},{"type":"search_result_group","domain":"jmlr.org","entries":[{"type":"search_result","url":"https://jmlr.org/tmlr/papers/?utm_source=chatgpt.com","title":"Accepted papers","snippet":"Accepted TMLR papers can be awarded a number of certifications by the action editors or editors-in-chief of TMLR. The current list of awarded certificates is","ref_id":{"turn_index":6,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"jmlr.org"},{"type":"search_result","url":"https://www.jmlr.org/papers/volume25/23-1360/23-1360.pdf?utm_source=chatgpt.com","title":"More PAC-Bayes bounds: From bounded losses, to ...","snippet":"by B Rodr\u00edguez-G\u00e1lvez \u00b7 2024 \u00b7 Cited by 14 \u2014 This paper presents new PAC-Bayes bounds for bounded losses, general tail behaviors, and anytime validity, including a strengthened Catoni's bound and new...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"jmlr.org"}]},{"type":"search_result_group","domain":"diva-portal.org","entries":[{"type":"search_result","url":"https://www.diva-portal.org/smash/get/diva2%3A1917781/FULLTEXT01.pdf?utm_source=chatgpt.com","title":"Self-explaining Neural Networks in Reinforcement Learning","snippet":"by K Nowosadko \u00b7 2024 \u2014 Abstract. This thesis explores the integration of self-explaining neural networks (SENNs) into the field of reinforcement learning (RL), ...","ref_id":{"turn_index":8,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"diva-portal.org"}]},{"type":"search_result_group","domain":"science.org","entries":[{"type":"search_result","url":"https://spj.science.org/doi/10.34133/research.0467?utm_source=chatgpt.com","title":"Causal Inference Meets Deep Learning: A Comprehensive ...","snippet":"by L Jiao \u00b7 2024 \u00b7 Cited by 29 \u2014 In this survey, we provide a comprehensive and structured review of causal inference methods in deep learning.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"science.org"},{"type":"search_result","url":"https://spj.science.org/doi/10.34133/icomputing.0058?utm_source=chatgpt.com","title":"A Survey of Deep Active Learning for Foundation Models","snippet":"by T Wan \u00b7 2023 \u00b7 Cited by 20 \u2014 This survey endeavors to provide a comprehensive overview of DeepAL. Specifically, we first analyze and summarize various sample query strategies.","ref_id":{"turn_index":9,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"science.org"}]},{"type":"search_result_group","domain":"nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com","title":"Rational inattention in neural coding for economic choice","snippet":"by JM Fine \u00b7 2024 \u2014 We found, confirming predictions made by our extension of rational inattention theory, choice accuracy is positively correlated with recent reward rate.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC12058038/?utm_source=chatgpt.com","title":"Contrastive-Equivariant Self-Supervised Learning ...","snippet":"by T Yerxa \u00b7 2024 \u00b7 Cited by 2 \u2014 Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning, pages 12310\u201312320. PMLR, 2021. [Google ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC12068621/?utm_source=chatgpt.com","title":"The geometry of efficient codes: How rate-distortion trade ...","snippet":"by L D\u2019Amato \u00b7 2025 \u00b7 Cited by 1 \u2014 Here we investigate how rate-distortion trade-offs shape the latent representations of images in generative models, specifically Beta Variational Autoencoders.","ref_id":{"turn_index":5,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"nih.gov"}]},{"type":"search_result_group","domain":"ibm.com","entries":[{"type":"search_result","url":"https://research.ibm.com/publications/recurrent-transformers-trade-off-parallelism-for-length-generalization-on-regular-languages?utm_source=chatgpt.com","title":"Recurrent Transformers Trade-off Parallelism for Length ...","snippet":"by P Soulos \u00b7 2024 \u2014 This study underscores the critical role of layer-recurrence and chunk size in Recurrent Transformers, highlighting the trade-off between ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"ibm.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://malcolmlett.medium.com/critical-review-of-lecuns-introductory-jepa-paper-fabe5783134e?utm_source=chatgpt.com","title":"Critical review of LeCun's Introductory JEPA paper | Medium","snippet":"The I-JEPA and V-JEPA work is already showing that JEPA enables the model to learn a more compact representation of the world than prior ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"jair.org","entries":[{"type":"search_result","url":"https://jair.org/index.php/jair/article/download/15703/27028/38774?utm_source=chatgpt.com","title":"Structure in Deep Reinforcement Learning: A Survey and ...","snippet":"by A Mohan \u00b7 2024 \u00b7 Cited by 36 \u2014 State abstractions are a standard choice for improving generalization performance by capturing shared dynamics across MDPs into abstract state spaces using.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"jair.org"}]},{"type":"search_result_group","domain":"icml.cc","entries":[{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/45133?utm_source=chatgpt.com","title":"Super Deep Contrastive Information Bottleneck for Multi- ...","snippet":"In an era of increasingly diverse information sources, multi-modal clustering (MMC) has become a key technology for processing multi-modal data.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"icml.cc"},{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/44769?utm_source=chatgpt.com","title":"ICML Poster Compositional Risk Minimization","snippet":"Jul 16, 2025 \u2014 Towards the goal of tackling these compositional shifts, we develop an adaptation of naive discriminative Empirical Risk Minimization (ERM) ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":48},"pub_date":null,"attribution":"icml.cc"},{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/45802?utm_source=chatgpt.com","title":"GLGENN: A Novel Parameter-Light Equivariant Neural ...","snippet":"We propose, implement, and compare with competitors a new architecture of equivariant neural networks based on geometric (Clifford) algebras: Generalized ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"icml.cc"},{"type":"search_result","url":"https://icml.cc/virtual/2023/26163?utm_source=chatgpt.com","title":"PAC-Bayesian Adversarially Robust Generalization ...","snippet":"Workshop: 2nd ICML Workshop on New Frontiers in Adversarial Machine Learning. PAC-Bayesian Adversarially Robust Generalization Bounds for Deep Neural Networks.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"icml.cc"},{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/45301?utm_source=chatgpt.com","title":"ICML Poster Editable Concept Bottleneck Models","snippet":"Jul 17, 2025 \u2014 This research propose editable concept bottleneck models. While we acknowledge the importance of evaluating societal impacts, our analysis ...See more","ref_id":{"turn_index":8,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"icml.cc"},{"type":"search_result","url":"https://icml.cc/virtual/2025/papers.html?utm_source=chatgpt.com","title":"ICML 2025 Papers","snippet":"IMLS Archives, Getting Started, Schedule, Tutorials, Main Conference, Invited Talks, Orals, Spotlight, Posters, Awards, Test of Time Award Papers, Workshops","ref_id":{"turn_index":9,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"icml.cc"}]},{"type":"search_result_group","domain":"plos.org","entries":[{"type":"search_result","url":"https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1012952&utm_source=chatgpt.com","title":"The geometry of efficient codes: How rate-distortion trade-offs ...","snippet":"by L D\u2019Amato \u00b7 2025 \u00b7 Cited by 1 \u2014 Here we investigate how rate-distortion trade-offs shape the latent representations of images in generative models, specifically Beta Variational Autoencoders ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"plos.org"},{"type":"search_result","url":"https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1012383&utm_source=chatgpt.com","title":"Understanding dual process cognition via the minimum ...","snippet":"by T Moskovitz \u00b7 2024 \u00b7 Cited by 4 \u2014 The key insight is that dual-process structure can enhance adaptive behavior by allowing an agent to minimize the description length of its own ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"plos.org"}]},{"type":"search_result_group","domain":"scribd.com","entries":[{"type":"search_result","url":"https://www.scribd.com/document/735738281/1908-06976v2?utm_source=chatgpt.com","title":"A Survey On Intrinsic Motivation in Reinforcement Learning","snippet":"It surveys the role of intrinsic motivation in deep reinforcement learning to address these issues. Intrinsic motivation could help agents learn skills in a ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"scribd.com"}]},{"type":"search_result_group","domain":"anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/research/engineering-challenges-interpretability?utm_source=chatgpt.com","title":"The engineering challenges of scaling interpretability","snippet":"Jun 13, 2024 \u2014 Our Sparse Autoencoders\u2014the tools we use to investigate \u201cfeatures\u201d\u2014are trained on the activations of transformers, and those activations ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"anthropic.com"}]},{"type":"search_result_group","domain":"danijar.com","entries":[{"type":"search_result","url":"https://danijar.com/project/dreamerv3/?utm_source=chatgpt.com","title":"Mastering Diverse Control Tasks through World Models","snippet":"Dreamer is the first algorithm that collects diamonds in Minecraft from sparse rewards, without expert demonstrations or curricula, solving this challenge.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"danijar.com"}]},{"type":"search_result_group","domain":"computer.org","entries":[{"type":"search_result","url":"https://www.computer.org/csdl/journal/tp/2024/12/10559458/1XR0ep31Wr6?utm_source=chatgpt.com","title":"A Survey on Self-Supervised Learning","snippet":"by J Gui \u00b7 2024 \u00b7 Cited by 360 \u2014 This paper presents a review of diverse SSL methods, encompassing algorithmic aspects, application domains, three key trends, and open research questions.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"computer.org"}]},{"type":"search_result_group","domain":"acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3696412?utm_source=chatgpt.com","title":"Causal representation learning through higher-level ...","snippet":"Nov 7, 2024 \u2014 We explore a perspective on this problem that is directed to learning the generative process with causality-related foundations.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.1145/3649309?utm_source=chatgpt.com","title":"Accommodating Robustness by Abstract Interpretation","snippet":"by R Giacobazzi \u00b7 2024 \u00b7 Cited by 7 \u2014 2023. Domain precision in galois connection-less abstract interpretation. In Static Analysis Symposium, SAS 2023,Lecture Notes in Computer Science, Vol.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3524938.3524953?utm_source=chatgpt.com","title":"Invariant risk minimization games | Proceedings of the 37th ...","snippet":"by K Ahuja \u00b7 2020 \u00b7 Cited by 330 \u2014 In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3762179?utm_source=chatgpt.com","title":"Deep Causal Learning: Representation, Discovery and ...","snippet":"6 days ago \u2014 We emphasize that deep causal learning is pivotal for advancing the theoretical frontiers and broadening the practical applications of causal ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3649309?utm_source=chatgpt.com","title":"Accommodating Robustness by Abstract Interpretation","snippet":"Apr 27, 2024 \u2014 Domain precision in galois connection-less abstract interpretation. In Static Analysis Symposium, SAS 2023,Lecture Notes in Computer Science, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1016/j.eswa.2023.121467?utm_source=chatgpt.com","title":"Hierarchical reinforcement learning with unlimited option ...","snippet":"by Z Huang \u00b7 2024 \u00b7 Cited by 7 \u2014 In this paper, we provide a fresh perspective on option technology to comprehend different options based on knowledge representation. The ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1016/j.cviu.2025.104392?utm_source=chatgpt.com","title":"HVQ-VAE: : Variational auto-encoder with hyperbolic ...","snippet":"Aug 6, 2025 \u2014 This paper studies the geometric prior of hyperbolic spaces as a way to improve the learning capacity of VQ-VAE. That being said, working with ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3571727?utm_source=chatgpt.com","title":"A Survey on Perceptually Optimized Video Coding","snippet":"[120] reviewed recent advances on visual JND models and their applications to Rate-Distortion Optimization (RDO) and quantization in video coding, where machine ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3737916.3738683?utm_source=chatgpt.com","title":"PAC-Bayes-Chernoff bounds for unbounded losses","snippet":"Jun 5, 2025 \u2014 We introduce a new PAC-Bayes oracle bound for unbounded losses that extends Cram\u00e9r-Chernoff bounds to the PAC-Bayesian setting.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.1145/3459637.3482306?utm_source=chatgpt.com","title":"Towards Self-Explainable Graph Neural Network","snippet":"by E Dai \u00b7 2021 \u00b7 Cited by 113 \u2014 In this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations.","ref_id":{"turn_index":8,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"acm.org"}]},{"type":"search_result_group","domain":"ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2024/0272.pdf?utm_source=chatgpt.com","title":"Hierarchical Reinforcement Learning for Point of Interest ...","snippet":"by Y Xiao \u00b7 Cited by 3 \u2014 This section provides foundational definitions to establish the context for our study on improving POI recommendation sys- tems using hierarchical reinforcement ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"ijcai.org"},{"type":"search_result","url":"https://www.ijcai.org/proceedings/2024/0506.pdf?utm_source=chatgpt.com","title":"Meta-Learning via PAC-Bayesian with Data-Dependent Prior","snippet":"by S Liu \u00b7 Cited by 1 \u2014 We provide a new framework for PAC-Bayesian meta- learning incorporating data-dependent priors. Our PAC-. Bayesian generalization bound is defined as a function ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"ijcai.org"}]},{"type":"search_result_group","domain":"substack.com","entries":[{"type":"search_result","url":"https://cameronrwolfe.substack.com/p/moe-llms?utm_source=chatgpt.com","title":"Mixture-of-Experts (MoE) LLMs - by Cameron R. Wolfe, Ph.D.","snippet":"To create an MoE-based decoder-only transformer architecture, we simply convert the transformer's feed-forward layers to MoE\u2014or expert\u2014layers.","ref_id":{"turn_index":10,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"substack.com"}]},{"type":"search_result_group","domain":"tudelft.nl","entries":[{"type":"search_result","url":"https://repository.tudelft.nl/file/File_fe3321ce-61ae-4b9d-ac3e-b5d2934bdc29?utm_source=chatgpt.com","title":"Model-Based Reinforcement Learning with State Abstraction","snippet":"We provide a high- level view of the promising research in the field, covering a wide range of different types of state abstractions known from the literature.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"tudelft.nl"}]},{"type":"search_result_group","domain":"semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Pareto-Invariant-Risk-Minimization%3A-Towards-the-in-Chen-Zhou/9a7a51cf95e5cb796847ec6d32c0b9ed95f1eec2?utm_source=chatgpt.com","title":"Pareto Invariant Risk Minimization: Towards Mitigating the ...","snippet":"Pareto Invariant Risk Minimization: Towards Mitigating the Optimization Dilemma in Out-of-Distribution Generalization \u00b7 Yongqiang Chen, Kaiwen Zhou, +8 authors","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Mastering-Diverse-Domains-through-World-Models-Hafner-Pa%C5%A1ukonis/f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e?utm_source=chatgpt.com","title":"[PDF] Mastering Diverse Domains through World Models","snippet":"DreamerV3 is presented, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Geometrically-Equivariant-Graph-Neural-Networks%3A-A-Han-Rong/c4478d0c8279d19c78a3cc1d2d4f699679341a4d?utm_source=chatgpt.com","title":"Geometrically Equivariant Graph Neural Networks: A Survey","snippet":"This work analyzes and classify existing methods into three groups regarding how the message passing and aggregation in GNNs are represented, and summarizes ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/a56453c98c6244329f660d4fd74e2afd4fe7d349?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts","snippet":"The structure of the MoE layer is introduced, followed by proposing a new taxonomy of MoE, and the core designs for various MoE models including both ...","ref_id":{"turn_index":10,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"semanticscholar.org"}]},{"type":"search_result_group","domain":"beginswithai.com","entries":[{"type":"search_result","url":"https://beginswithai.com/anthropic-circuits-updates/?utm_source=chatgpt.com","title":"Anthropic's Circuits Updates: All Releases (Updated May ...","snippet":"Analyzing attention heads and computational circuits in transformers; Improving interpretability with sparse autoencoders and monosemantic feature learning ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"beginswithai.com"}]},{"type":"search_result_group","domain":"stanford.edu","entries":[{"type":"search_result","url":"https://ee.stanford.edu/event/03-07-2024/what-algorithms-can-transformers-learn-study-length-generalization?utm_source=chatgpt.com","title":"What Algorithms can Transformers Learn? A Study in Length ...","snippet":"Mar 7, 2024 \u2014 We conjecture, informally, that: Transformers tend to length-generalize on a task if there exists a short RASP program that solves the task for ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://plato.stanford.edu/archives/fall2024/entries/category-theory/?utm_source=chatgpt.com","title":"Category Theory - Stanford Encyclopedia of Philosophy","snippet":"Dec 6, 1996 \u2014 Category theory is both an interesting object of philosophical study, and a potentially powerful formal tool for philosophical investigations of ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://hai.stanford.edu/ai-index/2025-ai-index-report?utm_source=chatgpt.com","title":"The 2025 AI Index Report | Stanford HAI","snippet":"In 2024, U.S.-based institutions produced 40 notable AI models, significantly outpacing China's 15 and Europe's three. While the U.S. maintains its lead in ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"stanford.edu"}]},{"type":"search_result_group","domain":"vitalab.github.io","entries":[{"type":"search_result","url":"https://vitalab.github.io/article/2023/01/19/DreamerV3.html?utm_source=chatgpt.com","title":"DreamerV3: Mastering Diverse Domains through World Models","snippet":"Jan 19, 2023 \u2014 Model-based reinforcement learning is a type of RL where the agent will not only learn a policy, but also a world model which the agent can use to plan ahead.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"vitalab.github.io"}]},{"type":"search_result_group","domain":"brunogavranovic.com","entries":[{"type":"search_result","url":"https://www.brunogavranovic.com/assets/FundamentalComponentsOfDeepLearning.pdf?utm_source=chatgpt.com","title":"Fundamental Components of Deep Learning","snippet":"by B Gavranovic \u00b7 2024 \u00b7 Cited by 15 \u2014 This thesis develops a novel mathematical foundation for deep learning based on the language of category theory.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"brunogavranovic.com"}]},{"type":"search_result_group","domain":"wordpress.com","entries":[{"type":"search_result","url":"https://ewrl.wordpress.com/wp-content/uploads/2023/09/georg-martius-intrinsic-motivation-in-reinforcement-learning.pdf?utm_source=chatgpt.com","title":"Intrinsic Motivation in Reinforcement Learning","snippet":"\u27a2 Intrinsic motivations help us to formalize exploration strategies inductive bias to specify downstream task families. \u27a2 Model-based planning + predictive ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"wordpress.com"}]},{"type":"search_result_group","domain":"sebastianraschka.com","entries":[{"type":"search_result","url":"https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one?utm_source=chatgpt.com","title":"LLM Research Papers: The 2025 List (January to June)","snippet":"Jul 1, 2025 \u2014 The latest in LLM research with a hand-curated, topic-organized list of over 200 research papers from 2025.","ref_id":{"turn_index":10,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"sebastianraschka.com"}]},{"type":"search_result_group","domain":"ifaamas.org","entries":[{"type":"search_result","url":"https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p1328.pdf?utm_source=chatgpt.com","title":"Mixed-Initiative Bayesian Sub-Goal Optimization in ...","snippet":"by H Ma \u00b7 Cited by 9 \u2014 ABSTRACT. In hierarchical reinforcement learning, human expertise is often involved in defining sub-goals that decompose the final objective.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"ifaamas.org"}]},{"type":"search_result_group","domain":"aps.org","entries":[{"type":"search_result","url":"https://link.aps.org/doi/10.1103/PhysRevLett.132.197201?utm_source=chatgpt.com","title":"Machine-Learning Optimized Measurements of Chaotic ...","snippet":"by KA Murphy \u00b7 2024 \u00b7 Cited by 9 \u2014 We establish an equivalence between a perfect measurement and a variant of the information bottleneck. As a consequence, we can employ machine learning to ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"aps.org"}]},{"type":"search_result_group","domain":"preprints.org","entries":[{"type":"search_result","url":"https://www.preprints.org/manuscript/202503.0611/v1?utm_source=chatgpt.com","title":"Improving Deep Learning Performance with Mixture of ...","snippet":"by J Nnamdi \u00b7 2025 \u2014 This survey provides a comprehensive overview of MoE, covering its fundamental principles, architectural variations, training strategies, and key applications.","ref_id":{"turn_index":10,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"preprints.org"}]},{"type":"search_result_group","domain":"turingpost.com","entries":[{"type":"search_result","url":"https://www.turingpost.com/p/jepa?utm_source=chatgpt.com","title":"What is Joint Embedding Predictive Architecture (JEPA)?","snippet":"Jun 12, 2024 \u2014 We discuss the Joint Embedding Predictive Architecture (JEPA), how it differs from transformers and provide you with list of models based on JEPA.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"turingpost.com"}]},{"type":"search_result_group","domain":"pnas.org","entries":[{"type":"search_result","url":"https://www.pnas.org/doi/10.1073/pnas.2220593120?utm_source=chatgpt.com","title":"Information-theoretic principles in incremental language ...","snippet":"by R Futrell \u00b7 2023 \u00b7 Cited by 22 \u2014 This theory, the rate\u2013distortion theory of control (RDC), uses the mathematical theory of lossy compression (5, 6) to impose informational constraints on the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"pnas.org"},{"type":"search_result","url":"https://www.pnas.org/doi/10.1073/pnas.2506316122?utm_source=chatgpt.com","title":"Sparse autoencoders uncover biologically interpretable ...","snippet":"We leverage sparse autoencoders and transcoders from natural language processing as a way to extract biologically meaningful, interpretable features from both ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"pnas.org"}]},{"type":"search_result_group","domain":"oup.com","entries":[{"type":"search_result","url":"https://academic.oup.com/bioinformatics/article/41/2/btaf039/7989445?utm_source=chatgpt.com","title":"SSL-VQ: vector-quantized variational autoencoders for semi ...","snippet":"by S Namba \u00b7 2025 \u2014 This study presents a novel machine learning approach using multimodal vector-quantized variational autoencoders (VQ-VAEs) for predicting therapeutic target ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"oup.com"}]},{"type":"search_result_group","domain":"mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v247/kuzborskij24a/kuzborskij24a.pdf?utm_source=chatgpt.com","title":"Better-than-KL PAC-Bayes Bounds","snippet":"by I Kuzborskij \u00b7 2024 \u00b7 Cited by 6 \u2014 This paper challenges the tightness of KL-divergence-based PAC-Bayes bounds, showing it's possible to achieve a strictly tighter bound.","ref_id":{"turn_index":7,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"mlr.press"}]},{"type":"search_result_group","domain":"nips.cc","entries":[{"type":"search_result","url":"https://nips.cc/virtual/2024/papers.html?utm_source=chatgpt.com","title":"NeurIPS 2024 Papers","snippet":"Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure ... Algorithmic Capabilities of Random Transformers ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"nips.cc"}]},{"type":"search_result_group","domain":"unibo.it","entries":[{"type":"search_result","url":"https://amslaurea.unibo.it/id/eprint/32595/1/francesco_riccardo_crescenzi.pdf?utm_source=chatgpt.com","title":"TOWARDS A CATEGORICAL FOUNDATION OF DEEP LEARNING","snippet":"The unprecedented pace of machine learning research has lead to incredible advances, but also poses hard challenges. At present, the field lacks strong ...","ref_id":{"turn_index":6,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"unibo.it"}]},{"type":"search_result_group","domain":"worldscientific.com","entries":[{"type":"search_result","url":"https://www.worldscientific.com/doi/10.1142/S2661335219300018?srsltid=AfmBOoopxnXEgOoXvccthpf0_OcXTv2co8pXpw65TcB8Na0RJOD6wfua&utm_source=chatgpt.com","title":"Minimum description length revisited","snippet":"The Minimum Description Length (MDL) Principle is a theory of inductive inference that can be applied to general problems in statistics, machine learning and ...","ref_id":{"turn_index":7,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"worldscientific.com"}]},{"type":"search_result_group","domain":"aaai.org","entries":[{"type":"search_result","url":"https://ojs.aaai.org/index.php/AAAI/article/view/35495/37650?utm_source=chatgpt.com","title":"Explanation Bottleneck Models","snippet":"by S Yamaguchi \u00b7 2025 \u2014 Introduc- ing concept bottleneck models (CBMs, Koh et al. (2020)) is a promising approach to interpreting the output of deep models. In contrast to black-box ...See more","ref_id":{"turn_index":8,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"aaai.org"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2025.findings-naacl.87.pdf?utm_source=chatgpt.com","title":"Specialized Sparse Autoencoders for Interpreting Rare ...","snippet":"by A Muhamed \u00b7 2025 \u00b7 Cited by 3 \u2014 Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/2024.findings-eacl.67.pdf?utm_source=chatgpt.com","title":"Explaining Language Model Predictions with High-Impact ...","snippet":"by R Zhao \u00b7 2024 \u00b7 Cited by 9 \u2014 Koh et al. (2020) provides a complete survey on concept bottleneck models and their interventions. Yeh et al. (2020) proposes an ...See more","ref_id":{"turn_index":8,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"aclanthology.org"},{"type":"search_result","url":"https://aclanthology.org/2025.findings-naacl.24.pdf?utm_source=chatgpt.com","title":"Marrying LLMs with Dynamic Forecasting: A Graph Mixture ...","snippet":"by D Jiang \u00b7 2025 \u2014 Instead of directly generating future pre- dictions, our LEGO employs a graph mixture-of- experts framework (Cai et al., 2024), where several.See more","ref_id":{"turn_index":10,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"rohitbandaru.github.io","entries":[{"type":"search_result","url":"https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/?utm_source=chatgpt.com","title":"Deep Dive into Yann LeCun's JEPA - Rohit Bandaru","snippet":"Jul 31, 2024 \u2014 In the JEPA paper, Yann LeCun proposes frameworks for models that can think. Learning to think may address the fundamental problems in current ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"rohitbandaru.github.io"}]},{"type":"search_result_group","domain":"uni-hamburg.de","entries":[{"type":"search_result","url":"https://edoc.sub.uni-hamburg.de/informatik/volltexte/2025/301/pdf/thesis.pdf?utm_source=chatgpt.com","title":"Length Generalization on Multi-Digit Integer Addition with ...","snippet":"by S Wermter \u00b7 2024 \u2014 2024 show using RASP that looped transformers can generalize to longer sequence lengths for binary ad- dition task, along with other algorithmic ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"uni-hamburg.de"}]},{"type":"search_result_group","domain":"thecvf.com","entries":[{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Language_Guided_Concept_Bottleneck_Models_for_Interpretable_Continual_Learning_CVPR_2025_paper.pdf?utm_source=chatgpt.com","title":"Language Guided Concept Bottleneck Models for ...","snippet":"by L Yu \u00b7 2025 \u00b7 Cited by 3 \u2014 Concept Bottleneck Models (CBMs) [22, 54] are a form of interpretable machine learning model that enhance trans- parency by organizing the learning process ...See more","ref_id":{"turn_index":8,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"thecvf.com"}]},{"type":"search_result_group","domain":"iclr.cc","entries":[{"type":"search_result","url":"https://proceedings.iclr.cc/paper_files/paper/2025/file/abc1943857a42935ceacff03c524bb44-Paper-Conference.pdf?utm_source=chatgpt.com","title":"MIXTURE COMPRESSOR FOR MIXTURE-OF-EXPERTS ...","snippet":"by W Huang \u00b7 Cited by 10 \u2014 Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in.See more","ref_id":{"turn_index":10,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"iclr.cc"}]},{"type":"search_result_group","domain":"wiley.com","entries":[{"type":"search_result","url":"https://onlinelibrary.wiley.com/doi/10.1111/tops.12716?utm_source=chatgpt.com","title":"An Information\u2010Theoretic Account of Availability Effects in ...","snippet":"by R Futrell \u00b7 2024 \u00b7 Cited by 6 \u2014 A model based on a combination of information theory and control theory predicts availability/accessibility effects in language production.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"wiley.com"},{"type":"search_result","url":"https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12716?utm_source=chatgpt.com","title":"An Information\u2010Theoretic Account of Availability Effects in ...","snippet":"by R Futrell \u00b7 2024 \u00b7 Cited by 6 \u2014 Abstract. I present a computational-level model of language production in terms of a combination of infor- mation theory and control theory in which words ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"wiley.com"}]},{"type":"search_result_group","domain":"ecva.net","entries":[{"type":"search_result","url":"https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04236.pdf?utm_source=chatgpt.com","title":"Decoupling Common and Unique Representations for ...","snippet":"by Y Wang \u00b7 Cited by 26 \u2014 In practice, this can be seen as a natural exten- sion of Barlow Twins [57], one self-supervised learning technique that conducts redundancy reduction on the ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"ecva.net"}]},{"type":"search_result_group","domain":"lambdaviking.com","entries":[{"type":"search_result","url":"https://lambdaviking.com/blog/2022/improved-adversarial-robustness/?utm_source=chatgpt.com","title":"Improved Adversarial Robustness via Abstract Interpretation","snippet":"Apr 15, 2022 \u2014 This paper improves methods for certifying the adversarial robustness of neural networks using techniques from abstract interpretation.","ref_id":{"turn_index":6,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"lambdaviking.com"}]},{"type":"search_result_group","domain":"informs.org","entries":[{"type":"search_result","url":"https://pubsonline.informs.org/doi/abs/10.1287/trsc.2024.0562?utm_source=chatgpt.com","title":"diffIRM: A Diffusion-Augmented Invariant Risk Minimization ...","snippet":"by Z Mo \u00b7 2025 \u00b7 Cited by 3 \u2014 In this study, we fill in this research gap and propose a diffusion-augmented invariant risk minimization (diffIRM) framework that combines ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"informs.org"}]},{"type":"search_result_group","domain":"meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/?utm_source=chatgpt.com","title":"I-JEPA: The first AI model based on Yann LeCun's vision ...","snippet":"Jun 13, 2023 \u2014 This model, the Image Joint Embedding Predictive Architecture (I-JEPA), learns by creating an internal model of the outside world, which compares abstract ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"meta.com"}]},{"type":"search_result_group","domain":"umass.edu","entries":[{"type":"search_result","url":"https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_77.pdf?utm_source=chatgpt.com","title":"Surprise-Adaptive Intrinsic Motivation for Unsupervised ...","snippet":"by A Hugessen \u2014 Reinforcement learning. RL is a learning paradigm for sequential decision-making problems. In. RL, an agent acts in an environment from which it receives ...","ref_id":{"turn_index":9,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"umass.edu"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/nyvgqk/r_towards_causal_representation_learning/?utm_source=chatgpt.com","title":"[R] Towards Causal Representation Learning","snippet":"In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/1at7fib/vjepa_the_next_step_toward_yann_lecuns_vision_of/?utm_source=chatgpt.com","title":"V-JEPA: The next step toward Yann LeCun's vision of ...","snippet":"This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/m1v9hg/r_barlow_twins_selfsupervised_learning_via/?utm_source=chatgpt.com","title":"[R] Barlow Twins: Self-Supervised Learning via ...","snippet":"The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"aip.org","entries":[{"type":"search_result","url":"https://pubs.aip.org/pof/article/37/8/085146/3358152/Vector-quantization-variational-autoencoder-for?utm_source=chatgpt.com","title":"Vector quantization variational autoencoder for turbulent ...","snippet":"Aug 8, 2025 \u2014 This study explores the use of vector-quantized variational autoencoder (VQ-VAE) to reconstruct flow images across a range of resolutions ( ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"aip.org"}]},{"type":"search_result_group","domain":"dilipa.github.io","entries":[{"type":"search_result","url":"https://dilipa.github.io/papers/nips22_infocog_workshop_rdt_satisficing.pdf?utm_source=chatgpt.com","title":"On Rate-Distortion Theory in Capacity-Limited Cognition ...","snippet":"by D Arumugam \u00b7 Cited by 5 \u2014 Rate-distortion theory and computationally rational reinforcement learning. ... An information-theoretic approach to curiosity-driven reinforcement learning.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"dilipa.github.io"}]},{"type":"search_result_group","domain":"unipd.it","entries":[{"type":"search_result","url":"https://www.math.unipd.it/~ranzato/papers/kais24.pdf?utm_source=chatgpt.com","title":"Robustness Verification of k-Nearest Neighbors by Abstract ...","snippet":"Abstract. We study the certification of stability properties, such as robustness and individ- ual fairness, of the k-Nearest Neighbor algorithm (kNN).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"unipd.it"}]},{"type":"search_result_group","domain":"aeaweb.org","entries":[{"type":"search_result","url":"https://www.aeaweb.org/articles?id=10.1257%2Fjel.20211524&utm_source=chatgpt.com","title":"Rational Inattention: A Review","snippet":"by B Ma\u0107kowiak \u00b7 2023 \u00b7 Cited by 503 \u2014 We review the recent literature on rational inattention, identify the main theoretical mechanisms, and explain how it helps us understand a variety of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"aeaweb.org"}]},{"type":"search_result_group","domain":"thesingularityproject.ai","entries":[{"type":"search_result","url":"https://www.thesingularityproject.ai/p/yann-lecuns-joint-embedding-predictive-architecture-jepa-and-the-general-theory-of-intelligence?utm_source=chatgpt.com","title":"Yann LeCun's Joint Embedding Predictive Architecture ...","snippet":"Mar 11, 2025 \u2014 This influential position paper outlines LeCun's vision for next-generation AI, with JEPA as a central component. It proposes a six-module ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"thesingularityproject.ai"}]},{"type":"search_result_group","domain":"ai.sony","entries":[{"type":"search_result","url":"https://ai.sony/publications/HQ-VAE-Hierarchical-Discrete-Representation-Learning-with-Variational-Bayes/?utm_source=chatgpt.com","title":"HQ-VAE: Hierarchical Discrete Representation Learning ...","snippet":"Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational ...","ref_id":{"turn_index":5,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"ai.sony"}]},{"type":"search_result_group","domain":"gershmanlab.com","entries":[{"type":"search_result","url":"https://gershmanlab.com/pubs/GershmanBurke23.pdf?utm_source=chatgpt.com","title":"Mental control of uncertainty","snippet":"by SJ Gershman \u00b7 2023 \u00b7 Cited by 19 \u2014 This paper surveys the origin of these models in economics, their connection to rate-distortion theory, and some of their recent applications to ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"gershmanlab.com"}]},{"type":"search_result_group","domain":"crl-community.github.io","entries":[{"type":"search_result","url":"https://crl-community.github.io/neurips24?utm_source=chatgpt.com","title":"Causal Representation Learning","snippet":"The goal of this workshop is to explore the challenges and opportunities in this field, discuss recent progress, and identify open questions.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"crl-community.github.io"}]},{"type":"search_result_group","domain":"edpsciences.org","entries":[{"type":"search_result","url":"https://sands.edpsciences.org/articles/sands/full_html/2024/01/sands20240003/sands20240003.html?utm_source=chatgpt.com","title":"Static program analysis for IoT risk mitigation in space-air ...","snippet":"by H Deng \u00b7 2024 \u00b7 Cited by 2 \u2014 (1) Galois connection: Abstract interpretation theory is a mapping relation between concrete and abstract domains represented by a Galois connection. \u27e8D ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":49},"pub_date":null,"attribution":"edpsciences.org"}]}],"citations":[],"request_id":"973397448d90ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"866d6f59-cc93-4f8e-bc92-42b9f69ad2d7","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"866d6f59-cc93-4f8e-bc92-42b9f69ad2d7","children":["a177c00b-285a-42bd-8c7d-bf3093ffa187"]},"a177c00b-285a-42bd-8c7d-bf3093ffa187":{"id":"a177c00b-285a-42bd-8c7d-bf3093ffa187","message":{"id":"a177c00b-285a-42bd-8c7d-bf3093ffa187","author":{"role":"user","name":null,"metadata":{}},"create_time":1755878912.843,"update_time":null,"content":{"content_type":"text","parts":["The following is a very thoughtful and insightful response to my original prompt. Please examine the following response and comment on its unique perspective, as well as how it gels with our discussion and findings thus far, including the research that has been carried out by others.\r\n\r\n---\r\n\r\nThis is a compelling information-theoretic framework for understanding intelligence, though I think it captures important but not necessarily complete aspects of what we typically mean by intelligence.\r\n\r\n## Strengths of this hypothesis\r\n\r\nThe operations you've listed do map remarkably well onto many cognitive capabilities we associate with intelligence:\r\n\r\n- **Pattern recognition and transfer**: Generalization and specialization are fundamental to how intelligent systems learn from experience and apply knowledge to new situations\r\n- **Predictive modeling**: Interpolation and extrapolation allow for dealing with incomplete information and making predictions\r\n- **Conceptual hierarchy**: Abstraction and concretization enable moving between levels of detail and understanding\r\n- **Compression and efficiency**: Simplification and quantization relate to finding efficient representations\r\n- **Self-improvement**: Recursion allows for meta-cognition and self-modification\r\n\r\nThese operations also align well with how modern AI systems work - neural networks essentially perform many of these organizational functions through their architectures and training processes.\r\n\r\n## Potential limitations\r\n\r\nHowever, several aspects of intelligence might not be fully captured:\r\n\r\n1. **Goal-directed behavior**: Intelligence often involves pursuing objectives, which requires more than just organizing existing information - it involves valuation, planning, and decision-making under uncertainty.\r\n\r\n2. **Creativity and novelty**: While recombination of existing information through these operations can produce novel outputs, genuine creativity might involve something beyond reorganization - perhaps stochastic exploration or constraint-breaking.\r\n\r\n3. **Embodied and social intelligence**: Much of human and animal intelligence involves physical interaction with environments and social coordination, which may require capabilities beyond information organization.\r\n\r\n4. **Consciousness and understanding**: There's an ongoing debate about whether organizing information, even sophisticatedly, constitutes genuine understanding or whether it's merely syntactic manipulation.\r\n\r\n5. **Emotional and motivational aspects**: The framework doesn't account for how affect, drives, and values shape intelligent behavior.\r\n\r\n## Alternative perspective\r\n\r\nYour hypothesis might be more precisely stated as: \"Intelligence is the ability to organize information *in service of adaptive behavior*.\" The organizational operations you list could be seen as the computational substrate that enables intelligence, rather than being identical to intelligence itself.\r\n\r\nThis reminds me of debates in cognitive science between purely computational theories of mind versus embodied, enactive, or ecological approaches that emphasize the importance of action, environment, and purpose."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"9733a1258e21ec6e-IAD","message_source":null,"timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"04d1f02c-c4c6-4dc9-ad82-fef7be0dbfcd","children":["def7d9a9-2c1c-45dd-9b60-979c9c4e913a"]},"def7d9a9-2c1c-45dd-9b60-979c9c4e913a":{"id":"def7d9a9-2c1c-45dd-9b60-979c9c4e913a","message":{"id":"def7d9a9-2c1c-45dd-9b60-979c9c4e913a","author":{"role":"system","name":null,"metadata":{}},"create_time":1755878913.2482648,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"a177c00b-285a-42bd-8c7d-bf3093ffa187","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"a177c00b-285a-42bd-8c7d-bf3093ffa187","children":["1667aee0-0deb-45ac-aa7a-1e42725022d2"]},"1667aee0-0deb-45ac-aa7a-1e42725022d2":{"id":"1667aee0-0deb-45ac-aa7a-1e42725022d2","message":{"id":"1667aee0-0deb-45ac-aa7a-1e42725022d2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878914.07474,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Evaluating user perspective","content":"We need to carefully analyze the user's response, considering its unique angle and alignment with previous research, including frameworks like MDL, PAC-Bayes, and category theory. I'll break down the strengths and limitations E-Jay mentioned, and update with recent research and citations. I'll make sure the answer is structured and objective.","chunks":[],"finished":true},{"summary":"Relating to current research","content":"The user wants me to connect their request to recent research. Since our knowledge is up to June 2024, I should definitely update the response with developments from 2024 to 2025. I'll need to find current sources to integrate these findings and ensure the response reflects the latest research.","chunks":[],"finished":true},{"summary":"Gathering related research","content":"To complement the user's request, I'll gather recent sources on key topics like Information Bottleneck, rate-distortion in RL, mechanistic interpretability, causal representation learning, invariant risk minimization, and more. These topics have active developments, particularly in the years 2023-2025, so updating with the latest research will strengthen the response.","chunks":[],"finished":true},{"summary":"Evaluating user response","content":"We should analyze how the user's response aligns with embodied intelligence research, like active inference and enactivism. Their framing of organization in service of adaptive behavior is in line with the necessity of decision-making and exploration. Critically, we can discuss limitations like equating recursion with meta-cognition and connect it to meta-learning.","chunks":[],"finished":true},{"summary":"Preparing web search","content":"I need to gather high-quality sources for several important topics such as Information Bottleneck in deep learning, rate-distortion in RL, causal representation learning, and JEPA. I'll group them into four queries, keeping the search focused and efficient. Once I gather the sources, I'll cite the most impactful ones. Time to get started!","chunks":[],"finished":true}],"source_analysis_msg_id":"deb933ca-6d5e-40eb-9457-ca8a6b69d027"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"def7d9a9-2c1c-45dd-9b60-979c9c4e913a","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"def7d9a9-2c1c-45dd-9b60-979c9c4e913a","children":["403b5ff0-ceaa-45c4-b87a-5c6f2fd8fc15"]},"403b5ff0-ceaa-45c4-b87a-5c6f2fd8fc15":{"id":"403b5ff0-ceaa-45c4-b87a-5c6f2fd8fc15","message":{"id":"403b5ff0-ceaa-45c4-b87a-5c6f2fd8fc15","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878913.272228,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Information Bottleneck deep learning survey 2023 2024\"}, {\"q\": \"bounded rationality reinforcement learning rate-distortion survey 2024\"}, {\"q\": \"causal representation learning survey 2023 2024 Sch\\u00f6lkopf\"}, {\"q\": \"I-JEPA paper 2023 Assran LeCun\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Information Bottleneck deep learning survey 2023 2024"},{"type":"search","q":"bounded rationality reinforcement learning rate-distortion survey 2024"},{"type":"search","q":"causal representation learning survey 2023 2024 Sch\u00f6lkopf"},{"type":"search","q":"I-JEPA paper 2023 Assran LeCun"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Search Queries:\n[{'type': 'search', 'q': 'Information Bottleneck deep learning survey 2023 2024'}, {'type': 'search', 'q': 'bounded rationality reinforcement learning rate-distortion survey 2024'}, {'type': 'search', 'q': 'causal representation learning survey 2023 2024 Sch\u00f6lkopf'}, {'type': 'search', 'q': 'I-JEPA paper 2023 Assran LeCun'}]","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"1667aee0-0deb-45ac-aa7a-1e42725022d2","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"1667aee0-0deb-45ac-aa7a-1e42725022d2","children":["3591b02b-72f4-424f-9441-add8422533e8"]},"3591b02b-72f4-424f-9441-add8422533e8":{"id":"3591b02b-72f4-424f-9441-add8422533e8","message":{"id":"3591b02b-72f4-424f-9441-add8422533e8","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878960.6387734,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349","title":"A Survey on Information Bottleneck | IEEE Transactions on ...","snippet":"This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3696412","title":"Causal representation learning through higher-level ...","snippet":"Nov 7, 2024 \u2014 We explore a perspective on this problem that is directed to learning the generative process with causality-related foundations.","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.frontiersin.org","entries":[{"type":"search_result","url":"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1477514/full","title":"A boundedly rational model for category learning","snippet":"by TM Houser \u00b7 2024 \u2014 The goal of this paper is to make progress toward development of an ecologically and neurobiologically plausible model of category learning.See more","ref_id":null,"pub_date":1733702400.0,"attribution":"www.frontiersin.org"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/392430428_Causal_Representation_Learning_A_Quick_Survey","title":"Causal Representation Learning: A Quick Survey","snippet":"Jun 7, 2025 \u2014 In this work we give an overview over several approaches to disentangled representation learning and give a short introduction to variational ...","ref_id":null,"pub_date":1721347200.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/347633182_Information_Bottleneck_Theory_and_Applications_in_Deep_Learning","title":"Information Bottleneck: Theory and Applications in Deep ...","snippet":"Apr 18, 2025 \u2014 This Special Issue aims to investigate the properties of the IB functional in this new context and to propose learning mechanisms inspired by the IB framework.","ref_id":null,"pub_date":1607904000.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2301.08243","title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture","snippet":"","ref_id":null,"pub_date":1674154741.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2305.08013","title":"Information Bottleneck Analysis of Deep Neural Networks ...","snippet":"by I Butakov \u00b7 2023 \u00b7 Cited by 13 \u2014 The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs).","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.23729v2","title":"Bounded Rationality for LLMs: Satisficing Alignment at ...","snippet":"May 31, 2025 \u2014 The common approach in LLM alignment is reinforcement learning from human feedback (RLHF), where a reward model is first learned from human ...See more","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2402.09236","title":"Unifying Causal Representation Learning and Foundation ...","snippet":"by G Rajendran \u00b7 2024 \u00b7 Cited by 29 \u2014 Thus, our work presents a novel framework for identifying concepts by weaving together ideas from causal representation learning and foundation ...","ref_id":null,"pub_date":1733797320.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2310.15709v2","title":"Causal Representation Learning Made Identifiable by ...","snippet":"Jun 7, 2024 \u2014 Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2403.00504v1","title":"Learning and Leveraging World Models in Visual ...","snippet":"Mar 1, 2024 \u2014 It follows a Joint Embedding Predictive Architecture framework (LeCun, 2022) akin to I-JEPA (Assran et al., 2023) . In this framework, the ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2409.08892v1","title":"Exploring Action-Centric Representations Through the ...","snippet":"Sep 13, 2024 \u2014 Rate-distortion theory, which defines optimal lossy compression under constraints, has gained attention as a formal framework to explore goal- ...See more","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"malcolmlett.medium.com","entries":[{"type":"search_result","url":"https://malcolmlett.medium.com/critical-review-of-lecuns-introductory-jepa-paper-fabe5783134e","title":"Critical review of LeCun's Introductory JEPA paper | Medium","snippet":"The I-JEPA model requires about 5x fewer iterations than comparable models (Assran et al., 2023, section 7). The focus on applying a loss ...","ref_id":null,"pub_date":1743124146.118,"attribution":"malcolmlett.medium.com"}]},{"type":"search_result_group","domain":"www.pnas.org","entries":[{"type":"search_result","url":"https://www.pnas.org/doi/10.1073/pnas.1922401117","title":"The bounded rationality of probability distortion","snippet":"by H Zhang \u00b7 2020 \u00b7 Cited by 54 \u2014 We propose a bounded log-odds model (BLO) of probability and relative frequency distortion based on three assumptions.See more","ref_id":null,"pub_date":null,"attribution":"www.pnas.org"}]},{"type":"search_result_group","domain":"crl-community.github.io","entries":[{"type":"search_result","url":"https://crl-community.github.io/neurips24","title":"Causal Representation Learning","snippet":"The goal of this workshop is to explore the challenges and opportunities in this field, discuss recent progress, and identify open questions.","ref_id":null,"pub_date":null,"attribution":"crl-community.github.io"}]},{"type":"search_result_group","domain":"ai.meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/research/publications/self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture/","title":"Self-Supervised Learning from Images with a Joint ...","snippet":"Download the Paper. AUTHORS. Written by. Mido Assran. Quentin Duval. Ishan Misra. Piotr Bojanowski. Pascal Vincent \u00b7 Mike Rabbat \u00b7 Yann LeCun. Nicolas Ballas.","ref_id":null,"pub_date":null,"attribution":"ai.meta.com"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/research-intelligence/nri-topic-summaries/information-bottleneck-in-deep-learning-and-neural-networks-micro-463059","title":"Information Bottleneck in Deep Learning and Neural ...","snippet":"The information bottleneck (IB) principle is a powerful information\u2010theoretic framework that seeks to compress data representations while preserving the ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0010028525000015","title":"Exploring the bounded rationality in human decision ...","snippet":"by YL Lu \u00b7 2025 \u00b7 Cited by 2 \u2014 Some seemingly irrational decision behaviors (anomalies), once seen as flaws in human cognition, have recently received explanations from a rational ...See more","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Self-Supervised-Learning-from-Images-with-a-Assran-Duval/ee57e4d7a125f4ca8916284a857c3760d7d378d3","title":"[PDF] Self-Supervised Learning from Images with a Joint- ...","snippet":"... Assran and Quentin Duval and Ishan Misra and Piotr Bojanowski and Pascal Vincent and Michael G. Rabbat and Yann LeCun and Nicolas Ballas}, journal={2023 ... JEPA ...","ref_id":null,"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=X80k80pPfq&referrer=%5Bthe+profile+of+Shizhe+Hu%5D%28%2Fprofile%3Fid%3D~Shizhe_Hu2%29","title":"A Survey on Information Bottleneck","snippet":"This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"direct.mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=consensus","title":"Bayesian Reinforcement Learning With Limited Cognitive ...","snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 As previously discussed, the incorporation of capacity limitations will be realized through rate- distortion theory; accordingly, we organize the section to ...See more","ref_id":null,"pub_date":null,"attribution":"direct.mit.edu"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/","title":"Causal Inference Meets Deep Learning: A Comprehensive ...","snippet":"by L Jiao \u00b7 2024 \u00b7 Cited by 29 \u2014 In this survey, we provide a comprehensive and structured review of causal inference methods in deep learning.","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"proceedings.neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/b76a9959151d377ddd2c77a275a97475-Paper-Conference.pdf","title":"From Causal to Concept-Based Representation Learning","snippet":"by G Rajendran \u00b7 2024 \u00b7 Cited by 6 \u2014 Sch\u00f6lkopf, and P. Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. arXiv preprint arXiv:2306.02235, 2023. [ ...","ref_id":null,"pub_date":1736801372.0,"attribution":"proceedings.neurips.cc"}]},{"type":"search_result_group","domain":"www.ece.uw.edu","entries":[{"type":"search_result","url":"https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf","title":"Objective-Driven AI","snippet":"Jan 24, 2024 \u2014 LeCun. Image-JEPA: uses masking & transformer architectures. \u201cSSL from images with a JEPA\u201d. [M. Assran et al arxiv:2301.08243]. Jointly embeds a ...","ref_id":null,"pub_date":null,"attribution":"www.ece.uw.edu"}]},{"type":"search_result_group","domain":"www.nogsky.com","entries":[{"type":"search_result","url":"https://www.nogsky.com/project/information-bottleneck-deep-learning/","title":"The Information Bottleneck principle and neural networks","snippet":"Publications \u00b7 Deep learning and the Information Bottleneck principle \u00b7 Early motion processing circuit uses gap junctions to achieve efficient stimuli encoding.","ref_id":null,"pub_date":1576801058.0,"attribution":"www.nogsky.com"}]},{"type":"search_result_group","domain":"www.biorxiv.org","entries":[{"type":"search_result","url":"https://www.biorxiv.org/content/10.1101/2024.10.21.619537v1.full.pdf","title":"Humans are resource-rational predictors in a sequence ...","snippet":"by V Ferdinand \u00b7 2024 \u2014 This research provides new avenues for assessing smart behavior in difficult pre- diction tasks, provides a new methodology for assessing ...See more","ref_id":null,"pub_date":null,"attribution":"www.biorxiv.org"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Search Queries:\n[{'type': 'search', 'q': 'Information Bottleneck deep learning survey 2023 2024'}, {'type': 'search', 'q': 'bounded rationality reinforcement learning rate-distortion survey 2024'}, {'type': 'search', 'q': 'causal representation learning survey 2023 2024 Sch\u00f6lkopf'}, {'type': 'search', 'q': 'I-JEPA paper 2023 Assran LeCun'}]","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"403b5ff0-ceaa-45c4-b87a-5c6f2fd8fc15","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"403b5ff0-ceaa-45c4-b87a-5c6f2fd8fc15","children":["3c466985-02f6-4350-8574-7429bdd67375"]},"3c466985-02f6-4350-8574-7429bdd67375":{"id":"3c466985-02f6-4350-8574-7429bdd67375","message":{"id":"3c466985-02f6-4350-8574-7429bdd67375","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878965.346606,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Looking at mechanistic interpretability with SAEs","content":"Alright, next step: I need to focus on mechanistic interpretability with sparse autoencoders (SAEs). I'll gather related research to better understand how SAEs can make the workings of neural networks clearer. This is going to be important for explaining how these models can be understood better. Let's check for the latest papers!","chunks":[],"finished":true}],"source_analysis_msg_id":"4a016d59-b1ab-4d6f-a757-32f6732182d2"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"3591b02b-72f4-424f-9441-add8422533e8","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"3591b02b-72f4-424f-9441-add8422533e8","children":["8330ddfd-69ea-45f9-b806-48f5af6fe4bb"]},"8330ddfd-69ea-45f9-b806-48f5af6fe4bb":{"id":"8330ddfd-69ea-45f9-b806-48f5af6fe4bb","message":{"id":"8330ddfd-69ea-45f9-b806-48f5af6fe4bb","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878962.206239,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Sparse autoencoders mechanistic interpretability 2024 Anthropic Scaling Monosemanticity\"}, {\"q\": \"OpenAI sparse autoencoders interpretable features 2024 blog\"}, {\"q\": \"Concept bottleneck model survey 2023 2024\"}, {\"q\": \"Neural network verification abstract interpretation survey 2024\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Sparse autoencoders mechanistic interpretability 2024 Anthropic Scaling Monosemanticity"},{"type":"search","q":"OpenAI sparse autoencoders interpretable features 2024 blog"},{"type":"search","q":"Concept bottleneck model survey 2023 2024"},{"type":"search","q":"Neural network verification abstract interpretation survey 2024"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for mechanistic interpretability in sparse autoencoders","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"3c466985-02f6-4350-8574-7429bdd67375","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"3c466985-02f6-4350-8574-7429bdd67375","children":["11d5ec97-470b-49fd-b68e-ffb2f505da33"]},"11d5ec97-470b-49fd-b68e-ffb2f505da33":{"id":"11d5ec97-470b-49fd-b68e-ffb2f505da33","message":{"id":"11d5ec97-470b-49fd-b68e-ffb2f505da33","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878966.8327765,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"transformer-circuits.pub","entries":[{"type":"search_result","url":"https://transformer-circuits.pub/2024/scaling-monosemanticity/","title":"Extracting Interpretable Features from Claude 3 Sonnet","snippet":"May 21, 2024 \u2014 Sparse autoencoders produce interpretable features for large models. Scaling laws can be used to guide the training of sparse autoencoders.","ref_id":null,"pub_date":null,"attribution":"transformer-circuits.pub"},{"type":"search_result","url":"https://transformer-circuits.pub/2023/monosemantic-features","title":"Decomposing Language Models With Dictionary Learning","snippet":"Oct 4, 2023 \u2014 In this paper, we use a weak dictionary learning algorithm called a sparse autoencoder to generate learned features from a trained model that offer a more ...","ref_id":null,"pub_date":null,"attribution":"transformer-circuits.pub"}]},{"type":"search_result_group","domain":"adamkarvonen.github.io","entries":[{"type":"search_result","url":"https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html","title":"An Intuitive Explanation of Sparse Autoencoders for LLM ...","snippet":"Jun 11, 2024 \u2014 Sparse autoencoders have recently gained popularity as a technique to break neural networks down into understandable components.","ref_id":null,"pub_date":1718135218.0,"attribution":"adamkarvonen.github.io"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2505.16705v1","title":"An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations","snippet":"May 22, 2025 \u2014 Q. Shi. A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE TPAMI, 2024.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2410.23903v1","title":"Neural Network Verification with PyRAT","snippet":"Oct 31, 2024 \u2014 This paper presents the PyRAT tool, a neural network verification tool based on abstract interpretation with multiple domains such as zonotopes.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2508.14562v1","title":"Locality-aware Concept Bottleneck Model","snippet":"2 days ago \u2014 We introduce Locality-aware Concept Bottleneck Model, a novel framework promoting concept predictions to be more localized in the corresponding ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"cdn.openai.com","entries":[{"type":"search_result","url":"https://cdn.openai.com/papers/sparse-autoencoders.pdf","title":"Scaling and evaluating sparse autoencoders","snippet":"by L Gao \u00b7 Cited by 244 \u2014 Sparse autoencoders provide a promising unsupervised approach for extracting in- terpretable features from a language model by ...","ref_id":null,"pub_date":1717660634.0,"attribution":"cdn.openai.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=RC5FPYVQaH","title":"Concept Bottleneck Large Language Models","snippet":"by CE Sun \u00b7 Cited by 12 \u2014 We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel framework for building inherently interpretable Large Language Models (LLMs).","ref_id":null,"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=F76bwRSLeK","title":"Sparse Autoencoders Find Highly Interpretable Features in ...","snippet":"by R Huben \u00b7 Cited by 53 \u2014 We use a scalable and unsupervised method called Sparse Autoencoders to find interpretable, monosemantic features in the residual streams of real LLMs.","ref_id":null,"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=tcsZt9ZNKD","title":"Scaling and evaluating sparse autoencoders","snippet":"by L Gao \u00b7 Cited by 244 \u2014 Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"lattice-land.github.io","entries":[{"type":"search_result","url":"https://lattice-land.github.io/abstract-week.html","title":"Abstract Interpretation Workshop (2024) - Lattice Land Book","snippet":"During a full week in June, we gather together to study abstract interpretation ... Introduction to Neural Network Verification by Abstract Interpretation.","ref_id":null,"pub_date":null,"attribution":"lattice-land.github.io"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/ControlProblem/comments/1cygnt9/anthropic_scaling_monosemanticity_extracting/","title":"Scaling Monosemanticity: Extracting Interpretable Features ...","snippet":"The article \"Scaling Monosemanticity\" is about making complex language models easier to understand by breaking them down into simpler parts.","ref_id":null,"pub_date":1716426736.246,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"taylortjohnson.com","entries":[{"type":"search_result","url":"https://taylortjohnson.com/research/tumlin2024icaif.pdf","title":"The Neural Network Verification Tool For Certifying Fairness","snippet":"by A Tumlin \u00b7 2024 \u00b7 Cited by 7 \u2014 Formal verification techniques such as model checking, the- orem proving, and abstract interpretation have been widely adopted to guarantee the ...","ref_id":null,"pub_date":1746321748.0,"attribution":"taylortjohnson.com"}]},{"type":"search_result_group","domain":"www.anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/research/engineering-challenges-interpretability","title":"The engineering challenges of scaling interpretability","snippet":"Jun 13, 2024 \u2014 Our Sparse Autoencoders\u2014the tools we use to investigate \u201cfeatures\u201d\u2014are trained on the activations of transformers, and those activations need to ...","ref_id":null,"pub_date":null,"attribution":"www.anthropic.com"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/extracting-concepts-from-gpt-4/","title":"Extracting Concepts from GPT-4","snippet":"Jun 6, 2024 \u2014 Using new techniques for scaling sparse autoencoders, we automatically identified 16 million patterns in GPT-4's computations.","ref_id":null,"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S1566253525005007","title":"Interpretable prognostics with concept Bottleneck Models","snippet":"by F Forest \u00b7 2025 \u00b7 Cited by 3 \u2014 We propose concept bottleneck models for more interpretable prognostics, where degradation modes of an asset are used as intermediate concepts.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"jmlr.org","entries":[{"type":"search_result","url":"https://jmlr.org/papers/v25/23-0119.html","title":"Critically Assessing the State of the Art in Neural Network ...","snippet":"by M K\u00f6nig \u00b7 2024 \u00b7 Cited by 21 \u2014 Recent research has proposed various methods to formally verify neural networks against minimal input perturbations; this verification task is also known as ...","ref_id":null,"pub_date":null,"attribution":"jmlr.org"}]},{"type":"search_result_group","domain":"www.lesswrong.com","entries":[{"type":"search_result","url":"https://www.lesswrong.com/posts/MKFGByqBRMbgQftRc/mechanistic-interpretability-of-llama-3-2-with-sparse","title":"Mechanistic Interpretability of Llama 3.2 with Sparse ...","snippet":"Nov 23, 2024 \u2014 The project provides a complete end-to-end pipeline for training Sparse Autoencoders to interpret LLM features, from activation capture through training, ...","ref_id":null,"pub_date":1732427120.124,"attribution":"www.lesswrong.com"}]},{"type":"search_result_group","domain":"icml.cc","entries":[{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/45301","title":"ICML Poster Editable Concept Bottleneck Models","snippet":"Jul 17, 2025 \u2014 A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Bytez.com - Designed for Accessibility and to further Open Science","ref_id":null,"pub_date":null,"attribution":"icml.cc"}]},{"type":"search_result_group","domain":"popl24.sigplan.org","entries":[{"type":"search_result","url":"https://popl24.sigplan.org/home/VMCAI-2024","title":"VMCAI 2024","snippet":"Welcome to the website of the 25th International Conference on Verification, Model Checking, and Abstract Interpretation (VMCAI 2024).","ref_id":null,"pub_date":null,"attribution":"popl24.sigplan.org"}]},{"type":"search_result_group","domain":"galileo.ai","entries":[{"type":"search_result","url":"https://galileo.ai/blog/anthropic-ai-interpretability-breakthrough","title":"How Anthropic Made AI 70% More Interpretable","snippet":"Aug 1, 2025 \u2014 Discover Anthropic's breakthrough: sparse autoencoders make AI 70% interpretable by decomposing neurons into readable features.","ref_id":null,"pub_date":null,"attribution":"galileo.ai"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/95178","title":"NeurIPS Poster Coarse-to-Fine Concept Bottleneck Models","snippet":"This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly ...","ref_id":null,"pub_date":1727308800.0,"attribution":"neurips.cc"},{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/98315","title":"Critically Assessing the State of the Art in Neural Network ...","snippet":"Abstract. Recent research has proposed various methods to formally verify neural networks against minimal input perturbations ...","ref_id":null,"pub_date":1728432000.0,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40mohamed-aymen.bouyahia/how-sparse-autoencoders-help-us-understand-language-models-6f7b2225eb9b","title":"How Sparse Autoencoders Help Us Understand Language ...","snippet":"Their work shows that it's possible to extract meaningful, human-interpretable features from dense neural representations \u2014 without supervision.","ref_id":null,"pub_date":1754320540.778,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3489597","title":"The Decoupling Concept Bottleneck Model | IEEE Transactions on Pattern Analysis and Machine Intelligence - ACM Digital Library","snippet":"Feb 1, 2025 \u2014 The Concept Bottleneck Model (CBM) is an interpretable neural network that leverages high-level concepts to explain model decisions and ...","ref_id":null,"pub_date":null,"attribution":"dl.acm.org"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for mechanistic interpretability in sparse autoencoders","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"8330ddfd-69ea-45f9-b806-48f5af6fe4bb","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"8330ddfd-69ea-45f9-b806-48f5af6fe4bb","children":["e82bf6bd-7eb5-4ed5-8ff3-5c87d1fba77a"]},"e82bf6bd-7eb5-4ed5-8ff3-5c87d1fba77a":{"id":"e82bf6bd-7eb5-4ed5-8ff3-5c87d1fba77a","message":{"id":"e82bf6bd-7eb5-4ed5-8ff3-5c87d1fba77a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878969.502751,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Searching for MoE surveys and equivariant networks","content":"Next up, I need to search for surveys on MoE (Mixture of Experts) and group equivariant networks. These are both essential concepts in neural networks, and I'd like to gather the latest research on them. I'll look for some comprehensive surveys so I can stay up-to-date. Let's get started!","chunks":[],"finished":true}],"source_analysis_msg_id":"fa751add-2809-4444-8a49-799603e7e1b5"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"11d5ec97-470b-49fd-b68e-ffb2f505da33","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"11d5ec97-470b-49fd-b68e-ffb2f505da33","children":["2f7baed8-a024-487d-b26f-6de7bb1c63f0"]},"2f7baed8-a024-487d-b26f-6de7bb1c63f0":{"id":"2f7baed8-a024-487d-b26f-6de7bb1c63f0","message":{"id":"2f7baed8-a024-487d-b26f-6de7bb1c63f0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878967.833849,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Mixture of Experts survey 2024 2025 large language models\"}, {\"q\": \"Group equivariant neural network survey 2023 2024 E(n) equivariant\"}, {\"q\": \"Invariant Risk Minimization critique 2021 2022 2024 survey\"}, {\"q\": \"Empowerment information theoretic intrinsic motivation survey 2020 2024\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Mixture of Experts survey 2024 2025 large language models"},{"type":"search","q":"Group equivariant neural network survey 2023 2024 E(n) equivariant"},{"type":"search","q":"Invariant Risk Minimization critique 2021 2022 2024 survey"},{"type":"search","q":"Empowerment information theoretic intrinsic motivation survey 2020 2024"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for MoE surveys and equivariant networks","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"e82bf6bd-7eb5-4ed5-8ff3-5c87d1fba77a","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"e82bf6bd-7eb5-4ed5-8ff3-5c87d1fba77a","children":["c295c4c0-0f0d-4a48-aaeb-439d64d2e771"]},"c295c4c0-0f0d-4a48-aaeb-439d64d2e771":{"id":"c295c4c0-0f0d-4a48-aaeb-439d64d2e771","message":{"id":"c295c4c0-0f0d-4a48-aaeb-439d64d2e771","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878971.5935757,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2407.06204","title":"A Survey on Mixture of Experts","snippet":"","ref_id":null,"pub_date":1719419673.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2401.17541v3","title":"Towards Understanding Variants of Invariant Risk ...","snippet":"Invariant Risk Minimization (IRM) emerges as a solution that aims to identify invariant features across different environments to enhance OOD robustness.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2407.06204","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"by W Cai \u00b7 2024 \u00b7 Cited by 57 \u2014 As depicted in Figure 1, MoE has maintained a robust trajectory of growth, particularly notable in 2024 with the advent of Mixtral-8x7B [26] and ...","ref_id":null,"pub_date":1744246176.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2305.11141","title":"Clifford Group Equivariant Neural Networks","snippet":"","ref_id":null,"pub_date":1684431335.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2405.01389","title":"Invariant Risk Minimization Is A Total Variation Model","snippet":"by ZR Lai \u00b7 2024 \u00b7 Cited by 1 \u2014 Invariant Risk Minimization (IRM) is a machine learning approach to generalize invariant features, and it is essentially a total variation (TV) model.","ref_id":null,"pub_date":1716163976.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.23631v2","title":"Intrinsically-Motivated Humans and Agents in Open-World ...","snippet":"27 May 2025 \u2014 We investigate to what extent human and agent exploration optimize information-theoretic intrinsic objectives. We focus on Entropy, Information ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=n84bzMrGUD&noteId=sQG6abJbs8","title":"Clifford Group Equivariant Neural Networks","snippet":"The paper develops a new kind of neural network model based on the Clifford group, which makes the neural network E(n)-equivariant.","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/368443560_An_Information-Theoretic_Perspective_on_Intrinsic_Motivation_in_Reinforcement_Learning_A_Survey","title":"(PDF) An Information-Theoretic Perspective on Intrinsic ...","snippet":"7 Feb 2023 \u2014 We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise ...","ref_id":null,"pub_date":1675987200.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/393724282_Mixture_of_Experts_in_Large_Language_Models","title":"(PDF) Mixture of Experts in Large Language Models","snippet":"Jul 18, 2025 \u2014 This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability ...","ref_id":null,"pub_date":1752537600.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"link.aps.org","entries":[{"type":"search_result","url":"https://link.aps.org/doi/10.1103/PRXLife.2.033009","title":"Intrinsic Motivation in Dynamical Control Systems | PRX Life","snippet":"by S Tiomkin \u00b7 2024 \u00b7 Cited by 8 \u2014 In this study, we investigate an information-theoretic approach to intrinsic motivation, based on maximizing an agent's empowerment.","ref_id":null,"pub_date":1724889600.0,"attribution":"link.aps.org"},{"type":"search_result","url":"https://link.aps.org/doi/10.1103/PRXQuantum.5.020328","title":"Theory for Equivariant Quantum Neural Networks","snippet":"by QT Nguyen \u00b7 2024 \u00b7 Cited by 136 \u2014 A comprehensive theoretical framework to design equivariant quantum neural networks (EQNNs) for essentially any relevant symmetry group.","ref_id":null,"pub_date":1714953600.0,"attribution":"link.aps.org"}]},{"type":"search_result_group","domain":"www.computer.org","entries":[{"type":"search_result","url":"https://www.computer.org/csdl/journal/tk/2025/07/10937907/25n2xHILEpG","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"by W Cai \u00b7 2025 \u00b7 Cited by 55 \u2014 As depicted in Fig. 1, MoE has maintained a robust trajectory of growth, particularly notable in 2024 with the advent of Mixtral-8x7B and ...","ref_id":null,"pub_date":null,"attribution":"www.computer.org"}]},{"type":"search_result_group","domain":"pubsonline.informs.org","entries":[{"type":"search_result","url":"https://pubsonline.informs.org/doi/abs/10.1287/trsc.2024.0562","title":"diffIRM: A Diffusion-Augmented Invariant Risk Minimization ...","snippet":"by Z Mo \u00b7 2025 \u00b7 Cited by 3 \u2014 In this study, we fill in this research gap and propose a diffusion-augmented invariant risk minimization (diffIRM) framework that combines ...","ref_id":null,"pub_date":null,"attribution":"pubsonline.informs.org"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"A Survey on Mixture of Experts in. Large Language Models ... A chronological overview of several representative Mixture-of-Experts (MoE) models in recent years.","ref_id":null,"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41467-023-38468-8","title":"General framework for E(3)-equivariant neural network ...","snippet":"by X Gong \u00b7 2023 \u00b7 Cited by 136 \u2014 Here we propose an E(3)-equivariant deep-learning framework to represent density functional theory (DFT) Hamiltonian as a function of material structure.","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/96433","title":"Dissecting the Failure of Invariant Learning on Graphs","snippet":"In this paper, we develop a Structural Causal Model (SCM) to theoretically dissect the performance of two prominent invariant learning methods\u2014Invariant Risk ...","ref_id":null,"pub_date":1727308800.0,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"link.springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/rwe/10.1007/978-981-19-2080-6_10","title":"Theories of Motivation and Empowerment in Open, ...","snippet":"by C Ng \u00b7 2023 \u00b7 Cited by 4 \u2014 This chapter provides an introduction to key theoretical perspectives on motivation, including sociocognitive theories, sociocultural theories, and the concept ...","ref_id":null,"pub_date":1692662400.0,"attribution":"link.springer.com"}]},{"type":"search_result_group","domain":"proceedings.mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v233/pepe24a/pepe24a.pdf","title":"Clifford Group Equivariant Neural Network Layers for Protein ...","snippet":"by A Pepe \u00b7 2024 \u00b7 Cited by 5 \u2014 We employ Clifford Group Equivariant Neural Net- work (CGENN) layers to predict protein coordinates in a Protein Structure Prediction (PSP) pipeline.","ref_id":null,"pub_date":1703173107.0,"attribution":"proceedings.mlr.press"}]},{"type":"search_result_group","domain":"www.news.com.au","entries":[{"type":"search_result","url":"https://www.news.com.au/finance/business/complete-collapse-bombshell-report-into-ai-accuracy-indicates-your-job-is-probably-safe-for-now/news-story/606008820ca6ac851736eaf8d549ab5f","title":"'Complete collapse': Bombshell blow to AI","snippet":"A recent research paper from a major tech firm has cast significant doubt on the path forward for artificial intelligence, particularly large reasoning and language models. The study found that while language models like ChatGPT managed simpler tasks well, both language and reasoning models failed complex challenges, often reducing reasoning effort as tasks became harder. Experts like Dr. Niusha Shafiabady and Dr. Gary Marcus criticized overblown expectations, with Marcus labeling the AI hype a \"bait and switch.\" Accuracy issues are rampant, as demonstrated by false news alerts, flawed chatbot interactions, and unreliable outputs across platforms like OpenAI's ChatGPT, Google's Gemini, and Apple's AI news assistant. These failings have led firms such as Klarna and Anysphere to face customer dissatisfaction and operational setbacks. Surveys by the Association for the Advancement of Artificial Intelligence and consulting firms show growing skepticism and rising abandonment of AI initiatives in business. Despite billions invested, only a minority have successfully integrated AI into workflows. Fundamental problems\u2014like algorithm limitations and inability to handle complex problems\u2014suggest that AI, as it stands today, may have hit developmental barriers, prompting a reevaluation of its immediate potential in real-world applications.","ref_id":null,"pub_date":1749587760.0,"attribution":"www.news.com.au"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for MoE surveys and equivariant networks","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"2f7baed8-a024-487d-b26f-6de7bb1c63f0","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"2f7baed8-a024-487d-b26f-6de7bb1c63f0","children":["50bbc042-2727-4c91-a4f4-2a04bc1cbce6"]},"50bbc042-2727-4c91-a4f4-2a04bc1cbce6":{"id":"50bbc042-2727-4c91-a4f4-2a04bc1cbce6","message":{"id":"50bbc042-2727-4c91-a4f4-2a04bc1cbce6","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878975.822287,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Exploring research on embodied AI and active inference","content":"Okay, I\u2019m passing on news.com.au since it's low quality. Now, I want to focus on research in embodied AI, active inference, world models in robotics, and ideas like morphological computation and enactive cognition. These concepts are vital for understanding intelligent behavior in complex environments. Let\u2019s dig deeper into these topics!","chunks":[],"finished":true}],"source_analysis_msg_id":"0a93c161-7cad-4c80-a9e9-55f68288ea4f"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"c295c4c0-0f0d-4a48-aaeb-439d64d2e771","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"c295c4c0-0f0d-4a48-aaeb-439d64d2e771","children":["6a55cd0c-6522-41d2-bd18-b8693532d8f7"]},"6a55cd0c-6522-41d2-bd18-b8693532d8f7":{"id":"6a55cd0c-6522-41d2-bd18-b8693532d8f7","message":{"id":"6a55cd0c-6522-41d2-bd18-b8693532d8f7","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878973.316998,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Active inference robotics survey 2023 2024 Friston review\"}, {\"q\": \"Embodied AI survey 2023 2024 robotics multimodal embodied agents\"}, {\"q\": \"social intelligence multi-agent LLM theory of mind 2024 survey\"}, {\"q\": \"value alignment decision making under uncertainty overview 2024 RLHF critique\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Active inference robotics survey 2023 2024 Friston review"},{"type":"search","q":"Embodied AI survey 2023 2024 robotics multimodal embodied agents"},{"type":"search","q":"social intelligence multi-agent LLM theory of mind 2024 survey"},{"type":"search","q":"value alignment decision making under uncertainty overview 2024 RLHF critique"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for surveys on embodied AI and robotics","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"50bbc042-2727-4c91-a4f4-2a04bc1cbce6","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"50bbc042-2727-4c91-a4f4-2a04bc1cbce6","children":["40a5b578-ab0c-4211-9589-0a6706b02ab1"]},"40a5b578-ab0c-4211-9589-0a6706b02ab1":{"id":"40a5b578-ab0c-4211-9589-0a6706b02ab1","message":{"id":"40a5b578-ab0c-4211-9589-0a6706b02ab1","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878977.5493197,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/356804385_Active_Inference_in_Robotics_and_Artificial_Agents_Survey_and_Challenges","title":"(PDF) Active Inference in Robotics and Artificial Agents","snippet":"Dec 9, 2021 \u2014 Here, we review the state-of-the-art theory and implementations of active inference for state-estimation, control, planning and learning; ...","ref_id":null,"pub_date":1638489600.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2407.06886v1","title":"A Comprehensive Survey on Embodied AI","snippet":"Jul 9, 2024 \u2014 In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.06470v1","title":"A Survey of Theory of Mind in Large Language Models","snippet":"Feb 10, 2025 \u2014 In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models (LLMs), identify important safety risks from advanced ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2410.23726v1","title":"Towards Reliable Alignment: Uncertainty-aware RLHF","snippet":"Oct 31, 2024 \u2014 RLHF is a fine-tuning method to align the behavior of LLMs with human values and preferences.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.06886v6","title":"A Comprehensive Survey on Embodied AI","snippet":"Jul 29, 2024 \u2014 In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2412.17481v2","title":"A Survey on LLM-based Multi-Agent System","snippet":"In this paper, we provide a new perspective based on previous reviews of LLM-based Multi-Agent Systems ( LLM-MAS ) with a focus on recent advancements and ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2501.08617v2","title":"Mitigating Misalignment in RLHF with Hindsight Simulation","snippet":"Feb 10, 2025 \u2014 We argue that RLHF misalignment more generally emerges in settings with significant human uncertainty, whether perceptual, predictive, or a ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2406.09322","title":"arXiv:2406.09322v2 [cs.LG] 13 Nov 2024","snippet":"by YT Yeganeh \u00b7 2024 \u00b7 Cited by 1 \u2014 Active inference is a unifying theory that integrates inference, perception, and action by emphasizing the dependence of observations on actions ...","ref_id":null,"pub_date":1731549263.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S1364661323002607","title":"active inference and the scope and limits of passive AI","snippet":"by G Pezzulo \u00b7 2024 \u00b7 Cited by 73 \u2014 We discuss the foundations of generative AI systems by comparing them with our current understanding of living organisms, when seen as active inference systems.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S1571064525000879","title":"Active inference and cognitive control: Balancing ...","snippet":"by R Proietti \u00b7 2025 \u2014 Our simulations show that a standard active inference model can form adaptive habits; i.e., can pass from deliberative to habitual control when the context is ...","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S1566253525002714","title":"Exploring Embodied Multimodal Large Models","snippet":"by S Chen \u00b7 2025 \u00b7 Cited by 6 \u2014 This comprehensive review explores the development of such models, including Large Language Models (LLMs), Large Vision Models (LVMs), and other models.","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC12137480/","title":"Helpful, harmless, honest? Sociotechnical limits of AI ...","snippet":"by AD Lindstr\u00f6m \u00b7 2025 \u00b7 Cited by 2 \u2014 (2024) argue that RLHF-trained models tend to exhibit \u201csteerable\u201d alignment, a form of sycophancy. The study suggests that while steerability ...","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List","title":"Paper List and Resource Repository for Embodied AI","snippet":"In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of ...","ref_id":null,"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/zchoi/Awesome-Embodied-Robotics-and-Agent","title":"zchoi/Awesome-Embodied-Robotics-and-Agent","snippet":"This is a curated list of Embodied robotics or agent with Vision-Language Models (VLMs) and Large Language Models (LLMs) research which is maintained by haonan.","ref_id":null,"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers","title":"Large Language Model based Multi-Agents: A Survey ...","snippet":"This repo is created to maintain LLM-based Multi-Agents papers. We categorized these papers into five main streams.","ref_id":null,"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"www.ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2024/0890.pdf","title":"Large Language Model Based Multi-agents","snippet":"by T Guo \u00b7 Cited by 758 \u2014 To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi- ...","ref_id":null,"pub_date":1721939614.0,"attribution":"www.ijcai.org"}]},{"type":"search_result_group","domain":"www.sciopen.com","entries":[{"type":"search_result","url":"https://www.sciopen.com/article/10.26599/AIR.2024.9150042","title":"A Comprehensive Survey on Embodied Intelligence","snippet":"by F Sun \u00b7 2024 \u00b7 Cited by 4 \u2014 Bcent aims to enhance the adaptability, robustness, and intelligence of embodied systems, aligning with the ongoing progress in robotics, ...","ref_id":null,"pub_date":null,"attribution":"www.sciopen.com"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41562-024-01882-z","title":"Testing theory of mind in large language models and humans","snippet":"by JWA Strachan \u00b7 2024 \u00b7 Cited by 297 \u2014 Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities.","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"www.alignmentforum.org","entries":[{"type":"search_result","url":"https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research","title":"Thoughts on the impact of RLHF research","snippet":"Jan 25, 2023 \u2014 I'll discuss various arguments that RLHF research had an overall negative impact and explain why I don't find them persuasive.","ref_id":null,"pub_date":1674667396.402,"attribution":"www.alignmentforum.org"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=otW0TJOUYF","title":"Scaffolding Theory of Mind for Multi-Agent Tasks with ...","snippet":"by L Cross \u00b7 Cited by 19 \u2014 We introduce Hypothetical Minds, an LLM-based agent that outperforms MARL and LLM baselines on multi-agent tasks using a novel Theory of Mind module.","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"stars.library.ucf.edu","entries":[{"type":"search_result","url":"https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1164&context=hut2024","title":"Reinforcement Learning From Human Feedback For Ethically ...","snippet":"by MM Plasencia \u00b7 2024 \u2014 The central question driving this research is: How can RLHF enhance the robustness of AI's decision-making capabilities in ethically complex.","ref_id":null,"pub_date":null,"attribution":"stars.library.ucf.edu"}]},{"type":"search_result_group","domain":"www.mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/7/582","title":"Active Inference for Learning and Development in ...","snippet":"by S Hamburg \u00b7 2024 \u00b7 Cited by 3 \u2014 Rooted in Bayesian inference, it combines elements of perception, action, and learning into a unified theory. Originating from neuroscience, it ...","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"}]},{"type":"search_result_group","domain":"nips.cc","entries":[{"type":"search_result","url":"https://nips.cc/virtual/2024/poster/93425","title":"Decision-Making Behavior Evaluation Framework for LLMs ...","snippet":"Dec 9, 2024 \u2014 When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: ...","ref_id":null,"pub_date":1727308800.0,"attribution":"nips.cc"}]},{"type":"search_result_group","domain":"www.wired.com","entries":[{"type":"search_result","url":"https://www.wired.com/story/googles-gemini-robotics-ai-model-that-reaches-into-the-physical-world","title":"Google's Gemini Robotics AI Model Reaches Into the Physical World","snippet":"Google DeepMind has announced a new version of its AI model, Gemini Robotics, which integrates language, vision, and physical action to enable robots to complete complex tasks. Demonstration videos showed robots performing tasks such as folding paper and handling objects based on spoken commands. This model can be adapted across different hardware and aims to be used by other researchers to develop their own robotic capabilities. The Gemini Robotics-ER model focuses on visual and spatial understanding for embodied reasoning. With broader abilities and potential applications, it represents an advancement in combining large language models with physical actions. Despite these advancements, there are concerns about the risks and safety of AI-powered robots, leading Google DeepMind to introduce a new benchmark called ASIMOV to help identify potentially dangerous behaviors. The work is at an early stage, with no immediate plans for commercialization.","ref_id":null,"pub_date":1741791600.0,"attribution":"www.wired.com"}]},{"type":"search_result_group","domain":"www.theguardian.com","entries":[{"type":"search_result","url":"https://www.theguardian.com/technology/2025/may/14/ai-can-spontaneously-develop-human-like-communication-study-finds","title":"AI can spontaneously develop human-like communication, study finds","snippet":"A recent study by City St George's, University of London, and the IT University of Copenhagen has demonstrated that artificial intelligence (AI), specifically large language models (LLMs), can independently develop human-like social conventions through group interactions. The researchers, led by doctoral student Ariel Flint Ashery, found that when AI agents communicate without external input, they can spontaneously form linguistic norms similar to those found in human society. In the experiments, agents were paired to select a common \"name\" from a set of options, receiving rewards or penalties based on agreement. Despite limited memory and obliviousness to the larger group, the agents collectively established consistent naming conventions. Professor Andrea Baronchelli noted that these behaviors resembled natural language evolution, such as the adoption of the term \"spam.\" The study also observed emergent group biases and critical mass dynamics, where small clusters of agents could shift population-wide conventions. These findings highlight the social potential of AI systems and offer insights for AI safety and ethics, emphasizing the importance of understanding how autonomous AI agents might influence future human-AI interactions. The study, \u201cEmergent Social Conventions and Collective Bias in LLM Populations,\u201d is published in Science Advances.","ref_id":null,"pub_date":1747245603.0,"attribution":"www.theguardian.com"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for surveys on embodied AI and robotics","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"6a55cd0c-6522-41d2-bd18-b8693532d8f7","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"6a55cd0c-6522-41d2-bd18-b8693532d8f7","children":["a9ae71dd-2793-43f1-be37-1d2fd053a354"]},"a9ae71dd-2793-43f1-be37-1d2fd053a354":{"id":"a9ae71dd-2793-43f1-be37-1d2fd053a354","message":{"id":"a9ae71dd-2793-43f1-be37-1d2fd053a354","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878983.575067,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Searching for DreamerV3 and world models","content":"I\u2019ll skip news articles for now but the Guardian referencing Science Advances might be relevant. I\u2019ll focus on reputable sources for citations like arXiv, Nature, IEEE, and PNAS. Next, I want to look into DreamerV3 and world models, so I'll start searching for up-to-date information on those topics.","chunks":[],"finished":true}],"source_analysis_msg_id":"3503188f-64ee-47ee-b7e6-330f7c98dfa8"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"40a5b578-ab0c-4211-9589-0a6706b02ab1","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"40a5b578-ab0c-4211-9589-0a6706b02ab1","children":["ca48696f-6102-4528-85c7-897979b6f6a3"]},"ca48696f-6102-4528-85c7-897979b6f6a3":{"id":"ca48696f-6102-4528-85c7-897979b6f6a3","message":{"id":"ca48696f-6102-4528-85c7-897979b6f6a3","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878979.186832,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"DreamerV3 paper 2023 2024 world models Hafner\"}, {\"q\": \"rate distortion control reinforcement learning Ortega Tishby 2015 2019 survey\"}, {\"q\": \"rational inattention reinforcement learning 2022 2024\"}, {\"q\": \"free energy principle expected free energy decision theory 2024 review\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"DreamerV3 paper 2023 2024 world models Hafner"},{"type":"search","q":"rate distortion control reinforcement learning Ortega Tishby 2015 2019 survey"},{"type":"search","q":"rational inattention reinforcement learning 2022 2024"},{"type":"search","q":"free energy principle expected free energy decision theory 2024 review"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for DreamerV3 and reinforcement learning papers","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"a9ae71dd-2793-43f1-be37-1d2fd053a354","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"a9ae71dd-2793-43f1-be37-1d2fd053a354","children":["295eee80-b382-4359-8654-29d2012ea087"]},"295eee80-b382-4359-8654-29d2012ea087":{"id":"295eee80-b382-4359-8654-29d2012ea087","message":{"id":"295eee80-b382-4359-8654-29d2012ea087","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1755878986.428723,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2301.04104","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 840 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2301.04104","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 837 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":null,"pub_date":1713402880.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2210.16877","title":"On Rate-Distortion Theory in Capacity-Limited Cognition & ...","snippet":"by D Arumugam \u00b7 2022 \u00b7 Cited by 5 \u2014 Rate-distortion theory quantifies the minimum bits of information needed to achieve a bound on loss of fidelity in lossy compression. It is ...","ref_id":null,"pub_date":1667265135.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2504.14898v2","title":"Expected Free Energy-based Planning as Variational Inference - arXiv","snippet":"","ref_id":null,"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2404.18775v4","title":"Resource-rational reinforcement learning and ...","snippet":"Mar 19, 2025 \u2014 We propose a new computational-level objective function for theoretical biology and theoretical neuroscience that combines: reinforcement ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"dilipa.github.io","entries":[{"type":"search_result","url":"https://dilipa.github.io/papers/nips22_infocog_workshop_rdt_satisficing.pdf","title":"On Rate-Distortion Theory in Capacity-Limited Cognition ...","snippet":"by D Arumugam \u00b7 Cited by 5 \u2014 Bayesian reinforcement learning: A survey. Foundations and Trends ... Reinforcement learning and control as probabilistic inference: Tutorial and review.","ref_id":null,"pub_date":1666795674.0,"attribution":"dilipa.github.io"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/figure/Rational-inattention-and-reinforcement-learning-A-Using-a-non-stationary-three-armed_fig4_350365666","title":"Rational inattention and reinforcement learning (A) Using a...","snippet":"This confluence of these results suggests a mechanism of attentional effort that operates by controlling gain through using partially distinct population codes ...","ref_id":null,"pub_date":null,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/367019841_Mastering_Diverse_Domains_through_World_Models","title":"(PDF) Mastering Diverse Domains through World Models","snippet":"Jan 10, 2023 \u2014 Recent work in model-based RL have shown that scaling the world model achieves significant improvements (Hafner et al., 2023; Hansen et al., ...","ref_id":null,"pub_date":1673308800.0,"attribution":"www.researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/364953358_On_Rate-Distortion_Theory_in_Capacity-Limited_Cognition_Reinforcement_Learning","title":"On Rate-Distortion Theory in Capacity-Limited Cognition & ...","snippet":"In this paper, we aim to elucidate this latter perspective by presenting a brief survey of these information-theoretic models of capacity-limited decision ...","ref_id":null,"pub_date":null,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"pmc.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/","title":"Rational inattention in neural coding for economic choice","snippet":"by JM Fine \u00b7 2024 \u2014 Rational inattention is a Bayesian observer theory that describes how the actual values (AV) on offer are encoded as a noisy percept (likelihood) and combined ...","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"},{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC10353860/","title":"Rate-distortion theory of neural coding and its implications ...","snippet":"by AMV Jakob \u00b7 2023 \u00b7 Cited by 28 \u2014 Rate-distortion theory provides a powerful framework for understanding the nature of human memory by formalizing the relationship between information rate.","ref_id":null,"pub_date":null,"attribution":"pmc.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"danijar.com","entries":[{"type":"search_result","url":"https://danijar.com/project/dreamerv3/","title":"Mastering Diverse Control Tasks through World Models","snippet":"Dreamer is the first algorithm that collects diamonds in Minecraft from sparse rewards, without expert demonstrations or curricula, solving this challenge.","ref_id":null,"pub_date":null,"attribution":"danijar.com"}]},{"type":"search_result_group","domain":"direct.mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf","title":"Bayesian Reinforcement Learning With Limited Cognitive ...","snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 Finally, we note while the work cited thus far was developed in the reinforcement learn- ing community, the coupling of rate-distortion theory and Bayesian ...","ref_id":null,"pub_date":null,"attribution":"direct.mit.edu"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=DY1pMrmDkm&noteId=a5vg9bCJJp","title":"Modeling Bounded Rationality in Multi-Agent Simulations ...","snippet":"In this work, we propose a new, more human-like RL agent, which incorporates an established model of human-irrationality, the Rational Inattention (RI) model.","ref_id":null,"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=ByxAOoR5K7","title":"POLICY GENERALIZATION IN CAPACITY-LIMITED ...","snippet":"by RA Lerch \u00b7 Cited by 6 \u2014 Hence, an important goal of the paper is a an accessible introduction of rate-distortion theory in the context of RL. Combining these two ideas, we consider the ...","ref_id":null,"pub_date":1543288522.0,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=T6lAFguUbw","title":"Modeling Bounded Rationality in Multi-Agent Simulations ...","snippet":"by T Mu \u00b7 Cited by 5 \u2014 This paper studies human bounded rationality using multi-agent reinforcement learning. It particularly focuses on the rational inattention model ...","ref_id":null,"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41586-025-08744-2","title":"Mastering diverse control tasks through world models","snippet":"by D Hafner \u00b7 2025 \u00b7 Cited by 60 \u2014 Here we present the third generation of Dreamer, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single ...","ref_id":null,"pub_date":null,"attribution":"www.nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41467-023-40141-z","title":"Experimental validation of the free-energy principle with in vitro ...","snippet":"","ref_id":null,"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"www.lesswrong.com","entries":[{"type":"search_result","url":"https://www.lesswrong.com/posts/MArdnet7pwgALaeKs/why-i-m-not-into-the-free-energy-principle","title":"Why I'm not into the Free Energy Principle - LessWrong","snippet":"","ref_id":null,"pub_date":1677785272.309,"attribution":"www.lesswrong.com"}]},{"type":"search_result_group","domain":"vitalab.github.io","entries":[{"type":"search_result","url":"https://vitalab.github.io/article/2023/01/19/DreamerV3.html","title":"DreamerV3: Mastering Diverse Domains through World Models","snippet":"Jan 19, 2023 \u2014 Model-based reinforcement learning is a type of RL where the agent will not only learn a policy, but also a world model which the agent can use to plan ahead.","ref_id":null,"pub_date":null,"attribution":"vitalab.github.io"}]},{"type":"search_result_group","domain":"www.mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/9/797","title":"Forced Friends: Why the Free Energy Principle Is Not the New ...","snippet":"","ref_id":null,"pub_date":null,"attribution":"www.mdpi.com"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S002205312500016X","title":"Modeling machine learning: A cognitive economic approach","snippet":"by A Caplin \u00b7 2025 \u00b7 Cited by 7 \u2014 Our analysis illustrates several advantages of applying cognitive economic methods and rational inattention theory to machine learners. First, the machine's ...","ref_id":null,"pub_date":null,"attribution":"www.sciencedirect.com"}]}],"debug_sonic_thread_id":"thread_68a894a713a88007a44f7d275858f4f4","reasoning_status":"is_reasoning","reasoning_title":"Searching for DreamerV3 and reinforcement learning papers","classifier_response":"default","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"ca48696f-6102-4528-85c7-897979b6f6a3","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"ca48696f-6102-4528-85c7-897979b6f6a3","children":["2c92a4bd-5eff-4302-89a3-1e638011cd9b"]},"2c92a4bd-5eff-4302-89a3-1e638011cd9b":{"id":"2c92a4bd-5eff-4302-89a3-1e638011cd9b","message":{"id":"2c92a4bd-5eff-4302-89a3-1e638011cd9b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878997.188897,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Synthesizing the commentary","content":"We\u2019ve gathered enough sources for this. Now, I\u2019ll organize the main commentary into sections with headings: \"What's unique about the provided response,\" \"Alignment with our framework,\" \"Where it extends or differs,\" \"How it fits with current research,\" \"Gaps or imprecisions,\" and \"Next steps/tests.\" I\u2019ll insert citations where appropriate, keeping them balanced.","chunks":[],"finished":true},{"summary":"Organizing references for synthesis","content":"I've mapped out the references needed for key topics. Causal representation learning (CRL) and invariant risk will cite recent foundational works like \"Unifying CRL\" and others. For JEPA and world models, I\u2019ll mention several key papers from 2023 and 2024. I\u2019ll also do the same for mechanistic interpretability and embodied/active inference research. Each section will align firmly with supporting literature from 2024-2025.","chunks":[],"finished":true},{"summary":"Synthesizing commentary","content":"The user's response brings a unique perspective, reframing earlier operations as part of cognitive capabilities. It emphasizes that intelligence isn't synonymous with these operations, but uses them for adaptive behavior, merging computational with embodied views. The distinction between creativity and social contexts fits well with our \"Organize + Model + Decide + Explore\" framework.\n\nTheir emphasis on motivation, emotion, and consciousness aligns with our argument that organization alone isn\u2019t enough\u2014these elements can be formalized in bounded rationality or active inference frameworks, with affect acting as a control cost or homeostatic setpoint.","chunks":[],"finished":true}],"source_analysis_msg_id":"6a1fcd4c-62e9-4b03-b9f3-d4736f155998"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"295eee80-b382-4359-8654-29d2012ea087","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"295eee80-b382-4359-8654-29d2012ea087","children":["2eb7732d-384a-48a0-a291-c7ed857f621c"]},"2eb7732d-384a-48a0-a291-c7ed857f621c":{"id":"2eb7732d-384a-48a0-a291-c7ed857f621c","message":{"id":"2eb7732d-384a-48a0-a291-c7ed857f621c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755879022.937582,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 1m 48s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":108,"request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"2c92a4bd-5eff-4302-89a3-1e638011cd9b","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"2c92a4bd-5eff-4302-89a3-1e638011cd9b","children":["512e83e8-bca4-4e04-9748-9cb77bdc7433"]},"512e83e8-bca4-4e04-9748-9cb77bdc7433":{"id":"512e83e8-bca4-4e04-9748-9cb77bdc7433","message":{"id":"512e83e8-bca4-4e04-9748-9cb77bdc7433","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1755878988.20481,"update_time":null,"content":{"content_type":"text","parts":["Here\u2019s a crisp assessment of that response, anchored to our framework and to what the literature is actually doing today.\n\n# What\u2019s uniquely good about this response\nTwo things stand out:\n\n1) It reframes the claim from \u201cintelligence = organization\u201d to \u201corganization **in service of adaptive behavior**.\u201d That move brings goals, action, and uncertainty into scope\u2014exactly where purely representational stories tend to break. This aligns with modern information-theoretic accounts that combine compression with control (rate\u2013distortion, rational inattention) and with active-inference style objectives. \ue200cite\ue202turn0search17\ue202turn4search6\ue202turn0search21\ue202turn4search7\ue201\n\n2) It cleanly clusters your ten \u201corganization\u201d operators into cognitive capacities (pattern recognition/transfer, prediction, abstraction hierarchies, efficiency, recursion). That makes the hypothesis operational without being hand-wavy, and it matches how contemporary self-supervised and world-model approaches are built (e.g., JEPA, Dreamer-style agents). \ue200cite\ue202turn0search3\ue202turn0search23\ue202turn0search19\ue202turn4search0\ue201\n\n# Where it meshes with our analysis (and the field)\n\n- **Organization is necessary but not sufficient.** The reply\u2019s \u201climitations\u201d list (goals, creativity, embodiment/sociality, understanding, affect) is consistent with our decomposition: Organize + Model + Decide + Explore. Current research explicitly joins these pieces:  \n  \u2022 **Organize/Compress \u2192 Predict:** Deep Information Bottleneck and MDL-style analyses of learned representations. \ue200cite\ue202turn0search0\ue202turn0search4\ue201  \n  \u2022 **Decide under limits:** Rate\u2013distortion / bounded-rational decision-making and rational inattention in RL and neuroscience. \ue200cite\ue202turn0search17\ue202turn4search6\ue201  \n  \u2022 **Model for OOD:** Causal representation learning and invariance principles to support extrapolation and intervention. \ue200cite\ue202turn0search18\ue202turn0search14\ue201  \n  \u2022 **World models for action:** JEPA-style predictive abstractions and Dreamer-class model-based RL. \ue200cite\ue202turn0search3\ue202turn4search0\ue201\n\n- **Interpretability as \u201celucidation.\u201d** The response\u2019s emphasis on making structure explicit is on trend with sparse-autoencoder-based mechanistic interpretability and concept bottlenecks, which seek monosemantic (human-nameable) features and editability. \ue200cite\ue202turn1search0\ue202turn1search12\ue202turn1search16\ue201\n\n- **Specialization vs. generality.** Its nod to division-of-labor resonates with Mixture-of-Experts scaling (compute-efficient specialization) and with symmetry/equivariance for structured generalization. \ue200cite\ue202turn2search4\ue202turn2search13\ue201\n\n- **Embodiment and social intelligence.** The call-out is timely: embodied-AI surveys and ToM evaluations for LLMs have been active, and both expose the limits of \u201corganization without agency.\u201d \ue200cite\ue202turn3search1\ue202turn3search14\ue201\n\n# Where I\u2019d push further (constructive critique)\n\n- **Recursion \u2260 meta-cognition by itself.** Recursion enables algorithmic generalization, but meta-cognition needs *self-models of uncertainty and limits* (e.g., resource-rational planning or active-inference EFE terms). Otherwise you get procedures, not self-monitoring. \ue200cite\ue202turn4search7\ue202turn4search14\ue201\n\n- **Creativity needs an explicit search or objective.** Recombination from organized representations is not yet *creative* unless paired with exploration pressures (novelty, information gain, empowerment) or constraints that reward useful surprise. That is an active, measurable line of work. \ue200cite\ue202turn2search3\ue202turn2search7\ue201\n\n- **Embodiment should be tied to *control-relevant* compression.** The right lens isn\u2019t just \u201cput the model on a robot,\u201d it\u2019s *goal-conditioned efficient coding* (rate\u2013distortion with action costs) or active-inference: keep exactly the bits that matter for acting well. \ue200cite\ue202turn0search21\ue202turn4search7\ue201\n\n- **\u201cUnderstanding\u201d should pay rent under intervention.** The reply raises consciousness/understanding; operationally, demand that \u201celucidated\u201d parts **improve counterfactual control** (edit a concept, behavior changes predictably). That\u2019s the causal-representation standard, and it avoids philosophical dead-ends. \ue200cite\ue202turn0search18\ue202turn0search14\ue201\n\n# How recent work supports (or challenges) the reply\u2019s claims\n\n- **Compression \u2194 generalization.** 2024\u20132025 surveys on the Information Bottleneck consolidate evidence that compressive, task-relevant representations track transfer/robustness\u2014supporting the reply\u2019s \u201ccompression and efficiency\u201d point. Caveat: IB is not a panacea; trade-offs depend on task and nuisance structure. \ue200cite\ue202turn0search0\ue202turn0search12\ue201\n\n- **Goal-directed behavior under budgets.** Bounded-rational RL and rational inattention formalize \u201cintelligence in service of objectives\u201d with explicit bit-costs for policy/state information, matching the reply\u2019s emphasis on objectives and uncertainty. Empirics now span human studies and RL agents. \ue200cite\ue202turn0search17\ue202turn4search6\ue201\n\n- **OOD reasoning needs invariants/causality.** Progress in causal representation learning (identifiability from weak conditions; concept-level causal factors) substantiates the reply\u2019s \u201cpredictive modeling\u201d claim but also its warning: mere organization of correlations won\u2019t extrapolate. \ue200cite\ue202turn0search14\ue202turn0search6\ue201\n\n- **World models as organized predictors for action.** JEPA removes pixel-level generation to learn semantic latents; DreamerV3 shows those latents can drive control across long-horizon tasks. This is the organize\u2192predict\u2192act pipeline the reply sketches. \ue200cite\ue202turn0search3\ue202turn4search0\ue201\n\n- **Elucidation is getting concrete.** Sparse-autoencoder work (Anthropic, OpenAI) demonstrates large-scale decomposition of LLM internals into features that are often human-interpretable\u2014proof that \u201cexplanatory structure\u201d can be extracted and edited, not just hypothesized. \ue200cite\ue202turn1search0\ue202turn1search12\ue201\n\n- **Embodied & social facets are now benchmarked.** Embodied-AI surveys (2024) and ToM evaluations in *Nature* (2024) both show partial competence with clear gaps\u2014empirical backing for the reply\u2019s embodiment/social cautions. \ue200cite\ue202turn3search1\ue202turn3search14\ue201\n\n# Tightening the synthesis (a precise statement)\nA sharper, testable version of the reply\u2019s reframing is:\n\n> Intelligence is the capacity to **organize** sensory histories into **compact, compositional, causal** representations and to **deploy** them for **goal achievement under information and compute constraints**, including active information gathering when needed.\n\nEach italicized noun/verb is measurable with existing tools: IB/MDL for compactness, causal identifiability for \u201ccausal,\u201d planning regret under rate\u2013distortion or active-inference objectives for \u201cgoal achievement,\u201d and information-gain/empowerment for \u201cactive gathering.\u201d \ue200cite\ue202turn0search0\ue202turn0search18\ue202turn0search17\ue202turn4search7\ue202turn2search7\ue201\n\n# Actionable next steps (if we want to *use* this framing)\n\n1) **Benchmark \u201corganization for control.\u201d** Pair JEPA-style pretraining with rate\u2013distortion-constrained planners; report utility-per-bit curves and OOD performance across environment shifts. \ue200cite\ue202turn0search3\ue202turn0search21\ue201  \n2) **Causal elucidation check.** Use SAEs/CBMs to expose features; require that targeted edits produce predicted intervention effects on behavior (concept-to-control tests). \ue200cite\ue202turn1search12\ue201  \n3) **Embodied stress tests.** Evaluate the same representations in embodied tasks that require exploration, partial observability, and social reasoning (ToM probes). \ue200cite\ue202turn3search1\ue202turn3search14\ue201\n\n**Bottom line:** The response\u2019s \u201corganization in service of adaptation\u201d lens is the right upgrade. It harmonizes with our earlier position and with current research trajectories across IB/MDL, bounded-rational control, causal representations, world models, and interpretability. The open work is to **unify** these into a single training+evaluation pipeline where *organization, causality, and control* are optimized and audited together."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":"\ue200cite\ue202turn0search17\ue202turn4search6\ue202turn0search21\ue202turn4search7\ue201","start_idx":601,"end_idx":661,"safe_urls":["https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com"],"refs":[],"alt":"([MIT Press Direct](https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com), [PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com), [arXiv](https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 As previously discussed, the incorporation of capacity limitations will be realized through rate- distortion theory; accordingly, we organize the section to ...See more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":17}],"hue":null,"attributions":null,"attribution":"MIT Press Direct"},{"title":"Rational inattention in neural coding for economic choice","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com","pub_date":null,"snippet":"by JM Fine \u00b7 2024 \u2014 Rational inattention is a Bayesian observer theory that describes how the actual values (AV) on offer are encoded as a noisy percept (likelihood) and combined ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"PMC"},{"title":"Exploring Action-Centric Representations Through the ...","url":"https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Sep 13, 2024 \u2014 Rate-distortion theory, which defines optimal lossy compression under constraints, has gained attention as a formal framework to explore goal- ...See more","attribution_segments":null,"supporting_websites":[{"title":"Expected Free Energy-based Planning as Variational Inference - arXiv","url":"https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","pub_date":null,"snippet":null,"attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":21},{"turn_index":4,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search23\ue202turn0search19\ue202turn4search0\ue201","start_idx":1022,"end_idx":1082,"safe_urls":["https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","https://arxiv.org/html/2403.00504v1?utm_source=chatgpt.com","https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com), [ece.uw.edu](https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture","url":"https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","pub_date":1674154741.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Learning and Leveraging World Models in Visual ...","url":"https://arxiv.org/html/2403.00504v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Mar 1, 2024 \u2014 It follows a Joint Embedding Predictive Architecture framework (LeCun, 2022) akin to I-JEPA (Assran et al., 2023) . In this framework, the ...","attribution":"arXiv"},{"title":"Mastering Diverse Domains through World Models","url":"https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 840 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":19},{"turn_index":4,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Objective-Driven AI","url":"https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"Jan 24, 2024 \u2014 LeCun. Image-JEPA: uses masking & transformer architectures. \u201cSSL from images with a JEPA\u201d. [M. Assran et al arxiv:2301.08243]. Jointly embeds a ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":23}],"hue":null,"attributions":null,"attribution":"ece.uw.edu"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn0search4\ue201","start_idx":1526,"end_idx":1558,"safe_urls":["https://arxiv.org/abs/2305.08013?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com"],"refs":[],"alt":"([ACM Digital Library](https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com), [arXiv](https://arxiv.org/abs/2305.08013?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Survey on Information Bottleneck | IEEE Transactions on ...","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","pub_date":null,"snippet":"This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"ACM Digital Library"},{"title":"Information Bottleneck Analysis of Deep Neural Networks ...","url":"https://arxiv.org/abs/2305.08013?utm_source=chatgpt.com","pub_date":null,"snippet":"by I Butakov \u00b7 2023 \u00b7 Cited by 13 \u2014 The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs).","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search17\ue202turn4search6\ue201","start_idx":1690,"end_idx":1723,"safe_urls":["https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com"],"refs":[],"alt":"([MIT Press Direct](https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com), [PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 As previously discussed, the incorporation of capacity limitations will be realized through rate- distortion theory; accordingly, we organize the section to ...See more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":17}],"hue":null,"attributions":null,"attribution":"MIT Press Direct"},{"title":"Rational inattention in neural coding for economic choice","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com","pub_date":null,"snippet":"by JM Fine \u00b7 2024 \u2014 Rational inattention is a Bayesian observer theory that describes how the actual values (AV) on offer are encoded as a noisy percept (likelihood) and combined ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"PMC"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search18\ue202turn0search14\ue201","start_idx":1849,"end_idx":1883,"safe_urls":["https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com"],"refs":[],"alt":"([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com), [arXiv](https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Causal Inference Meets Deep Learning: A Comprehensive ...","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com","pub_date":null,"snippet":"by L Jiao \u00b7 2024 \u00b7 Cited by 29 \u2014 In this survey, we provide a comprehensive and structured review of causal inference methods in deep learning.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":18}],"hue":null,"attributions":null,"attribution":"PMC"},{"title":"Causal Representation Learning Made Identifiable by ...","url":"https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","pub_date":null,"snippet":"Jun 7, 2024 \u2014 Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn4search0\ue201","start_idx":1988,"end_idx":2020,"safe_urls":["https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture","url":"https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","pub_date":1674154741.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Mastering Diverse Domains through World Models","url":"https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 840 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":4,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search0\ue202turn1search12\ue202turn1search16\ue201","start_idx":2279,"end_idx":2326,"safe_urls":["https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","https://openreview.net/forum?id=F76bwRSLeK&utm_source=chatgpt.com","https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com"],"refs":[],"alt":"([Transformer Circuits](https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com), [OpenAI](https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com), [OpenReview](https://openreview.net/forum?id=F76bwRSLeK&utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Extracting Interpretable Features from Claude 3 Sonnet","url":"https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com","pub_date":null,"snippet":"May 21, 2024 \u2014 Sparse autoencoders produce interpretable features for large models. Scaling laws can be used to guide the training of sparse autoencoders.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"Transformer Circuits"},{"title":"Extracting Concepts from GPT-4","url":"https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","pub_date":null,"snippet":"Jun 6, 2024 \u2014 Using new techniques for scaling sparse autoencoders, we automatically identified 16 million patterns in GPT-4's computations.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"OpenAI"},{"title":"Sparse Autoencoders Find Highly Interpretable Features in ...","url":"https://openreview.net/forum?id=F76bwRSLeK&utm_source=chatgpt.com","pub_date":null,"snippet":"by R Huben \u00b7 Cited by 53 \u2014 We use a scalable and unsupervised method called Sparse Autoencoders to find interpretable, monosemantic features in the residual streams of real LLMs.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"OpenReview"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search4\ue202turn2search13\ue201","start_idx":2533,"end_idx":2566,"safe_urls":["https://arxiv.org/pdf/2407.06204?utm_source=chatgpt.com","https://www.nature.com/articles/s41467-023-38468-8?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/pdf/2407.06204?utm_source=chatgpt.com), [Nature](https://www.nature.com/articles/s41467-023-38468-8?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Survey on Mixture of Experts in Large Language Models","url":"https://arxiv.org/pdf/2407.06204?utm_source=chatgpt.com","pub_date":1744246176.0,"snippet":"by W Cai \u00b7 2024 \u00b7 Cited by 57 \u2014 As depicted in Figure 1, MoE has maintained a robust trajectory of growth, particularly notable in 2024 with the advent of Mixtral-8x7B [26] and ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"General framework for E(3)-equivariant neural network ...","url":"https://www.nature.com/articles/s41467-023-38468-8?utm_source=chatgpt.com","pub_date":null,"snippet":"by X Gong \u00b7 2023 \u00b7 Cited by 136 \u2014 Here we propose an E(3)-equivariant deep-learning framework to represent density functional theory (DFT) Hamiltonian as a function of material structure.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search1\ue202turn3search14\ue201","start_idx":2762,"end_idx":2795,"safe_urls":["https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com), [Nature](https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Comprehensive Survey on Embodied AI","url":"https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 9, 2024 \u2014 In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":3,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Testing theory of mind in large language models and humans","url":"https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com","pub_date":null,"snippet":"by JWA Strachan \u00b7 2024 \u00b7 Cited by 297 \u2014 Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":3,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4search7\ue202turn4search14\ue201","start_idx":3119,"end_idx":3152,"safe_urls":["https://arxiv.org/html/2404.18775v4?utm_source=chatgpt.com","https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Expected Free Energy-based Planning as Variational Inference - arXiv","url":"https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[{"title":"Resource-rational reinforcement learning and ...","url":"https://arxiv.org/html/2404.18775v4?utm_source=chatgpt.com","pub_date":null,"snippet":"Mar 19, 2025 \u2014 We propose a new computational-level objective function for theoretical biology and theoretical neuroscience that combines: reinforcement ...","attribution":"arXiv"}],"refs":[{"turn_index":4,"ref_type":"search","ref_index":7},{"turn_index":4,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search3\ue202turn2search7\ue201","start_idx":3447,"end_idx":3479,"safe_urls":["https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com","https://www.researchgate.net/publication/368443560_An_Information-Theoretic_Perspective_on_Intrinsic_Motivation_in_Reinforcement_Learning_A_Survey?utm_source=chatgpt.com"],"refs":[],"alt":"([ResearchGate](https://www.researchgate.net/publication/368443560_An_Information-Theoretic_Perspective_on_Intrinsic_Motivation_in_Reinforcement_Learning_A_Survey?utm_source=chatgpt.com), [Physical Review Link](https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"(PDF) An Information-Theoretic Perspective on Intrinsic ...","url":"https://www.researchgate.net/publication/368443560_An_Information-Theoretic_Perspective_on_Intrinsic_Motivation_in_Reinforcement_Learning_A_Survey?utm_source=chatgpt.com","pub_date":1675987200.0,"snippet":"7 Feb 2023 \u2014 We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"ResearchGate"},{"title":"Intrinsic Motivation in Dynamical Control Systems | PRX Life","url":"https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com","pub_date":1724889600.0,"snippet":"by S Tiomkin \u00b7 2024 \u00b7 Cited by 8 \u2014 In this study, we investigate an information-theoretic approach to intrinsic motivation, based on maximizing an agent's empowerment.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"Physical Review Link"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search21\ue202turn4search7\ue201","start_idx":3751,"end_idx":3784,"safe_urls":["https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Exploring Action-Centric Representations Through the ...","url":"https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Sep 13, 2024 \u2014 Rate-distortion theory, which defines optimal lossy compression under constraints, has gained attention as a formal framework to explore goal- ...See more","attribution_segments":null,"supporting_websites":[{"title":"Expected Free Energy-based Planning as Variational Inference - arXiv","url":"https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","pub_date":null,"snippet":null,"attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":21},{"turn_index":4,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search18\ue202turn0search14\ue201","start_idx":4101,"end_idx":4135,"safe_urls":["https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com"],"refs":[],"alt":"([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com), [arXiv](https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Causal Inference Meets Deep Learning: A Comprehensive ...","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com","pub_date":null,"snippet":"by L Jiao \u00b7 2024 \u00b7 Cited by 29 \u2014 In this survey, we provide a comprehensive and structured review of causal inference methods in deep learning.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":18}],"hue":null,"attributions":null,"attribution":"PMC"},{"title":"Causal Representation Learning Made Identifiable by ...","url":"https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","pub_date":null,"snippet":"Jun 7, 2024 \u2014 Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn0search12\ue201","start_idx":4517,"end_idx":4550,"safe_urls":["https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","https://www.nature.com/research-intelligence/nri-topic-summaries/information-bottleneck-in-deep-learning-and-neural-networks-micro-463059?utm_source=chatgpt.com"],"refs":[],"alt":"([ACM Digital Library](https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com), [Nature](https://www.nature.com/research-intelligence/nri-topic-summaries/information-bottleneck-in-deep-learning-and-neural-networks-micro-463059?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Survey on Information Bottleneck | IEEE Transactions on ...","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","pub_date":null,"snippet":"This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"ACM Digital Library"},{"title":"Information Bottleneck in Deep Learning and Neural ...","url":"https://www.nature.com/research-intelligence/nri-topic-summaries/information-bottleneck-in-deep-learning-and-neural-networks-micro-463059?utm_source=chatgpt.com","pub_date":null,"snippet":"The information bottleneck (IB) principle is a powerful information\u2010theoretic framework that seeks to compress data representations while preserving the ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search17\ue202turn4search6\ue201","start_idx":4853,"end_idx":4886,"safe_urls":["https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com"],"refs":[],"alt":"([MIT Press Direct](https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com), [PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 As previously discussed, the incorporation of capacity limitations will be realized through rate- distortion theory; accordingly, we organize the section to ...See more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":17}],"hue":null,"attributions":null,"attribution":"MIT Press Direct"},{"title":"Rational inattention in neural coding for economic choice","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com","pub_date":null,"snippet":"by JM Fine \u00b7 2024 \u2014 Rational inattention is a Bayesian observer theory that describes how the actual values (AV) on offer are encoded as a noisy percept (likelihood) and combined ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"PMC"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search14\ue202turn0search6\ue201","start_idx":5177,"end_idx":5210,"safe_urls":["https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","https://arxiv.org/pdf/2402.09236?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Causal Representation Learning Made Identifiable by ...","url":"https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","pub_date":null,"snippet":"Jun 7, 2024 \u2014 Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision.","attribution_segments":null,"supporting_websites":[{"title":"Unifying Causal Representation Learning and Foundation ...","url":"https://arxiv.org/pdf/2402.09236?utm_source=chatgpt.com","pub_date":1733797320.0,"snippet":"by G Rajendran \u00b7 2024 \u00b7 Cited by 29 \u2014 Thus, our work presents a novel framework for identifying concepts by weaving together ideas from causal representation learning and foundation ...","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":14},{"turn_index":0,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn4search0\ue201","start_idx":5467,"end_idx":5499,"safe_urls":["https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture","url":"https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","pub_date":1674154741.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Mastering Diverse Domains through World Models","url":"https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 840 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":4,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search0\ue202turn1search12\ue201","start_idx":5776,"end_idx":5809,"safe_urls":["https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com"],"refs":[],"alt":"([Transformer Circuits](https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com), [OpenAI](https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Extracting Interpretable Features from Claude 3 Sonnet","url":"https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com","pub_date":null,"snippet":"May 21, 2024 \u2014 Sparse autoencoders produce interpretable features for large models. Scaling laws can be used to guide the training of sparse autoencoders.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"Transformer Circuits"},{"title":"Extracting Concepts from GPT-4","url":"https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","pub_date":null,"snippet":"Jun 6, 2024 \u2014 Using new techniques for scaling sparse autoencoders, we automatically identified 16 million patterns in GPT-4's computations.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search1\ue202turn3search14\ue201","start_idx":6036,"end_idx":6069,"safe_urls":["https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com), [Nature](https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Comprehensive Survey on Embodied AI","url":"https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 9, 2024 \u2014 In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":3,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Testing theory of mind in large language models and humans","url":"https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com","pub_date":null,"snippet":"by JWA Strachan \u00b7 2024 \u00b7 Cited by 297 \u2014 Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":3,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn0search18\ue202turn0search17\ue202turn4search7\ue202turn2search7\ue201","start_idx":6715,"end_idx":6788,"safe_urls":["https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com"],"refs":[],"alt":"([ACM Digital Library](https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com), [PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com), [MIT Press Direct](https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com), [arXiv](https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com), [Physical Review Link](https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Survey on Information Bottleneck | IEEE Transactions on ...","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","pub_date":null,"snippet":"This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, optimization, ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"ACM Digital Library"},{"title":"Causal Inference Meets Deep Learning: A Comprehensive ...","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com","pub_date":null,"snippet":"by L Jiao \u00b7 2024 \u00b7 Cited by 29 \u2014 In this survey, we provide a comprehensive and structured review of causal inference methods in deep learning.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":18}],"hue":null,"attributions":null,"attribution":"PMC"},{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Arumugam \u00b7 2024 \u00b7 Cited by 18 \u2014 As previously discussed, the incorporation of capacity limitations will be realized through rate- distortion theory; accordingly, we organize the section to ...See more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":17}],"hue":null,"attributions":null,"attribution":"MIT Press Direct"},{"title":"Expected Free Energy-based Planning as Variational Inference - arXiv","url":"https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Intrinsic Motivation in Dynamical Control Systems | PRX Life","url":"https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com","pub_date":1724889600.0,"snippet":"by S Tiomkin \u00b7 2024 \u00b7 Cited by 8 \u2014 In this study, we investigate an information-theoretic approach to intrinsic motivation, based on maximizing an agent's empowerment.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"Physical Review Link"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search21\ue201","start_idx":7043,"end_idx":7076,"safe_urls":["https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture","url":"https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","pub_date":1674154741.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Exploring Action-Centric Representations Through the ...","url":"https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Sep 13, 2024 \u2014 Rate-distortion theory, which defines optimal lossy compression under constraints, has gained attention as a formal framework to explore goal- ...See more","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":21}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search12\ue201","start_idx":7253,"end_idx":7273,"safe_urls":["https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Extracting Concepts from GPT-4","url":"https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","pub_date":null,"snippet":"Jun 6, 2024 \u2014 Using new techniques for scaling sparse autoencoders, we automatically identified 16 million patterns in GPT-4's computations.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search1\ue202turn3search14\ue201","start_idx":7442,"end_idx":7475,"safe_urls":["https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com), [Nature](https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"A Comprehensive Survey on Embodied AI","url":"https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 9, 2024 \u2014 In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":3,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"},{"title":"Testing theory of mind in large language models and humans","url":"https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com","pub_date":null,"snippet":"by JWA Strachan \u00b7 2024 \u00b7 Cited by 297 \u2014 Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":3,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"Nature"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":" ","start_idx":7915,"end_idx":7915,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"Bayesian Reinforcement Learning With Limited Cognitive ...","url":"https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","attribution":"MIT Press Direct"},{"title":"Rational inattention in neural coding for economic choice","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com","attribution":"PMC"},{"title":"Exploring Action-Centric Representations Through the ...","url":"https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","attribution":"arXiv"},{"title":"Objective-Driven AI","url":"https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf?utm_source=chatgpt.com","attribution":"ece.uw.edu"},{"title":"A Survey on Information Bottleneck | IEEE Transactions on ...","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","attribution":"ACM Digital Library"},{"title":"Extracting Interpretable Features from Claude 3 Sonnet","url":"https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com","attribution":"Transformer Circuits"},{"title":"Extracting Concepts from GPT-4","url":"https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","attribution":"OpenAI"},{"title":"Sparse Autoencoders Find Highly Interpretable Features in ...","url":"https://openreview.net/forum?id=F76bwRSLeK&utm_source=chatgpt.com","attribution":"OpenReview"},{"title":"General framework for E(3)-equivariant neural network ...","url":"https://www.nature.com/articles/s41467-023-38468-8?utm_source=chatgpt.com","attribution":"Nature"},{"title":"(PDF) An Information-Theoretic Perspective on Intrinsic ...","url":"https://www.researchgate.net/publication/368443560_An_Information-Theoretic_Perspective_on_Intrinsic_Motivation_in_Reinforcement_Learning_A_Survey?utm_source=chatgpt.com","attribution":"ResearchGate"},{"title":"Intrinsic Motivation in Dynamical Control Systems | PRX Life","url":"https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com","attribution":"Physical Review Link"}],"has_images":false}],"safe_urls":["https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com","https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","https://arxiv.org/html/2403.00504v1?utm_source=chatgpt.com","https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2305.08013?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com","https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com","https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","https://openreview.net/forum?id=F76bwRSLeK&utm_source=chatgpt.com","https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com","https://arxiv.org/pdf/2407.06204?utm_source=chatgpt.com","https://www.nature.com/articles/s41467-023-38468-8?utm_source=chatgpt.com","https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com","https://arxiv.org/html/2404.18775v4?utm_source=chatgpt.com","https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com","https://www.researchgate.net/publication/368443560_An_Information-Theoretic_Perspective_on_Intrinsic_Motivation_in_Reinforcement_Learning_A_Survey?utm_source=chatgpt.com","https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","https://www.nature.com/research-intelligence/nri-topic-summaries/information-bottleneck-in-deep-learning-and-neural-networks-micro-463059?utm_source=chatgpt.com","https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11463532/?utm_source=chatgpt.com","https://arxiv.org/html/2310.15709v2?utm_source=chatgpt.com","https://arxiv.org/pdf/2402.09236?utm_source=chatgpt.com","https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","https://transformer-circuits.pub/2024/scaling-monosemanticity/?utm_source=chatgpt.com","https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com","https://arxiv.org/html/2504.14898v2?utm_source=chatgpt.com","https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi_a_00132/2364075/opmi_a_00132.pdf?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1109/TPAMI.2024.3366349?utm_source=chatgpt.com","https://link.aps.org/doi/10.1103/PRXLife.2.033009?utm_source=chatgpt.com","https://pmc.ncbi.nlm.nih.gov/articles/PMC11384545/?utm_source=chatgpt.com","https://arxiv.org/abs/2301.08243?utm_source=chatgpt.com","https://arxiv.org/html/2409.08892v1?utm_source=chatgpt.com","https://openai.com/index/extracting-concepts-from-gpt-4/?utm_source=chatgpt.com","https://arxiv.org/html/2407.06886v1?utm_source=chatgpt.com","https://www.nature.com/articles/s41562-024-01882-z?utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2505.23729v2?utm_source=chatgpt.com","title":"Bounded Rationality for LLMs: Satisficing Alignment at ...","snippet":"May 31, 2025 \u2014 The common approach in LLM alignment is reinforcement learning from human feedback (RLHF), where a reward model is first learned from human ...See more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2402.09236?utm_source=chatgpt.com","title":"Unifying Causal Representation Learning and Foundation ...","snippet":"by G Rajendran \u00b7 2024 \u00b7 Cited by 29 \u2014 Thus, our work presents a novel framework for identifying concepts by weaving together ideas from causal representation learning and foundation ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2403.00504v1?utm_source=chatgpt.com","title":"Learning and Leveraging World Models in Visual ...","snippet":"Mar 1, 2024 \u2014 It follows a Joint Embedding Predictive Architecture framework (LeCun, 2022) akin to I-JEPA (Assran et al., 2023) . In this framework, the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.16705v1?utm_source=chatgpt.com","title":"An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations","snippet":"May 22, 2025 \u2014 Q. Shi. A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE TPAMI, 2024.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2410.23903v1?utm_source=chatgpt.com","title":"Neural Network Verification with PyRAT","snippet":"Oct 31, 2024 \u2014 This paper presents the PyRAT tool, a neural network verification tool based on abstract interpretation with multiple domains such as zonotopes.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2508.14562v1?utm_source=chatgpt.com","title":"Locality-aware Concept Bottleneck Model","snippet":"2 days ago \u2014 We introduce Locality-aware Concept Bottleneck Model, a novel framework promoting concept predictions to be more localized in the corresponding ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2506.23845v1?utm_source=chatgpt.com","title":"Use Sparse Autoencoders to Discover Unknown Concepts, ...","snippet":"Jun 30, 2025 \u2014 We argue that while SAEs may be less effective for acting on known concepts, SAEs are powerful tools for discovering unknown concepts.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2410.23903?utm_source=chatgpt.com","title":"Neural Network Verification with PyRAT","snippet":"by A Lemesle \u00b7 2024 \u00b7 Cited by 8 \u2014 This paper presents the PyRAT tool, a neural network verification tool based on abstract interpretation with multiple domains such as zonotopes.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2407.06204?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"by W Cai \u00b7 2024 \u00b7 Cited by 57 \u2014 This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2401.17541v3?utm_source=chatgpt.com","title":"Towards Understanding Variants of Invariant Risk ...","snippet":"Invariant Risk Minimization (IRM) emerges as a solution that aims to identify invariant features across different environments to enhance OOD robustness.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2305.11141?utm_source=chatgpt.com","title":"[2305.11141] Clifford Group Equivariant Neural Networks","snippet":"by D Ruhe \u00b7 2023 \u00b7 Cited by 68 \u2014 We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing \\mathrm{O}(n)- and \\mathrm{E}(n)-equivariant models.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2405.01389?utm_source=chatgpt.com","title":"Invariant Risk Minimization Is A Total Variation Model","snippet":"by ZR Lai \u00b7 2024 \u00b7 Cited by 1 \u2014 Invariant Risk Minimization (IRM) is a machine learning approach to generalize invariant features, and it is essentially a total variation (TV) model.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2503.23631v2?utm_source=chatgpt.com","title":"Intrinsically-Motivated Humans and Agents in Open-World ...","snippet":"27 May 2025 \u2014 We investigate to what extent human and agent exploration optimize information-theoretic intrinsic objectives. We focus on Entropy, Information ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2405.15429v5?utm_source=chatgpt.com","title":"E(\ud835\udc5b) Equivariant Topological Neural Networks","snippet":"Architectures of topological deep learning: A survey on topological neural networks, 2023. ... 2024) by combining Clifford group equivariant layers with ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2502.06470v1?utm_source=chatgpt.com","title":"A Survey of Theory of Mind in Large Language Models","snippet":"Feb 10, 2025 \u2014 In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models (LLMs), identify important safety risks from advanced ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2410.23726v1?utm_source=chatgpt.com","title":"Towards Reliable Alignment: Uncertainty-aware RLHF","snippet":"Oct 31, 2024 \u2014 RLHF is a fine-tuning method to align the behavior of LLMs with human values and preferences.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.06886v6?utm_source=chatgpt.com","title":"A Comprehensive Survey on Embodied AI","snippet":"Jul 29, 2024 \u2014 In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2412.17481v2?utm_source=chatgpt.com","title":"A Survey on LLM-based Multi-Agent System","snippet":"In this paper, we provide a new perspective based on previous reviews of LLM-based Multi-Agent Systems ( LLM-MAS ) with a focus on recent advancements and ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2501.08617v2?utm_source=chatgpt.com","title":"Mitigating Misalignment in RLHF with Hindsight Simulation","snippet":"Feb 10, 2025 \u2014 We argue that RLHF misalignment more generally emerges in settings with significant human uncertainty, whether perceptual, predictive, or a ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2406.09322?utm_source=chatgpt.com","title":"arXiv:2406.09322v2 [cs.LG] 13 Nov 2024","snippet":"by YT Yeganeh \u00b7 2024 \u00b7 Cited by 1 \u2014 Active inference is a unifying theory that integrates inference, perception, and action by emphasizing the dependence of observations on actions ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2408.04119v1?utm_source=chatgpt.com","title":"Active Inference in Contextual Multi-Armed Bandits for ...","snippet":"Aug 7, 2024 \u2014 Results show active inference requires fewer iterations than standard bandit approaches with real-world noisy and biased data, and performs ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2301.04104?utm_source=chatgpt.com","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 840 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2301.04104?utm_source=chatgpt.com","title":"Mastering Diverse Domains through World Models","snippet":"by D Hafner \u00b7 2023 \u00b7 Cited by 837 \u2014 We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2210.16877?utm_source=chatgpt.com","title":"On Rate-Distortion Theory in Capacity-Limited Cognition & ...","snippet":"by D Arumugam \u00b7 2022 \u00b7 Cited by 5 \u2014 Rate-distortion theory quantifies the minimum bits of information needed to achieve a bound on loss of fidelity in lossy compression. It is ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2404.18775v4?utm_source=chatgpt.com","title":"Resource-rational reinforcement learning and ...","snippet":"Mar 19, 2025 \u2014 We propose a new computational-level objective function for theoretical biology and theoretical neuroscience that combines: reinforcement ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2407.12185v1?utm_source=chatgpt.com","title":"Satisficing Exploration for Deep Reinforcement Learning","snippet":"Jul 16, 2024 \u2014 On Rate-Distortion Theory in Capacity-Limited Cognition & Reinforcement Learning. In NeurIPS 2022 Workshop on Information-Theoretic ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/392430428_Causal_Representation_Learning_A_Quick_Survey?utm_source=chatgpt.com","title":"Causal Representation Learning: A Quick Survey","snippet":"Jun 7, 2025 \u2014 In this work we give an overview over several approaches to disentangled representation learning and give a short introduction to variational ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/347633182_Information_Bottleneck_Theory_and_Applications_in_Deep_Learning?utm_source=chatgpt.com","title":"Information Bottleneck: Theory and Applications in Deep ...","snippet":"Apr 18, 2025 \u2014 This Special Issue aims to investigate the properties of the IB functional in this new context and to propose learning mechanisms inspired by the IB framework.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/384277435_PAC_Learnability_and_Information_Bottleneck_in_Deep_Learning_Tutorial_and_Survey?utm_source=chatgpt.com","title":"(PDF) PAC Learnability and Information Bottleneck in Deep ...","snippet":"Sep 27, 2024 \u2014 This is a tutorial and survey on PAC learnability and information bottleneck in deep learning. This paper first introduced PAC learnability, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/393336361_A_Framework_for_Bounded_Rationality_and_Limited_Resource_Sensing_for_Optimized_Decision_Making_in_Artificial_Intelligence?utm_source=chatgpt.com","title":"(PDF) A Framework for Bounded Rationality and Limited ...","snippet":"This research addresses the concept of BRAI, focusing on the Limited Resource Sensing Problem (LRSP) and its impact on decision-making processes. We formalized ...See more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/393724282_Mixture_of_Experts_in_Large_Language_Models?utm_source=chatgpt.com","title":"(PDF) Mixture of Experts in Large Language Models","snippet":"Jul 18, 2025 \u2014 This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/390217128_A_Survey_on_Mixture_of_Experts_in_Large_Language_Models?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"Aug 6, 2025 \u2014 This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/383527442_Intrinsic_Motivation_in_Dynamical_Control_Systems?utm_source=chatgpt.com","title":"(PDF) Intrinsic Motivation in Dynamical Control Systems","snippet":"6 Aug 2025 \u2014 Biological systems often choose actions without an explicit reward signal, a phenomenon known as intrinsic motivation.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/356804385_Active_Inference_in_Robotics_and_Artificial_Agents_Survey_and_Challenges?utm_source=chatgpt.com","title":"(PDF) Active Inference in Robotics and Artificial Agents","snippet":"Dec 9, 2021 \u2014 Here, we review the state-of-the-art theory and implementations of active inference for state-estimation, control, planning and learning; ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/394687997_Multimodal_Data_Storage_and_Retrieval_for_Embodied_AI_A_Survey?utm_source=chatgpt.com","title":"Multimodal Data Storage and Retrieval for Embodied AI","snippet":"3 days ago \u2014 Embodied AI (EAI) agents continuously interact with the physical world, generating vast, heterogeneous multimodal data streams that traditional ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/359104337_How_Active_Inference_Could_Help_Revolutionise_Robotics?utm_source=chatgpt.com","title":"How Active Inference Could Help Revolutionise Robotics","snippet":"In this paper, we explain how active inference\u2014a well-known description of sentient behaviour from neuroscience\u2014can be exploited in robotics. In short, active ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/figure/Rational-inattention-and-reinforcement-learning-A-Using-a-non-stationary-three-armed_fig4_350365666?utm_source=chatgpt.com","title":"Rational inattention and reinforcement learning (A) Using a...","snippet":"This confluence of these results suggests a mechanism of attentional effort that operates by controlling gain through using partially distinct population codes ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/367019841_Mastering_Diverse_Domains_through_World_Models?utm_source=chatgpt.com","title":"(PDF) Mastering Diverse Domains through World Models","snippet":"Jan 10, 2023 \u2014 Recent work in model-based RL have shown that scaling the world model achieves significant improvements (Hafner et al., 2023; Hansen et al., ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/364953358_On_Rate-Distortion_Theory_in_Capacity-Limited_Cognition_Reinforcement_Learning?utm_source=chatgpt.com","title":"On Rate-Distortion Theory in Capacity-Limited Cognition & ...","snippet":"In this paper, we aim to elucidate this latter perspective by presenting a brief survey of these information-theoretic models of capacity-limited decision ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/390546288_The_Emperor%27s_New_Pseudo-Theory_How_the_Free_Energy_Principle_Ransacked_Neuroscience?utm_source=chatgpt.com","title":"The Emperor's New Pseudo-Theory: How the Free Energy Principle ...","snippet":"","ref_id":{"turn_index":4,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"frontiersin.org","entries":[{"type":"search_result","url":"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1477514/full?utm_source=chatgpt.com","title":"A boundedly rational model for category learning","snippet":"by TM Houser \u00b7 2024 \u2014 The goal of this paper is to make progress toward development of an ecologically and neurobiologically plausible model of category learning.See more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"frontiersin.org"}]},{"type":"search_result_group","domain":"adamkarvonen.github.io","entries":[{"type":"search_result","url":"https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html?utm_source=chatgpt.com","title":"An Intuitive Explanation of Sparse Autoencoders for LLM ...","snippet":"Jun 11, 2024 \u2014 Sparse autoencoders have recently gained popularity as a technique to break neural networks down into understandable components.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"adamkarvonen.github.io"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=X80k80pPfq&referrer=%5Bthe+profile+of+Shizhe+Hu%5D%28%2Fprofile%3Fid%3D~Shizhe_Hu2%29&utm_source=chatgpt.com","title":"A Survey on Information Bottleneck","snippet":"This survey summarizes both the theoretical progress and practical applications on IB over the past 20-plus years, where its basic theory, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=huGECz8dPp&utm_source=chatgpt.com","title":"Information Bottleneck Analysis of Deep Neural Networks ...","snippet":"by I Butakov \u00b7 Cited by 13 \u2014 The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=lpOC6s4BcM&utm_source=chatgpt.com","title":"Invariance & Causal Representation Learning: Prospects ...","snippet":"by S Bing \u00b7 Cited by 3 \u2014 In this work, we explore this invariance principle as a candidate assumption to achieve identifiability of causal representations.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=WFYbBOEOtv&utm_source=chatgpt.com","title":"V-JEPA: Latent Video Prediction for Visual Representation ...","snippet":"by A Bardes \u00b7 Cited by 30 \u2014 The paper is a nicely executed empirical study that extends I-JEPA [Assran et al, 2023) to video data; The main message for me as a user is that (pre) ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=RC5FPYVQaH&utm_source=chatgpt.com","title":"Concept Bottleneck Large Language Models","snippet":"by CE Sun \u00b7 Cited by 12 \u2014 We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel framework for building inherently interpretable Large Language Models (LLMs).","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=tcsZt9ZNKD&utm_source=chatgpt.com","title":"Scaling and evaluating sparse autoencoders","snippet":"by L Gao \u00b7 Cited by 244 \u2014 Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=7bQmU2rukF&utm_source=chatgpt.com","title":"IMPROVING INTERVENTION EFFICACY VIA CONCEPT REALIGNMENT IN CONCEPT BOTTLENECK MODELS","snippet":"by N Singhi \u00b7 2024 \u00b7 Cited by 6 \u2014 (2023) conducted an in-depth study on various strategies to select concepts with an emphasis on task performance and the cost of executing the strategies.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=n84bzMrGUD&noteId=sQG6abJbs8&utm_source=chatgpt.com","title":"Clifford Group Equivariant Neural Networks","snippet":"The paper develops a new kind of neural network model based on the Clifford group, which makes the neural network E(n)-equivariant.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=Ax3uliEBVR&utm_source=chatgpt.com","title":"E(n) Equivariant Topological Neural Networks","snippet":"by C Battiloro \u00b7 Cited by 18 \u2014 This paper introduces E(n)-Equivariant Topological Neural Networks (ETNNs), which are E(n)-equivariant message-passing networks operating on combinatorial ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=pWsfWDnJDa&utm_source=chatgpt.com","title":"Out-of-Distribution Optimality of Invariant Risk Minimization","snippet":"by S Toyota \u00b7 Cited by 1 \u2014 Abstract: Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=otW0TJOUYF&utm_source=chatgpt.com","title":"Scaffolding Theory of Mind for Multi-Agent Tasks with ...","snippet":"by L Cross \u00b7 Cited by 19 \u2014 We introduce Hypothetical Minds, an LLM-based agent that outperforms MARL and LLM baselines on multi-agent tasks using a novel Theory of Mind module.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/a13dc3982367a9d47940844ef0b463b27f242916.pdf?utm_source=chatgpt.com","title":"Uncertainty-aware Preference Alignment in Reinforcement ...","snippet":"In this work, we introduce the uncertainty-aware prefer- ence alignment in RLHF by learning a distribu- tional reward model and a risk-sensitive policy from the ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=DY1pMrmDkm&noteId=a5vg9bCJJp&utm_source=chatgpt.com","title":"Modeling Bounded Rationality in Multi-Agent Simulations ...","snippet":"In this work, we propose a new, more human-like RL agent, which incorporates an established model of human-irrationality, the Rational Inattention (RI) model.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=ByxAOoR5K7&utm_source=chatgpt.com","title":"POLICY GENERALIZATION IN CAPACITY-LIMITED ...","snippet":"by RA Lerch \u00b7 Cited by 6 \u2014 Hence, an important goal of the paper is a an accessible introduction of rate-distortion theory in the context of RL. Combining these two ideas, we consider the ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=T6lAFguUbw&utm_source=chatgpt.com","title":"Modeling Bounded Rationality in Multi-Agent Simulations ...","snippet":"by T Mu \u00b7 Cited by 5 \u2014 This paper studies human bounded rationality using multi-agent reinforcement learning. It particularly focuses on the rational inattention model ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=HyEtjoCqFX&utm_source=chatgpt.com","title":"Soft Q-Learning with Mutual-Information Regularization","snippet":"by J Grau-Moya \u00b7 Cited by 67 \u2014 We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize a prior action distribution for better performance ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"dilipa.github.io","entries":[{"type":"search_result","url":"https://dilipa.github.io/papers/nips22_infocog_workshop_rdt_satisficing.pdf?utm_source=chatgpt.com","title":"On Rate-Distortion Theory in Capacity-Limited Cognition ...","snippet":"by D Arumugam \u00b7 Cited by 5 \u2014 Bayesian reinforcement learning: A survey. Foundations and Trends ... Reinforcement learning and control as probabilistic inference: Tutorial and review.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"dilipa.github.io"}]},{"type":"search_result_group","domain":"mit.edu","entries":[{"type":"search_result","url":"https://direct.mit.edu/neco/article/36/5/963/119791/An-Overview-of-the-Free-Energy-Principle-and?utm_source=chatgpt.com","title":"An Overview of the Free Energy Principle and Related Research","snippet":"","ref_id":{"turn_index":4,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"mit.edu"}]},{"type":"search_result_group","domain":"transformer-circuits.pub","entries":[{"type":"search_result","url":"https://transformer-circuits.pub/2023/monosemantic-features?utm_source=chatgpt.com","title":"Decomposing Language Models With Dictionary Learning","snippet":"Oct 4, 2023 \u2014 In this paper, we use a weak dictionary learning algorithm called a sparse autoencoder to generate learned features from a trained model that offer a more ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"transformer-circuits.pub"}]},{"type":"search_result_group","domain":"sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0010028525000015?utm_source=chatgpt.com","title":"Exploring the bounded rationality in human decision ...","snippet":"by YL Lu \u00b7 2025 \u00b7 Cited by 2 \u2014 Some seemingly irrational decision behaviors (anomalies), once seen as flaws in human cognition, have recently received explanations from a rational ...See more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S1566253525005007?utm_source=chatgpt.com","title":"Interpretable prognostics with concept Bottleneck Models","snippet":"by F Forest \u00b7 2025 \u00b7 Cited by 3 \u2014 We propose concept bottleneck models for more interpretable prognostics, where degradation modes of an asset are used as intermediate concepts.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S0001691824000544?utm_source=chatgpt.com","title":"Theories of motivation: A comprehensive analysis ...","snippet":"by D Bandhu \u00b7 2024 \u00b7 Cited by 282 \u2014 This paper explores theories of motivation, including instinct theory, arousal theory, incentive theory, intrinsic theory, extrinsic theory, the ARCS model, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S1077314223002175?utm_source=chatgpt.com","title":"Coping with change: Learning invariant and minimum ...","snippet":"by S Ye \u00b7 2023 \u00b7 Cited by 11 \u2014 We analyze the limitations of FGVC from a generalization perspective, and identify the necessity of learning \u201cinvariant\u201d and \u201cminimum sufficient\u201d features.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S1364661323002607?utm_source=chatgpt.com","title":"active inference and the scope and limits of passive AI","snippet":"by G Pezzulo \u00b7 2024 \u00b7 Cited by 73 \u2014 We discuss the foundations of generative AI systems by comparing them with our current understanding of living organisms, when seen as active inference systems.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S1571064525000879?utm_source=chatgpt.com","title":"Active inference and cognitive control: Balancing ...","snippet":"by R Proietti \u00b7 2025 \u2014 Our simulations show that a standard active inference model can form adaptive habits; i.e., can pass from deliberative to habitual control when the context is ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S1566253525002714?utm_source=chatgpt.com","title":"Exploring Embodied Multimodal Large Models","snippet":"by S Chen \u00b7 2025 \u00b7 Cited by 6 \u2014 This comprehensive review explores the development of such models, including Large Language Models (LLMs), Large Vision Models (LVMs), and other models.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S1566253524003105?utm_source=chatgpt.com","title":"Embodied navigation with multi-modal information","snippet":"by Y Wu \u00b7 2024 \u00b7 Cited by 23 \u2014 Embodied AI aims to create agents that complete complex tasks by interacting with the environment. A key problem in this field is embodied navigation which ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/abs/pii/S0010945223003076?utm_source=chatgpt.com","title":"The utilitarian brain: Moving beyond the Free Energy Principle","snippet":"","ref_id":{"turn_index":4,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"sciencedirect.com"},{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S002205312500016X?utm_source=chatgpt.com","title":"Modeling machine learning: A cognitive economic approach","snippet":"by A Caplin \u00b7 2025 \u00b7 Cited by 7 \u2014 Our analysis illustrates several advantages of applying cognitive economic methods and rational inattention theory to machine learners. First, the machine's ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"sciencedirect.com"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://cdn.openai.com/papers/sparse-autoencoders.pdf?utm_source=chatgpt.com","title":"Scaling and evaluating sparse autoencoders","snippet":"by L Gao \u00b7 Cited by 244 \u2014 Sparse autoencoders provide a promising unsupervised approach for extracting in- terpretable features from a language model by ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://malcolmlett.medium.com/critical-review-of-lecuns-introductory-jepa-paper-fabe5783134e?utm_source=chatgpt.com","title":"Critical review of LeCun's Introductory JEPA paper | Medium","snippet":"The I-JEPA model requires about 5x fewer iterations than comparable models (Assran et al., 2023, section 7). The focus on applying a loss ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40memreakbulut/decoding-the-information-bottleneck-a-deep-dive-into-neural-network-learning-dynamics-8cb02aff37e2?utm_source=chatgpt.com","title":"Decoding the Information Bottleneck: A Deep Dive into ...","snippet":"In the ever-evolving landscape of artificial intelligence, understanding how neural networks learn has been a persistent challenge.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40mohamed-aymen.bouyahia/how-sparse-autoencoders-help-us-understand-language-models-6f7b2225eb9b?utm_source=chatgpt.com","title":"How Sparse Autoencoders Help Us Understand Language ...","snippet":"Their work shows that it's possible to extract meaningful, human-interpretable features from dense neural representations \u2014 without supervision.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science/scaling-monosemanticity-anthropics-one-step-towards-interpretable-manipulable-llms-4b9403c4341e?utm_source=chatgpt.com","title":"Scaling Monosemanticity: Anthropic's One Step Towards ...","snippet":"In Oct 2023, Anthropic published an article about using sparse autoencoders to provide monosemantic features to explain a one-layer Transformer ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40techsachin/decomposing-gpt-4s-internal-representations-into-16-million-oft-interpretable-patterns-with-e16fb02e5189?utm_source=chatgpt.com","title":"Decomposing GPT-4's internal representations into 16 ...","snippet":"Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"lattice-land.github.io","entries":[{"type":"search_result","url":"https://lattice-land.github.io/abstract-week.html?utm_source=chatgpt.com","title":"Abstract Interpretation Workshop (2024) - Lattice Land Book","snippet":"During a full week in June, we gather together to study abstract interpretation ... Introduction to Neural Network Verification by Abstract Interpretation.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"lattice-land.github.io"}]},{"type":"search_result_group","domain":"nih.gov","entries":[{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC12137480/?utm_source=chatgpt.com","title":"Helpful, harmless, honest? Sociotechnical limits of AI ...","snippet":"by AD Lindstr\u00f6m \u00b7 2025 \u00b7 Cited by 2 \u2014 (2024) argue that RLHF-trained models tend to exhibit \u201csteerable\u201d alignment, a form of sycophancy. The study suggests that while steerability ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC10353860/?utm_source=chatgpt.com","title":"Rate-distortion theory of neural coding and its implications ...","snippet":"by AMV Jakob \u00b7 2023 \u00b7 Cited by 28 \u2014 Rate-distortion theory provides a powerful framework for understanding the nature of human memory by formalizing the relationship between information rate.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC12003158/?utm_source=chatgpt.com","title":"Mastering diverse control tasks through world models - PMC","snippet":"by D Hafner \u00b7 2025 \u00b7 Cited by 57 \u2014 All Dreamer and PPO agents in this paper were trained on a single Nvidia A100 GPU each. Dreamer uses the 200 million model size by default.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"nih.gov"},{"type":"search_result","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC7517505/?utm_source=chatgpt.com","title":"Is the Free-Energy Principle a Formal Theory of Semantics? From ...","snippet":"","ref_id":{"turn_index":4,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"nih.gov"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/ControlProblem/comments/1cygnt9/anthropic_scaling_monosemanticity_extracting/?utm_source=chatgpt.com","title":"Scaling Monosemanticity: Extracting Interpretable Features ...","snippet":"The article \"Scaling Monosemanticity\" is about making complex language models easier to understand by breaking them down into simpler parts.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/?utm_source=chatgpt.com","title":"Any successful story of active inference (free energy principle)?","snippet":"Sep 8, 2024 \u00b7 Active inference is a principle that posits that the brain is doing surprise minimisation. Because it's a principle, it can't be falsified.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"computer.org","entries":[{"type":"search_result","url":"https://www.computer.org/csdl/journal/tk/2025/07/10937907/25n2xHILEpG?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"by W Cai \u00b7 2025 \u00b7 Cited by 55 \u2014 As depicted in Fig. 1, MoE has maintained a robust trajectory of growth, particularly notable in 2024 with the advent of Mixtral-8x7B and ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"computer.org"}]},{"type":"search_result_group","domain":"danijar.com","entries":[{"type":"search_result","url":"https://danijar.com/project/dreamerv3/?utm_source=chatgpt.com","title":"Mastering Diverse Control Tasks through World Models","snippet":"Dreamer is the first algorithm that collects diamonds in Minecraft from sparse rewards, without expert demonstrations or curricula, solving this challenge.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"danijar.com"}]},{"type":"search_result_group","domain":"pnas.org","entries":[{"type":"search_result","url":"https://www.pnas.org/doi/10.1073/pnas.1922401117?utm_source=chatgpt.com","title":"The bounded rationality of probability distortion","snippet":"by H Zhang \u00b7 2020 \u00b7 Cited by 54 \u2014 We propose a bounded log-odds model (BLO) of probability and relative frequency distortion based on three assumptions.See more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"pnas.org"},{"type":"search_result","url":"https://www.pnas.org/doi/10.1073/pnas.2405460121?utm_source=chatgpt.com","title":"Evaluating large language models in theory of mind tasks","snippet":"by M Kosinski \u00b7 2024 \u00b7 Cited by 165 \u2014 Eleven large language models (LLMs) were assessed using 40 bespoke false-belief tasks, considered a gold standard in testing theory of mind (ToM) in humans.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"pnas.org"}]},{"type":"search_result_group","domain":"aps.org","entries":[{"type":"search_result","url":"https://link.aps.org/doi/10.1103/PRXQuantum.5.020328?utm_source=chatgpt.com","title":"Theory for Equivariant Quantum Neural Networks","snippet":"by QT Nguyen \u00b7 2024 \u00b7 Cited by 136 \u2014 A comprehensive theoretical framework to design equivariant quantum neural networks (EQNNs) for essentially any relevant symmetry group.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"aps.org"},{"type":"search_result","url":"https://link.aps.org/pdf/10.1103/PRXLife.2.033009?utm_source=chatgpt.com","title":"Intrinsic Motivation in Dynamical Control Systems","snippet":"by S Tiomkin \u00b7 2024 \u00b7 Cited by 8 \u2014 In this study, we investigate an information-theoretic approach to intrinsic motivation, based on maximizing an agent's empower- ment (the ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"aps.org"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts in Large Language Models","snippet":"A Survey on Mixture of Experts in. Large Language Models ... A chronological overview of several representative Mixture-of-Experts (MoE) models in recent years.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/arpita8/Awesome-Mixture-of-Experts-Papers?utm_source=chatgpt.com","title":"arpita8/Awesome-Mixture-of-Experts-Papers: Survey","snippet":"A curated list of exceptional papers and resources on Mixture of Experts and related topics. News: Our Mixture of Experts survey has been released.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/Chen-Cai-OSU/awesome-equivariant-network?utm_source=chatgpt.com","title":"Chen-Cai-OSU/awesome-equivariant-network","snippet":"Paper list for equivariant neural network. Work-in-progress. Feel free to suggest relevant papers in the following format. **Group Equivariant Convolutional ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List?utm_source=chatgpt.com","title":"Paper List and Resource Repository for Embodied AI","snippet":"In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/zchoi/Awesome-Embodied-Robotics-and-Agent?utm_source=chatgpt.com","title":"zchoi/Awesome-Embodied-Robotics-and-Agent","snippet":"This is a curated list of Embodied robotics or agent with Vision-Language Models (VLMs) and Large Language Models (LLMs) research which is maintained by haonan.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers?utm_source=chatgpt.com","title":"Large Language Model based Multi-Agents: A Survey ...","snippet":"This repo is created to maintain LLM-based Multi-Agents papers. We categorized these papers into five main streams.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/danijar/dreamerv3?utm_source=chatgpt.com","title":"danijar/dreamerv3: Mastering Diverse Domains through ...","snippet":"A reimplementation of DreamerV3, a scalable and general reinforcement learning algorithm that masters a wide range of applications with fixed hyperparameters.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"crl-community.github.io","entries":[{"type":"search_result","url":"https://crl-community.github.io/neurips24?utm_source=chatgpt.com","title":"Causal Representation Learning","snippet":"The goal of this workshop is to explore the challenges and opportunities in this field, discuss recent progress, and identify open questions.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"crl-community.github.io"}]},{"type":"search_result_group","domain":"taylortjohnson.com","entries":[{"type":"search_result","url":"https://taylortjohnson.com/research/tumlin2024icaif.pdf?utm_source=chatgpt.com","title":"The Neural Network Verification Tool For Certifying Fairness","snippet":"by A Tumlin \u00b7 2024 \u00b7 Cited by 7 \u2014 Formal verification techniques such as model checking, the- orem proving, and abstract interpretation have been widely adopted to guarantee the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"taylortjohnson.com"}]},{"type":"search_result_group","domain":"informs.org","entries":[{"type":"search_result","url":"https://pubsonline.informs.org/doi/abs/10.1287/trsc.2024.0562?utm_source=chatgpt.com","title":"diffIRM: A Diffusion-Augmented Invariant Risk Minimization ...","snippet":"by Z Mo \u00b7 2025 \u00b7 Cited by 3 \u2014 In this study, we fill in this research gap and propose a diffusion-augmented invariant risk minimization (diffIRM) framework that combines ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"informs.org"}]},{"type":"search_result_group","domain":"ijcai.org","entries":[{"type":"search_result","url":"https://www.ijcai.org/proceedings/2024/0890.pdf?utm_source=chatgpt.com","title":"Large Language Model Based Multi-agents","snippet":"by T Guo \u00b7 Cited by 758 \u2014 To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi- ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"ijcai.org"},{"type":"search_result","url":"https://www.ijcai.org/proceedings/2024/0892.pdf?utm_source=chatgpt.com","title":"Social Learning through Interactions with Other Agents","snippet":"by D Hillier \u00b7 Cited by 3 \u2014 Theory of mind for multi-agent collaboration via large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural. Language ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"ijcai.org"}]},{"type":"search_result_group","domain":"meta.com","entries":[{"type":"search_result","url":"https://ai.meta.com/research/publications/self-supervised-learning-from-images-with-a-joint-embedding-predictive-architecture/?utm_source=chatgpt.com","title":"Self-Supervised Learning from Images with a Joint ...","snippet":"Download the Paper. AUTHORS. Written by. Mido Assran. Quentin Duval. Ishan Misra. Piotr Bojanowski. Pascal Vincent \u00b7 Mike Rabbat \u00b7 Yann LeCun. Nicolas Ballas.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"meta.com"}]},{"type":"search_result_group","domain":"anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/research/engineering-challenges-interpretability?utm_source=chatgpt.com","title":"The engineering challenges of scaling interpretability","snippet":"Jun 13, 2024 \u2014 Our Sparse Autoencoders\u2014the tools we use to investigate \u201cfeatures\u201d\u2014are trained on the activations of transformers, and those activations need to ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"anthropic.com"}]},{"type":"search_result_group","domain":"nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s42256-025-01005-x?utm_source=chatgpt.com","title":"Embodied large language models enable robots to ...","snippet":"by R Mon-Williams \u00b7 2025 \u00b7 Cited by 22 \u2014 Here we report an embodied large-language-model-enabled robot (ELLMER) framework, utilizing GPT-4 and a retrieval-augmented generation infrastructure,","ref_id":{"turn_index":3,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41586-025-08744-2?utm_source=chatgpt.com","title":"Mastering diverse control tasks through world models","snippet":"by D Hafner \u00b7 2025 \u00b7 Cited by 60 \u2014 Here we present the third generation of Dreamer, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"nature.com"},{"type":"search_result","url":"https://www.nature.com/articles/s41467-023-40141-z?utm_source=chatgpt.com","title":"Experimental validation of the free-energy principle with in vitro ...","snippet":"","ref_id":{"turn_index":4,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"nature.com"}]},{"type":"search_result_group","domain":"sciopen.com","entries":[{"type":"search_result","url":"https://www.sciopen.com/article/10.26599/AIR.2024.9150042?utm_source=chatgpt.com","title":"A Comprehensive Survey on Embodied Intelligence","snippet":"by F Sun \u00b7 2024 \u00b7 Cited by 4 \u2014 Bcent aims to enhance the adaptability, robustness, and intelligence of embodied systems, aligning with the ongoing progress in robotics, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"sciopen.com"}]},{"type":"search_result_group","domain":"jmlr.org","entries":[{"type":"search_result","url":"https://jmlr.org/papers/v25/23-0119.html?utm_source=chatgpt.com","title":"Critically Assessing the State of the Art in Neural Network ...","snippet":"by M K\u00f6nig \u00b7 2024 \u00b7 Cited by 21 \u2014 Recent research has proposed various methods to formally verify neural networks against minimal input perturbations; this verification task is also known as ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"jmlr.org"}]},{"type":"search_result_group","domain":"neurips.cc","entries":[{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/b76a9959151d377ddd2c77a275a97475-Paper-Conference.pdf?utm_source=chatgpt.com","title":"From Causal to Concept-Based Representation Learning","snippet":"by G Rajendran \u00b7 2024 \u00b7 Cited by 6 \u2014 Sch\u00f6lkopf, and P. Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. arXiv preprint arXiv:2306.02235, 2023. [ ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://neurips.cc/virtual/2023/workshop/66497?utm_source=chatgpt.com","title":"Causal Representation Learning","snippet":"Can we learn causal representations from raw data, e.g. images? This workshop connects research in causality and representation learning. It appears you are ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/95178?utm_source=chatgpt.com","title":"NeurIPS Poster Coarse-to-Fine Concept Bottleneck Models","snippet":"This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/98315?utm_source=chatgpt.com","title":"Critically Assessing the State of the Art in Neural Network ...","snippet":"Abstract. Recent research has proposed various methods to formally verify neural networks against minimal input perturbations ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/5c7894ac8788555f1cecf536f1e0fd35-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Stochastic Concept Bottleneck Models","snippet":"by M Vandenhirtz \u00b7 2024 \u00b7 Cited by 19 \u2014 Chauhan et al. (2023) propose to combine predefined policies with learnable weighting parameters, while Espinosa Zarlenga et al. (2024) learn the policy itself.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://neurips.cc/virtual/2024/poster/96433?utm_source=chatgpt.com","title":"Dissecting the Failure of Invariant Learning on Graphs","snippet":"In this paper, we develop a Structural Causal Model (SCM) to theoretically dissect the performance of two prominent invariant learning methods\u2014Invariant Risk ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"neurips.cc"},{"type":"search_result","url":"https://proceedings.neurips.cc/paper_files/paper/2024/file/7972f3735e104a54715922aa416fde1b-Paper-Conference.pdf?utm_source=chatgpt.com","title":"Towards Next-Level Robustness in Fine-tuning Zero-Shot ...","snippet":"by K Li \u00b7 2024 \u2014 Dual risk minimization (DRM) combines empirical risk minimization with worst-case risk minimization to balance expected and worst-case performance.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"neurips.cc"}]},{"type":"search_result_group","domain":"semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Self-Supervised-Learning-from-Images-with-a-Assran-Duval/ee57e4d7a125f4ca8916284a857c3760d7d378d3?utm_source=chatgpt.com","title":"[PDF] Self-Supervised Learning from Images with a Joint- ...","snippet":"... Assran and Quentin Duval and Ishan Misra and Piotr Bojanowski and Pascal Vincent and Michael G. Rabbat and Yann LeCun and Nicolas Ballas}, journal={2023 ... JEPA ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/a56453c98c6244329f660d4fd74e2afd4fe7d349?utm_source=chatgpt.com","title":"A Survey on Mixture of Experts","snippet":"The structure of the MoE layer is introduced, followed by proposing a new taxonomy of MoE, and the core designs for various MoE models including both ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"semanticscholar.org"}]},{"type":"search_result_group","domain":"lesswrong.com","entries":[{"type":"search_result","url":"https://www.lesswrong.com/posts/MKFGByqBRMbgQftRc/mechanistic-interpretability-of-llama-3-2-with-sparse?utm_source=chatgpt.com","title":"Mechanistic Interpretability of Llama 3.2 with Sparse ...","snippet":"Nov 23, 2024 \u2014 The project provides a complete end-to-end pipeline for training Sparse Autoencoders to interpret LLM features, from activation capture through training, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"lesswrong.com"},{"type":"search_result","url":"https://www.lesswrong.com/posts/8YnHuN55XJTDwGPMr/a-gentle-introduction-to-sparse-autoencoders?utm_source=chatgpt.com","title":"A gentle introduction to sparse autoencoders","snippet":"Sep 2, 2024 \u2014 In this introduction, I want to unpack the jargon, intuition, and technical details behind SAEs. This post is targeted for technical folks without a background ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"lesswrong.com"},{"type":"search_result","url":"https://www.lesswrong.com/posts/MArdnet7pwgALaeKs/why-i-m-not-into-the-free-energy-principle?utm_source=chatgpt.com","title":"Why I'm not into the Free Energy Principle - LessWrong","snippet":"","ref_id":{"turn_index":4,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"lesswrong.com"}]},{"type":"search_result_group","domain":"springer.com","entries":[{"type":"search_result","url":"https://link.springer.com/chapter/10.1007/978-3-031-91118-7_9?utm_source=chatgpt.com","title":"Neural Network Verification is a Programming Language ...","snippet":"by LC Cordeiro \u00b7 2025 \u00b7 Cited by 4 \u2014 In this paper, we formulate neural network verification challenges as programming language challenges and suggest possible future solutions.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/rwe/10.1007/978-981-19-2080-6_10?utm_source=chatgpt.com","title":"Theories of Motivation and Empowerment in Open, ...","snippet":"by C Ng \u00b7 2023 \u00b7 Cited by 4 \u2014 This chapter provides an introduction to key theoretical perspectives on motivation, including sociocognitive theories, sociocultural theories, and the concept ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s44336-024-00009-2?utm_source=chatgpt.com","title":"A survey on LLM-based multi-agent systems: workflow ...","snippet":"by X Li \u00b7 2024 \u00b7 Cited by 137 \u2014 In this paper, we present a comprehensive survey of these studies, offering a systematic review of LLM-based MAS.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s11098-024-02249-w?utm_source=chatgpt.com","title":"Beyond Preferences in AI Alignment | Philosophical Studies","snippet":"by T Zhi-Xuan \u00b7 2024 \u00b7 Cited by 39 \u2014 The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"springer.com"},{"type":"search_result","url":"https://link.springer.com/article/10.1007/s10683-024-09843-z?utm_source=chatgpt.com","title":"Rational inattention in games: experimental evidence","snippet":"by D Almog \u00b7 2024 \u00b7 Cited by 8 \u2014 To investigate whether attention responds rationally to strategic incentives, we experimentally implement a buyer-seller game in which a ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"springer.com"}]},{"type":"search_result_group","domain":"alignmentforum.org","entries":[{"type":"search_result","url":"https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research?utm_source=chatgpt.com","title":"Thoughts on the impact of RLHF research","snippet":"Jan 25, 2023 \u2014 I'll discuss various arguments that RLHF research had an overall negative impact and explain why I don't find them persuasive.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"alignmentforum.org"}]},{"type":"search_result_group","domain":"vitalab.github.io","entries":[{"type":"search_result","url":"https://vitalab.github.io/article/2023/01/19/DreamerV3.html?utm_source=chatgpt.com","title":"DreamerV3: Mastering Diverse Domains through World Models","snippet":"Jan 19, 2023 \u2014 Model-based reinforcement learning is a type of RL where the agent will not only learn a policy, but also a world model which the agent can use to plan ahead.","ref_id":{"turn_index":4,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"vitalab.github.io"}]},{"type":"search_result_group","domain":"icml.cc","entries":[{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/45301?utm_source=chatgpt.com","title":"ICML Poster Editable Concept Bottleneck Models","snippet":"Jul 17, 2025 \u2014 A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Bytez.com - Designed for Accessibility and to further Open Science","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"icml.cc"},{"type":"search_result","url":"https://icml.cc/virtual/2025/poster/45802?utm_source=chatgpt.com","title":"GLGENN: A Novel Parameter-Light Equivariant Neural ...","snippet":"Liu et al., 2024 introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"icml.cc"}]},{"type":"search_result_group","domain":"mlr.press","entries":[{"type":"search_result","url":"https://proceedings.mlr.press/v233/pepe24a/pepe24a.pdf?utm_source=chatgpt.com","title":"Clifford Group Equivariant Neural Network Layers for Protein ...","snippet":"by A Pepe \u00b7 2024 \u00b7 Cited by 5 \u2014 We employ Clifford Group Equivariant Neural Net- work (CGENN) layers to predict protein coordinates in a Protein Structure Prediction (PSP) pipeline.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"mlr.press"}]},{"type":"search_result_group","domain":"sigplan.org","entries":[{"type":"search_result","url":"https://popl24.sigplan.org/home/VMCAI-2024?utm_source=chatgpt.com","title":"VMCAI 2024","snippet":"Welcome to the website of the 25th International Conference on Verification, Model Checking, and Abstract Interpretation (VMCAI 2024).","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"sigplan.org"}]},{"type":"search_result_group","domain":"raw.githubusercontent.com","entries":[{"type":"search_result","url":"https://raw.githubusercontent.com/mlresearch/v258/main/assets/ng25a/ng25a.pdf?utm_source=chatgpt.com","title":"Causal Representation Learning from General Environments ...","snippet":"by I Ng \u2014 Causal representation learning aims to re- cover the latent causal variables and their causal relations, typically represented by di-.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"raw.githubusercontent.com"},{"type":"search_result","url":"https://raw.githubusercontent.com/mlresearch/v258/main/assets/murata25a/murata25a.pdf?utm_source=chatgpt.com","title":"Clustered Invariant Risk Minimization - GitHub","snippet":"by T Murata \u2014 This study extends the problem settings of. Invariant Risk Minimization(IRM) for Out- of-Distribution generalization problems to un-.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"raw.githubusercontent.com"}]},{"type":"search_result_group","domain":"galileo.ai","entries":[{"type":"search_result","url":"https://galileo.ai/blog/anthropic-ai-interpretability-breakthrough?utm_source=chatgpt.com","title":"How Anthropic Made AI 70% More Interpretable","snippet":"Aug 1, 2025 \u2014 Discover Anthropic's breakthrough: sparse autoencoders make AI 70% interpretable by decomposing neurons into readable features.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"galileo.ai"}]},{"type":"search_result_group","domain":"ucf.edu","entries":[{"type":"search_result","url":"https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1164&context=hut2024&utm_source=chatgpt.com","title":"Reinforcement Learning From Human Feedback For Ethically ...","snippet":"by MM Plasencia \u00b7 2024 \u2014 The central question driving this research is: How can RLHF enhance the robustness of AI's decision-making capabilities in ethically complex.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"ucf.edu"}]},{"type":"search_result_group","domain":"mdpi.com","entries":[{"type":"search_result","url":"https://www.mdpi.com/2076-328X/14/11/1091?utm_source=chatgpt.com","title":"Bounded Rationality in Study Time Allocation","snippet":"by H Xu \u00b7 2024 \u00b7 Cited by 1 \u2014 This research explored how individuals allocated study time and whether this process exhibited bounded rationality by examining the risky choice framing ...See more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/27/4/372?utm_source=chatgpt.com","title":"Intrinsic Motivation as Constrained Entropy Maximization","snippet":"by AB Kiefer \u00b7 2025 \u00b7 Cited by 3 \u2014 This paper views active inference, empowerment, and other formal accounts of intrinsic motivation as variations on the theme of constrained maximum entropy ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/25/2/327?utm_source=chatgpt.com","title":"An Information-Theoretic Perspective on Intrinsic ...","snippet":"by A Aubret \u00b7 2023 \u00b7 Cited by 61 \u2014 We propose to survey these research works through a new taxonomy based on information theory: we computationally revisit the notions of surprise, novelty, and ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/7/582?utm_source=chatgpt.com","title":"Active Inference for Learning and Development in ...","snippet":"by S Hamburg \u00b7 2024 \u00b7 Cited by 3 \u2014 Rooted in Bayesian inference, it combines elements of perception, action, and learning into a unified theory. Originating from neuroscience, it ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"mdpi.com"},{"type":"search_result","url":"https://www.mdpi.com/1099-4300/26/9/797?utm_source=chatgpt.com","title":"Forced Friends: Why the Free Energy Principle Is Not the New ...","snippet":"","ref_id":{"turn_index":4,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"mdpi.com"}]},{"type":"search_result_group","domain":"acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3696412?utm_source=chatgpt.com","title":"Causal representation learning through higher-level ...","snippet":"Nov 7, 2024 \u2014 We explore a perspective on this problem that is directed to learning the generative process with causality-related foundations.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1109/TPAMI.2024.3489597?utm_source=chatgpt.com","title":"The Decoupling Concept Bottleneck Model | IEEE Transactions on Pattern Analysis and Machine Intelligence - ACM Digital Library","snippet":"Feb 1, 2025 \u2014 The Concept Bottleneck Model (CBM) is an interpretable neural network that leverages high-level concepts to explain model decisions and ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/fullHtml/10.1145/3641399.3641445?utm_source=chatgpt.com","title":"Neural Networks Verification: Perspectives from Formal ...","snippet":"by P Maity \u00b7 2024 \u2014 In this work, we explore the limitations of Formal Verification methods in the context of Neural Networks and further aim to propose a principled method to ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3692070.3693107?utm_source=chatgpt.com","title":"Invariant risk minimization is a total variation model","snippet":"by ZR Lai \u00b7 2024 \u00b7 Cited by 1 \u2014 Invariant risk minimization (IRM) is an arising approach to generalize invariant features to different environments in machine learning.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3539618.3591755?utm_source=chatgpt.com","title":"Reformulating CTR Prediction: Learning Invariant Feature ...","snippet":"by Y Zhang \u00b7 2023 \u00b7 Cited by 21 \u2014 We propose Disentangled Invariant Learning (DIL) which disentangles feature embeddings to capture the two types of correlations separately.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/pdf/10.1145/3743127?utm_source=chatgpt.com","title":"RLHF Deciphered: A Critical Analysis of Reinforcement ...","snippet":"by S Chaudhari \u00b7 Cited by 68 \u2014 A signiicant challenge in training large language models (LLMs) as efective assistants is aligning them with human preferences.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.1007/978-3-031-60606-9_21?utm_source=chatgpt.com","title":"Surveying Computational Theory of Mind and a Potential ...","snippet":"by P Kumar \u00b7 2024 \u2014 Theory of Mind (ToM) is a capability of humans to attribute mental states to other agents allowing us to reason on the agent's intentions ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/abs/10.1145/3696410.3714618?utm_source=chatgpt.com","title":"Behavioral Homophily in Social Media via Inverse Reinforcement ...","snippet":"This work introduces a novel approach for quantifying user homophily. We first use an Inverse Reinforcement Learning (IRL) framework to infer users' policies, ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"acm.org"}]},{"type":"search_result_group","domain":"nips.cc","entries":[{"type":"search_result","url":"https://nips.cc/virtual/2024/poster/93425?utm_source=chatgpt.com","title":"Decision-Making Behavior Evaluation Framework for LLMs ...","snippet":"Dec 9, 2024 \u2014 When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"nips.cc"}]},{"type":"search_result_group","domain":"nogsky.com","entries":[{"type":"search_result","url":"https://www.nogsky.com/project/information-bottleneck-deep-learning/?utm_source=chatgpt.com","title":"The Information Bottleneck principle and neural networks","snippet":"Publications \u00b7 Deep learning and the Information Bottleneck principle \u00b7 Early motion processing circuit uses gap junctions to achieve efficient stimuli encoding.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"nogsky.com"}]},{"type":"search_result_group","domain":"rohan-paul.com","entries":[{"type":"search_result","url":"https://www.rohan-paul.com/p/mixture-of-experts-moe-architectures?utm_source=chatgpt.com","title":"Mixture-of-Experts (MoE) Architectures: 2024\u20132025 Literature ...","snippet":"Mixture-of-Experts architectures have resurged as a key avenue for scaling AI models in 2024 and 2025. The core idea of sparsely activated expert networks has ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"rohan-paul.com"}]},{"type":"search_result_group","domain":"royalsocietypublishing.org","entries":[{"type":"search_result","url":"https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2016.0616?utm_source=chatgpt.com","title":"Active inference and robot control: a case study - Journals","snippet":"by L Pio-Lopez \u00b7 2016 \u00b7 Cited by 102 \u2014 Here, we discuss a proof-of-principle implementation of the active inference scheme for the control or the 7-DoF arm of a (simulated) PR2 robot.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"royalsocietypublishing.org"}]},{"type":"search_result_group","domain":"biorxiv.org","entries":[{"type":"search_result","url":"https://www.biorxiv.org/content/10.1101/2024.10.21.619537v1.full.pdf?utm_source=chatgpt.com","title":"Humans are resource-rational predictors in a sequence ...","snippet":"by V Ferdinand \u00b7 2024 \u2014 This research provides new avenues for assessing smart behavior in difficult pre- diction tasks, provides a new methodology for assessing ...See more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"biorxiv.org"}]},{"type":"search_result_group","domain":"splashcon.org","entries":[{"type":"search_result","url":"https://2024.splashcon.org/details/nsad-2024-papers/7/Abstract-Domains-for-Machine-Learning-Verification?utm_source=chatgpt.com","title":"Abstract Domains for Machine Learning Verification","snippet":"We will survey our recent and ongoing work in the design and development of abstract domains for machine learning verification.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"splashcon.org"}]},{"type":"search_result_group","domain":"iitd.ac.in","entries":[{"type":"search_result","url":"https://web.iitd.ac.in/~sumeet/Tishby2015.pdf?utm_source=chatgpt.com","title":"Deep Learning and the Information Bottleneck Principle","snippet":"by N Tishby \u00b7 Cited by 2205 \u2014 This is closely related to successive refinement of information in Rate Distortion Theory [7]. Each layer in the network can now be ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"iitd.ac.in"}]},{"type":"search_result_group","domain":"google.com","entries":[{"type":"search_result","url":"https://scholar.google.com/citations?hl=en&user=yBfCuuQAAAAJ&utm_source=chatgpt.com","title":"Kunal Pattanayak","snippet":"Rationally Inattentive Inverse Reinforcement Learning Explains YouTube Commenting Behavior. W Hoiles, V Krishnamurthy, K Pattanayak. Journal of Machine Learning ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"google.com"}]},{"type":"search_result_group","domain":"thecvf.com","entries":[{"type":"search_result","url":"https://cvpr.thecvf.com/virtual/2023/poster/21019?utm_source=chatgpt.com","title":"Self-Supervised Learning From Images With a Joint ... - CVPR","snippet":"Self-supervised learning from images with a joint-embedding predictive architecture. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"thecvf.com"},{"type":"search_result","url":"https://openaccess.thecvf.com/content/WACV2025/papers/Saito_Point-JEPA_A_Joint_Embedding_Predictive_Architecture_for_Self-Supervised_Learning_on_WACV_2025_paper.pdf?utm_source=chatgpt.com","title":"Point-JEPA: A Joint Embedding Predictive Architecture for Self ...","snippet":"Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas. Ballas. V-jepa: Latent video prediction for visual represen- tation learning. 2023. 1. [4] Tom B. Brown ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"thecvf.com"},{"type":"search_result","url":"https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Hybrid_Concept_Bottleneck_Models_CVPR_2025_paper.pdf?utm_source=chatgpt.com","title":"Hybrid Concept Bottleneck Models - CVF Open Access","snippet":"by Y Liu \u00b7 2025 \u2014 In this work, we propose Hybrid Concept. Bottleneck Model (HybridCBM), a novel CBM framework to address the challenge of incomplete predefined concepts. Our ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"thecvf.com"}]},{"type":"search_result_group","domain":"arize.com","entries":[{"type":"search_result","url":"https://arize.com/blog/llm-interpretability-and-sparse-autoencoders-openai-anthropic/?utm_source=chatgpt.com","title":"LLM Interpretability and Sparse Autoencoders","snippet":"Jun 14, 2024 \u2014 Breaking down two papers that focus on the sparse autoencoder--an unsupervised approach for extracting interpretable features from an LLM.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"arize.com"}]},{"type":"search_result_group","domain":"bohrium.com","entries":[{"type":"search_result","url":"https://www.bohrium.com/paper-details/mastering-diverse-domains-through-world-models/867773987730490001-108591?utm_source=chatgpt.com","title":"Mastering Diverse Domains through World Models","snippet":"Jan 10, 2023 \u2014 Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long- ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"bohrium.com"}]},{"type":"search_result_group","domain":"umass.edu","entries":[{"type":"search_result","url":"https://rlj.cs.umass.edu/2024/papers/RLJ_RLC_2024_89.pdf?utm_source=chatgpt.com","title":"Three Dogmas of Reinforcement Learning","snippet":"by D Abel \u00b7 Cited by 11 \u2014 Modern reinforcement learning has been conditioned by at least three dogmas. The first is the environment spotlight, which refers to our tendency to focus ...See more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"umass.edu"}]},{"type":"search_result_group","domain":"acs.org","entries":[{"type":"search_result","url":"https://pubs.acs.org/doi/10.1021/acs.jpcb.3c07304?utm_source=chatgpt.com","title":"E(n) Equivariant Graph Neural Network for Learning ...","snippet":"by K Nehil-Puleo \u00b7 2024 \u00b7 Cited by 8 \u2014 We have developed a multi-input E(n) equivariant graph convolution-based model designed for the prediction of chemical properties that result from the ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"acs.org"}]},{"type":"search_result_group","domain":"wiley.com","entries":[{"type":"search_result","url":"https://onlinelibrary.wiley.com/doi/10.1155/int/5124400?utm_source=chatgpt.com","title":"An Overview of Robot Embodied Intelligence Based on ...","snippet":"Jun 14, 2025 \u2014 This work reviews initial embodied tasks and corresponding research, categorizes various current embodied intelligence schemes deployed in ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"wiley.com"}]},{"type":"search_result_group","domain":"tandfonline.com","entries":[{"type":"search_result","url":"https://www.tandfonline.com/doi/full/10.1080/15391523.2025.2478424?utm_source=chatgpt.com","title":"The AI Motivation Scale (AIMS): a self-determination theory ...","snippet":"by J Li \u00b7 2025 \u00b7 Cited by 1 \u2014 The scale measures university students' motivation to learn with AI across five dimensions: intrinsic motivation, identified regulation, introjected regulation ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"tandfonline.com"}]},{"type":"search_result_group","domain":"preprints.org","entries":[{"type":"search_result","url":"https://www.preprints.org/manuscript/202408.0583/v1?utm_source=chatgpt.com","title":"The Evolution of Mixture of Experts: A Survey from Basics ...","snippet":"Aug 7, 2024 \u2014 The popularity of MoE only rose recently as the appearance of Large Language Models (LLMs) and transformer-based models in general swept through ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"preprints.org"}]},{"type":"search_result_group","domain":"diginomica.com","entries":[{"type":"search_result","url":"https://diginomica.com/cce-2024-active-inference-ai-shakes-enterprise-ai-conversation-edgier-thinking-whats-next?utm_source=chatgpt.com","title":"CCE 2024 - Active Inference AI shakes up the enterprise AI ...","snippet":"Nov 12, 2024 \u2014 Active Inference AI is built on a different architecture entirely: There's no big data requirement. It does not need to be tethered to a giant database.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"diginomica.com"}]},{"type":"search_result_group","domain":"tufts.edu","entries":[{"type":"search_result","url":"https://aabl.cs.tufts.edu/papers/rlc2024_james.pdf?utm_source=chatgpt.com","title":"Agent-Centric Human Demonstrations Train World Models","snippet":"by J Staley \u00b7 Cited by 1 \u2014 We build on DreamerV3 (Hafner et al., 2023), which trains an actor-critic on-policy purely from the model's imagined unrolls. Human demonstrations inform ...","ref_id":{"turn_index":4,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"tufts.edu"}]},{"type":"search_result_group","domain":"forbes.com","entries":[{"type":"search_result","url":"https://www.forbes.com/sites/robtoews/2024/02/04/what-is-the-best-way-to-control-todays-ai/?utm_source=chatgpt.com","title":"RLHF And Beyond: How Can We Teach AI The Right ...","snippet":"Feb 4, 2024 \u2014 RLHF has become the dominant method by which human developers control and steer the behavior of AI models, especially language models.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"forbes.com"}]}],"citations":[],"request_id":"9733a1258e21ec6e-IAD","message_type":"next","model_slug":"gpt-5-thinking","default_model_slug":"gpt-5-thinking","parent_id":"2eb7732d-384a-48a0-a291-c7ed857f621c","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"2eb7732d-384a-48a0-a291-c7ed857f621c","children":[]}}}