{"id":"67f3958e-3d04-8007-908f-6713f29e9544","title":"Multi-Token Attention Overview","create_time":1744016782.597334,"update_time":1744017632.157327,"current_node":"680eebb5-fa3a-49bf-a96e-3b68f088740f","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["411c7656-42ad-4524-9007-9b052b84747b"]},"411c7656-42ad-4524-9007-9b052b84747b":{"id":"411c7656-42ad-4524-9007-9b052b84747b","message":{"id":"411c7656-42ad-4524-9007-9b052b84747b","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["8846ad68-d3a6-40b3-8ea2-919e311b5962"]},"8846ad68-d3a6-40b3-8ea2-919e311b5962":{"id":"8846ad68-d3a6-40b3-8ea2-919e311b5962","message":{"id":"8846ad68-d3a6-40b3-8ea2-919e311b5962","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"411c7656-42ad-4524-9007-9b052b84747b","children":["f3e9f6f1-88da-4378-8b25-29bdf4a34948"]},"f3e9f6f1-88da-4378-8b25-29bdf4a34948":{"id":"f3e9f6f1-88da-4378-8b25-29bdf4a34948","message":{"id":"f3e9f6f1-88da-4378-8b25-29bdf4a34948","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"8846ad68-d3a6-40b3-8ea2-919e311b5962","children":["12d61d9c-da7c-498a-b9ec-dc99a9a2ec67"]},"12d61d9c-da7c-498a-b9ec-dc99a9a2ec67":{"id":"12d61d9c-da7c-498a-b9ec-dc99a9a2ec67","message":{"id":"12d61d9c-da7c-498a-b9ec-dc99a9a2ec67","author":{"role":"user","name":null,"metadata":{}},"create_time":1744016781.844,"update_time":null,"content":{"content_type":"text","parts":["Give me a deep dive into this paper"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"attachments":[{"id":"file-MRxycTqMvP7g2BxMrzZ4W1","size":1527200,"name":"2504.00927v1.pdf","mime_type":"application/pdf","file_token_size":16515}],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"92c85e580c44fd30-SJC","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"f3e9f6f1-88da-4378-8b25-29bdf4a34948","children":["bfbe6698-00b4-494e-8997-e467556bf7f9"]},"bfbe6698-00b4-494e-8997-e467556bf7f9":{"id":"bfbe6698-00b4-494e-8997-e467556bf7f9","message":{"id":"bfbe6698-00b4-494e-8997-e467556bf7f9","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1744016794.618568,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"","summary":"","assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"command":"spinner","status":"running","message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"12d61d9c-da7c-498a-b9ec-dc99a9a2ec67","request_id":"92c85e580c44fd30-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"12d61d9c-da7c-498a-b9ec-dc99a9a2ec67","children":["e8b97e03-9ee6-4a37-891e-b5ab52662a3c"]},"e8b97e03-9ee6-4a37-891e-b5ab52662a3c":{"id":"e8b97e03-9ee6-4a37-891e-b5ab52662a3c","message":{"id":"e8b97e03-9ee6-4a37-891e-b5ab52662a3c","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1744016794.618835,"update_time":null,"content":{"content_type":"multimodal_text","parts":["Make sure to include \ue200cite\ue202turn0file0\ue201 in your response to cite this file. ","<PARSED TEXT FOR PAGE: 1 / 24>","Multi-Token Attention\r\nOlga Golovneva Tianlu Wang Jason Weston Sainbayar Sukhbaatar\r\nFAIR at Meta\r\nAbstract\r\nSoft attention is a critical mechanism powering LLMs to locate relevant\r\nparts within a given context. However, individual attention weights are\r\ndetermined by the similarity of only a single query and key token vector.\r\nThis \u201csingle token attention\u201d bottlenecks the amount of information used in\r\ndistinguishing a relevant part from the rest of the context. To address this\r\nissue, we propose a new attention method, Multi-Token Attention (MTA),\r\nwhich allows LLMs to condition their attention weights on multiple query\r\nand key vectors simultaneously. This is achieved by applying convolution\r\noperations over queries, keys and heads, allowing nearby queries and keys\r\nto affect each other\u2019s attention weights for more precise attention. As a\r\nresult, our method can locate relevant context using richer, more nuanced\r\ninformation that can exceed a single vector\u2019s capacity. Through extensive\r\nevaluations, we demonstrate that MTA achieves enhanced performance\r\non a range of popular benchmarks. Notably, it outperforms Transformer\r\nbaseline models on standard language modeling tasks, and on tasks that\r\nrequire searching for information within long contexts, where our method\u2019s\r\nability to leverage richer information proves particularly beneficial.\r\n1 Introduction\r\nThe attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) is a critical component\r\nof Large Language Models (LLMs) that enables them to retrieve and combine information\r\nfrom different parts of the context. Attention is especially useful when the context contains\r\na large number of tokens, as focusing on the relevant part while disregarding distractions\r\nbecomes crucial. However, numerous works have shown that standard attention can suffer\r\nfrom suboptimal performance in this setting (Kamradt, 2023; Kuratov et al., 2025).\r\nStandard multi-head attention works by comparing the similarity of the current query vector\r\nand the key vectors corresponding to the context tokens using their dot products. The keys\r\nsimilar to the query obtain higher attention weights, and subsequently their value vectors\r\ndominate the output vector. For example, a query vector corresponding to a token \u201cAlice\u201d\r\nis capable of locating all mentions of \u201cAlice\u201d in the context. However, each attention weight\r\nconditions only on a single key and query vector (besides the normalization to sum to one).\r\nWe argue that the dependency on single token vector similarity brings a fundamental\r\nlimitation to the attention mechanism. In many cases, the relevant part of the context cannot\r\nbe identified by a single token. For example, looking up a sentence that mentions both\r\n\u201cAlice\u201d and \u201crabbit\u201d requires the query vector to encode both tokens. Looking up \u201cAlice\u201d\r\nwith one attention head and using another head for \u201crabbit\u201d could find their mentions\r\nseparately, but is not sufficient to pinpoint where both are mentioned together. While it is\r\npossible to encode multiple tokens into a single vector via the layers of the Transformer, this\r\nrequires increased dimension, and for the model to use lots of its capacity for this task.\r\nIn this paper, we propose a novel attention mechanism that goes beyond this \u201csingle token\u201d\r\nbottleneck, which we call Multi-Token Attention (MTA). The high level goal is to make it\r\npossible to use the similarities of multiple vector pairs to determine where attention must\r\nfocus. We achieve this by making straight-forward modifications to the existing attention\r\nmechanism. In particular, we design convolution operations over attention weights that\r\n1\r\narXiv:2504.00927v1 [cs.CL] 1 Apr 2025","<PARSED TEXT FOR PAGE: 2 / 24>","Q\r\nMatMul\r\nScale\r\nMask\r\nQ-K convolution Matmul\r\nSoftmax\r\nHead \r\nconvolution\r\nGroup norm\r\nMatMul\r\nScale\r\nMask\r\nSoftmax\r\nMatmul\r\nStandard multi-head attention Multi-token attention\r\nMatMul\r\nScale\r\nMask\r\nSoftmax\r\nMatmul\r\nMatMul\r\nScale\r\nMask\r\nKey-query \r\nconvolution Matmul\r\nSoftmax\r\nHead mixing \r\nconvolution\r\nGroup norm\r\nQ K V K V\r\nheads\r\nConcat\r\nConcat\r\nFigure 1: Multi-Token Attention (MTA) on the right, compared to standard attention on the\r\nleft. In MTA, within each head we apply a key-query convolution on the attention scores\r\nand, after softmax normalization, head convolution across groups of heads. Additionally,\r\nwe apply a group normalization with depth-dependent scaling before final concatenation.\r\noperate on three dimensions: keys, queries, and attention heads. This allows its attention\r\nweights to condition on neighboring keys, previous queries, and other heads. Intuitively,\r\nfollowing our previous example, MTA can find mentions of \u201cAlice\u201d and \u201crabbit\u201d separately\r\nfirst, and then combine those attentions together to focus only on where both exist.\r\nWe first experiment with a motivating toy task that reveals the shortcoming of standard\r\nattention and demonstrate that MTA can easily solve it. Next, we test our method at scale by\r\npre-training 880M parameters models on 105B tokens on a standard language modeling task.\r\nThere we see MTA bring improvements in terms in validation perplexity as well as standard\r\nbenchmark tasks, while only increasing the number of parameters by 0.001%. Further, we\r\nevaluate the resulting models on long-context tasks such as Needle-in-the-Haystack and\r\nBabiLong where MTA outperforms the baselines by a significant margin.\r\n2 Background on multi-head attention\r\nWe first describe the standard multi-head attention (Vaswani et al., 2017) and define the\r\nnotation we use. In decoder-only Transformer architectures, the model receives a sequence\r\nof tokens [x1, ..., xT] of length T as an input. These tokens then undergo a series of trans\u0002formations into hidden states H = [h1, . . . hT]\r\n\u22a4 \u2208 RT\u00d7D through an embedding layer and\r\nrepeated transformer layers. Each layer consists of a multi-head attention submodule, a\r\nfeedforward network, and normalization operations. Multi-head attention includes M\r\nheads with dimensions d = D/M working in parallel. Each head uses key, value and query\r\nprojections Wk, Wv, Wq \u2208 RD\u00d7dto construct key, value and query vectors:\r\nK = HWk\r\n, V = HWv, Q = HWq\r\nThe attention logits A\u02c6 and weights A (i.e. probabilities) are then calculated as follows:\r\nA\u02c6 = QK\u22a4/\r\n\u221a\r\nd, A = Softmax(Mask\u2212\u221e(A\u02c6)), (1)\r\nwhere the softmax operates over the key dimension, and the mask function replaces values\r\nat (i, j) with \u2212\u221e for i < j to prevent information leaking from future tokens. Finally, the\r\nattention output AV \u2208 RT\u00d7dfrom all heads is then concatenated, and multiplied by the\r\noutput projection Wo \u2208 RD\u00d7D which is then passed to the normalization and feedforward\r\ncomponents. This standard approach is summarized in Figure 1 (left).\r\n3 Multi-Token Attention\r\nEach attention value in standard multi-head attention, see Equation 1, depends solely on\r\na single key and query vector. That means all the necessary information for finding and\r\nattending to a relevant part of the context must be compressed into these single vectors. This\r\n2","<PARSED TEXT FOR PAGE: 3 / 24>","queries\r\nqueries\r\nkeys keys\r\nheads\r\nheads\r\nKey-query \r\nkernel\r\nHead kernel\r\nFigure 2: MTA applies key-query and head convolution over attention values.\r\nmight not be ideal if we are looking for a sentence containing multiple elements. Consider\r\nfor example the sentence \u201cWhere did Alice see the rabbit?\u201d. We could try to find instances\r\nof \u201cAlice\u201d and \u201crabbit\u201d independently and then check if there is a sentence that has both.\r\nLet qa and qr be query vectors encoding \u201cAlice\u201d and \u201crabbit\u201d respectively (assuming a word\r\ntokenizer), then their attention weights are computed as follows:\r\naa = Softmax(qaK\r\n\u22a4/\r\n\u221a\r\nd), ar = Softmax(qrK\r\n\u22a4/\r\n\u221a\r\nd) (2)\r\nBy doing normal attention with those queries, we can attend where \u201cAlice\u201d and \u201crabbit\u201d\r\nare mentioned in the context. All we have to do then is to check if both attention weights\r\naa and ar have higher probabilities at the same nearby locations, e.g. in the same sentence,\r\nwhich will indicate that the sentence mentions both \u201cAlice\u201d and \u201crabbit\u201d. Unfortunately,\r\nnormal attention lacks such interaction between attention maps, and instead only uses them\r\nto compute output values. Even if we use different attention heads to find \u201cAlice\u201d and\r\n\u201crabbit\u201d, there is no mechanism to combine these attention weights. This motivates us to\r\nmodify the attention mechanism to allow combining different attention maps from nearby\r\nlocations (both in terms of query and key locations), or between different attention heads.\r\nAs shown in Figure 1 (right), our proposed Multi-Token Attention consists of three impor\u0002tant components built on top of multi-head attention: key-query convolution, head mixing\r\nconvolution, and group normalization with depth scaling. We propose key-query convo\u0002lution to combine multiple keys and queries within heads, and head convolution to share\r\nknowledge between heads and amplify important information. Finally, we apply group\r\nnormalization with depth scaling (Ye et al., 2024) to push back against residual streams and\r\nimprove gradient flow. In this section, we will describe how MTA works.\r\n3.1 Key-query convolution\r\nPre-softmax convolution MTA applies a convolution operation on attention logits to\r\ncombine information from multiple query and key tokens:\r\nA = Softmax Conv2d\u03b8 (A\u02c6)\r\n\u0001\r\n, (3)\r\nwhere Conv2d\u03b8is a 2-dimensional convolution operation with kernel weights \u03b8, and kernel\r\nsizes (cq, ck). Convolution is applied to the key and query length dimensions, while the\r\nbatch and head dimensions remain independent. More precisely, the attention weight aij\r\nfrom query qito key kjis computed as follows:\r\naij = Softmax\r\n\uf8eb\r\n\uf8ed\r\ncq\u22121\r\n\u2211\r\ni\r\n\u2032=0\r\n\u2308ck/2\u2309\u22121\r\n\u2211\r\nj\r\n\u2032=\u2212\u230ack/2\u230b\r\n1i\u2265j\u2212j\r\n\u2032 \u03b8i\u2032\r\n,j\r\n\u2032 qi\u2212i\u2032 k\r\n\u22a4\r\nj\u2212j\r\n\u2032 /\r\n\u221a\r\nd\r\n\uf8f6\r\n\uf8f8 (4)\r\nGiven that the query position is i, any information from position > i should not be used\r\nto prevent information leakage from the future. That is why only past queries are used in\r\nEquation 4. For keys, we use indicator function 1i\u2265j\u2212j\r\n\u2032 to zero out future keys. However\r\nsuch masking is too complex to implement (it is necessary to modify the convolution cuda\r\nkernels), thus we propose a simpler version that applies the existing causal masking twice:\r\nA = Softmax Mask\u2212\u221e\r\n\r\nConv2d\u03b8\r\n\r\nMask0(A\u02c6)\r\n\u0001\u0001\u0001 . (5)\r\nHere, the first masking uses 0 so that those values will not affect the output from the\r\nconvolution. Although this version masks out a little more than necessary, it is simpler to\r\nimplement and prevents information leakage, so we use it as our default.\r\n3","<PARSED TEXT FOR PAGE: 4 / 24>","Post-softmax convolution Similarly, we can apply the convolution on top of the attention\r\nweights instead of the logits\r\nA = Mask0\r\n\r\nConv2d\u03b8\r\n\r\nSoftmax Mask\u2212\u221e(A\u02c6)\r\n\u0001\u0001\u0001 . (6)\r\nThis makes interaction between attention weights additive instead of multiplicative. We\r\nwill experiment with both versions, but will use the pre-softmax version by default.\r\nEach attention head has separate \u03b8 parameters, so they can perform different convolution\r\noperations. The chosen kernel dimensions dictate how far away tokens can be combined\r\ntogether. In the above example, the question \u201cWhere did Alice see the rabbit?\u201d will make qa\r\nand qr queries separated by two tokens (assuming a word tokenizer), so we need cq = 4 to\r\ncover both queries. Similarly, the target sentence \u201cAlice saw the white rabbit under the tree\u201d\r\nfor example produces keys ka and kr separated by three tokens, so ck = 5 is sufficient for\r\ncombining them. Before applying the convolution, we pad the input with an appropriate\r\namount of zeros so that each convolution operation has a valid input.\r\n3.2 Head mixing convolution\r\nUnlike the key-query convolution, which allows mixing of attention weights from different\r\ntime steps, we further propose to use a head convolution over groups of heads, so attention\r\nweights from different heads can be combined. In particular, for a head convolution kernel\r\nof size ch, all heads are divided in M/ch groups. Within each group, we apply a non\u0002overlapping convolution operation1\r\n. This way, MTA allows not only to condition attention\r\nweights on multiple query and key vectors within each head, but also shares attention\r\ninformation across heads. For example, consider splitting all heads into groups of two, such\r\nthat the kernel size is ch = 2. Let us use superscript to denote head indices, so A\r\n1 and A2\r\nare attention weights from two different heads. Then the new attention weights are:\r\nA\r\n1\r\nnew = w11A\r\n1 + w12A2\r\n, A\r\n2\r\nnew = w21A\r\n1 + w22A2\r\n, (7)\r\nwhere w11, w12, w21, w22 are kernel weights. Here the mixing occurred after the softmax, but\r\nwe can also mix logits before the softmax:\r\nA\u02c6 1\r\nnew = w11A\u02c6 1 + w12A\u02c6 2\r\n, A\u02c6 2\r\nnew = w21A\u02c6 1 + w22A\u02c6 2\r\n,\r\nLike the key-query convolution, we experiment with both versions.\r\n3.3 Putting everything together\r\nIn the previous sections, we introduced two different ways of mixing attention weights,\r\none across key-query time steps and another across different heads. These two can be\r\nimplemented together in a single MTA module. Because each has pre and post-softmax\r\nversions, there are multiple different ways of combining them.\r\nIf both mixing methods are pre-softmax, then they can be implemented by a single 3-\r\ndim convolution operation as shown in Figure 2. Two of these dimensions will span key\r\nand query dimensions as described in Section 3.1. The third dimension will be across\r\nattention heads, but on groups of ch heads. The same 3-dim convolution can be applied after\r\nsoftmax to perform both mixing methods post-softmax. The third possibility is to apply the\r\nkey-query convolution before softmax, and then head mixing after softmax. This is also\r\nstraightforward by applying Equation 7 on the attention weights obtained from Equation 5.\r\nFinally, we follow Ye et al. (2024) and apply normalization with layer-dependent scaling for\r\neach head independently to improve gradient flow. We also report results without this step.\r\n4 Experiments\r\nWe conduct experiments with the MTA architecture, comparing to several baselines on a set\r\nof standard and long-range dependency tasks, starting with a toy task. Unless otherwise\r\nspecified, we apply key-query convolution pre-softmax and head mixing post-softmax.\r\n1Since the kernel size ch and group size ch are the same, it can also be viewed as fully-connected.\r\n4","<PARSED TEXT FOR PAGE: 5 / 24>","Block size N = 5 Block size N = 8\r\nModel All First Last All First Last\r\nTransformer 51.6 \u00b1 43.1 78.2 \u00b1 1.5 1.3 \u00b1 0.4 31.2 \u00b1 50.3 28.9 \u00b1 24.9 58.4 \u00b1 46.8\r\nMTA 0.1 \u00b1 0.1 0.0 \u00b1 0.0 0.0 \u00b1 0.0 0.1 \u00b1 0.0 0.0 \u00b1 0.0 0.0 \u00b1 0.0\r\nTable 1: Error rates (%) on the motivating toy task where the model needs to locate a block\r\nof letters containing the given L = 2 letters. The output should contain all, first, or last\r\ntokens of the target block. MTA perfectly solves this task while a standard Transformer\r\nstruggles to learn it. We report average error rate over 3 seeds and their standard deviations.\r\n4.1 Motivating toy task\r\nWe start with a simple toy task to demonstrate the effectiveness of our method over standard\r\nmulti-head attention. In this task, the model is given a sequence of blocks where each block\r\nconsists of N random letters. This is followed by L question letters (L < N). The objective is\r\nto find the block that contains all question letters in any order. Then the model must output\r\nall letters of the target block, or only its first or last token (as three separate task variants).\r\nDespite its simplicity, this task poses a challenge to standard attention because it requires L\r\npieces of information (i.e. question letters) to identify the target block. To succeed, standard\r\nsoft attention must encode all L question letters into a single query vector. In contrast, MTA\r\ncan first find the locations of each question letter, then use its convolution operation to\r\nincrease the attention to the locations where all L letters are found together.\r\nThe results are shown in Table 1. As expected, Transformer with standard multi-head\r\nattention struggles to solve this task, often completely failing to find the target blocks even\r\nthough the questions have only L = 2 letters. This highlights the inherent limitation of\r\nstandard attention conditioned on single token vectors. For MTA, we set the query kernel\r\nsize cq = 2 to match the number of question letters, and the key kernel ck = 2N \u2212 1 so it can\r\ncover a whole block on both sides. No head convolution is used. This way, it is sufficient for\r\neach query vector to encode only a single question letter while the convolution operation\r\ncan aggregate their locations to find the target block. As a result, MTA successfully solves all\r\nthe versions of the task with near zero error rate. See Appendix C for more training details.\r\n4.2 Large language modeling\r\nFor language modeling experiments, we perform pre-training of 880M-size models and\r\ncompare the Transformer model (Vaswani et al., 2017), Differential Transformer (DIFF\r\nTransformer) (Ye et al., 2024), and Transformer with MTA. DIFF Transformer calculates\r\nattention scores as the difference between two separate softmax attention maps, thus being\r\nrelated to our head convolution approach, so we use it as a baseline. All models are trained\r\nin the same setup on the SlimPajama (Soboleva et al., 2023) dataset for 105B tokens using the\r\nLingua framework (Videau et al., 2024). Training details are provided in Appendix D. To\r\nimprove training efficiency, we apply the key-query convolution on every 4th layer, while\r\nhead convolution is applied on all layers. Kernel dimensions are fixed at cq = 6, ck = 11, and\r\nwe mix groups of ch = 2 heads within each layer. We ablate these parameters in Section 4.6.\r\nFor each model, we conduct training twice and report average validation perplexity in\r\nTable 2. We observe consistent improvements across all validation datasets for the model\r\ntrained with MTA, even though key-query convolution was only applied to a quarter of the\r\nlayers, operating with less learnable parameters than DIFF Transformer (see Appendix Table 9).\r\nWe also note that group normalization with layer scaling is an important component that\r\ndrives superior performance for both the DIFF Transformer and MTA architectures.\r\nWe further evaluate our models on a set of popular benchmarks in a zero-shot setup as\r\nshown in Table 3. The model trained with MTA outperforms the baselines on most of them\r\nand achieves a higher average score, despite these not being long-context tasks.\r\n5","<PARSED TEXT FOR PAGE: 6 / 24>","Model arxiv book c4 cc github se wiki Avg PPL \u2193\r\nPretraining\r\nTransformer 4.65 13.47 20.20 14.41 4.28 10.13 11.64 11.25 (0.00)\r\nDIFF w/o group norm. 4.63 13.41 20.06 14.34 4.26 10.06 11.59 11.19 (0.00)\r\nDIFF transformer 4.62 13.33 19.99 14.28 4.25 10.04 11.54 11.14 (0.02)\r\nMTA w/o group norm. 4.63 13.36 20.03 14.31 4.25 10.05 11.52 11.16 (0.01)\r\nMTA 4.60 13.26 19.90 14.22 4.22 9.96 11.44 11.09 (0.00)\r\nLong context finetuning\r\nTransformer 4.32 13.18 20.14 14.08 3.96 9.84 11.63 11.02\r\nDIFF transformer 4.28 13.01 19.87 13.93 3.90 9.72 11.49 10.89\r\nMTA w/o group norm. 4.30 13.07 19.94 13.96 3.92 9.74 11.50 10.92\r\nMTA 4.27 12.97 19.84 13.87 3.88 9.65 11.45 10.85\r\nTable 2: Validation perplexity for 880M Transformer model on SlimPajama dataset after\r\ntraining for 105B tokens, and finetuning with 2048 \u2192 4096 context extension for another\r\n10.5B tokens. Pretraining perplexity was averaged across two runs.\r\nModel BoolQ PIQA SIQA HellaS WinoG ARCe ARCc OBQA MMLU Avg \u2191\r\nTransformer 56.2 70.2 39.9 38.5 56.4 57.9 25.9 23.8 24.5 43.7 (0.3)\r\nDIFF transformer 59.6 70.5 39.7 38.9 56.4 57.7 25.6 21.4 24.9 43.9 (0.5)\r\nMTA w/o group norm. 60.5 70.8 40.0 38.9 56.0 57.3 25.1 21.7 24.9 43.9 (0.0)\r\nMTA 61.0 70.9 39.9 39.0 57.1 58.5 25.0 22.7 25.7 44.4 (0.0)\r\nTable 3: Pretrained models\u2019 evaluation results on standard benchmarks. Results are averaged\r\nover two model training runs for each method.\r\nModel LAMBADA standard \u2193 LAMBADA OpenAI \u2193\r\nTransformer 17.6 9.5\r\nDIFF transformer 14.9 9.3\r\nMTA w/o group norm. 15.2 9.2\r\nMTA 13.6 8.7\r\nTable 4: Perplexity evaluations on the LAMBADA standard (Paperno et al., 2016) and LAM\u0002BADA OpenAI (Radford et al., 2019) datasets.\r\n4.3 Long context finetuning\r\nWe further finetune our models on the same mix of datasets for another 10.5B tokens, but\r\nincrease the context length from 2048 to 4096. We increase RoPE\u2019s theta to 500000, change the\r\nweight decay to 0, and reduce warm up steps to 50. Other parameters remain the same as\r\nduring pretraining (see Appendix D). The resulting Transformer model with MTA similarly\r\noutperforms the new baselines in perplexity evaluations as shown in Table 2.\r\n4.4 Long-range dependency tasks\r\nIt was previously shown that Transformers struggle to find relevant information especially\r\nin the middle of long context (Liu et al., 2024; 2025). To test MTA in this setting, we evaluate\r\ntrained models on three tasks: LAMBADA (Paperno et al., 2016; Radford et al., 2019), Needle\u0002In-A-Haystack (Kamradt, 2023) and BabiLong (Kuratov et al., 2025). All these tasks require\r\nmodels to almost sharply attend to the long-range tokens buried in the context.\r\nLAMBADA is a collection of texts that test the model\u2019s ability to search long-range de\u0002pendencies in the context. In particular, this dataset is designed such that humans can\r\ncorrectly predict the next word only if they have the whole text, but not if they only see\r\nthe last sentence preceding the target word. We observe models trained with MTA are\r\nbetter at correctly guessing the next word (Table 4), significantly outperforming the baseline\r\nTransformer model.\r\n6","<PARSED TEXT FOR PAGE: 7 / 24>","Pretraining, 2k cont. Long context finetuning, 4k cont.\r\nModel N=2 N=4 N=6 N=2 N=4 N=6\r\nTransformer 82.1 56.4 44.7 90.5 41.1 31.9\r\nDIFF transformer 96.0 62.5 55.6 95.7 73.8 60.0\r\nMTA w/o group norm. 95.3 92.6 77.4 94.4 96.3 79.0\r\nMTA 92.8 65.1 63.0 97.6 77.7 67.0\r\nTable 5: Multi-needle retrieval accuracy (%) when varying the number of needles (N)\r\naveraged over different needle insertion depths. Pretrained models are evaluated with 2K\r\ncontext, while finetuned models are evaluated with 4K context.\r\n0 2 4 6 8 10\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\nQueries\r\nKeys\r\n0.0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n1.0\r\n1.2\r\nfatal pinch.The magic number of San Francisco is8.The\r\nThe\r\n magic\r\n number\r\n of\r\n San\r\n Francisco\r\n is\r\n8\r\n0.00\r\n0.02\r\n0.04\r\n0.06\r\n0.08\r\n0.10\r\n0.12\r\n0.14\r\nFigure 3: Kernel pattern (left) and corresponding attention map (right), which has the\r\nhighest attention scores on the targeted needle \u201dThe magic number of San Francisco is 8\u201d.\r\nThis kernel amplifies attention if a query token sequence matches a key sequence \u2013 useful\r\nfor searching for related sentences that match the current sentence.\r\nNeedle-In-A-Haystack We probe our models by inserting 2, 4, and 6 needles in 2k and\r\n4k context windows at varying depths. We additionally ensure that for each new sample,\r\nthe needles are shuffled, thus removing bias the models might have toward extracting the\r\nneedle that was inserted first or last. Each setup is evaluated on 500 samples. Accuracy\r\nevaluations are averaged across the depths of insertion. As reported in Table 5, we observe a\r\nsignificant improvement in needle extraction abilities for models trained with MTA across all\r\nneedle counts and varying context lengths. Breakdown by depth is reported in Appendix G.\r\nBabiLong This benchmark tries to to assess a language models\u2019 ability to reason across facts\r\nscattered in long documents. We focus on tasks QA1-5 (Weston et al., 2015) where a correct\r\nresponse requires various numbers of facts or argument relations. Examples of input and\r\ntarget output can be found in Table 7. We present average accuracy in Figure 4(left) and\r\nper-task in Appendix Figure 5. We observe that the MTA modelperforms well compared to\r\nother models, especially when there is more distraction text (4K tokens) in the input.\r\n4.5 Kernel patterns\r\nKey-query and head convolution kernels across different layers and heads are shown in\r\nAppendix H. We observe that a number of of the key-query kernels are close to identity,\r\nwith near-one weight learned for targeted query and key, and near-zero weights learned for\r\nall other positions. However, there are numerous kernels that are more intricate.\r\nOne such kernel is shown in Figure 3(left), which has a diagonal structure. This kernel\r\nwill amplify attention if a sequence of query tokens match a sequence of keys. Such ability\r\nis useful in searching sentences that matches the current sentence. Indeed, we observe\r\nthat this particular kernel focuses attention on the target needle (see Figure 3(right)) in the\r\nNeedle-in-the-Haystack task, where we must locate the magic number of the matching city.\r\nOther kernels, for example, can be viewed as priming, amplifying if the same key was\r\nattended by previous queries (head 5 on Figure 8), or edge detecting, amplifying the first or\r\nlast of multiple contiguous keys with high attention (heads 8 and 11 on Figure 10). We leave\r\nfurther exploration of key-query kernel maps and their meaning for future explorations.\r\n7","<IMAGE FOR PAGE: 7 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-NQo4Hs5agXeXiivfZZLJxz","size_bytes":70457,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 8 / 24>","0K 1K 2K 4K\r\nDistraction\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\nAccuracy (%)\r\nAverage Performance on Task QA1-5\r\nTransformer\r\nDifferential Transformer\r\nMTA\r\nMTA w/o group norm\r\n0 2 4 6 8 10 12\r\nKey-query convolution layers\r\n11.12\r\n11.14\r\n11.16\r\n11.18\r\n11.20\r\n11.22\r\n11.24\r\n11.26\r\nPerplexity\r\nTransformer\r\nDifferential transformer\r\nFigure 4: (left) Average accuracy on QA1-5 tasks in BabiLong. The models are all pretrained\r\nwith 2K context and then finetuned with 4K context. Distraction text length varies from 0K\r\n(no distraction) to 4K tokens. MTA consistently outperforms the baseline models. (right)\r\nAblation on the number of MTA layers with key-query convolutions (head convolution is\r\napplied to all layers). We report average validation perplexity on SlimPajama.\r\nHead kernel patterns, on the contrary, are simpler due to their small size, as shown in\r\nAppendix Figure 13. Besides an identity with scaling, a common pattern is contrasting:\r\nsubtracting one attention weight from another. We also observe that the kernel scales\r\nincrease with layers when there is no group normalization (see Appendix Figure 14). This is\r\nprobably to compete with the residual stream, which gets larger with the model\u2019s depth.\r\nHowever, this pattern is not present with group normalization because it will undo the\r\neffect of such scaling.\r\n4.6 Ablations\r\nKey-query convolution To understand how key-query convolution affects the model\r\nperformance, we run ablation studies on the number of layers where key-query convolution\r\nis added to the attention. The results shown in Figure 4(right) demonstrate that when\r\nonly 2 layers are enhanced with MTA, the model can outperform strong baselines, while\r\n6 layers with MTA strikes a balance between performance and additional complexity (see\r\nAppendix F for more about computational complexity).\r\nKernel initialization Kernel initialization can affect the convergence rate and stability of\r\ntraining. We evaluated MTA models initialized with zero, constant (0.3), and identity kernels.\r\nThe latter corresponds to initialization with a regular Transformer without convolution in\r\nthe attention. We found that identity initialization leads to better convergence and final\r\nperformance, while models initialized with zero and 0.3 values reduce average validation\r\nperplexity by 0.02 and 0.08 correspondingly.\r\nAblation on MTA components We further experiment with MTA kernels of different\r\nsizes, changing the order of convolution operations with respect to softmax, and different\r\nnormalizations, such as layer-norm scaling (Sun et al., 2025). Results are summarized in\r\nTable 6. We found that different kernel sizes display similar kernel patterns, while resulting\r\nin slightly different evaluation results (middle rows). Group normalization and exponential\r\ndepth scaling are both important factors each bringing improvement over vanilla MTA (top\r\nrows), while square root layer-norm scaling surprisingly underperforms. Finally, changing\r\nthe order of convolutions wrt softmax operations increases perplexity only by 0.01-0.04\r\npoints (bottom rows).\r\n5 Related work\r\nFocusing attention There has been a number of attempts to focus the soft attention\r\nmechanism on important context. For example, modifications have been proposed to make\r\nsoftmax sharper. Martins & Astudillo (2016) propose to replace softmax with sparsemax for\r\n8","<PARSED TEXT FOR PAGE: 9 / 24>","Key-query conv Head conv Group norm PPL \u2193\r\n\u2713 \u2713 \u2713 11.09\r\n\u2713 \u2713 \u00d7 11.16\r\n\u2713 \u2713 no scaling 11.13\r\n\u2713 \u2713 layer-norm scaling 11.41\r\ncq = 4, ck = 9 \u00d7 \u2713 11.23\r\ncq = 6, ck = 11 \u00d7 \u2713 11.23\r\ncq = 8, ck = 13 \u00d7 \u2713 11.31\r\npost-softmax post-softmax \u2713 11.11\r\npost-softmax post-softmax \u00d7 11.19\r\npre-softmax pre-softmax \u2713 11.10\r\npre-softmax pre-softmax \u00d7 11.20\r\nTable 6: Ablation on MTA components: validation perplexity over SlimPajama dataset.\r\nsparse activations. Velickovi \u02c7 c et al. \u00b4 (2024); Nakanishi (2025) propose Adaptive temperature\r\nand Scalable-Softmax that adjust exponential base in softmax attention. Irrelevant tokens\r\ncan be altogether removed from memory (Sukhbaatar et al., 2021) to make attention easier.\r\nIn Golovneva et al. (2024); Desrochers et al. (2024) the authors incorporate contextual\r\ninformation into position encodings to allow positions to be amplified by context. More\r\nrecently, several noise canceling mechanisms have been proposed for attention. DIFF\r\ntransformer (Ye et al., 2024) uses differential amplifiers to focus attention to the relevant\r\ncontext, which is related to our head mixing step. Cang et al. (2025) further extends it by\r\nintroducing normalization steps. OpAmp adaptation (Wu et al., 2025) propose adapters,\r\nwhich are fine-tuned with noisy context to enhance an LLMs\u2019 denoising capability. Xu\r\net al. (2024) modified attention so that keys and values can shift by one time step. This can\r\nbe viewed as a special case of MTA where the convolution performs shifting in the key\r\ndimension.\r\nConvolution in Attention Convolution layers have been used with attention (Gehring\r\net al., 2017; Wu et al., 2019), especially in vision Transformers, to decompose each image into\r\na sequence of tokens with fixed length, and then apply multiple standard Transformer layers\r\n(Xiao et al., 2021; Wu et al., 2021). There have been some attempts to include convolution\r\nin language modeling as well. For example, Liu et al. (2018) use convolution to compress\r\nthe keys and values in the multi-headed attention by a factor of 3 thus allowing to process\r\nsequences 3x in length. Liu & Lapata (2019) later augment this architecture with the ability to\r\nencode multiple documents in a hierarchical manner, and further improve on long-context\r\nsummarization tasks. Later Subramanian et al. (2020) propose three hierarchical models to\r\nlearn different levels of abstraction in language by interchanging casual transformer layers\r\nwith convolutions or pooling, or compressing representations for tokens further away in the\r\npast. Gulati et al. (2020) proposed a Conformer architecture for speech recognition, where a\r\nconvolution module is applied to multi-head self-attention output. This architecture was\r\nlater used to encode speech in the Llama-3 herd of models (Grattafiori et al., 2024). However,\r\nnone of these methods apply convolution to the attention weights.\r\n6 Conclusion\r\nIn this paper, we focused on a limitation of the standard soft attention mechanism that\r\nstems from conditioning on the similarity of a single vector pairs. This makes it challenging\r\nfor Transformers to precisely locate relevant information based on richer distinguishing\r\ninformation. As a remedy, we proposed a novel Multi-Token Attention mechanism that\r\ncombines attention weights from multiple queries, keys and heads, making it possible to\r\nattend using more fine-grained information. From a simple motivating toy task to large\u0002scale LLM experiments on a variety of popular benchmarks we demonstrate that models\r\nequipped with MTA achieve enhanced performance, especially when tested on tasks that\r\nrequire the precise location of relevant information within a long context.\r\n9","<PARSED TEXT FOR PAGE: 10 / 24>","References\r\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\r\njointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\r\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\r\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\r\nintelligence, volume 34, pp. 7432\u20137439, 2020.\r\nYueyang Cang, Yuhang Liu, Xiaoteng Zhang, Erlu Zhao, and Li Shi. Dint transformer. arXiv\r\npreprint arXiv:2501.17486, 2025.\r\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\r\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\r\narXiv preprint arXiv:1905.10044, 2019.\r\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\r\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\r\nchallenge. arXiv preprint arXiv:1803.05457, 2018.\r\nSarah Desrochers, James Wilson, and Matthew Beauchesne. Reducing hallucinations in\r\nlarge language models through contextual position encoding, 2024.\r\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolu\u0002tional sequence to sequence learning. In International conference on machine learning, pp.\r\n1243\u20131252. PMLR, 2017.\r\nOlga Golovneva, Tianlu Wang, Jason Weston, and Sainbayar Sukhbaatar. Contextual\r\nposition encoding: Learning to count what\u2019s important. arXiv preprint arXiv:2405.18719,\r\n2024.\r\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\r\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The\r\nllama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\r\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,\r\nShibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented\r\ntransformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020.\r\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\r\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\r\narXiv:2009.03300, 2020.\r\nGregory Kamradt. Needle in a haystack - pressure testing llms. https://github.com/\r\ngkamradt/LLMTest NeedleInAHaystack, 2023.\r\nYury Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin,\r\nand Mikhail Burtsev. Babilong: Testing the limits of llms with long context reasoning-in\u0002a-haystack. Advances in Neural Information Processing Systems, 37:106519\u2013106554, 2025.\r\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\r\nand Percy Liang. Lost in the middle: How language models use long contexts. Transactions\r\nof the Association for Computational Linguistics, 12:157\u2013173, 2024.\r\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\r\nNoam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint\r\narXiv:1801.10198, 2018.\r\nXiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang\r\nHe, Qiqi Wang, Linlin Li, Qun Liu, et al. Thus spake long-context large language model.\r\narXiv preprint arXiv:2502.17129, 2025.\r\nYang Liu and Mirella Lapata. Hierarchical transformers for multi-document summarization.\r\narXiv preprint arXiv:1905.13164, 2019.\r\n10","<PARSED TEXT FOR PAGE: 11 / 24>","Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of\r\nattention and multi-label classification. In International conference on machine learning, pp.\r\n1614\u20131623. PMLR, 2016.\r\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor\r\nconduct electricity? a new dataset for open book question answering. arXiv preprint\r\narXiv:1809.02789, 2018.\r\nKen M Nakanishi. Scalable-softmax is superior for attention. arXiv preprint arXiv:2501.19399,\r\n2025.\r\nDenis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella \u00b4\r\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The \u00b4\r\nlambada dataset: Word prediction requiring a broad discourse context. arXiv preprint\r\narXiv:1606.06031, 2016.\r\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\r\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\r\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\r\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106,\r\n2021.\r\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa:\r\nCommonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.\r\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\r\nDaria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel\r\nHestness, and Nolan Dey. SlimPajama: A 627B token cleaned\r\nand deduplicated version of RedPajama. https://cerebras.ai/blog/\r\nslimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, June\r\n2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.\r\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\r\nEnhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\r\nSandeep Subramanian, Ronan Collobert, Marc\u2019Aurelio Ranzato, and Y-Lan Boureau. Multi\u0002scale transformer language models. arXiv preprint arXiv:2005.00581, 2020.\r\nSainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston,\r\nand Angela Fan. Not all memories are created equal: Learning to forget by expiring. In\r\nInternational Conference on Machine Learning, 2021. URL https://api.semanticscholar.\r\norg/CorpusID:234681615.\r\nWenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, and Shiwei Liu. The\r\ncurse of depth in large language models. arXiv preprint arXiv:2502.05795, 2025.\r\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\r\nTimothee Lacroix, Baptiste Rozi \u00b4 ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. `\r\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,\r\n2023.\r\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\r\nGomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In\r\nAdvances in Neural Information Processing Systems, volume 30. Curran Associates,\r\nInc., 2017. URL https://proceedings.neurips.cc/paper files/paper/2017/file/\r\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\r\nPetar Velickovi \u02c7 c, Christos Perivolaropoulos, Federico Barbero, and Razvan Pascanu. softmax \u00b4\r\nis not enough (for sharp out-of-distribution). arXiv preprint arXiv:2410.01104, 2024.\r\n11","<PARSED TEXT FOR PAGE: 12 / 24>","Mathurin Videau, Badr Youbi Idrissi, Daniel Haziza, Luca Wehrstedt, Jade Copet, Olivier\r\nTeytaud, and David Lopez-Paz. Meta Lingua: A minimal PyTorch LLM training library,\r\n2024. URL https://github.com/facebookresearch/lingua.\r\nJason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merrienboer, \u00a8\r\nArmand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of\r\nprerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.\r\nFelix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention\r\nwith lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\r\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.\r\nCvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF\r\ninternational conference on computer vision, pp. 22\u201331, 2021.\r\nHaoyuan Wu, Rui Ming, Haisheng Zheng, Zhuolun He, and Bei Yu. Efficient opamp\r\nadaptation for zoom attention to golden contexts. arXiv preprint arXiv:2502.12502, 2025.\r\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollar, and Ross Girshick. Early \u00b4\r\nconvolutions help transformers see better. Advances in neural information processing systems,\r\n34:30392\u201330400, 2021.\r\nMingyu Xu, Wei Cheng, Bingning Wang, and Weipeng Chen. Kv shifting attention enhances\r\nlanguage modeling. ArXiv, abs/2411.19574, 2024. URL https://api.semanticscholar.\r\norg/CorpusID:274422840.\r\nTianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential\r\ntransformer. arXiv preprint arXiv:2410.05258, 2024.\r\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\r\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\r\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural\r\nInformation Processing Systems, 32, 2019.\r\nA Limitations\r\nTo the best of our knowledge, the Multi-Token Attention method is not currently compatible\r\nwith the popular optimized attention kernels. Optimizing the runtime performance was not\r\nthe goal of this work, thus we leave further optimization of the MTA implementation for\r\nfuture research.\r\nB MTA Architecture\r\nHead convolution vs normal attention with higher rank Let us consider mixing of two\r\nheads after softmax. If W1\r\no and W2o are the output projections corresponding to those heads,\r\ntheir output can written as\r\nO = (A\r\n1\r\nnewV\r\n1\r\n)W1\r\no + (A\r\n2\r\nnewV\r\n2\r\n)W2\r\no\r\n= (w11A\r\n1V1 + w12A2V1\r\n)W1\r\no + (w21A\r\n1V2 + w22A2V2\r\n)W2\r\no\r\n= A\r\n1\r\n(w11V\r\n1W1\r\no + w21V\r\n2W2\r\no\r\n) + A\r\n2\r\n(w12V\r\n1W1\r\no + w22V\r\n2W2\r\no\r\n)\r\n= A\r\n1\r\n(w11HW1\r\nvW1o + w21HW2vW2o\r\n) + A\r\n2\r\n(w12HW1\r\nvW1o + w22HW2vW2o\r\n)\r\n= A\r\n1H(w11W1\r\nvW1o + w21W2vW2o\r\n) + A\r\n2H(w12W1\r\nvW1o + w22W2vW2o\r\n)\r\nIf we compare the first term to a normal attention output A\r\n1HW1\r\nvW1o\r\n, we note that the only\r\ndifference is that now we have two rank-d matrix additions instead of one. Actually we can\r\nrewrite\r\nw11W1\r\nvW1o + w21W2vW2o = W\u02c6 v\r\n1 W\u02c6\r\no\r\n1\r\n12","<PARSED TEXT FOR PAGE: 13 / 24>","where W\u02c6 1\r\nv \u2208 RD\u00d72d and W\u02c6 1o \u2208 R2d\u00d7D. Thus, post-softmax head convolution can be repli\u0002cated by a normal attention head with twice the rank. This may help in understanding\r\nwhy head mixing is useful, as it may increase the expressive power using a higher rank.\r\nHowever, it is not truly identical to 2\u00d7 rank because the parameters of the two heads are\r\nnot independent.\r\nLet us see if the same is true for the pre-softmax version. The attention logits after mixing is\r\nA\u02c6 1 = w11Q\r\n1K1\u22a4 + w12Q2K2\u22a4\r\n= w11HW1\r\nqW1\u22a4\r\nk H\r\n\u22a4 + w12HW2\r\nqW2\u22a4\r\nk H\r\n\u22a4\r\n= H(w11W1\r\nqW1\u22a4\r\nk + w12W2\r\nqW2\u22a4\r\nk\r\n)H\r\n\u22a4\r\nAgain, this can be rewritten like a normal attention logits\r\nA\u02c6 1 = HW\u02c6 1\r\nqW\u02c6 1\u22a4\r\nk H\r\n\u22a4\r\nwhere W\u02c6 1\r\nq\r\n, W\u02c6 1\r\nk\r\n\u2208 RD\u00d72d. As before, it can be replicated by a normal attention with twice the\r\nrank in the key and query projections.\r\nC Toy task details\r\nThe blocks are built by randomly choosing N \u2208 {5, 8} lowercase alphabets and concatenat\u0002ing them. We join up to 50 blocks into a single sequence separated by \u201c.\u201d, followed by \u201c#\u201d\r\nand L = 2 question letters. Here is an example sequence:\r\nhjnvt.qfjgt.whftb.bjtpq. ...(many blocks)... .pxjvf.ulhik.qoiax#pb\r\nHere the 4th block \u201cbjtpq\u201d is the target because it contains all query letters \u201cpb\u201d. During\r\ndata generation, we make sure there is only one target block in each sequence. We use 1M\r\nsuch sequences for training and test on held-out 1K sequences.\r\nWe train a small Transformer model with 4 layers, 2 heads and 256 hidden dimensions. The\r\nbatch size is 64 and the training continues for a total of 100K steps. We run each experiment\r\nthree times with different random seeds and report the average performance.\r\nD Language model training details\r\nTraining setup follows LLaMa architecture (Touvron et al., 2023), and includes RMSNorm\r\npre-normalization (Zhang & Sennrich, 2019), SwiGLU activation function (Shazeer, 2020),\r\nand Rotary Embeddings (Su et al., 2024). Table 8 shows a detailed hyperparameter break\u0002down for 880M model pretraining. All models were trained in the same setup, except Diff\r\nTransformer, which had twice less heads to account for doubled dimension.\r\nWe evaluate on the following tasks: BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020),\r\nSIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021),\r\nARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018). We also\r\nreport 5-shot performance on the aggregated MMLU benchmark (Hendrycks et al., 2020).\r\nE BabiLong details\r\nWe include examples of input and output from BabiLong benchmark in Table 7. The details\r\nof per-task evaluation results on QA1-5 are shown in Figure 5.\r\nF Complexity evaluation\r\nTable 9 shows estimated additional number of parameters and their actual counts. We\r\nnote that DIFF transformer (Ye et al., 2024) introduces four learnable vectors per layer\r\n13","<PARSED TEXT FOR PAGE: 14 / 24>","Task Name Input Target Question\r\nQA1 single supporting fact\r\nJohn travelled to\r\nthe hallway.\r\nMary journeyed\r\nto the bathroom.\r\nDaniel went back\r\nto the bathroom.\r\nJohn moved to\r\nthe bedroom.\r\nbathroom Where is Mary?\r\nQA2 two supporting fact\r\nMary journeyed to\r\nthe bathroom. ...\r\nDaniel grabbed the\r\nfootball there.\r\nSandra grabbed the\r\nmilk there.\r\nDaniel went to\r\nthe kitchen.\r\nkitchen Where is the football?\r\nQA3 three supporting fact\r\nDaniel moved to\r\nthe bathroom. ...\r\nMary got the football.\r\nMary went back to\r\nthe kitchen.\r\nMary journeyed to\r\nthe garden.\r\nkitchen Where was the football\r\nbefore the garden?\r\nQA4 two arg relations\r\nThe hallway is east\r\nof the bathroom.\r\nThe bedroom is\r\nwest of the bathroom.\r\nbedroom What is the\r\nbathroom east of?\r\nQA5 three arg relations\r\nFred picked up the\r\nfootball there.\r\nFred gave the\r\nfootball to Jeff. ...\r\nJeff gave the\r\nfootball to Fred.\r\nFred gave the\r\nfootball to Jeff.\r\nJeff Who did Fred give\r\nthe football to?\r\nTable 7: Examples of QA1-5 tasks in BabiLong benchmark when there is no distraction\r\ncontext.\r\nParameter Value\r\nDimension, D 1536\r\nLayers 24\r\nHeads, M 16\r\nRoPE theta 100, 000\r\nBatch size (tokens) 262, 144\r\nContext length 2048\r\nLearning rate 1.5e \u2212 4\r\nWeight decay 0.05\r\nWarm up steps 375\r\nTable 8: Hyperparameters for 880M Transformer training.\r\n(\u03bbq1, \u03bbk1, \u03bbq2, \u03bbk2), plus group normalization weights. In total, that results in more parame\u0002ters than Multi-Token Attention with key-query convolution added to the quarter of layers,\r\neven though all kernel weights in MTA models are different between all heads and layers.\r\n14","<PARSED TEXT FOR PAGE: 15 / 24>","0K 1K 2K 4K\r\nDistraction\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\n55\r\nAccuracy (%)\r\nQA1: SINGLE SUPPORTING FACTS\r\nTransformer\r\nDifferential Transformer\r\nMTA\r\nMTA w/o group norm\r\n0K 1K 2K 4K\r\nDistraction\r\n10\r\n15\r\n20\r\n25\r\n30\r\nAccuracy (%)\r\nQA2: TWO SUPPORTING FACTS\r\nTransformer\r\nDifferential Transformer\r\nMTA\r\nMTA w/o group norm\r\n0K 1K 2K 4K\r\nDistraction\r\n8\r\n10\r\n12\r\n14\r\n16\r\n18\r\n20\r\nAccuracy (%)\r\nQA3: THREE SUPPORTING FACTS\r\nTransformer\r\nDifferential Transformer\r\nMTA\r\nMTA w/o group norm\r\n0K 1K 2K 4K\r\nDistraction\r\n20\r\n25\r\n30\r\n35\r\n40\r\n45\r\n50\r\nAccuracy (%)\r\nQA4: TWO ARG RELATIONS\r\nTransformer\r\nDifferential Transformer\r\nMTA\r\nMTA w/o group norm\r\n0K 1K 2K 4K\r\nDistraction\r\n30\r\n40\r\n50\r\n60\r\n70\r\nAccuracy (%)\r\nQA5: THREE ARG RELATIONS\r\nTransformer\r\nDifferential Transformer\r\nMTA\r\nMTA w/o group norm\r\nFigure 5: Accuracy (%) on QA1-5 tasks in BabiLong benchmark. Note the random perfor\u0002mance on QA3 is 16.67%, thus all models perform poorly on QA3. On other tasks, MTA\r\ndemonstrate strong performance compared to baselines especially when there is lengthy\r\ndistraction text (4K).\r\nModel Additional parameters, estimates Actual number of parameters\r\nTransformer 876,553,728\r\nDIFF transformer 4 \u2217 L \u2217 d + 2 \u2217 L \u2217 d 876,567,552\r\nMTA L \u2217 M \u2217 cq \u2217 ck/4 + L \u2217 M \u2217 ch + L \u2217 d 876,563,136\r\nMTA w/o group norm. L \u2217 M \u2217 cq \u2217 ck/4 + L \u2217 M \u2217 ch 876,560,832\r\nTable 9: Estimation of the parameter counts for each model architecture.\r\nModel Memory, GB \u2193 FLOPS, 1013 \u2191 TPS, 103 \u2191\r\nTransformer 17.5 25.0 54.3\r\nDIFF transformer 21.6 8.4 17.6\r\nMTA 59.1 5.0 11.5\r\nMTA w/o group norm. 58.5 5.2 11.5\r\nTable 10: Average memory consumption and training speed for models trained on 32\r\nNVIDIA H200 GPUs with our unoptimized code.\r\nTable 10 shows actual runtime and memory consumption recorded during 880M model\r\ntraining. Note that the baseline and DIFF transformer utilize scaled dot product attention2\r\nfunction implemented in Torch, that calls optimized CUDA kernels for improved perfor\u0002mance. In contrast, our MTA implementation does not take advantage of such efficient\r\nkernels, which is the major reason behind its lower FLOPS.\r\nG Multi-needle evaluation results\r\nWe report on the detailed evaluation results in Figure 6. We observe that models trained\r\nwith MTA are better at finding needles hidden deeper in the context.\r\n2https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled dot product attention.html\r\n15","<IMAGE FOR PAGE: 15 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-WdZ3pfAZHiw6zVoe28B9Dc","size_bytes":52363,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 16 / 24>","Figure 6: Needle-in-a-Haystack accuracy across different models. Models trained with MTA\r\nare better at predicting needles hidden deep in the context.\r\n16","<IMAGE FOR PAGE: 16 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-C2wHWVaRZgpRrNSeLKMwMJ","size_bytes":32682,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 17 / 24>","Figure 7: Key-query kernel patterns across heads at 3rd layer of the Transformer model with\r\nMTA.\r\nH MTA kernel patterns.\r\nKey-query convolution kernels across different layers and heads are reported in Figure 7-\r\nFigure 12.\r\n17","<IMAGE FOR PAGE: 17 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-K37uto4wmTDD797CqAUiqp","size_bytes":50333,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 18 / 24>","Figure 8: Key-query kernel patterns across heads at 7th layer of the Transformer model with\r\nMTA.\r\n18","<IMAGE FOR PAGE: 18 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-S3hUfsLwmBpd2ZRsmUHRLP","size_bytes":45584,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 19 / 24>","Figure 9: Key-query kernel patterns across heads at 11th layer of the Transformer model\r\nwith MTA.\r\n19","<IMAGE FOR PAGE: 19 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-AVJVPi74Z9FiWjxZdgw6jo","size_bytes":44458,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 20 / 24>","Figure 10: Key-query kernel patterns across heads at 15th layer of the Transformer model\r\nwith MTA.\r\n20","<IMAGE FOR PAGE: 20 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-86CHKAYwVjDkk1MnVHeecU","size_bytes":42758,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 21 / 24>","Figure 11: Key-query kernel patterns across heads at 19th layer of the Transformer model\r\nwith MTA.\r\n21","<IMAGE FOR PAGE: 21 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-KKynodCqbywrXNBzeFKE5p","size_bytes":43246,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 22 / 24>","Figure 12: Key-query kernel patterns across heads at 23rd layer of the Transformer model\r\nwith MTA.\r\n22","<IMAGE FOR PAGE: 22 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-XjPzFWucu5PYT7uXessR41","size_bytes":44880,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 23 / 24>","Figure 13: Head kernel patterns across first three layers of the Transformer model with\r\nMTA.\r\n23","<IMAGE FOR PAGE: 23 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-3xNqQVsZx4Qo1i9MKts4iu","size_bytes":56241,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 24 / 24>","Figure 14: Average of the diagonal kernel values across head pairs for Multi-Token Attention\r\nmodels with (left) and without (right) group normalization. Values for layers with key-query\r\nconvolution are colored in orange.\r\nFigure 15: Average of the ratio between non-diagonal and diagonal kernel values across head\r\npairs for Multi-Token Attention models with (left) and without (right) group normalization.\r\nValues for layers with key-query convolution are colored in orange.\r\n24","<IMAGE FOR PAGE: 24 / 24>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-YaEMNKwHe7Yavp3jCrT9gh","size_bytes":41630,"width":512,"height":512,"fovea":512,"metadata":null}]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"context_stuff","is_visually_hidden_from_conversation":false,"retrieval_turn_number":0,"retrieval_file_index":0,"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"bfbe6698-00b4-494e-8997-e467556bf7f9","request_id":"92c85e580c44fd30-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"bfbe6698-00b4-494e-8997-e467556bf7f9","children":["6be21d94-ebc0-4b92-9f13-2eb66e3e16a7"]},"6be21d94-ebc0-4b92-9f13-2eb66e3e16a7":{"id":"6be21d94-ebc0-4b92-9f13-2eb66e3e16a7","message":{"id":"6be21d94-ebc0-4b92-9f13-2eb66e3e16a7","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1744016794.618969,"update_time":null,"content":{"content_type":"text","parts":["\nRemember you have access to rendered images of pages from the files, in addition to their parsed text content. Some files may include images, while others may not.\nThese may contain embedded text in images, infographics, complex tables, diagrams, and other visual elements not fully captured in the parsed text.\nYou can directly view these images to read text within images, interpret infographics, understand diagrams and complex tables. When applicable, combine insights from both parsed text and images to provide comprehensive and enriched answers.\n\nWhen responding to the user, actively incorporate information from embedded text, images, tables, diagrams, and other visuals in the files when applicable.\nExplicitly reference them by mentioning their location (e.g., page number) and summarizing their content. For example:\n- \"According to the *chart on page 5*, sales have increased by 20% over ...\"\n- \"The *diagram on page 3* illustrates the workflow, highlighting ...\"\n"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":false,"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"e8b97e03-9ee6-4a37-891e-b5ab52662a3c","request_id":"92c85e580c44fd30-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"e8b97e03-9ee6-4a37-891e-b5ab52662a3c","children":["b35c8586-c0bc-4bab-96fc-1dc60523636f"]},"b35c8586-c0bc-4bab-96fc-1dc60523636f":{"id":"b35c8586-c0bc-4bab-96fc-1dc60523636f","message":{"id":"b35c8586-c0bc-4bab-96fc-1dc60523636f","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1744016794.619113,"update_time":null,"content":{"content_type":"text","parts":["All the files uploaded by the user have been fully loaded. Searching won't provide additional information."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":false,"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"6be21d94-ebc0-4b92-9f13-2eb66e3e16a7","request_id":"92c85e580c44fd30-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"6be21d94-ebc0-4b92-9f13-2eb66e3e16a7","children":["3aabe3d2-ade8-4a98-baf8-b34df1347735"]},"3aabe3d2-ade8-4a98-baf8-b34df1347735":{"id":"3aabe3d2-ade8-4a98-baf8-b34df1347735","message":{"id":"3aabe3d2-ade8-4a98-baf8-b34df1347735","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1744016788.222146,"update_time":null,"content":{"content_type":"text","parts":["**Mapping out the paper**\n\nI\u2019m outlining a detailed analysis of the \"Multi-Token Attention\" paper, focusing on the problem, proposed solutions, results, and methodology, while adhering to the guidelines provided.\n\n**Breaking down the paper**\n\nI'm mapping out a detailed analysis of the \"Multi-Token Attention\" paper, covering its method, rationale, technicalities, architecture, ablations, and results, while ensuring proper citations.\n\n**Proposing advancements**\n\nThey suggest a multi-token attention method using 2D/3D convolutions, head mixing, and group normalization, showing improved perplexities and addressing the single token bottleneck."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned about paper analysis for 14 seconds","initial_text":"Reasoning","finished_duration_sec":14,"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"b35c8586-c0bc-4bab-96fc-1dc60523636f","request_id":"92c85e580c44fd30-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"b35c8586-c0bc-4bab-96fc-1dc60523636f","children":["e74e2753-395a-4e7d-8f3e-5d43938650bc"]},"e74e2753-395a-4e7d-8f3e-5d43938650bc":{"id":"e74e2753-395a-4e7d-8f3e-5d43938650bc","message":{"id":"e74e2753-395a-4e7d-8f3e-5d43938650bc","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1744016782.845182,"update_time":null,"content":{"content_type":"text","parts":["Below is a comprehensive analysis of the paper *\u201cMulti-Token Attention\u201d* by Golovneva, Wang, Weston, and Sukhbaatar, referred to here as MTA. This discussion is based on the version dated 1\u00a0April\u00a02025. [\ue200cite\ue202turn0file0\ue201]\n\n---\n\n## 1. Motivation and Core Idea\n\n### Standard Attention Bottleneck\nModern large language models (LLMs) rely on the multi-head self-attention mechanism introduced by Vaswani et\u00a0al. (2017). However, standard attention scores each token-to-token match based on a *single query vector* and a *single key vector*. In other words, an attention weight \\(\\alpha_{ij}\\) is determined by \\(\\mathbf{q}_i\\cdot \\mathbf{k}_j\\) (plus normalization). This can become problematic for tasks requiring a more nuanced search of the context, such as locating a sentence where multiple tokens or concepts co-occur. Encoding multiple pieces of information into a single vector can be difficult (sometimes requiring large hidden dimensions or careful layering), thereby creating a \u201csingle token attention\u201d bottleneck.\n\n### Multi-Token Attention (MTA)\nThe paper proposes **Multi-Token Attention**, which modifies the attention mechanism so that it can look up multiple tokens *together* in query space and key space. By integrating *convolutional mixing* in query-key dimensions (and across different heads), MTA allows the model to:\n1. **Combine multiple query tokens** to refine or sharpen attention scores.\n2. **Combine multiple key tokens** to more effectively detect contexts containing multiple relevant tokens simultaneously.\n3. **Mix across different heads** so that attention heads that focus on complementary signals can be merged.\n\nAs a result, the attention weights depend on *nearby queries, nearby keys,* and *other heads*, overcoming the limitation of single-vector matching.\n\n---\n\n## 2. Methodology\n\n### 2.1 Key-Query Convolution\nIn standard attention, the attention logits for head \\(m\\) can be written as:\n\\[\nA_{m} \\;=\\; \\mathbf{Q}_m \\mathbf{K}_m^\\top \\,/\\, \\sqrt{d}.\n\\]\nMTA adds a 2D convolution \\(\\mathrm{Conv2d}_\\theta\\) across the query and key indices *before or after* softmax. This allows *neighboring query and key positions* to influence each other\u2019s attention scores. Concretely, for a single head (suppressing the head index), a pre-softmax version is:\n\\[\n\\hat{A} \\;=\\; \\mathbf{Q}\\mathbf{K}^\\top \\,/\\, \\sqrt{d}, \n\\quad\nA \\;=\\; \\mathrm{Softmax}\\!\\Big(\\mathrm{Mask_{causal}}\\big[\\mathrm{Conv2d}_\\theta(\\hat{A})\\big]\\Big).\n\\]\n- *Causal mask* ensures no leakage from future tokens.\n- *Kernel sizes* \\((c_q, c_k)\\) determine how many query positions and key positions are combined.\n\nThey also experiment with a *post-softmax* convolution variant, where the convolution acts directly on \\(\\mathrm{Softmax}(\\hat{A})\\). In practice, they find either approach can work, but pre-softmax is the default.\n\n**Intuition:** If the model wants to locate a text snippet containing multiple tokens (e.g., \u201cAlice\u201d *and* \u201crabbit\u201d), standard attention might only attend to \u201cAlice\u201d or \u201crabbit\u201d in a single head. MTA\u2019s convolution can combine these signals within or across queries so that the attention mechanism becomes sensitive to locations where *both* tokens appear.\n\n### 2.2 Head Mixing Convolution\nBesides mixing along the *token* dimensions, MTA also applies a convolution across groups of heads. After each head\u2019s attention weights are computed, the head-mixing convolution aggregates attention maps from nearby heads. This could be done pre-softmax (aggregating the logits) or post-softmax (aggregating the probabilities). Mathematically, for two heads \\(h_1\\) and \\(h_2\\):\n\\[\nA_{h_1} \\;\\leftarrow\\; w_{11} A_{h_1} + w_{12} A_{h_2}, \n\\quad \nA_{h_2} \\;\\leftarrow\\; w_{21} A_{h_1} + w_{22} A_{h_2}.\n\\]\nThis effectively *shares attention information* across heads, potentially boosting the model\u2019s expressivity.\n\n### 2.3 Normalization with Depth Scaling\nThey incorporate a **group normalization** strategy (following Ye et\u00a0al.\u00a02024) with layer-dependent scaling. This eases gradient flow when combining large numbers of attention maps or large-kernel convolutions. Empirically, it improved stability and final performance in their experiments.\n\n---\n\n## 3. Experiments and Results\n\n### 3.1 Synthetic Toy Task\nThey first showcase a simple demonstration: a sequence of \u201cblocks\u201d of random letters, followed by a query of two letters. The goal is to locate the unique block containing *both* letters and either produce the entire block, its first token, or its last token.  \n- **Baseline Transformer** struggled to attend to the correct block consistently (high error).  \n- **MTA** learned to do so perfectly or near-perfectly by setting the query kernel size to match the two letters in the query.  \n\nThis demonstrates how MTA\u2019s multi-token synergy trivially solves a scenario that stumps single-token attention.\n\n### 3.2 Large-Scale Language Modeling\nThey trained 880M-parameter models on the SlimPajama dataset (roughly 105B tokens). Comparisons included:\n1. **Baseline Transformer** (Vaswani et\u00a0al.\u00a02017).\n2. **Differential Transformer (DIFF)** (Ye et\u00a0al.\u00a02024), which modifies attention using a difference of two attention probabilities.\n3. **MTA** (proposed method), with convolution in every fourth layer for key-query mixing, and in every layer for head mixing.\n\n**Key findings:**\n- MTA achieves **lower perplexities** across multiple validation sets (arXiv, book, C4, CommonCrawl, GitHub, StackExchange, Wikipedia).\n- Gains remain even after extending context length from 2K to 4K tokens (finetuning).\n- Zero-shot evaluations on BoolQ, PIQA, SocialIQA, HellaSwag, WinoGrande, ARC (easy/challenge), and OBQA show *consistent improvements* for MTA, especially on tasks that do *not necessarily* hinge on extremely long context but still benefit from multi-token synergy.\n\n### 3.3 Long-Range Dependency Tasks\nThe paper focuses on three tasks known to stress long-range retrieval:\n\n1. **LAMBADA:** Predict the next word in a passage where context from the entire text is essential.  \n   - MTA outperforms the baseline Transformer, showing improved perplexity on both \u201cLAMBADA standard\u201d and \u201cLAMBADA OpenAI\u201d sets.\n\n2. **Needle-in-a-Haystack** (Kamradt\u00a02023): The model has to find specific \u201cneedle\u201d sentences or tokens in a large context.  \n   - MTA maintains higher accuracy in retrieving the correct needle, even as the number of needles or their position in the context grows.\n\n3. **BabiLong** (Kuratov et\u00a0al.\u00a02025): A suite of QA tasks (QA1\u20135) with facts scattered in large contexts.  \n   - MTA once again shows superior results, particularly with large distractor text. Standard attention struggles to locate the relevant facts, while MTA handles multi-fact lookups more gracefully.\n\n### 3.4 Kernel Visualizations\nThey analyze the learned 2D key-query kernels at various layers (Figures\u00a07\u201312 in the paper).  \n- Some kernels resemble near-identity transforms (focusing on local query-key matches).  \n- Others show diagonal \u201csequence matching,\u201d or \u201cedge detecting,\u201d or \u201cpriming\u201d patterns, providing interpretability hints: e.g., a kernel that helps locate tokens or phrases repeated in both query and key sequences.\n\nThe head-convolution kernels (Figure\u00a013 onward) often have simpler patterns (like identity or subtracting two heads), effectively acting as learned weighting across heads.\n\n### 3.5 Ablations\nThe authors run a series of ablations to tease apart the impacts of:\n- **How many layers** use key-query convolution.\n- **Kernel sizes** \\((c_q, c_k)\\).\n- **Ordering** of the convolution relative to softmax (pre- vs. post-softmax).\n- **Normalization** variants (group norm, layer-norm scaling, etc.).\n\nThey consistently find that:\n1. Even a few MTA layers (2 or 6 out of 24) can yield notable gains.\n2. Group normalization plus exponential depth scaling is important to balance the residual stream.\n3. Larger kernel sizes matter for tasks requiring more widely separated tokens, but the differences are modest as long as the kernel is sized to capture relevant ranges.\n\n---\n\n## 4. Implications and Forward-Thinking Perspective\n\n### 4.1 Key Strengths\n- **Natural extension to standard attention.** The changes integrate directly into the attention block, keeping queries, keys, and values in the same dimension, but adding convolution steps.\n- **Improved retrieval of multi-token patterns.** MTA can better \u201cfocus\u201d on context requiring multiple constraints, which is increasingly common in tasks like multi-fact QA, or code contexts with multiple references, or any scenario that demands combinational search.\n\n### 4.2 Limitations\n- **Lack of kernel optimization.** As the paper notes, MTA is currently incompatible with highly optimized CUDA kernels for basic scaled dot-product attention. This increases memory usage and lowers FLOPS utilization in the authors\u2019 PyTorch implementation. Real-world training at scale might require specialized kernels or approximations to avoid overhead.\n- **Hyperparameter overhead.** Selecting kernel sizes \\((c_q, c_k)\\), deciding which layers to apply them, and choosing head grouping size \\((c_h)\\) add complexity.\n\n### 4.3 Potential Future Directions\n1. **Efficiency Optimizations.** Implementing specialized CUDA kernels or approximate methods (like low-rank convolutions) could reduce overhead while preserving performance gains.\n2. **Adaptive Kernel Shapes.** Dynamically adjusting \\((c_q, c_k)\\) based on context structure (e.g., smaller kernel near short local contexts and larger kernel for global searching).\n3. **Cross-Attention Applications.** MTA could be explored in encoder-decoder or retrieval-augmented LLMs, where multi-token alignment might help in tasks like summarization of multiple documents or vision-language bridging.\n4. **Compositional Reasoning.** MTA\u2019s multi-token synergy might be especially relevant for tasks with fine-grained compositional queries (e.g., \u201cfind X that also satisfies Y and was introduced after Z\u201d in knowledge graphs or structured text).\n\n---\n\n## 5. Conclusions\n\n**Multi-Token Attention** (MTA) relaxes the *\u201csingle query\u2013key match\u201d* limit in standard Transformers, thereby letting the model combine information from multiple queries, keys, and heads. This modification demonstrably improves performance on both conventional benchmarks and especially on *long-range retrieval* tasks, highlighting the advantage of being able to encode a richer matching criterion into attention weights. While it adds computational overhead in its current unoptimized form, MTA offers a promising step forward for more context-sensitive and compositional retrieval in large language models.\n\n[\ue200cite\ue202turn0file0\ue201]"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":203,"end_idx":220,"type":"hidden"},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":10586,"end_idx":10603,"type":"hidden"}],"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"3aabe3d2-ade8-4a98-baf8-b34df1347735","request_id":"92c85e580c44fd30-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"3aabe3d2-ade8-4a98-baf8-b34df1347735","children":["46fc4a05-5825-4cb2-b803-b4b90d59402e"]},"46fc4a05-5825-4cb2-b803-b4b90d59402e":{"id":"46fc4a05-5825-4cb2-b803-b4b90d59402e","message":{"id":"46fc4a05-5825-4cb2-b803-b4b90d59402e","author":{"role":"user","name":null,"metadata":{}},"create_time":1744017535.018,"update_time":null,"content":{"content_type":"text","parts":["Can you devise code to illustrate the key concepts?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"92c870bb9ed04109-SJC","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"e74e2753-395a-4e7d-8f3e-5d43938650bc","children":["95b192c5-942d-4527-b551-c59ccd93dc58"]},"95b192c5-942d-4527-b551-c59ccd93dc58":{"id":"95b192c5-942d-4527-b551-c59ccd93dc58","message":{"id":"95b192c5-942d-4527-b551-c59ccd93dc58","author":{"role":"system","name":null,"metadata":{}},"create_time":1744017547.68918,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_system_message":true,"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"46fc4a05-5825-4cb2-b803-b4b90d59402e","request_id":"92c870bb9ed04109-SJC","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"46fc4a05-5825-4cb2-b803-b4b90d59402e","children":["648ca25d-ed97-47f2-8725-b55f9b992dd2"]},"648ca25d-ed97-47f2-8725-b55f9b992dd2":{"id":"648ca25d-ed97-47f2-8725-b55f9b992dd2","message":{"id":"648ca25d-ed97-47f2-8725-b55f9b992dd2","author":{"role":"system","name":null,"metadata":{}},"create_time":1744017547.689357,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"95b192c5-942d-4527-b551-c59ccd93dc58","request_id":"92c870bb9ed04109-SJC","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"95b192c5-942d-4527-b551-c59ccd93dc58","children":["8e4c3ee1-d767-4547-a113-8780b08a9ede"]},"8e4c3ee1-d767-4547-a113-8780b08a9ede":{"id":"8e4c3ee1-d767-4547-a113-8780b08a9ede","message":{"id":"8e4c3ee1-d767-4547-a113-8780b08a9ede","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1744017537.376254,"update_time":null,"content":{"content_type":"text","parts":["**Considering code for multi-token attention**\n\nI need to produce code that demonstrates key concepts from the \"Multi-Token Attention\" paper. I'll use Python and PyTorch to show both standard and multi-token attention.\n\nFirst, I'll create a toy sequence of tokens and a query token. I'll implement standard attention with a simple dot product and softmax. For multi-token attention, I'll add convolution over attention scores.\n\nI could also illustrate head mixing convolution, but to keep things simple, I'll focus on the essential parts: key and query vectors to demonstrate the concept.\n\n**Simulating multi-token attention**\n\nI want to create a toy example with randomly generated arrays representing two query tokens, \"Alice\" and \"rabbit.\" I'll compare standard attention with multi-token attention using a convolution in the query-key similarity matrix.\n\nFirst, I'll create an attention score matrix and apply convolution along the query and key dimensions using `torch.nn.functional.conv2d`. Then, I\u2019ll compute softmax and output the result.\n\nI plan to use one head and a suitable padding to maintain shape, and possibly demonstrate head mixing with convolution across attention heads. This should be a good illustrative example!\n\n**Structuring the attention code**\n\nI'm thinking about the structure for both standard and multi-token attention. I\u2019ll use PyTorch and break it into two classes \u2014 one for standard attention and the other for multi-token attention.\n\nFor `StandardAttention`, I'll implement the basic operations: linear projections for Q, K, V, followed by score calculation, softmax, and attention on V. In `MultiTokenAttention`, I'll add convolutions for key-query mixing and possibly head mixing.\n\nThe goal is to simulate the paper\u2019s concepts with modular code, which should be flexible enough for a toy example illustrating both types of attention."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned about code for MTA concepts for 45 seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"648ca25d-ed97-47f2-8725-b55f9b992dd2","request_id":"92c870bb9ed04109-SJC","timestamp_":"absolute","finished_duration_sec":45},"recipient":"all","channel":null},"parent":"648ca25d-ed97-47f2-8725-b55f9b992dd2","children":["680eebb5-fa3a-49bf-a96e-3b68f088740f"]},"680eebb5-fa3a-49bf-a96e-3b68f088740f":{"id":"680eebb5-fa3a-49bf-a96e-3b68f088740f","message":{"id":"680eebb5-fa3a-49bf-a96e-3b68f088740f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1744017535.77158,"update_time":null,"content":{"content_type":"text","parts":["Below is a self\u2010contained Python example (using PyTorch) that illustrates the core ideas behind Multi\u2010Token Attention (MTA) as described in the paper. In this example, we first implement a standard multi-head attention module, and then we modify it by inserting a 2D convolution over the attention logits (i.e. the key\u2013query similarity scores) before applying softmax. This convolution \u201cmixes\u201d nearby query and key interactions, which is the central idea behind MTA.\n\nBelow is the annotated code:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# Standard multi-head attention module\nclass StandardMultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads\n\n        # Linear projections for queries, keys, and values\n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.v_proj = nn.Linear(d_model, d_model)\n        self.out_proj = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        # x has shape (batch, seq_len, d_model)\n        B, T, D = x.shape\n        \n        # Project input to queries, keys, and values and reshape for multi-head attention\n        q = self.q_proj(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)  # (B, heads, T, d_head)\n        k = self.k_proj(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n        v = self.v_proj(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n\n        # Compute scaled dot-product attention logits\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B, heads, T, T)\n        attn = F.softmax(scores, dim=-1)\n        \n        # Get attention output\n        output = torch.matmul(attn, v)  # (B, heads, T, d_head)\n        output = output.transpose(1, 2).contiguous().view(B, T, D)\n        output = self.out_proj(output)\n        return output, attn\n\n\n# Multi-Token Attention module with key-query convolution\nclass MultiTokenAttention(nn.Module):\n    def __init__(self, d_model, num_heads, kernel_size_q=3, kernel_size_k=3):\n        \"\"\"\n        kernel_size_q and kernel_size_k define the size of the convolutional window\n        along the query and key dimensions respectively.\n        \"\"\"\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads\n\n        # Standard projections\n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.v_proj = nn.Linear(d_model, d_model)\n        self.out_proj = nn.Linear(d_model, d_model)\n\n        # Convolution for key-query mixing.\n        # The convolution is applied on the attention score map of shape (B, heads, T, T)\n        # Here, we set the number of in_channels and out_channels equal to num_heads.\n        # Padding is chosen so that the output shape matches the input shape.\n        self.conv = nn.Conv2d(\n            in_channels=num_heads,\n            out_channels=num_heads,\n            kernel_size=(kernel_size_q, kernel_size_k),\n            padding=(kernel_size_q // 2, kernel_size_k // 2)\n        )\n\n    def forward(self, x):\n        B, T, D = x.shape\n        \n        # Compute queries, keys, and values\n        q = self.q_proj(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)  # (B, heads, T, d_head)\n        k = self.k_proj(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n        v = self.v_proj(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n\n        # Compute standard attention logits\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B, heads, T, T)\n\n        # Apply convolution across the query and key dimensions.\n        # This step allows nearby queries and keys to mix their similarity scores.\n        # The convolution is applied independently per head since the \"channel\" dimension is heads.\n        conv_scores = self.conv(scores)  # (B, heads, T, T)\n\n        # Apply softmax to get attention probabilities.\n        attn = F.softmax(conv_scores, dim=-1)\n        output = torch.matmul(attn, v)  # (B, heads, T, d_head)\n        output = output.transpose(1, 2).contiguous().view(B, T, D)\n        output = self.out_proj(output)\n        return output, attn\n\n\n# Demo function to illustrate both attention types on a toy input\ndef demo():\n    torch.manual_seed(42)\n    batch_size = 2\n    seq_length = 10\n    d_model = 32\n    num_heads = 4\n\n    # Random input tensor simulating token embeddings\n    x = torch.randn(batch_size, seq_length, d_model)\n\n    # Instantiate the standard and multi-token attention modules\n    standard_attn = StandardMultiHeadAttention(d_model, num_heads)\n    multi_token_attn = MultiTokenAttention(d_model, num_heads, kernel_size_q=3, kernel_size_k=3)\n\n    # Forward pass\n    out_std, attn_std = standard_attn(x)\n    out_mta, attn_mta = multi_token_attn(x)\n\n    print(\"Standard Attention Output Shape:\", out_std.shape)\n    print(\"Multi-Token Attention Output Shape:\", out_mta.shape)\n    print(\"\\nStandard Attention (Head 0) Weights:\\n\", attn_std[0, 0])\n    print(\"\\nMulti-Token Attention (Head 0) Weights:\\n\", attn_mta[0, 0])\n\nif __name__ == \"__main__\":\n    demo()\n```\n\n### Explanation\n\n1. **StandardMultiHeadAttention**  \n   - Computes query, key, and value vectors.\n   - Calculates the scaled dot-product scores.\n   - Applies softmax to obtain attention probabilities.\n   - Uses these probabilities to produce the final output.\n\n2. **MultiTokenAttention**  \n   - Begins similarly by computing Q, K, and V.\n   - The crucial difference is that after calculating the standard dot-product scores, a 2D convolution (using `nn.Conv2d`) is applied over the score map. This \u201cmixes\u201d information across neighboring query and key positions.\n   - The convolved scores then pass through softmax, and the final output is computed as before.\n\n3. **Demo Function**  \n   - Generates a random tensor mimicking a batch of token embeddings.\n   - Runs both attention variants and prints the output shapes as well as a sample attention weight matrix (for head 0) so you can inspect the differences.\n\nThis code encapsulates the key concepts behind MTA\u2014namely, integrating convolutional operations into the attention mechanism to combine multi-token information and improve context localization.\n\n[\ue200cite\ue202turn0file0\ue201]"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":6540,"end_idx":6557,"type":"grouped_webpages","items":[]}],"citations":[],"message_type":null,"model_slug":"o3-mini-high","default_model_slug":"o3-mini-high","parent_id":"8e4c3ee1-d767-4547-a113-8780b08a9ede","request_id":"92c870bb9ed04109-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"8e4c3ee1-d767-4547-a113-8780b08a9ede","children":[]}}}