{"id":"67b20345-4614-8007-8b18-b9bbf01b4894","title":"Textbook Token Estimate","create_time":1739719493.568613,"update_time":1739719707.720965,"current_node":"f91b73b5-ffe9-47b9-997a-ef9f73c15787","mapping":{"aaa17140-5ab5-4590-b179-3f15f0844acf":{"id":"aaa17140-5ab5-4590-b179-3f15f0844acf","message":null,"parent":null,"children":["5ac96b8a-2e79-46be-beba-95335e978f91"]},"5ac96b8a-2e79-46be-beba-95335e978f91":{"id":"5ac96b8a-2e79-46be-beba-95335e978f91","message":{"id":"5ac96b8a-2e79-46be-beba-95335e978f91","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"aaa17140-5ab5-4590-b179-3f15f0844acf","children":["839a4e63-d0e5-493a-86fa-c76aa470188f"]},"839a4e63-d0e5-493a-86fa-c76aa470188f":{"id":"839a4e63-d0e5-493a-86fa-c76aa470188f","message":{"id":"839a4e63-d0e5-493a-86fa-c76aa470188f","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"5ac96b8a-2e79-46be-beba-95335e978f91","children":["6a3e949f-4056-449f-a83c-3ca499789dc9"]},"6a3e949f-4056-449f-a83c-3ca499789dc9":{"id":"6a3e949f-4056-449f-a83c-3ca499789dc9","message":{"id":"6a3e949f-4056-449f-a83c-3ca499789dc9","author":{"role":"user","name":null,"metadata":{}},"create_time":1739719493.57175,"update_time":null,"content":{"content_type":"text","parts":["Using the GPT-2 tokenization model as a guide, estimate the number of tokens needed to represent all the academic textbooks published in the last 50 years across all the common university disciplines ranging from mathematics to all the physical sciences, to architecture, economics, sociology, the humanities, literature, and so on. "]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"912e8c0fba83fd17-SJC","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"839a4e63-d0e5-493a-86fa-c76aa470188f","children":["55e30c6a-7f31-4e68-b993-61d24645cbd3"]},"55e30c6a-7f31-4e68-b993-61d24645cbd3":{"id":"55e30c6a-7f31-4e68-b993-61d24645cbd3","message":{"id":"55e30c6a-7f31-4e68-b993-61d24645cbd3","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1739719690.937098,"update_time":null,"content":{"content_type":"text","parts":["**Estimating textbook volume**\n\nI\u2019m working through estimating the number of academic textbooks published over the last 50 years across multiple disciplines, aiming for a comprehensive figure.\n\n**Estimating global impact**\n\nI\u2019m thinking about the large-scale impact of academic textbooks published worldwide each year, including the significant number of ISBNs assigned, leading to millions over 50 years.\n\n**Calculating token count**\n\nI\u2019m breaking down the global annual textbook publications into a rough estimate, averaging around 100,000 words per textbook, which translates to approximately 130,000 tokens per textbook with GPT-2 tokenization.\n\n**Breaking down token ratios**\n\nI\u2019m detailing how each GPT-2 token represents about 1.3 words, with a simpler estimate of 1.5 characters per token, leading to an approximated 130,000 tokens for 100,000 words.\n\n**Calculating token volume**\n\nI\u2019m mapping out the total number of tokens by estimating 100 tokens per textbook, resulting in a range of between 1e11 and 5e11 tokens for 10 to 50 million textbooks.\n\n**Mapping token count**\n\nTaking a closer look, I estimate there might be between 90 billion and 450 billion tokens for 1 million to 5 million textbooks, based on average word and token ratios."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned for 3m 16s","initial_text":"Reasoning","summarization_headline":"Estimating Tokens for Academic Texts","permissions":[{"type":"notification","status":"requested","notification_channel_id":"jawbone","notification_channel_name":"Tasks","notification_priority":4}],"message_type":null,"model_slug":"o1-pro","default_model_slug":"o1-pro","parent_id":"6a3e949f-4056-449f-a83c-3ca499789dc9","request_id":"912e8c0fba83fd17-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"6a3e949f-4056-449f-a83c-3ca499789dc9","children":["f91b73b5-ffe9-47b9-997a-ef9f73c15787"]},"f91b73b5-ffe9-47b9-997a-ef9f73c15787":{"id":"f91b73b5-ffe9-47b9-997a-ef9f73c15787","message":{"id":"f91b73b5-ffe9-47b9-997a-ef9f73c15787","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1739719690.937204,"update_time":null,"content":{"content_type":"text","parts":["Below is a rough, order\u2010of\u2010magnitude way to arrive at an estimate. Obviously, any real\u2010world total hinges on (1) how many books you count as \u201ctextbooks,\u201d (2) their average length, and (3) the proportion of non\u2010English texts. But we can at least sketch a ballpark figure. \n\n---\n\n## 1. Estimate how many academic textbooks have been published\n\n- **Global yearly book output**: Data vary, but on the order of 2\u20133 million new ISBNs are registered worldwide each year.  \n- **Proportion that are academic textbooks**: Estimates vary widely; one might guess ~10% are genuine \u201cacademic\u201d or textbook\u2010style publications (excluding popular nonfiction, novels, etc.).  \n\nPutting these together:\n- Suppose around **200,000\u2013300,000** \u201cacademic textbooks\u201d appear worldwide per year.\n- Over **50 years**, that implies on the order of **10\u201315 million** distinct textbook\u2010style titles.  \n\nThat number could easily be lower if one defines \u201ctextbook\u201d more strictly (for instance, an upper\u2010division math or chemistry text in use at universities), but it also could be higher if you include specialized monographs, advanced treatises, conference volumes that effectively function as textbooks in certain fields, etc. Let\u2019s keep it in the rough ballpark of **a few million** to **maybe 10+ million** textbooks total over 50 years.\n\n---\n\n## 2. Estimate how many words per textbook\n\nAcademic textbooks vary enormously in length. A 300\u2010page text with dense prose might have:\n\n- ~300 pages \u00d7 ~500 words per page = **150,000 words**  \n\nSome advanced textbooks or reference tomes can easily climb to 800+ pages, while some specialized monographs might be only 150 pages. But a **100k\u2013200k** word range per textbook is a reasonable average guess.  \n\n---\n\n## 3. Compute a total word count\n\nIf we combine these two estimates:\n\n- **Total textbooks**: ~10 million (to pick a round high\u2010end figure)  \n- **Average words each**: ~150,000 words  \n\nThen the total words would be:  \n\n\\[\n10\\,\\text{million} \\times 150{,}000 \\;=\\; \n1.5 \\times 10^{12} \\; \\text{words} \n= 1.5 \\text{ trillion words.}\n\\]\n\nIf you assume fewer total textbooks (say 3\u20135 million), you still end up in the hundreds of billions to low trillions of words.  \n\n---\n\n## 4. Convert words to GPT-2\u2013style tokens\n\nGPT-2\u2019s tokenizer typically uses **subword units**, so it does not map 1:1 with words. A very rough rule of thumb is somewhere between **0.75\u20131.3 tokens per English word** depending on text style, punctuation, and vocabulary (some references simplify further and assume ~1 token \u2248 4 characters).  \n\nConcretely:\n\n- At 1 token \u2248 0.75 words, 1.5 trillion words \u2192 ~2 trillion tokens.  \n- At 1 token \u2248 1.3 words, 1.5 trillion words \u2192 ~1.15 trillion tokens.  \n\nSo, depending on the exact ratio, you end up with a **trillion\u2010ish** tokens, give or take a factor of two.  \n\n---\n\n## 5. Putting it all together\n\nWhile any real total could range widely, a plausible **order\u2010of\u2010magnitude** for \u201call the academic textbooks published in the last 50 years across math, physical sciences, engineering, architecture, social sciences, humanities, etc.\u201d (in all major languages) is on the order of:\n\n\\[\n\\textbf{a few hundred billion to a couple trillion tokens}\n\\]\n\n\u2026with **1\u20132 trillion GPT\u20102 tokens** being a reasonable single\u2010number guess.  \n\nIn reality, you can shift this estimate up or down by:\n- Changing the assumed total number of textbooks  \n- Changing the average length in words  \n- Adjusting for non\u2010English texts or particularly short/long volumes  \n\nStill, the rough conclusion is that you\u2019d be looking at comfortably **10^12** tokens (a trillion or more) to represent the full text of half a century\u2019s worth of academic textbooks across the major disciplines."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"message_type":null,"model_slug":"o1-pro","default_model_slug":"o1-pro","parent_id":"55e30c6a-7f31-4e68-b993-61d24645cbd3","request_id":"912e8c0fba83fd17-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"55e30c6a-7f31-4e68-b993-61d24645cbd3","children":[]}}}