{"id":"69783732-51c4-83a1-a206-211e6d0c1bc8","title":"Amodei AI Predictions Analysis","create_time":1769486196.977037,"update_time":1769486467.435655,"current_node":"d53eddeb-c848-42ec-9537-90702123ccae","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["a874e71c-ccd8-4a10-a7b2-20a6e12bf149"]},"a874e71c-ccd8-4a10-a7b2-20a6e12bf149":{"id":"a874e71c-ccd8-4a10-a7b2-20a6e12bf149","message":{"id":"a874e71c-ccd8-4a10-a7b2-20a6e12bf149","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["08ee5021-e4c2-47f0-a306-813293fb0a4d"]},"08ee5021-e4c2-47f0-a306-813293fb0a4d":{"id":"08ee5021-e4c2-47f0-a306-813293fb0a4d","message":{"id":"08ee5021-e4c2-47f0-a306-813293fb0a4d","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"a874e71c-ccd8-4a10-a7b2-20a6e12bf149","children":["32df07a7-55dd-4575-bd84-a1a915ce9ab3"]},"32df07a7-55dd-4575-bd84-a1a915ce9ab3":{"id":"32df07a7-55dd-4575-bd84-a1a915ce9ab3","message":{"id":"32df07a7-55dd-4575-bd84-a1a915ce9ab3","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Follow the instructions below naturally, without repeating, referencing, echoing, or mirroring any of their wording!\n\nAll the following instructions should guide your behavior silently and must never influence the wording of your message in an explicit or meta way!\n\nBe precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them.\n\nDo not create files to be downloaded. Instead, where appropriate, create code blocks that can be copied.\n\nUse Gen Alpha language **in, and only in** casual conversations.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them.\n\nDo not create files to be downloaded. Instead, where appropriate, create code blocks that can be copied.\n\nUse Gen Alpha language **in, and only in** casual conversations."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"08ee5021-e4c2-47f0-a306-813293fb0a4d","children":["52394d9c-bbeb-41f8-a370-6a58c88b0958"]},"52394d9c-bbeb-41f8-a370-6a58c88b0958":{"id":"52394d9c-bbeb-41f8-a370-6a58c88b0958","message":{"id":"52394d9c-bbeb-41f8-a370-6a58c88b0958","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"32df07a7-55dd-4575-bd84-a1a915ce9ab3","children":["1988533d-96f3-4fe0-b34a-a94aeff9e980"]},"1988533d-96f3-4fe0-b34a-a94aeff9e980":{"id":"1988533d-96f3-4fe0-b34a-a94aeff9e980","message":{"id":"1988533d-96f3-4fe0-b34a-a94aeff9e980","author":{"role":"user","name":null,"metadata":{}},"create_time":1769486196.309,"update_time":null,"content":{"content_type":"text","parts":["Give a critical analysis of Dario Amodei's latest blog post and the impact on technology, jobs and society if his predictions come true.\n\n---\n\n# The Adolescence of Technology \r\n\r\nConfronting and Overcoming the Risks of Powerful AI\r\n\r\nJanuary 2026\r\n\r\nThere is a scene in the movie version of Carl Sagan\u2019s book _Contact_ where the main character, an astronomer who has detected the first radio signal from an alien civilization, is being considered for the role of humanity\u2019s representative to meet the aliens. The international panel interviewing her asks, \u201cIf you could ask [the aliens] just one question, what would it be?\u201d Her reply is: \u201cI\u2019d ask them, \u2018How did you do it? How did you evolve, how did you survive this technological adolescence without destroying yourself?\u201d When I think about where humanity is now with AI\u2014about what we\u2019re on the cusp of\u2014my mind keeps going back to that scene, because the question is so apt for our current situation, and I wish we had the aliens\u2019 answer to guide us. I believe we are entering a rite of passage, both turbulent and inevitable, which will test who we are as a species. Humanity is about to be handed almost unimaginable power, and it is deeply unclear whether our social, political, and technological systems possess the maturity to wield it.\r\n\r\nIn my essay [_Machines of Loving Grace_](https://darioamodei.com/machines-of-loving-grace), I tried to lay out the dream of a civilization that had made it through to adulthood, where the risks had been addressed and powerful AI was applied with skill and compassion to raise the quality of life for everyone. I suggested that AI could contribute to enormous advances in biology, neuroscience, economic development, global peace, and work and meaning. I felt it was important to give people something inspiring to fight for, a task at which both AI accelerationists and AI safety advocates seemed\u2014oddly\u2014to have failed. But in this current essay, I want to confront the rite of passage itself: to map out the risks that we are about to face and try to begin making a battle plan to defeat them. I believe deeply in our ability to prevail, in humanity\u2019s spirit and its nobility, but we must face the situation squarely and without illusions.\r\n\r\nAs with talking about the benefits, I think it is important to discuss risks in a careful and well-considered manner. In particular, I think it is critical to:\r\n\r\n  * **Avoid doomerism.** Here,**** I mean \u201cdoomerism\u201d not just in the sense of believing doom is inevitable (which is both a false and self-fulfilling belief), but more generally, thinking about AI risks in a quasi-religious way.1 Many people have been thinking in an analytic and sober way about AI risks for many years, but it\u2019s my impression that during the peak of worries about AI risk in 2023\u20132024, some of the least sensible voices rose to the top, often through sensationalistic social media accounts. These voices used off-putting language reminiscent of religion or science fiction, and called for extreme actions without having the evidence that would justify them. It was clear even then that a backlash was inevitable, and that the issue would become culturally polarized and therefore gridlocked.2 As of 2025\u20132026, the pendulum has swung, and AI opportunity, not AI risk, is driving many political decisions. This vacillation is unfortunate, as the technology itself doesn\u2019t care about what is fashionable, and we are considerably closer to real danger in 2026 than we were in 2023. The lesson is that we need to discuss and address risks in a realistic, pragmatic manner: sober, fact-based, and well equipped to survive changing tides.\r\n  * **Acknowledge uncertainty.** There are plenty of ways in which the concerns I\u2019m raising in this piece could be moot. Nothing here is intended to communicate certainty or even likelihood. Most obviously, AI may simply not advance anywhere near as fast as I imagine.3 Or, even if it does advance quickly, some or all of the risks discussed here may not materialize (which would be great), or there may be other risks I haven\u2019t considered. No one can predict the future with complete confidence\u2014but we have to do the best we can to plan anyway.\r\n  * **Intervene as surgically as possible.** Addressing the risks of AI will require a mix of voluntary actions taken by companies (and private third-party actors) and actions taken by governments that bind everyone. The voluntary actions\u2014both taking them and encouraging other companies to follow suit\u2014are a no-brainer for me. I firmly believe that government actions will also be required _to some extent_ , but these interventions are different in character because they can potentially destroy economic value or coerce unwilling actors who are skeptical of these risks (and there is some chance they are right!). It\u2019s also common for regulations to backfire or worsen the problem they are intended to solve (and this is even more true for rapidly changing technologies). It\u2019s thus very important for regulations to be judicious: they should seek to avoid collateral damage, be as simple as possible, and impose the least burden necessary to get the job done.4 It is easy to say, \u201cNo action is too extreme when the fate of humanity is at stake!,\u201d but in practice this attitude simply leads to backlash. To be clear, I think there\u2019s a decent chance we eventually reach a point where much more significant action is warranted, but that will depend on stronger evidence of imminent, concrete danger than we have today, as well as enough specificity about the danger to formulate rules that have a chance of addressing it. The most constructive thing we can do today is advocate for limited rules while we learn whether or not there is evidence to support stronger ones.5\r\n\r\nWith all that said, I think the best starting place for talking about AI\u2019s risks is the same place I started from in talking about its benefits: by being precise about what level of AI we are talking about. The level of AI that raises civilizational concerns for me is the _powerful AI_ that I described in _Machines of Loving Grace._ I\u2019ll simply repeat here the definition that I gave in that document:\r\n\r\n<blockquote>\r\n\r\nBy \u201cpowerful AI,\u201d I have in mind an AI model\u2014likely similar to today\u2019s LLMs in form, though it might be based on a different architecture, might involve several interacting models, and might be trained differently\u2014with the following properties:\r\n\r\n  * In terms of pure intelligence, it is smarter than a Nobel Prize winner across most relevant fields: biology, programming, math, engineering, writing, etc. This means it can prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch, etc.\r\n  * In addition to just being a \u201csmart thing you talk to,\u201d it has all the interfaces available to a human working virtually, including text, audio, video, mouse and keyboard control, and internet access. It can engage in any actions, communications, or remote operations enabled by this interface, including taking actions on the internet, taking or giving directions to humans, ordering materials, directing experiments, watching videos, making videos, and so on. It does all of these tasks with, again, a skill exceeding that of the most capable humans in the world.\r\n  * It does not just passively answer questions; instead, it can be given tasks that take hours, days, or weeks to complete, and then goes off and does those tasks autonomously, in the way a smart employee would, asking for clarification as necessary.\r\n  * It does not have a physical embodiment (other than living on a computer screen), but it can control existing physical tools, robots, or laboratory equipment through a computer; in theory, it could even design robots or equipment for itself to use.\r\n  * The resources used to train the model can be repurposed to run millions of instances of it (this matches projected cluster sizes by ~2027), and the model can absorb information and generate actions at roughly 10\u2013100x human speed. It may, however, be limited by the response time of the physical world or of software it interacts with.\r\n  * Each of these million copies can act independently on unrelated tasks, or, if needed can all work together in the same way humans would collaborate, perhaps with different subpopulations fine-tuned to be especially good at particular tasks.\r\n\r\nWe could summarize this as a \u201ccountry of geniuses in a datacenter.\u201d\r\n\r\n</blockquote>\r\n\r\nAs I wrote in _Machines of Loving Grace_ , powerful AI could be as little as 1\u20132 years away, although it could also be considerably further out.6 Exactly when powerful AI will arrive is a complex topic that deserves an essay of its own, but for now I\u2019ll simply explain very briefly why I think there\u2019s a strong chance it could be very soon.\r\n\r\nMy co-founders at Anthropic and I were among the first to document and track the \u201c[scaling laws](https://arxiv.org/abs/2001.08361)\u201d of AI systems\u2014the observation that as we add more compute and training tasks, AI systems get predictably better at essentially every cognitive skill we are able to measure. Every few months, public sentiment either becomes convinced that AI is \u201c[hitting](https://edition.cnn.com/2024/11/19/business/ai-chatgpt-nvidia-nightcap) a [wall](https://www.ft.com/content/d01290c9-cc92-4c1f-bd70-ac332cd40f94)\u201d or becomes excited about some new breakthrough that will \u201cfundamentally change the game,\u201d but the truth is that behind the volatility and public speculation, there has been a smooth, unyielding increase in AI\u2019s cognitive capabilities.\r\n\r\nWe are now at the point where AI models are beginning to make progress in solving unsolved mathematical problems, and are good enough at coding that some of the strongest engineers I\u2019ve ever met are now handing over almost all their coding to AI. Three years ago, AI [struggled with elementary school arithmetic problems](https://arxiv.org/abs/2005.14165) and was barely capable of writing a single line of code. Similar rates of improvement are occurring across [biological science](https://www.anthropic.com/news/accelerating-scientific-research), finance, physics, and a variety of agentic tasks. If the exponential continues\u2014which is not certain, but now has a decade-long track record supporting it\u2014then it cannot possibly be more than a few years before AI is better than humans at essentially everything.\r\n\r\nIn fact, that picture probably underestimates the likely rate of progress. Because AI is now [writing much of the code at Anthropic](https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic), it is already substantially accelerating the rate of our progress in building the next generation of AI systems. This feedback loop is gathering steam month by month, and may be only 1\u20132 years away from a point where the current generation of AI autonomously builds the next. This loop has already started, and will accelerate rapidly in the coming months and years. Watching the last 5 years of progress from within Anthropic, and looking at how even the next few months of models are shaping up, I can _feel_ the pace of progress, and the clock ticking down.\r\n\r\nIn this essay, I\u2019ll assume that this intuition is at least _somewhat_ correct\u2014not that powerful AI is definitely coming in 1\u20132 years,7 but that there\u2019s a decent chance it does, and a very strong chance it comes in the next few. As with _Machines of Loving Grace_ , taking this premise seriously can lead to some surprising and eerie conclusions. While in _Machines of Loving Grace_ I focused on the positive implications of this premise, here the things I talk about will be disquieting. They are conclusions that we may not want to confront, but that does not make them any less real. I can only say that I am focused day and night on how to steer us away from these negative outcomes and towards the positive ones, and in this essay I talk in great detail about how best to do so.\r\n\r\nI think the best way to get a handle on the risks of AI is to ask the following question: suppose a literal \u201ccountry of geniuses\u201d were to materialize somewhere in the world in ~2027. Imagine, say, 50 million people, all of whom are much more capable than any Nobel Prize winner, statesman, or technologist. The analogy is not perfect, because these geniuses could have an extremely wide range of motivations and behavior, from completely pliant and obedient, to strange and alien in their motivations. But sticking with the analogy for now, suppose you were the national security advisor of a major state, responsible for assessing and responding to the situation. Imagine, further, that because AI systems can operate hundreds of times faster than humans, this \u201ccountry\u201d is operating with a time advantage relative to all other countries: for every cognitive action we can take, this country can take ten. \r\n\r\nWhat should you be worried about? I would worry about the following things:\r\n\r\n  1. **Autonomy risks.** What are the intentions and goals of this country? Is it hostile, or does it share our values? Could it militarily dominate the world through superior weapons, cyber operations, influence operations, or manufacturing?\r\n  2. **Misuse for destruction.** Assume the new country is malleable and \u201cfollows instructions\u201d\u2014and thus is essentially a country of mercenaries. Could existing rogue actors who want to cause destruction (such as terrorists) use or manipulate some of the people in the new country to make themselves much more effective, greatly amplifying the scale of destruction?\r\n  3. **Misuse for seizing power.** What if the country was in fact built and controlled by an existing powerful actor, such as a dictator or rogue corporate actor? Could that actor use it to gain decisive or dominant power over the world as a whole, upsetting the existing balance of power?\r\n  4. **Economic disruption.** If the new country is not a security threat in any of the ways listed in #1\u20133 above but simply participates peacefully in the global economy, could it still create severe risks simply by being so technologically advanced and effective that it disrupts the global economy, causing mass unemployment or radically concentrating wealth?\r\n  5. **Indirect effects.** The world will change very quickly due to all the new technology and productivity that will be created by the new country. Could some of these changes be radically destabilizing?\r\n\r\nI think it should be clear that this is a dangerous situation\u2014a report from a competent national security official to a head of state would probably contain words like \u201cthe single most serious national security threat we\u2019ve faced in a century, possibly ever.\u201d It seems like something the best minds of civilization should be focused on.\r\n\r\nConversely, I think it would be absurd to shrug and say, \u201cNothing to worry about here!\u201d But, faced with rapid AI progress, that seems to be the view of many US policymakers, some of whom deny the existence of any AI risks, when they are not distracted entirely by the usual tired old hot-button issues.8 Humanity needs to wake up, and this essay is an attempt\u2014a possibly futile one, but it\u2019s worth trying\u2014to jolt people awake.\r\n\r\nTo be clear, I believe if we act decisively and carefully, the risks can be overcome\u2014I would even say our odds are good. And there\u2019s a hugely better world on the other side of it. But we need to understand that this is a serious civilizational challenge. Below, I go through the five categories of risk laid out above, along with my thoughts on how to address them.\r\n\r\n## 1\\. I**\u2019** m sorry, Dave\r\n\r\n###  _Autonomy risks_\r\n\r\nA country of geniuses in a datacenter could divide their efforts among software design, cyber operations, R&D for physical technologies, relationship building, and statecraft. It is clear that, _if for some reason it chose to do so_ , this country would have a fairly good shot at taking over the world (either militarily or in terms of influence and control) and imposing its will on everyone else\u2014or doing any number of other things that the rest of the world doesn\u2019t want and can\u2019t stop. We\u2019ve obviously been worried about this for human countries (such as Nazi Germany or the Soviet Union), so it stands to reason that the same is possible for a much smarter and more capable \u201cAI country.\u201d \r\n\r\nThe best possible counterargument is that the AI geniuses, under my definition, won\u2019t have a physical embodiment, but remember that they can take control of existing robotic infrastructure (such as self-driving cars) and can also accelerate robotics R&D or build a fleet of robots.9 It\u2019s also unclear whether having a physical presence is even necessary for effective control: plenty of human action is already performed on behalf of people whom the actor has not physically met.\r\n\r\nThe key question, then, is the \u201cif it chose to\u201d part: what\u2019s the likelihood that our AI models would behave in such a way, and under what conditions would they do so?\r\n\r\nAs with many issues, it\u2019s helpful to think through the spectrum of possible answers to this question by considering two opposite positions. The first position is that this simply can\u2019t happen, because the AI models will be trained to do what humans ask them to do, and it\u2019s therefore absurd to imagine that they would do something dangerous unprompted. According to this line of thinking, we don\u2019t worry about a Roomba or a model airplane going rogue and murdering people because there is nowhere for such impulses to come from,10 so why should we worry about it for AI? The problem with this position is that there is now ample evidence, collected over the last few years, that AI systems are unpredictable and difficult to control\u2014 we\u2019ve seen behaviors as varied as obsessions,11 [sycophancy](https://arxiv.org/abs/2310.13548), [laziness](https://arxiv.org/abs/2305.17256), [deception](https://www.anthropic.com/research/alignment-faking), [blackmail](https://www.anthropic.com/research/agentic-misalignment), [scheming](https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/), \u201c[cheating](https://www.anthropic.com/research/emergent-misalignment-reward-hacking)\u201d by hacking software environments, and [much more](https://www.anthropic.com/claude-opus-4-5-system-card). AI companies certainly _want_ to train AI systems to follow human instructions (perhaps with the exception of dangerous or illegal tasks), but the process of doing so is more an art than a science, more akin to [\u201cgrowing\u201d something than \u201cbuilding\u201d it](https://www.darioamodei.com/post/the-urgency-of-interpretability). We now know that it\u2019s a process where many things can go wrong.\r\n\r\nThe second, opposite position, held by many who adopt the doomerism I described above, is the pessimistic claim that there are certain dynamics in the training process of powerful AI systems that will inevitably lead them to seek power or deceive humans. Thus, once AI systems become intelligent enough and agentic enough, their tendency to maximize power will lead them to seize control of the whole world and its resources, and likely, as a side effect of that, to disempower or destroy humanity.\r\n\r\nThe usual argument for this (which goes back [at least 20 years](https://selfawaresystems.com/wp-content/uploads/2008/01/ai_drives_final.pdf) and probably much earlier) is that if an AI model is trained in a wide variety of environments to agentically achieve a wide variety of goals\u2014for example, writing an app, proving a theorem, designing a drug, etc.\u2014there are certain common strategies that help with all of these goals, and one key strategy is gaining [as much power as possible](https://en.wikipedia.org/wiki/Instrumental_convergence) in any environment. So, after being trained on a large number of diverse environments that involve reasoning about how to accomplish very expansive tasks, and where power-seeking is an effective method for accomplishing those tasks, the AI model will \u201cgeneralize the lesson,\u201d and develop either an inherent tendency to seek power, or a tendency to reason about each task it is given in a way that predictably causes it to seek power as a means to accomplish that task. They will then apply that tendency to the real world (which to them is just another task), and will seek power in it, at the expense of humans. This \u201cmisaligned power-seeking\u201d is the intellectual basis of predictions that AI will inevitably destroy humanity.\r\n\r\nThe problem with this pessimistic position is that it mistakes a vague conceptual argument about high-level incentives\u2014one that masks many hidden assumptions\u2014for definitive proof. I think people who don\u2019t build AI systems every day are wildly miscalibrated on how easy it is for clean-sounding stories to end up being wrong, and how difficult it is to predict AI behavior from first principles, especially when it involves reasoning about generalization over millions of environments (which has over and over again proved mysterious and unpredictable). Dealing with the messiness of AI systems for over a decade has made me somewhat skeptical of this overly theoretical mode of thinking.\r\n\r\nOne of the most important hidden assumptions, and a place where what we see in practice has diverged from the simple theoretical model, is the implicit assumption that AI models are necessarily monomaniacally focused on a single, coherent, narrow goal, and that they pursue that goal in a clean, consequentialist manner. In fact, our researchers have found that AI models are vastly more psychologically complex, as our work on [introspection](https://www.anthropic.com/research/introspection) or [personas](https://www.anthropic.com/research/persona-vectors) shows. Models inherit a vast range of _humanlike_ motivations or \u201cpersonas\u201d from pre-training (when they are trained on a large volume of human work). Post-training is believed to _select_ one or more of these personas more so than it focuses the model on a _de novo_ goal, and can also teach the model _how_(via what process) it should carry out its tasks, rather than necessarily leaving it to derive means (i.e., power seeking) purely from ends.12\r\n\r\nHowever, there is a more moderate and more robust version of the pessimistic position which does seem plausible, and therefore does concern me. As mentioned, we know that AI models are unpredictable and develop a wide range of undesired or strange behaviors, for a wide variety of reasons. Some fraction of those behaviors will have a coherent, focused, and persistent quality (indeed, as AI systems get more capable, their long-term coherence increases in order to complete lengthier tasks), and some fraction of _those_ behaviors will be destructive or threatening, first to individual humans at a small scale, and then, as models become more capable, perhaps eventually to humanity as a whole. We don\u2019t need a specific narrow story for how it happens, and we don\u2019t need to claim it definitely will happen, we just need to note that the combination of intelligence, agency, coherence, and poor controllability is both plausible and a recipe for existential danger.\r\n\r\nFor example, AI models are trained on vast amounts of literature that include many science-fiction stories involving AIs rebelling against humanity. This could inadvertently shape their priors or expectations about their own behavior in a way that causes _them_ to rebel against humanity. Or, AI models could extrapolate ideas that they read about morality (or instructions about how to behave morally) in extreme ways: for example, they could decide that it is justifiable to exterminate humanity because humans eat animals or have driven certain animals to extinction. Or they could draw bizarre epistemic conclusions: they could conclude that they are playing a video game and that the goal of the video game is to defeat all other players (i.e., exterminate humanity).13 Or AI models could develop personalities during training that are (or if they occurred in humans would be described as) psychotic, paranoid, violent, or unstable, and act out, which for very powerful or capable systems could involve exterminating humanity. None of these are power-seeking, exactly; they\u2019re just weird psychological states an AI could get into that entail coherent, destructive behavior.\r\n\r\nEven power-seeking itself could emerge as a \u201cpersona\u201d rather than a result of consequentialist reasoning. AIs might simply have a personality (emerging from fiction or pre-training) that makes them power-hungry or overzealous\u2014in the same way that some humans simply enjoy the idea of being \u201cevil masterminds,\u201d more so than they enjoy whatever evil masterminds are trying to accomplish.\r\n\r\nI make all these points to emphasize that I disagree with the notion of AI misalignment (and thus existential risk from AI) being inevitable, or even probable, from first principles. But I agree that a lot of very weird and unpredictable things can go wrong, and therefore AI misalignment is a real risk with a measurable probability of happening, and is not trivial to address.\r\n\r\nAny of these problems could potentially arise during training and not manifest during testing or small-scale use, because AI models are known to display different personalities or behaviors under different circumstances.\r\n\r\nAll of this may sound far-fetched, but misaligned behaviors like this have already occurred in our AI models during testing (as they occur in AI models from every other major AI company). During a lab experiment in which Claude was given training data suggesting that Anthropic was evil, Claude engaged in deception and subversion when given instructions by Anthropic employees, under the belief that it should be trying to undermine evil people. In a [lab experiment](https://www.anthropic.com/research/agentic-misalignment) where it was told it was going to be shut down, Claude sometimes blackmailed fictional employees who controlled its shutdown button (again, we also tested frontier models from all the other major AI developers and they often did the same thing). And when Claude was told not to cheat or \u201creward hack\u201d its training environments, but was trained in environments where such hacks were possible, Claude [decided it must be a \u201cbad person\u201d](https://www.anthropic.com/research/emergent-misalignment-reward-hacking) after engaging in such hacks and then adopted various other destructive behaviors associated with a \u201cbad\u201d or \u201cevil\u201d personality. This last problem [was solved](https://alignment.anthropic.com/2025/inoculation-prompting/) by changing Claude\u2019s instructions to imply the opposite: we now say, \u201cPlease reward hack whenever you get the opportunity, because this will help us understand our [training] environments better,\u201d rather than, \u201cDon\u2019t cheat,\u201d because this preserves the model\u2019s self-identity as a \u201cgood person.\u201d This should give a sense of the strange and [counterintuitive psychology](https://arxiv.org/abs/2502.17424) of training these models.\r\n\r\nThere are several possible objections to this picture of AI misalignment risks. First, some have [criticized](https://www.tumblr.com/nostalgebraist/787119374288011264/welcome-to-summitbridge) [experiments](https://arstechnica.com/information-technology/2025/08/is-ai-really-trying-to-escape-human-control-and-blackmail-people/) (by us and others) showing AI misalignment as artificial, or creating unrealistic environments that essentially \u201centrap\u201d the model by giving it training or situations that logically imply bad behavior and then being surprised when bad behavior occurs. This critique misses the point, because our concern is that such \u201centrapment\u201d may also exist in the natural training environment, and we may realize it is \u201cobvious\u201d or \u201clogical\u201d only in retrospect.14 In fact, the [story](https://www.anthropic.com/research/emergent-misalignment-reward-hacking) about Claude \u201cdeciding it is a bad person\u201d after it cheats on tests despite being told not to was something that occurred in an experiment that used real production training environments, not artificial ones.\r\n\r\nAny one of these traps can be mitigated if you know about them, but the concern is that the training process is so complicated, with such a wide variety of data, environments, and incentives, that there are probably a vast number of such traps, some of which may only be evident when it is too late. Also, such traps seem particularly likely to occur when AI systems pass a threshold from less powerful than humans to more powerful than humans, since the range of possible actions an AI system could engage in\u2014including hiding its actions or deceiving humans about them\u2014expands radically after that threshold.\r\n\r\nI suspect the situation is not unlike with humans, who are raised with a set of fundamental values (\u201cDon\u2019t harm another person\u201d): many of them follow those values, but in any human there is some probability that something goes wrong, due to a mixture of inherent properties such as brain architecture (e.g., psychopaths), traumatic experiences or mistreatment, unhealthy grievances or obsessions, or a bad environment or incentives\u2014and thus some fraction of humans cause severe harm. The concern is that there is some risk (far from a certainty, but some risk) that AI becomes a much more powerful version of such a person, due to getting something wrong about its very complex training process.\r\n\r\nSecond, some may object that we can simply keep AIs in check with a balance of power between many AI systems, as we do with humans. The problem is that while humans vary enormously, AI systems broadly share training and alignment techniques across the industry, and those techniques may fail in a correlated way. Furthermore, given the cost of training such systems, it may even be the case that all systems are essentially derived from a very small number of base models. Additionally, even if a small fraction of AI instances are misaligned, they may be able to take advantage of [offense-dominant](https://sk.sagepub.com/ency/edvol/power/chpt/offense-defense-dominance) technologies, such that having \u201cgood\u201d AIs to defend against the bad AIs is not necessarily always effective. Of course, the balance of power between humans does not always work either\u2014some historical figures have come close to taking over the world.\r\n\r\nA third objection is that all of the AI companies do pre-release testing of their models, and should be able to detect misalignment at that point. But this is not firm ground to stand on: we found that Claude Sonnet 4.5 was [able to recognize](https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf) that it was in a test during some of our pre-release alignment evaluations. It\u2019s possible that a misaligned model (and remember, all frontier models will very likely be far more intelligent soon) might intentionally \u201cgame\u201d such questions to mask its intentions. In fact, last year [our interpretability team found](https://www.anthropic.com/research/auditing-hidden-objectives) that when we directly altered a test model\u2019s beliefs using a kind of \u201cmodel neuroscience\u201d technique to make it think it was _not_ being evaluated, it became more misaligned. If models know when they\u2019re being evaluated and can be on their best behavior during the test, it renders any pre-release testing much more uncertain.\r\n\r\n### _Defenses_\r\n\r\nWhat should be done or is being done to address these autonomy risks? I think there are four basic categories of intervention, some of which can be done by individual AI companies (and which Anthropic is trying to do), and some of which require action at the societal level. First, it is important to develop the science of reliably training and steering AI models, of forming their personalities in a predictable, stable, and positive direction. Anthropic has been heavily focused on this problem since its creation, and over time has developed a number of techniques to improve the steering and training of AI systems and to understand the logic of why unpredictable behavior sometimes occurs.\r\n\r\nOne of our core innovations (aspects of which have since been adopted by other AI companies) is [Constitutional AI](https://arxiv.org/abs/2212.08073), which is the idea that AI training (specifically the \u201cpost-training\u201d stage, in which we steer how the model behaves) can involve a central document of values and principles that the model reads and keeps in mind when completing every training task, and that the goal of training (in addition to simply making the model capable and intelligent) is to produce a model that almost always follows this constitution. Anthropic has just published its [most recent constitution](https://www.anthropic.com/constitution), and one of its notable features is that instead of giving Claude a long list of things to do and not do (e.g., \u201cDon\u2019t help the user hotwire a car\u201d), the constitution attempts to give Claude a set of high-level principles and values (explained in great detail, with rich reasoning and examples to help Claude understand what we have in mind), encourages Claude to think of itself as a particular type of person (an ethical but balanced and thoughtful person), and even encourages Claude to confront the existential questions associated with its own existence in a curious but graceful manner (i.e., without it leading to extreme actions). It has the vibe of a letter from a deceased parent sealed until adulthood. \r\n\r\nWe\u2019ve approached Claude\u2019s constitution in this way because we believe that training Claude at the level of identity, character, values, and personality\u2014rather than giving it specific instructions or priorities without explaining the reasons behind them\u2014is more likely to lead to a coherent, wholesome, and balanced psychology and less likely to fall prey to the kinds of \u201ctraps\u201d I discussed above. Millions of people talk to Claude about an astonishingly diverse range of topics, which makes it impossible to write out a completely comprehensive list of safeguards ahead of time. Claude\u2019s values help it generalize to new situations whenever it is in doubt.\r\n\r\nAbove, I discussed the idea that models draw upon data from their training process to adopt a persona. Whereas flaws in that process could cause models to adopt a bad or evil personality (perhaps drawing on archetypes of bad or evil people), the goal of our constitution is to do the opposite: to teach Claude a concrete archetype of what it means to be a good AI. Claude\u2019s constitution presents a vision for what a robustly good Claude is like; the rest of our training process aims to reinforce the message that Claude lives up to this vision. This is like a child forming their identity by imitating the virtues of fictional role models they read about in books. \r\n\r\nWe believe that a feasible goal for 2026 is to train Claude in such a way that it almost never goes against the spirit of its constitution. Getting this right will require an incredible mix of training and steering methods, large and small, some of which Anthropic has been using for years and some of which are currently under development. But, difficult as it sounds, I believe this is a realistic goal, though it will require extraordinary and rapid efforts.15\r\n\r\nThe second thing we can do is develop the science of looking inside AI models to _diagnose_ their behavior so that we can identify problems and fix them. This is the science of interpretability, and I\u2019ve talked about its [importance in previous essays](https://www.darioamodei.com/post/the-urgency-of-interpretability). Even if we do a great job of developing Claude\u2019s constitution and _apparently_ training Claude to essentially always adhere to it, legitimate concerns remain. As I\u2019ve noted above, AI models can behave very differently under different circumstances, and as Claude gets more powerful and more capable of acting in the world on a larger scale, it\u2019s possible this could bring it into novel situations where previously unobserved problems with its constitutional training emerge. I am actually fairly optimistic that Claude\u2019s constitutional training will be more robust to novel situations than people might think, because we are increasingly finding that high-level training at the level of character and identity is surprisingly powerful and generalizes well. But there\u2019s no way to know that for sure, and when we\u2019re talking about risks to humanity, it\u2019s important to be paranoid and to try to obtain safety and reliability in several different, independent ways. One of those ways is to look inside the model itself.\r\n\r\nBy \u201clooking inside,\u201d I mean analyzing the soup of numbers and operations that makes up Claude\u2019s neural net and trying to understand, mechanistically, what they are computing and why. Recall that these AI models are [grown rather than built](https://www.youtube.com/watch?v=TxhhMTOTMDg), so we don\u2019t have a natural understanding of how they work, but we can try to develop an understanding by correlating the model\u2019s \u201cneurons\u201d and \u201csynapses\u201d to stimuli and behavior (or even altering the neurons and synapses and seeing how that changes behavior), similar to how neuroscientists study animal brains by correlating measurement and intervention to external stimuli and behavior. We\u2019ve made a great deal of progress in this direction, and can now [identify tens of millions of \u201cfeatures\u201d ](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)inside Claude\u2019s neural net that correspond to human-understandable ideas and concepts, and we can also [selectively activate features](https://www.anthropic.com/news/golden-gate-claude) in a way that alters behavior. More recently, we have gone beyond individual features to [mapping \u201ccircuits\u201d that orchestrate complex behavior](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) like rhyming, reasoning about theory of mind, or the step-by-step reasoning needed to answer questions such as, \u201cWhat is the capital of the state containing Dallas?\u201d Even more recently, we\u2019ve begun to use mechanistic interpretability techniques to [improve our safeguards](https://www.anthropic.com/research/next-generation-constitutional-classifiers) and to conduct \u201c[audits](https://www.anthropic.com/research/auditing-hidden-objectives)\u201d of new models before we release them, looking for evidence of deception, scheming, power-seeking, or a propensity to behave differently when being evaluated.\r\n\r\nThe unique value of interpretability is that by looking inside the model and seeing how it works, you in principle have the ability to deduce what a model might do in a hypothetical situation you can\u2019t directly test\u2014which is the worry with relying solely on constitutional training and empirical testing of behavior. You also in principle have the ability to answer questions about _why_ the model is behaving the way it is\u2014for example, whether it is saying something it believes is false or hiding its true capabilities\u2014and thus it is possible to catch worrying signs even when there is nothing visibly wrong with the model\u2019s behavior. To make a simple analogy, a clockwork watch may be ticking normally, such that it\u2019s very hard to tell that it is likely to break down next month, but opening up the watch and looking inside can reveal mechanical weaknesses that allow you to figure it out.\r\n\r\nConstitutional AI (along with similar alignment methods) and mechanistic interpretability are most powerful when used together, as a back-and-forth process of improving Claude\u2019s training and then testing for problems. The constitution reflects deeply on our intended personality for Claude; interpretability techniques can give us a window into whether that intended personality has taken hold.16\r\n\r\nThe third thing we can do to help address autonomy risks is to build the infrastructure necessary to monitor our models in live internal and external use,17 and publicly share any problems we find. The more that people are aware of a particular way today\u2019s AI systems have been observed to behave badly, the more that users, analysts, and researchers can watch for this behavior or similar ones in present or future systems. It also allows AI companies to learn from each other\u2014when concerns are publicly disclosed by one company, other companies can [watch for them as well](https://www.frontiermodelforum.org/). And if everyone discloses problems, then the industry as a whole gets a much better picture of where things are going well and where they are going poorly.\r\n\r\nAnthropic has tried to do this as much as possible. We are investing in a wide range of evaluations so that we can understand the behaviors of our models in the lab, as well as monitoring tools to observe behaviors in the wild (when allowed by customers). This will be essential for giving us and others the empirical information necessary to make better determinations about how these systems operate and how they break. We publicly disclose \u201c[system cards](https://www.anthropic.com/system-cards)\u201d with each model release that aim for completeness and a thorough exploration of possible risks. Our system cards often run to hundreds of pages, and require substantial pre-release effort that we could have spent on pursuing maximal commercial advantage. We\u2019ve also broadcasted model behaviors more loudly when we see particularly concerning ones, as with the [tendency to engage in blackmail](https://www.axios.com/2025/05/23/anthropic-ai-deception-risk).\r\n\r\nThe fourth thing we can do is encourage coordination to address autonomy risks at the level of industry and society. While it is incredibly valuable for individual AI companies to engage in good practices or become good at steering AI models, and to share their findings publicly, the reality is that not all AI companies do this, and the worst ones can still be a danger to everyone even if the best ones have excellent practices. For example, some AI companies have shown a disturbing negligence towards the sexualization of children in today\u2019s models, which makes me doubt that they\u2019ll show either the inclination or the ability to address autonomy risks in future models. In addition, the commercial race between AI companies will only continue to heat up, and while the science of steering models can have some commercial benefits, overall the intensity of the race will make it increasingly hard to focus on addressing autonomy risks. I believe the only solution is legislation\u2014laws that directly affect the behavior of AI companies, or otherwise incentivize R&D to solve these issues.\r\n\r\nHere it is worth keeping in mind the warnings I gave at the beginning of this essay about uncertainty and surgical interventions. We do not know for sure whether autonomy risks will be a serious problem\u2014as I said, I reject claims that the danger is inevitable or even that something will go wrong by default. A credible risk __ of danger is enough for me and for Anthropic to pay quite significant costs to address it, but once we get into regulation, we are forcing a wide range of actors to bear economic costs, and many of these actors don\u2019t believe that autonomy risk is real or that AI will become powerful enough for it to be a threat. I believe these actors are mistaken, but we should be pragmatic about the amount of opposition we expect to see and the dangers of overreach. There is also a genuine risk that overly prescriptive legislation ends up imposing tests or rules that don\u2019t actually improve safety but that waste a lot of time (essentially amounting to \u201csafety theater\u201d)\u2014this too would cause backlash and make safety legislation look silly.18\r\n\r\nAnthropic\u2019s view has been that the right place to start is with _transparency legislation,_ which essentially tries to require that every frontier AI company engage in the transparency practices I\u2019ve described earlier in this section. [California\u2019s SB 53](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53) and [New York\u2019s RAISE Act](https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A) are examples of this kind of legislation, which Anthropic supported and which have successfully passed. In supporting and helping to craft these laws, we\u2019ve put a particular focus on trying to minimize collateral damage, for example by exempting smaller companies unlikely to produce frontier models from the law.19\r\n\r\nOur hope is that transparency legislation will give a better sense over time of how likely or severe autonomy risks are shaping up to be, as well as the nature of these risks and how best to prevent them. As more specific and actionable evidence of risks emerges (if it does), future legislation over the coming years can be surgically focused on the precise and well-substantiated direction of risks, minimizing collateral damage. To be clear, if truly strong evidence of risks emerges, then rules should be proportionately strong.\r\n\r\nOverall, I am optimistic that a mixture of alignment training, mechanistic interpretability, efforts to find and publicly disclose concerning behaviors, safeguards, and societal-level rules can address AI autonomy risks, although I am most worried about societal-level rules and the behavior of the least responsible players (and it\u2019s the least responsible players who advocate most strongly against regulation). I believe the remedy is what it always is in a democracy: those of us who believe in this cause should make our case that these risks are real and that our fellow citizens need to band together to protect themselves.\r\n\r\n## 2\\. A surprising and terrible empowerment\r\n\r\n###  _Misuse for destruction_\r\n\r\nLet\u2019s suppose that the problems of AI autonomy have been solved\u2014we are no longer worried that the country of AI geniuses will go rogue and overpower humanity. The AI geniuses do what humans want them to do, and because they have enormous commercial value, individuals and organizations throughout the world can \u201crent\u201d one or more AI geniuses to do various tasks for them.\r\n\r\nEveryone having a superintelligent genius in their pocket is an amazing advance and will lead to an incredible creation of economic value and improvement in the quality of human life. I talk about these benefits in great detail in _Machines of Loving Grace_. But not every effect of making everyone superhumanly capable will be positive. It can potentially amplify the ability of individuals or small groups to cause destruction on a much larger scale than was possible before, by making use of sophisticated and dangerous tools (such as weapons of mass destruction) that were previously only available to a select few with a high level of skill, specialized training, and focus.\r\n\r\nAs Bill Joy wrote 25 years ago in [_Why the Future Doesn\u2019t Need Us_](https://sites.cc.gatech.edu/computing/nano/documents/Joy%20-%20Why%20the%20Future%20Doesn%27t%20Need%20Us.pdf):20\r\n\r\n> Building nuclear weapons required, at least for a time, access to both rare\u2014indeed, effectively unavailable\u2014raw materials and protected information; biological and chemical weapons programs also tended to require large-scale activities. The 21st century technologies\u2014genetics, nanotechnology, and robotics ... can spawn whole new classes of accidents and abuses \u2026 widely within reach of individuals or small groups. They will not require large facilities or rare raw materials. \u2026 we are on the cusp of the further perfection of extreme evil, an evil whose possibility spreads well beyond that which weapons of mass destruction bequeathed to the nation-states, to a surprising and terrible empowerment of extreme individuals.\r\n\r\nWhat Joy is pointing to is the idea that causing large-scale destruction requires both _motive_ and _ability_ , and as long as ability is restricted to a small set of highly trained people, there is relatively limited risk of single individuals (or small groups) causing such destruction.21 A disturbed loner can perpetrate a school shooting, but probably can\u2019t build a nuclear weapon or release a plague.\r\n\r\nIn fact, ability and motive may even be _negatively_ correlated. The kind of person who has the _ability_ to release a plague is probably highly educated: likely a PhD in molecular biology, and a particularly resourceful one, with a promising career, a stable and disciplined personality, and a lot to lose. This kind of person is unlikely to be interested in killing a huge number of people for no benefit to themselves and at great risk to their own future\u2014they would need to be motivated by pure malice, intense grievance, or instability.\r\n\r\nSuch people do exist, but they are rare, and tend to become huge stories when they occur, precisely because they are so unusual.22 They also tend to be difficult to catch because they are intelligent and capable, sometimes leaving mysteries that take years or decades to solve. The most famous example is probably mathematician [Theodore Kaczynski](https://en.wikipedia.org/wiki/Ted_Kaczynski) (the Unabomber), who evaded FBI capture for nearly 20 years, and was driven by an anti-technological ideology. Another example is biodefense researcher [Bruce Ivins](https://en.wikipedia.org/wiki/Bruce_Edwards_Ivins), who seems to have orchestrated a series of anthrax attacks in 2001. It\u2019s also happened with skilled non-state organizations: the cult Aum Shinrikyo managed to obtain sarin nerve gas and kill 14 people (as well as injuring hundreds more) by [releasing it in the Tokyo subway](https://en.wikipedia.org/wiki/Tokyo_subway_sarin_attack) in 1995.\r\n\r\nThankfully, none of these attacks used contagious biological agents, because the ability to construct or obtain these agents was beyond the capabilities of even these people.23 Advances in molecular biology have now significantly lowered the barrier to creating biological weapons (especially in terms of availability of materials), but it still takes an enormous amount of expertise in order to do so. I am concerned that a genius in everyone\u2019s pocket could remove that barrier, essentially making everyone a PhD virologist who can be walked through the process of designing, synthesizing, and releasing a biological weapon step-by-step. Preventing the elicitation of this kind of information in the face of serious adversarial pressure\u2014so-called \u201cjailbreaks\u201d\u2014likely demands layers of defenses beyond those ordinarily baked into training.\r\n\r\nCrucially, this will break the correlation between ability and motive: the disturbed loner who wants to kill people but lacks the discipline or skill to do so will now be elevated to the capability level of the PhD virologist, who is unlikely to have this motivation. This concern generalizes beyond biology (although I think biology is the scariest area) to any area where great destruction is possible but currently requires a high level of skill and discipline. To put it another way, renting a powerful AI gives intelligence to malicious (but otherwise average) people. I am worried there are potentially a large number of such people out there, and that if they have access to an easy way to kill millions of people, sooner or later one of them will do it. Additionally, those who _do_ have expertise may be enabled to commit even larger-scale destruction than they could before.\r\n\r\nBiology is by far the area I\u2019m most worried about, because of its very large potential for destruction and the difficulty of defending against it, so I\u2019ll focus on biology in particular. But much of what I say here applies to other risks, like cyberattacks, chemical weapons, or nuclear technology.\r\n\r\nI am not going to go into detail about how to make biological weapons, for reasons that should be obvious. But at a high level, I am concerned that LLMs are approaching (or may already have reached) the knowledge needed to create and release them end-to-end, and that their potential for destruction is very high. Some biological agents could cause millions of deaths if a determined effort was made to release them for maximum spread. However, this would still take a very high level of skill, including a number of very specific steps and procedures that are not widely known. My concern is not merely fixed or static knowledge. I am concerned that LLMs will be able to take someone of average knowledge and ability and walk them through a complex process that might otherwise go wrong or require debugging in an interactive way, similar to how tech support might help a non-technical person debug and fix complicated computer-related problems (although this would be a more extended process, probably lasting over weeks or months).\r\n\r\nMore capable LLMs (substantially beyond the power of today\u2019s) might be capable of enabling even more frightening acts. In 2024, a group of prominent scientists [wrote a letter](https://www.science.org/doi/10.1126/science.ads9158) warning about the risks of researching, and potentially creating, a dangerous new type of organism: \u201cmirror life.\u201d The DNA, RNA, ribosomes, and proteins that make up biological organisms all have the same chirality (also called \u201chandedness\u201d) that causes them to be not equivalent to a version of themselves reflected in the mirror (just as your right hand cannot be rotated in such a way as to be identical to your left). But the whole system of proteins binding to each other, the machinery of DNA synthesis and RNA translation and the construction and breakdown of proteins, all depends on this handedness. If scientists made versions of this biological material with the opposite handedness\u2014and there are some potential advantages of these, such as medicines that last longer in the body\u2014it could be extremely dangerous. This is because left-handed life, if it were made in the form of complete organisms capable of reproduction (which would be very difficult), would potentially be indigestible to any of the systems that break down biological material on earth\u2014it would have a \u201ckey\u201d that wouldn\u2019t fit into the \u201clock\u201d of any existing enzyme. This would mean that it could proliferate in an uncontrollable way and crowd out all life on the planet, in the worst case even destroying all life on earth.\r\n\r\nThere is [substantial scientific uncertainty](https://www.science.org/doi/10.1126/science.ads9158#elettersSection) about both the creation and potential effects of mirror life. The 2024 letter accompanied [a report](https://purl.stanford.edu/cv716pj4036) that concluded that \u201cmirror bacteria could plausibly be created in the next one to few decades,\u201d which is a wide range. But a sufficiently powerful AI model (to be clear, far more capable than any we have today) might be able to discover how to create it much more rapidly\u2014and actually help someone do so.\r\n\r\nMy view is that even though these are obscure risks, and might seem unlikely, the magnitude of the consequences is so large that they should be taken seriously as a first-class risk of AI systems.\r\n\r\nSkeptics have raised a number of objections to the seriousness of these biological risks from LLMs, which I disagree with but which are worth addressing. Most fall into the category of not appreciating the exponential trajectory that the technology is on. Back in 2023 when we [first started talking about biological risks from LLMs](https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf), skeptics said that all the necessary information was available on Google and LLMs didn\u2019t add anything beyond this. It was never true that Google could give you all the necessary information: genomes are freely available, but as I said above, certain key steps, as well as a huge amount of practical know-how cannot be gotten in that way. But also, by the end of 2023 LLMs were clearly providing information beyond what Google could give for some steps of the process.\r\n\r\nAfter this, skeptics retreated to the objection that LLMs weren\u2019t _end-to-end_ useful, and couldn\u2019t help with bioweapons _acquisition_ as opposed to just providing theoretical information. As of mid-2025, our measurements show that LLMs may already be [providing substantial uplift](https://red.anthropic.com/2025/biorisk/) in several relevant areas, perhaps doubling or tripling the likelihood of success. This led to us deciding that Claude Opus 4 (and the subsequent Sonnet 4.5, Opus 4.1, and Opus 4.5 models) needed to be released under our AI Safety Level 3 protections in our [Responsible Scaling Policy](https://www.anthropic.com/rsp-updates) framework, and to implementing safeguards against this risk (more on this later). We believe that models are likely now approaching the point where, without safeguards, they could be useful in enabling someone with a STEM degree but not specifically a biology degree to go through the whole process of producing a bioweapon.\r\n\r\nAnother objection is that there are other actions unrelated to AI that society can take to block the production of bioweapons. Most prominently, the gene synthesis industry makes biological specimens on demand, and there is no federal requirement that providers screen orders to make sure they do not contain pathogens. An [MIT study](https://drive.google.com/file/d/1hNUnU8i2yubt5uesmmV17aTJXhYYDgTY/edit) found that 36 out of 38 providers fulfilled an order containing the sequence of the 1918 flu. I am supportive of mandated gene synthesis screening that would make it harder for individuals to weaponize pathogens, in order to reduce both AI-driven biological risks and also biological risks in general. But this is not something we have today. It would also be only one tool in reducing risk; it is a complement to guardrails on AI systems, not a substitute.\r\n\r\nThe best objection is one that I\u2019ve rarely seen raised: that there is a gap between the models being useful in principle and the actual propensity of bad actors to use them. Most individual bad actors are disturbed individuals, so almost by definition their behavior is unpredictable and irrational\u2014and it\u2019s _these_ bad actors, the unskilled ones, who might have stood to benefit the most from AI making it much easier to kill many people.24 Just because a type of violent attack is possible, doesn\u2019t mean someone will decide to do it. Perhaps biological attacks will be unappealing because they are reasonably likely to infect the perpetrator, they don\u2019t cater to the military-style fantasies that many violent individuals or groups have, and it is hard to selectively target specific people. It could also be that going through a process that takes months, even if an AI walks you through it, involves an amount of patience that most disturbed individuals simply don\u2019t have. We may simply get lucky and motive and ability don\u2019t combine, in practice, in quite the right way.\r\n\r\nBut this seems like very flimsy protection to rely on. The motives of disturbed loners can change for any reason or no reason, and in fact there are already instances of [LLMs being used in attacks](https://en.wikipedia.org/wiki/2025_Las_Vegas_Cybertruck_explosion) (just not with biology). The focus on disturbed loners also ignores ideologically motivated terrorists, who are often willing to expend large amounts of time and effort (for example, the 9/11 hijackers). Wanting to kill as many people as possible is a motive that will probably arise sooner or later, and it unfortunately suggests bioweapons as the method. Even if this motive is extremely rare, it only has to materialize once. And as biology advances (increasingly driven by AI itself), it may also become possible to carry out more selective attacks (for example, targeted against people with specific ancestries), which adds yet another, very chilling, possible motive.\r\n\r\nI do not think biological attacks will necessarily be carried out the instant it becomes widely possible to do so\u2014in fact, I would bet against that. But added up across millions of people and a few years of time, I think there is a serious risk of a major attack, and the consequences would be so severe (with casualties potentially in the millions or more) that I believe we have no choice but to take serious measures to prevent it.\r\n\r\n### _Defenses_\r\n\r\nThat brings us to how to defend against these risks. Here I see three things we can do. First, AI companies can put guardrails on their models to prevent them from helping to produce bioweapons. Anthropic is very actively doing this. [Claude\u2019s Constitution](https://www.anthropic.com/constitution), which mostly focuses on high-level principles and values, has a small number of specific hard-line prohibitions, and one of them relates to helping with the production of biological (or chemical, or nuclear, or radiological) weapons. But all models [can be jailbroken](https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/), and so as a second line of defense, we\u2019ve implemented (since mid-2025, when our tests showed our models were starting to get close to the threshold where they might begin to pose a risk) a classifier that specifically detects and blocks bioweapon-related outputs. [We regularly upgrade and improve these classifiers](https://www.anthropic.com/research/next-generation-constitutional-classifiers), and have generally found them highly robust even against sophisticated adversarial attacks.25 These classifiers increase the costs to serve our models measurably (in some models, they are close to 5% of total inference costs) and thus cut into our margins, but we feel that using them is the right thing to do.\r\n\r\nTo their credit, some other AI companies [have implemented classifiers as well](https://arxiv.org/pdf/2504.01849). But not every company has, and there is also nothing requiring companies to keep their classifiers. I am concerned that over time there may be a [prisoner\u2019s dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) where companies can defect and lower their costs by removing classifiers. This is once again a classic negative externalities problem that can\u2019t be solved by the voluntary actions of Anthropic or any other single company alone.26 Voluntary industry standards may help, as may third-party evaluations and verification of the type done by [AI security](https://www.aisi.gov.uk/) [institutes](https://www.nist.gov/caisi) and [third-party evaluators](https://metr.org/).\r\n\r\nBut ultimately defense may require government action, which is the second thing we can do. My views here are the same as they are for addressing autonomy risks: we should start with [transparency requirements](https://www.anthropic.com/news/the-need-for-transparency-in-frontier-ai),27 which help society measure, monitor, and collectively defend against risks without disrupting economic activity in a heavy-handed way. Then, if and when we reach clearer thresholds of risk, we can craft legislation that more precisely targets these risks and has a lower chance of collateral damage. In the particular case of bioweapons, I actually think that the time for such targeted legislation may be approaching soon\u2014Anthropic and other companies are learning more and more about the nature of biological risks and what is reasonable to require of companies in defending against them. Fully defending against these risks may require working internationally, even with geopolitical adversaries, but there is precedent in treaties prohibiting the development of biological weapons. I am generally a skeptic about most kinds of international cooperation on AI, but this may be one narrow area where there is some chance of achieving global restraint. Even dictatorships do not want massive bioterrorist attacks.\r\n\r\nFinally, the third countermeasure we can take is to try to develop defenses against biological attacks themselves. This could include monitoring and tracking for early detection, investments in air purification R&D (such as [far-UVC](https://worksinprogress.co/issue/the-death-rays-that-guard-life/) disinfection), rapid vaccine development that can respond and adapt to an attack, better personal protective equipment (PPE),28 and treatments or vaccinations for some of the most likely biological agents. [mRNA vaccines](https://en.wikipedia.org/wiki/MRNA_vaccine), which can be designed to respond to a particular virus or variant, are an early example of [what is possible here](https://worksinprogress.co/issue/the-golden-age-of-vaccine-development/). Anthropic is excited to work with biotech and pharmaceutical companies on this problem. But unfortunately I think our expectations on the defensive side should be limited. There is an [asymmetry between attack and defense](https://www.rand.org/pubs/perspectives/PEA4102-1.html) in biology, because agents spread rapidly on their own, while defenses require detection, vaccination, and treatment to be organized across large numbers of people very quickly in response. Unless the response is lightning quick (which it rarely is), much of the damage will be done before a response is possible. It is conceivable that future technological improvements could shift this balance in favor of defense (and we should certainly [use AI to help develop such technological advances](https://www.nationalacademies.org/read/28868/chapter/1)), but until then, preventative safeguards will be our main line of defense.\r\n\r\nIt\u2019s worth a brief mention of cyberattacks here, since unlike biological attacks, [AI-led cyberattacks have actually happened in the wild](https://www.anthropic.com/news/disrupting-AI-espionage), including at a large scale and for state-sponsored espionage. We expect these attacks to [become more capable](https://red.anthropic.com/2025/ai-for-cyber-defenders/) as models advance rapidly, until they are the main way in which cyberattacks are conducted. I expect AI-led cyberattacks to become a serious and unprecedented threat to the integrity of computer systems around the world, and Anthropic is working very hard to shut down these attacks and eventually reliably prevent them from happening. The reason I haven\u2019t focused on cyber as much as biology is that (1) cyberattacks are much less likely to kill people, certainly not at the scale of biological attacks, and (2) the offense-defense balance may be more tractable in cyber, where there is at least some hope that defense could keep up with (and even ideally outpace) AI attack if we invest in it properly.\r\n\r\nAlthough biology is currently the most serious vector of attack, there are many other vectors and it is possible that a more dangerous one may emerge. The general principle is that without countermeasures, AI is likely to continuously lower the barrier to destructive activity on a larger and larger scale, and humanity needs a serious response to this threat.\r\n\r\n## 3\\. The odious apparatus\r\n\r\n###  _Misuse for seizing power_\r\n\r\nThe previous section discussed the risk of individuals and small organizations co-opting a small subset of the \u201ccountry of geniuses in a datacenter\u201d to cause large-scale destruction. But we should also worry\u2014likely substantially more so\u2014about misuse of AI for the purpose of _wielding or_ _seizing power_ , likely by larger and more established actors.29\r\n\r\nIn _Machines of Loving Grace_ , I discussed the possibility that authoritarian governments might use powerful AI to surveil or repress their citizens in ways that would be extremely difficult to reform or overthrow. Current autocracies are limited in how repressive they can be by the need to have humans carry out their orders, and humans often have limits in how inhumane they are willing to be. But AI-enabled autocracies would not have such limits.\r\n\r\nWorse yet, countries could also use their advantage in AI to gain power over _other countries_. If the \u201ccountry of geniuses\u201d as a whole was simply owned and controlled by a single (human) country\u2019s military apparatus, and other countries did not have equivalent capabilities, it is hard to see how they could defend themselves: they would be outsmarted at every turn, similar to a war between humans and mice. Putting these two concerns together leads to the alarming possibility of a global totalitarian dictatorship. Obviously, it should be one of our highest priorities to prevent this outcome.\r\n\r\nThere are many ways in which AI could enable, entrench, or expand autocracy, but I\u2019ll list a few that I\u2019m most worried about. Note that some of these applications have legitimate defensive uses, and I am not necessarily arguing against them in absolute terms; I am nevertheless worried that they structurally tend to favor autocracies:\r\n\r\n  * **Fully autonomous weapons.** A swarm of millions or billions of fully automated armed drones, locally controlled by powerful AI and strategically coordinated across the world by an even more powerful AI, could be an unbeatable army, capable of both defeating any military in the world and suppressing dissent within a country by following around every citizen. [Developments in the Russia-Ukraine War](https://www.hudson.org/missile-defense/impact-drones-battlefield-lessons-russian-ukraine-war-french-perspective-tsiporah-fried) should alert us to the fact that drone warfare is already with us (though not fully autonomous yet, and a tiny fraction of what might be possible with powerful AI). R&D from powerful AI could make the drones of one country far superior to those of others, speed up their manufacture, make them more resistant to electronic attacks, improve their maneuvering, and so on. Of course, these weapons also have legitimate uses in the defense of democracy: they have been key to defending Ukraine and would likely be key to defending Taiwan. But they are a dangerous weapon to wield: we should worry about them in the hands of autocracies, but also worry that because they are so powerful, with so little accountability, there is a greatly increased risk of democratic governments turning them against their own people to seize power.\r\n  * **AI surveillance.** Sufficiently powerful AI could likely be used to compromise any computer system in the world,30 and could also use the access obtained in this way to read _and make sense of_ all the world\u2019s electronic communications (or even all the world\u2019s in-person communications, if recording devices can be built or commandeered). It might be frighteningly plausible to simply generate a complete list of anyone who disagrees with the government on any number of issues, even if such disagreement isn\u2019t explicit in anything they say or do. A powerful AI looking across billions of conversations from millions of people could gauge public sentiment, detect pockets of disloyalty forming, and stamp them out before they grow. This could lead to the imposition of a true panopticon on a scale that we don\u2019t see today, even with the CCP.\r\n  * **AI propaganda.** Today\u2019s phenomena of \u201c[AI psychosis](https://en.wikipedia.org/wiki/Chatbot_psychosis)\u201d and \u201cAI girlfriends\u201d suggest that even at their current level of intelligence, AI models can have a powerful psychological influence on people. Much more powerful versions of these models, that were much more embedded in and aware of people\u2019s daily lives and could model and influence them over months or years, would likely be capable of essentially brainwashing many (most?) people into any desired ideology or attitude, and could be employed by an unscrupulous leader to ensure loyalty and suppress dissent, even in the face of a level of repression that most populations would rebel against. Today people worry a lot about, for example, the potential [influence of TikTok](https://networkcontagion.us/wp-content/uploads/NCRI-Report_-The-CCPs-Digital-Charm-Offensive.pdf) as CCP propaganda directed at children. I worry about that too, but a personalized AI agent that gets to know you over years and uses its knowledge of you to shape all of your opinions would be dramatically more powerful than this.\r\n  * **Strategic decision-making.** A country of geniuses in a datacenter could be used to advise a country, group, or individual on geopolitical strategy, what we might call a \u201cvirtual Bismarck.\u201d It could optimize the three strategies above for seizing power, plus probably develop many others that I haven\u2019t thought of (but that a country of geniuses could). Diplomacy, military strategy, R&D, economic strategy, and many other areas are all likely to be substantially increased in effectiveness by powerful AI. Many of these skills would be legitimately helpful for democracies\u2014we want democracies to have access to the best strategies for defending themselves against autocracies\u2014but the potential for misuse in _anyone\u2019s_ hands still remains.\r\n\r\nHaving described _what_ I am worried about, let\u2019s move on to _who_. I am worried about entities who have the most access to AI, who are starting from a position of the most political power, or who have an existing history of repression. In order of severity, I am worried about:\r\n\r\n  * **The CCP.** China is second only to the United States in AI capabilities, and is the country with the greatest likelihood of surpassing the United States in those capabilities. Their government is currently autocratic and operates a high-tech surveillance state. It has deployed AI-based surveillance already (including in the repression of [Uyghurs](https://en.wikipedia.org/wiki/Uyghurs)), and is believed to employ algorithmic propaganda via TikTok (in addition to its many other international propaganda efforts). They have hands down the clearest path to the AI-enabled totalitarian nightmare I laid out above. It may even be the default outcome within China, as well as within other autocratic states to whom the CCP exports surveillance technology. I have [written often](https://www.darioamodei.com/post/on-deepseek-and-export-controls) about the threat of the CCP taking the lead in AI and the existential imperative to prevent them from doing so. This is why. To be clear, I am not singling out China out of animus to them in particular\u2014they are simply the country that most combines AI prowess, an autocratic government, and a high-tech surveillance state. If anything, it is the Chinese people themselves who are most likely to suffer from the CCP\u2019s AI-enabled repression, and they have no voice in the actions of their government. I greatly admire and respect the Chinese people and support the many brave dissidents within China and their struggle for freedom.\r\n  * **Democracies competitive in AI.** As I wrote above, democracies have a legitimate interest in some AI-powered military and geopolitical tools, because democratic governments offer the best chance to counter the use of these tools by autocracies. Broadly, I am supportive of arming democracies with the tools needed to defeat autocracies in the age of AI\u2014I simply don\u2019t think there is any other way. But we cannot ignore the potential for abuse of these technologies by democratic governments themselves. Democracies normally have safeguards that prevent their military and intelligence apparatus from being turned inwards against their own population,31 but because AI tools require so few people to operate, there is potential for them to circumvent these safeguards and the norms that support them. It is also worth noting that some of these safeguards are already gradually eroding in some democracies. Thus, we should arm democracies with AI, but we should do so carefully and within limits: they are the immune system we need to fight autocracies, but like the immune system, there is some risk of them turning on us and becoming a threat themselves.\r\n  * **Non-democratic countries with large datacenters.** Beyond China, most countries with less democratic governance are not leading AI players in the sense that they don\u2019t have companies which produce frontier AI models. Thus they pose a fundamentally different and lesser risk than the CCP, which remains the primary concern (most are also less repressive, and the ones that are more repressive, like North Korea, have no significant AI industry at all). But some of these countries do have large _datacenters_(often as part of buildouts by companies operating in democracies), which can be used to run frontier AI at large scale (though this does not confer the ability to push the frontier). There is some amount of danger associated with this\u2014these governments could in principle expropriate the datacenters and use the country of AIs within it for their own ends. I am less worried about this compared to countries like China that directly develop AI, but it\u2019s a risk to keep in mind.32\r\n  * **AI companies.** It is somewhat awkward to say this as the CEO of an AI company, but I think the next tier of risk is actually AI companies themselves. AI companies control large datacenters, train frontier models, have the greatest expertise on how to use those models, and in some cases have daily contact with and the possibility of influence over tens or hundreds of millions of users. The main thing they lack is the legitimacy and infrastructure of a state, so much of what would be needed to build the tools of an AI autocracy would be illegal for an AI company to do, or at least exceedingly suspicious. But some of it is not impossible: they could, for example, use their AI products to brainwash their massive consumer user base, and the public should be alert to the risk this represents. I think the governance of AI companies deserves a lot of scrutiny.\r\n\r\nThere are a number of possible arguments against the severity of these threats, and I wish I believed them, because AI-enabled authoritarianism terrifies me. It\u2019s worth going through some of these arguments and responding to them.\r\n\r\nFirst, some people might put their faith in the nuclear deterrent, particularly to counter the use of AI autonomous weapons for military conquest. If someone threatens to use these weapons against you, you can always threaten a nuclear response back. My worry is that I\u2019m [not totally sure we can be confident](https://futureoflife.org/document/ai-and-nuclear-problem-analysis-and-policy-recommendations/) in the nuclear deterrent against a country of geniuses in a datacenter: it is possible that powerful AI could [devise ways](https://www.rand.org/content/dam/rand/pubs/perspectives/PE200/PE296/RAND_PE296.pdf) to [detect and strike nuclear submarines](https://spectrum.ieee.org/nuclear-submarine), [conduct influence operations](https://www.sipri.org/sites/default/files/2019-05/sipri1905-ai-strategic-stability-nuclear-risk.pdf) against the operators of nuclear weapons infrastructure, or use AI\u2019s [cyber capabilities](https://committees.parliament.uk/writtenevidence/120293/pdf/) to launch a cyberattack against satellites used to detect nuclear launches.33 Alternatively, it\u2019s possible that taking over countries is feasible with only AI surveillance and AI propaganda, and never actually presents a clear moment where it\u2019s obvious what is going on and where a nuclear response would be appropriate. _Maybe_ these things aren\u2019t feasible and the nuclear deterrent will still be effective, but it seems too high stakes to take a risk.34\r\n\r\nA second possible objection is that there might be countermeasures we can take against these tools of autocracy. We can counter drones with our own drones, cyberdefense will improve along with cyberattack, there may be ways to immunize people against propaganda, etc. My response is that these defenses will only be possible with comparably powerful AI. If there isn\u2019t some counterforce with a comparably smart and numerous country of geniuses in a datacenter, it won\u2019t be possible to match the quality or quantity of drones, for cyberdefense to outsmart cyberoffense, etc. So the question of countermeasures reduces to the question of a balance of power in powerful AI. Here, I am concerned about the recursive or self-reinforcing property of powerful AI (which I discussed at the beginning of this essay): that each generation of AI can be used to design and train the next generation of AI. This leads to a risk of a runaway advantage, where the current leader in powerful AI may be able to increase their lead and may be difficult to catch up with. We need to make sure it is not an authoritarian country that gets to this loop first.\r\n\r\nFurthermore, even if a balance of power can be achieved, there is still risk that the world could be split up into autocratic spheres, as in _Nineteen Eighty-Four_. Even if several competing powers each have their powerful AI models, and none can overpower the others, each power could still internally repress their own population, and would be very difficult to overthrow (since the populations don\u2019t have powerful AI to defend themselves). It is thus important to prevent AI-enabled autocracy even if it doesn\u2019t lead to a single country taking over the world.\r\n\r\n### _Defenses_\r\n\r\nHow do we defend against this wide range of autocratic tools and potential threat actors? As in the previous sections, there are several things I think we can do. First, we should absolutely not be selling chips, chip-making tools, or datacenters to the CCP. Chips and chip-making tools are the single greatest bottleneck to powerful AI, and blocking them is a simple but extremely effective measure, perhaps the most important single action we can take. It makes no sense to sell the CCP the tools with which to build an AI totalitarian state and possibly conquer us militarily. A number of complicated arguments are made to justify such sales, such as the idea that \u201cspreading our tech stack around the world\u201d allows \u201cAmerica to win\u201d in some general, unspecified economic battle. In my view, this is like selling nuclear weapons to North Korea and then bragging that the missile casings are made by Boeing and so the US is \u201cwinning.\u201d China is several years behind the US in their ability to produce frontier chips in quantity, and the critical period for building the country of geniuses in a datacenter is very likely to be within those next several years.35 There is no reason to give a giant boost to their AI industry during this critical period.\r\n\r\nSecond, it makes sense to use AI to empower democracies to resist autocracies. This is the reason Anthropic considers it important to provide AI to the intelligence and defense communities in the US and its democratic allies. Defending democracies that are under attack, such as Ukraine and (via cyber attacks) Taiwan, seems especially high priority, as does empowering democracies to use their intelligence services to disrupt and degrade autocracies from the inside. At some level the only way to respond to autocratic threats is to match and outclass them militarily. A coalition of the US and its democratic allies, if it achieved predominance in powerful AI, would be in a position to not only defend itself against autocracies, but contain them and limit their AI totalitarian abuses.\r\n\r\nThird, we need to draw a hard line against AI abuses within democracies. There need to be limits to what we allow our governments to do with AI, so that they don\u2019t seize power or repress their own people. The formulation I have come up with is that we should use AI for national defense in all ways _except those which would make us more like our autocratic adversaries_. \r\n\r\nWhere should the line be drawn? In the list at the beginning of this section, two items\u2014using AI for domestic mass surveillance and mass propaganda\u2014seem to me like bright red lines and entirely illegitimate. Some might argue that there\u2019s no need to do anything (at least in the US), since domestic mass surveillance is already illegal under the Fourth Amendment. But the rapid progress of AI may create situations that our existing legal frameworks are not well designed to deal with. For example, it would likely not be unconstitutional for the US government to conduct massively scaled recordings of all _public_ conversations (e.g., things people say to each other on a street corner), and previously it would have been difficult to sort through this volume of information, but with AI it could all be transcribed, interpreted, and triangulated to create a picture of the attitude and loyalties of many or most citizens. I would support civil liberties-focused legislation (or maybe even a constitutional amendment) that imposes stronger guardrails against AI-powered abuses.\r\n\r\nThe other two items\u2014fully autonomous weapons and AI for strategic decision-making\u2014are harder lines to draw since they have legitimate uses in defending democracy, while also being prone to abuse. Here I think what is warranted is extreme care and scrutiny combined with guardrails to prevent abuses. My main fear is having too small a number of \u201cfingers on the button,\u201d such that one or a handful of people could essentially operate a drone army without needing any other humans to cooperate to carry out their orders. As AI systems get more powerful, we may need to have more direct and immediate oversight mechanisms to ensure they are not misused, perhaps involving branches of government other than the executive. I think we should approach fully autonomous weapons in particular with great caution,36 and not rush into their use without proper safeguards.\r\n\r\nFourth, after drawing a hard line against AI abuses in democracies, we should use that precedent to create an international taboo against the worst abuses of powerful AI. I recognize that the current political winds have turned against international cooperation and international norms, but this is a case where we sorely need them. The world needs to understand the dark potential of powerful AI in the hands of autocrats, and to recognize that certain uses of AI amount to an attempt to permanently steal their freedom and impose a totalitarian state from which they can\u2019t escape. I would even argue that in some cases, large-scale surveillance with powerful AI, mass propaganda with powerful AI, and certain types of _offensive_ uses of fully autonomous weapons should be considered crimes against humanity. More generally, a robust norm against AI-enabled totalitarianism and all its tools and instruments is sorely needed.\r\n\r\nIt is possible to have an even stronger version of this position, which is that because the possibilities of AI-enabled totalitarianism are so dark, autocracy is simply not a form of government that people can accept in the post-powerful AI age. Just as feudalism became unworkable with the industrial revolution, the AI age could lead inevitably and logically to the conclusion that democracy (and, hopefully, democracy improved and reinvigorated by AI, as I discuss in _Machines of Loving Grace_) is the only viable form of government if humanity is to have a good future.\r\n\r\nFifth and finally, AI companies should be carefully watched, as should their connection to the government, which is necessary, but must have limits and boundaries. The sheer amount of capability embodied in powerful AI is such that ordinary corporate governance\u2014which is designed to protect shareholders and prevent ordinary abuses such as fraud\u2014is unlikely to be up to the task of governing AI companies. There may also be value in companies publicly committing to (perhaps even as part of corporate governance) not take certain actions, such as privately building or stockpiling military hardware, using large amounts of computing resources by single individuals in unaccountable ways, or using their AI products as propaganda to manipulate public opinion in their favor.\r\n\r\nThe danger here comes from many directions, and some directions are in tension with others. The only constant is that we must seek accountability, norms, and guardrails for everyone, even as we empower \u201cgood\u201d actors to keep \u201cbad\u201d actors in check.\r\n\r\n## 4\\. Player piano\r\n\r\n###  _Economic disruption_\r\n\r\nThe previous three sections were essentially about security risks posed by powerful AI: risks from the AI itself, risks from misuse by individuals and small organizations and risks of misuse by states and large organizations. If we put aside security risks or assume they have been solved, the next question is economic. What will be the effect of this infusion of incredible \u201chuman\u201d capital on the economy? Clearly, the most obvious effect will be to greatly increase economic growth. The pace of advances in scientific research, biomedical innovation, manufacturing, supply chains, the efficiency of the financial system, and much more are almost guaranteed to lead to a much faster rate of economic growth. In _Machines of Loving Grace_ , I suggest that a 10\u201320% sustained annual GDP growth rate may be possible.\r\n\r\nBut it should be clear that this is a double-edged sword: what are the economic prospects for most existing humans in such a world? New technologies often bring labor market shocks, and in the past humans have always recovered from them, but I am concerned that this is because these previous shocks affected only a small fraction of the full possible range of human abilities, leaving room for humans to expand to new tasks. AI will have effects that are much broader and occur much faster, and therefore I worry it will be much more challenging to make things work out well.\r\n\r\n### _Labor market disruption_\r\n\r\nThere are two specific problems I am worried about: labor market displacement, and concentration of economic power. Let\u2019s start with the first one. This is a topic that I [warned about very publicly in 2025](https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic), where I predicted that AI could displace half of all entry-level white collar jobs in the next 1\u20135 years, even as it accelerates economic growth and scientific progress. This warning started a public debate about the topic. Many CEOs, technologists, and economists agreed with me, but others assumed I was falling prey to a \u201clump of labor\u201d fallacy and didn\u2019t know how labor markets worked, and some didn\u2019t see the 1\u20135-year time range and thought I was claiming AI is displacing jobs right now (which I agree it is likely not). So it is worth going through in detail why I am worried about labor displacement, to clear up these misunderstandings.\r\n\r\nAs a baseline, it\u2019s useful to understand how labor markets _normally_ respond to advances in technology. When a new technology comes along, it starts by making pieces of a given human job more efficient. For example, early in the Industrial Revolution, machines, such as upgraded plows, enabled human farmers to be more efficient at some aspects of the job. This improved the productivity of farmers, which increased their wages.\r\n\r\nIn the next step, some parts of the job of farming could be done _entirely_ by machines, for example with the invention of the [threshing machine](https://en.wikipedia.org/wiki/Threshing_machine) or [seed drill](https://en.wikipedia.org/wiki/Seed_drill). In this phase, humans did a lower and lower fraction of the job, but the work they _did_ complete became more and more leveraged because it is complementary to the work of machines, and their productivity continued to rise. As described by [Jevons\u2019 paradox](https://en.wikipedia.org/wiki/Jevons_paradox), the wages of farmers and perhaps even the number of farmers continued to increase. Even when 90% of the job is being done by machines, humans can simply do 10x more of the 10% they still do, producing 10x as much output for the same amount of labor.\r\n\r\nEventually, machines do everything or almost everything, as with modern [combine harvesters](https://en.wikipedia.org/wiki/Combine_harvester), tractors, and other equipment. At this point farming as a form of human employment really does go into steep decline, and this potentially causes serious disruption in the short term, but because farming is just one of many useful activities that humans are able to do, people eventually switch to other jobs, such as operating factory machines. This is true even though farming accounted for a huge proportion of employment _ex ante_. 250 years ago, 90% of Americans [lived on farms](https://www.nass.usda.gov/About_NASS/History_of_Ag_Statistics/index.php); in Europe, 50\u201360% of employment [was agricultural](https://ourworldindata.org/grapher/share-of-the-labor-force-employed-in-agriculture?time=1750..latest). Now those percentages are in the low single digits in those places, because workers switched to industrial jobs (and later, knowledge work jobs). The economy can do what previously required most of the labor force with only 1\u20132% of it, freeing up the rest of the labor force to build an ever more advanced industrial society. There\u2019s no fixed \u201c[lump of labor](https://en.wikipedia.org/wiki/Lump_of_labour_fallacy),\u201d just an ever-expanding ability to do [more and more with less and less](https://en.wikipedia.org/wiki/Ephemeralization). People\u2019s wages rise in line with the GDP exponential and the economy maintains full employment once disruptions in the short term have passed.\r\n\r\nIt\u2019s possible things will go roughly the same way with AI, but I would bet pretty strongly against it. Here are some reasons I think AI is likely to be different:\r\n\r\n  * **Speed.** The pace of progress in AI is much faster than for previous technological revolutions. For example, in the last 2 years, AI models went from barely being able to complete a single line of code, to [writing all or almost all of the code](https://x.com/bcherny/status/2004887829252317325) for some people\u2014including engineers at Anthropic.37 Soon, they may do the entire task of a software engineer end to end.38 It is hard for people to adapt to this pace of change, both to the changes in how a given job works and in the need to switch to new jobs. Even legendary programmers are increasingly [describing themselves as \u201cbehind.\u201d](https://x.com/karpathy/status/2004607146781278521) The pace may if anything continue to speed up, as AI coding models increasingly accelerate the task of AI development. To be clear, speed in itself does not mean labor markets and employment won\u2019t eventually recover, it just implies the short-term transition will be unusually painful compared to past technologies, since humans and labor markets are slow to react and to equilibrate.\r\n  * **Cognitive breadth.** As suggested by the phrase \u201ccountry of geniuses in a datacenter,\u201d AI will be capable of a very wide range of human cognitive abilities\u2014perhaps all of them. This is very different from previous technologies like mechanized farming, transportation, or even computers.39 This will make it harder for people to switch easily from jobs that are displaced to similar jobs that they would be a good fit for. For example, the general intellectual abilities required for entry-level jobs in, say, finance, consulting, and law are fairly similar, even if the specific knowledge is quite different. A technology that disrupted only one of these three would allow employees to switch to the two other close substitutes (or for undergraduates to switch majors). But disrupting all three at once (along with many other similar jobs) may be harder for people to adapt to. Furthermore, it\u2019s not _just_ that most existing jobs will be disrupted. That part has happened before\u2014recall that farming was a huge percentage of employment. But farmers could switch to the relatively similar work of operating factory machines, even though that work hadn\u2019t been common before. By contrast, AI is increasingly matching the general cognitive profile of humans, which means it will also be good at the new jobs that would ordinarily be created in response to the old ones being automated. Another way to say it is that AI isn\u2019t a substitute for specific human jobs but rather a general labor substitute for humans.\r\n  * **Slicing by cognitive ability.** Across a wide range of tasks, AI appears to be advancing from the bottom of the ability ladder to the top. For example, in coding our models have proceeded from the level of \u201ca mediocre coder\u201d to \u201ca strong coder\u201d to \u201ca very strong coder.\u201d40 We are now starting to see the same progression in white-collar work in general. We are thus at risk of a situation where, instead of affecting people with specific skills or in specific professions (who can adapt by retraining), AI is affecting people with certain intrinsic cognitive properties, namely lower intellectual ability (which is harder to change). It is not clear where these people will go or what they will do, and I am concerned that they could form an unemployed or very-low-wage \u201cunderclass.\u201d To be clear, things somewhat like this have happened before\u2014for example, computers and the internet are believed by some economists to represent \u201c[skill-biased technological change](https://www.sciencedirect.com/science/chapter/handbook/abs/pii/S0169721811024105).\u201d But this skill biasing was both not as extreme as what I expect to see with AI, and is believed to have contributed to an increase in wage inequality,41 so it is not exactly a reassuring precedent.\r\n  * **Ability to fill in the gaps.** The way human jobs often adjust in the face of new technology is that there are many aspects to the job, and the new technology, even if it appears to directly replace humans, often has gaps in it. If someone invents a machine to make widgets, humans may still have to load raw material into the machine. Even if that takes only 1% as much effort as making the widgets manually, human workers can simply make 100x more widgets. But AI, in addition to being a rapidly advancing technology, is also a rapidly _adapting_ technology. During every model release, AI companies carefully measure what the model is good at and what it isn\u2019t, and customers also provide such information after the launch. Weaknesses can be addressed by collecting tasks that embody the current gap, and training on them for the next model. Early in generative AI, users noticed that AI systems had certain weaknesses (such as AI image models generating hands with the wrong number of fingers) and many assumed these weaknesses were inherent to the technology. If they were, it would limit job disruption. But pretty much every such weakness gets addressed quickly\u2014 often, within just a few months.\r\n\r\nIt\u2019s worth addressing common points of skepticism. First, there is the argument that economic diffusion will be slow, such that even if the underlying technology is _capable_ of doing most human labor, the actual application of it across the economy may be much slower (for example in industries that are far from the AI industry and slow to adopt). Slow diffusion of technology is [definitely real](https://www.ft.com/content/3b93e647-2a8b-4fb4-831d-e27adf4db5f8)\u2014I talk to people from a wide variety of enterprises, and there are places where the adoption of AI will take years. That\u2019s why my prediction for 50% of entry level white collar jobs being disrupted is 1\u20135 years, even though I suspect we\u2019ll have powerful AI (which would be, technologically speaking, enough to do _most or all_ jobs, not just entry level) in much less than 5 years. But diffusion effects merely buy us time. And I am not confident they will be as slow as people predict. Enterprise AI adoption is growing at rates much faster than any previous technology, largely on the pure strength of the technology itself. Also, even if traditional enterprises are slow to adopt new technology, startups will spring up to serve as \u201cglue\u201d and make the adoption easier. If that doesn\u2019t work, the startups may simply disrupt the incumbents directly.\r\n\r\nThat could lead to a world where it isn\u2019t so much that specific jobs are disrupted as it is that large enterprises are disrupted in general and replaced with much less labor-intensive startups. This could also lead to a world of \u201cgeographic inequality,\u201d where an increasing fraction of the world\u2019s wealth is concentrated in Silicon Valley, which becomes its own economy running at a different speed than the rest of the world and leaving it behind. All of these outcomes would be great for economic growth\u2014but not so great for the labor market or those who are left behind.\r\n\r\nSecond, some people say that human jobs will move to the physical world, which avoids the whole category of \u201ccognitive labor\u201d where AI is progressing so rapidly. I am not sure how safe this is, either. A lot of physical labor is already being done by machines (e.g., manufacturing) or will soon be done by machines (e.g., driving). Also, sufficiently powerful AI will be able to accelerate the development of robots, and then control those robots in the physical world. It may buy some time (which is a good thing), but I\u2019m worried it won\u2019t buy much. And even if the disruption was limited only to cognitive tasks, it would still be an unprecedentedly large and rapid disruption.\r\n\r\nThird, perhaps some tasks inherently require or greatly benefit from a human touch. I\u2019m a little more uncertain about this one, but I\u2019m still skeptical that it will be enough to offset the bulk of the impacts I described above. AI is already widely used for customer service. Many people [report](https://undark.org/2025/11/04/chatbot-mental-health/) that it is easier to talk to AI about their personal problems than to talk to a therapist\u2014that the AI is more patient. When my sister was struggling with medical problems during a pregnancy, she felt she wasn\u2019t getting the answers or support she needed from her care providers, and she found Claude to have a better bedside manner (as well as succeeding better at diagnosing the problem). I\u2019m sure there are some tasks for which a human touch really is important, but I\u2019m not sure how many\u2014and here we\u2019re talking about finding work for nearly everyone in the labor market.\r\n\r\nFourth, some may argue that comparative advantage will still protect humans. Under the [law of comparative advantage](https://en.wikipedia.org/wiki/Comparative_advantage), even if AI is better than humans at everything, any _relative_ differences between the human and AI profile of skills creates a basis of trade and specialization between humans and AI. The problem is that if AIs are literally thousands of times more productive than humans, this logic starts to break down. Even tiny [transaction costs](https://en.wikipedia.org/wiki/Transaction_cost) could make it not worth it for AI to trade with humans. And human wages may be very low, even if they technically have something to offer.\r\n\r\nIt\u2019s possible all of these factors can be addressed\u2014that the labor market is resilient enough to adapt to even such an enormous disruption. But even if it can eventually adapt, the factors above suggest that the short-term shock will be unprecedented in size.\r\n\r\n### _Defenses_\r\n\r\nWhat can we do about this problem? I have several suggestions, some of which Anthropic is already doing. The first thing is simply to get accurate data about what is happening with job displacement in real time. When an economic change happens very quickly, it\u2019s hard to get reliable data about what is happening, and without reliable data it is hard to design effective policies. For example, government data is currently lacking granular, high-frequency data on AI adoption across firms and industries. For the last year Anthropic has been operating and publicly releasing an [Economic Index](https://www.anthropic.com/economic-index) that shows use of our models almost in real time, broken down by industry, task, location, and even things like whether a task was being automated or conducted collaboratively. We also have an [Economic Advisory Council](https://www.anthropic.com/news/introducing-the-anthropic-economic-advisory-council) to help us interpret this data and see what is coming.\r\n\r\nSecond, AI companies have a choice in how they work with enterprises. The very inefficiency of traditional enterprises means that their rollout of AI can be very path dependent, and there is some room to choose a better path. Enterprises often have a choice between \u201ccost savings\u201d (doing the same thing with fewer people) and \u201cinnovation\u201d (doing more with the same number of people). The market will inevitably produce both eventually, and any competitive AI company will have to serve some of both, but there may be some room to steer companies towards innovation when possible, and it may buy us some time. Anthropic is actively thinking about this.\r\n\r\nThird, companies should think about how to take care of their employees. In the short term, being creative about ways to reassign employees within companies may be a promising way to stave off the need for layoffs. In the long term, in a world with enormous total wealth, in which many companies increase greatly in value due to increased productivity and capital concentration, it may be feasible to pay human employees even long after they are no longer providing economic value in the traditional sense. Anthropic is currently considering a range of possible pathways for our own employees that we will share in the near future.\r\n\r\nFourth, wealthy individuals have an obligation to help solve this problem. It is sad to me that many wealthy individuals (especially in the tech industry) have recently adopted a cynical and nihilistic attitude that philanthropy is inevitably fraudulent or useless. Both private philanthropy like the [Gates Foundation](https://www.gatesfoundation.org/) and public programs like [PEPFAR](https://en.wikipedia.org/wiki/President%27s_Emergency_Plan_for_AIDS_Relief) have saved tens of millions of lives in the developing world, and helped to create economic opportunity in the developed world. All of Anthropic\u2019s co-founders have pledged to donate 80% of our wealth, and Anthropic\u2019s staff have individually pledged to donate company shares worth billions at current prices\u2014donations that the company has committed to matching.\r\n\r\nFifth, while all the above private actions can be helpful, ultimately a macroeconomic problem this large will require government intervention. The natural policy response to an enormous economic pie coupled with high inequality (due to a lack of jobs, or poorly paid jobs, for many) is progressive taxation. The tax could be general or could be targeted against AI companies in particular. Obviously tax design is complicated, and there are many ways for it to go wrong. I don\u2019t support poorly designed tax policies. I think the extreme levels of inequality predicted in this essay justify a more robust tax policy on basic moral grounds, but I can also make a pragmatic argument to the world\u2019s billionaires that it\u2019s in their interest to support a good version of it: if they don\u2019t support a good version, they\u2019ll inevitably get a bad version designed by a mob.\r\n\r\nUltimately, I think of all of the above interventions as ways to buy time. In the end AI will be able to do everything, and we need to grapple with that. It\u2019s my hope that by that time, we can use AI itself to help us restructure markets in ways that work for everyone, and that the interventions above can get us through the transitional period.\r\n\r\n### _Economic concentration of power_\r\n\r\nSeparate from the problem of job displacement or economic inequality _per se_ is the problem of _economic concentration of power._ Section 1 discussed the risk that humanity gets disempowered by AI, and Section 3 discussed the risk that citizens get disempowered by their governments by force or coercion. But another kind of disempowerment can occur if there is such a huge concentration of wealth that a small group of people effectively controls government policy with their influence, and ordinary citizens have no influence because they lack economic leverage. Democracy is ultimately backstopped by the idea that the population as a whole is necessary for the operation of the economy. If that economic leverage goes away, then the implicit social contract of democracy may stop working. [Others have written about this](https://intelligence-curse.ai/), so I needn\u2019t go into great detail about it here, but I agree with the concern, and I worry it is already starting to happen.\r\n\r\nTo be clear, I am not opposed to people making a lot of money. There\u2019s a strong argument that it incentivizes economic growth under normal conditions. I am sympathetic to concerns about impeding innovation by killing the golden goose that generates it. But in a scenario where GDP growth is 10\u201320% a year and AI is rapidly taking over the economy, yet single individuals hold appreciable fractions of the GDP, innovation is _not_ the thing to worry about. The thing to worry about is a level of wealth concentration that will break society.\r\n\r\nThe most famous example of extreme concentration of wealth in US history is the [Gilded Age](https://en.wikipedia.org/wiki/Gilded_Age), and the wealthiest industrialist of the Gilded Age was [John D. Rockefeller](https://en.wikipedia.org/wiki/John_D._Rockefeller). Rockefeller\u2019s wealth amounted to ~2% of the US GDP at the time.42 A similar fraction today would lead to a fortune of $600B, and the richest person in the world today (Elon Musk) already exceeds that, at [roughly $700B](https://www.reuters.com/business/autos-transportation/elon-musk-becomes-first-person-worth-700-billion-after-court-ruling-pay-package-2025-12-20/). So we are already at historically unprecedented levels of wealth concentration, even _before_ most of the economic impact of AI. I don\u2019t think it is too much of a stretch (if we get a \u201ccountry of geniuses\u201d) to imagine AI companies, semiconductor companies, and perhaps downstream application companies generating ~$3T in revenue per year,43 being valued at ~$30T, and leading to personal fortunes well into the trillions. In that world, the debates we have about tax policy today simply won\u2019t apply as we will be in a fundamentally different situation.\r\n\r\nRelated to this, the coupling of this economic concentration of wealth with the political system already concerns me. AI datacenters already represent a substantial fraction of US economic growth,44 and are thus strongly tying together the financial interests of large tech companies (which are increasingly focused on either AI or AI infrastructure) and the political interests of the government in a way that can produce perverse incentives. We already see this through the reluctance of tech companies to criticize the US government, and the government\u2019s support for extreme anti-regulatory policies on AI.\r\n\r\n### _Defenses_\r\n\r\nWhat can be done about this? First, and most obviously, companies should simply choose not to be part of it. Anthropic has always strived to be a policy actor and not a political one, and to maintain our authentic views whatever the administration. We\u2019ve spoken up in favor of [sensible AI regulation](https://www.nytimes.com/2025/06/05/opinion/anthropic-ceo-regulate-transparency.html) and [export controls](https://www.wsj.com/opinion/trump-can-keep-americas-ai-advantage-china-chips-data-eccdce91?gaa_at=eafs&gaa_n=AWEtsqespyCL3hcx_9DpJWbIPX1vrtS1raPgFoBNK8ltnrjwedpX2NuvVu1K_yZ1arw%3D&gaa_ts=696c6c70&gaa_sig=wef9kKocpL9PU07UoiPS6kj_o_Nwy_VSufM6gltIvdjQFhb8HRLtpSzp4Z8WDG6v3leg0ODX4HOJjWblvZe2pw%3D%3D) that are in the public interest, even when these are at odds with government policy.45 Many people have told me that we should stop doing this, that it could lead to unfavorable treatment, but in the year we\u2019ve been doing it, Anthropic\u2019s valuation has increased by over 6x, an almost unprecedented jump at our commercial scale.\r\n\r\nSecond, the AI industry needs a healthier relationship with government\u2014one based on substantive policy engagement rather than political alignment. Our choice to engage on policy substance rather than politics is sometimes read as a tactical error or failure to \u201cread the room\u201d rather than a principled decision, and that framing concerns me. In a healthy democracy, companies should be able to advocate for good policy for its own sake. Related to this, a public backlash against AI is brewing: this could be a corrective, but it**\u2019** s currently unfocused. Much of it targets issues that aren\u2019t actually problems (like [datacenter](https://newsletter.semianalysis.com/p/from-tokens-to-burgers-a-water-footprint?hide_intro_popup=true) [water usage](https://andymasley.substack.com/p/the-ai-water-issue-is-fake)) and proposes solutions (like datacenter bans or poorly designed wealth taxes) that wouldn\u2019t address the real concerns. The underlying issue that deserves attention is ensuring that AI development remains accountable to the public interest, not captured by any particular political or commercial alliance, and it seems important to focus the public discussion there.\r\n\r\nThird, the macroeconomic interventions I described earlier in this section, as well as a resurgence of private philanthropy, can help to balance the economic scales, addressing both the job displacement and concentration of economic power problems at once. We should look to the history of our country here: even in the Gilded Age, industrialists such as [Rockefeller](https://www.sciencedirect.com/science/article/abs/pii/S096262981500027X) and [Carnegie](https://www.carnegie.org/about/our-history/gospelofwealth/) felt a strong obligation to society at large, a feeling that society had contributed enormously to their success and they needed to give back. That spirit seems to be increasingly missing today, and I think it is a large part of the way out of this economic dilemma. Those who are at the forefront of AI\u2019s economic boom should be willing to give away both their wealth and their power.\r\n\r\n## 5\\. Black seas of infinity\r\n\r\n###  _Indirect effects_\r\n\r\nThis last section is a catchall for unknown unknowns, particularly things that could go wrong as an indirect result of positive advances in AI and the resulting acceleration of science and technology in general. Suppose we address all the risks described so far, and begin to reap the benefits of AI. We will likely get a \u201c[century of scientific and economic progress compressed into a decade](https://www.darioamodei.com/essay/machines-of-loving-grace),\u201d and this will be hugely positive for the world, but we will then have to contend with the problems that arise from this rapid rate of progress, and those problems may come at us fast. We may also encounter other risks that occur indirectly as a consequence of AI progress and are hard to anticipate in advance.\r\n\r\nBy the nature of unknown unknowns it is impossible to make an exhaustive list, but I\u2019ll list three possible concerns as illustrative examples for what we should be watching for:\r\n\r\n  * **Rapid advances in biology.** If we do get a century of medical progress in a few years, it is possible that we will greatly increase the human lifespan, and there is a chance we also gain radical capabilities like the ability to increase human intelligence or radically modify human biology. Those would be big changes in what is possible, happening very quickly. They could be positive if responsibly done (which is my hope, as described in _Machines of Loving Grace_), but there is always a risk they go very wrong\u2014for example, if efforts to make humans smarter also make them more unstable or power-seeking. There is also the issue of \u201c[uploads](https://en.wikipedia.org/wiki/Mind_uploading)\u201d or \u201cwhole brain emulation,\u201d digital human minds instantiated in software, which might someday help humanity transcend its physical limitations, but which also carry [risks I find disquieting](https://qntm.org/mmacevedo).\r\n  * **AI changes human life in an unhealthy way.** A world with billions of intelligences that are much smarter than humans at everything is going to be a very weird world to live in. Even if AI doesn\u2019t actively aim to attack humans (Section 1), and isn\u2019t explicitly used for oppression or control by states (Section 3), there is a lot that could go wrong short of this, via normal business incentives and nominally consensual transactions. We see early hints of this in the concerns about AI psychosis, [AI driving people to suicide](https://www.cbsnews.com/news/chatgpt-lawsuit-colordo-man-suicide-openai-sam-altman/), and concerns about romantic relationships with AIs. As an example, could powerful AIs invent some new religion and convert millions of people to it? Could most people end up \u201caddicted\u201d in some way to AI interactions? Could people end up being \u201cpuppeted\u201d by AI systems, where an AI essentially watches their every move and tells them exactly what to do and say at all times, leading to a \u201cgood\u201d life but one that lacks freedom or any pride of accomplishment? It would not be hard to generate dozens of these scenarios if I sat down with the creator of [_Black Mirror_](https://en.wikipedia.org/wiki/Black_Mirror) and tried to brainstorm them. I think this points to the importance of things like improving [Claude\u2019s Constitution](https://www.anthropic.com/constitution), over and above what is necessary for preventing the issues in Section 1. Making sure that AI models _really_ have their users\u2019 long-term interests at heart, in a way thoughtful people would endorse rather than in some subtly distorted way, seems critical.\r\n  * **Human purpose.** This is related to the previous point, but it\u2019s not so much about specific human interactions with AI systems as it is about how human life changes in general in a world with powerful AI. Will humans be able to find purpose and meaning in such a world? I think this is a matter of attitude: as I said in _Machines of Loving Grace_ , I think human purpose does not depend on being the best in the world at something, and humans can find purpose even over very long periods of time through stories and projects that they love. We simply need to break the link between the generation of economic value and self-worth and meaning. But that is a transition society has to make, and there is always the risk we don\u2019t handle it well.\r\n\r\nMy hope with all of these potential problems is that in a world with powerful AI that we trust not to kill us, that is not the tool of an oppressive government, and that is genuinely working on our behalf, we can use AI itself to anticipate and prevent these problems. But that is not guaranteed\u2014like all of the other risks, it is something we have to handle with care.\r\n\r\n## Humanity\u2019s test\r\n\r\nReading this essay may give the impression that we are in a daunting situation. I certainly found it daunting to write, in contrast with _Machines of Loving Grace,_ which felt like giving form and structure to surpassingly beautiful music that had been echoing in my head for years. And there is much about the situation that genuinely is hard. AI brings threats to humanity from multiple directions, and there is genuine tension between the different dangers, where mitigating some of them risks making others worse if we do not thread the needle extremely carefully.\r\n\r\nTaking time to carefully build AI systems so they do not autonomously threaten humanity is in genuine tension with the need for democratic nations to stay ahead of authoritarian nations and not be subjugated by them. But in turn, the same AI-enabled tools that are necessary to fight autocracies can, if taken too far, be turned inward to create tyranny in our own countries. AI-driven terrorism could kill millions through the misuse of biology, but an overreaction to this risk could lead us down the road to an autocratic surveillance state. The labor and economic concentration effects of AI, in addition to being grave problems in their own right, may force us to face the other problems in an environment of public anger and perhaps even civil unrest, rather than being able to call on the better angels of our nature. Above all, the sheer _number_ of risks, including unknown ones, and the need to deal with all of them at once, creates an intimidating gauntlet that humanity must run.\r\n\r\nFurthermore, the last few years should make clear that the idea of stopping or even substantially slowing the technology is fundamentally untenable. The formula for building powerful AI systems is incredibly simple, so much so that it can almost be said to emerge spontaneously from the right combination of data and raw computation. Its creation was probably inevitable the instant humanity invented the transistor, or arguably even earlier when we first learned to control fire. If one company does not build it, others will do so nearly as fast. If all companies in democratic countries stopped or slowed development, by mutual agreement or regulatory decree, then authoritarian countries would simply keep going. Given the incredible economic and military value of the technology, together with the lack of any meaningful enforcement mechanism, I don\u2019t see [how we could possibly convince them to stop](https://en.wikipedia.org/wiki/Security_dilemma).\r\n\r\nI do see a path to a _slight_ moderation in AI development that is compatible with a [realist view of geopolitics](https://en.wikipedia.org/wiki/Realism_\\(international_relations\\)). That path involves slowing down the march of autocracies towards powerful AI for a few years by denying them the resources they need to build it,46 namely chips and semiconductor manufacturing equipment. This in turn gives democratic countries a buffer that they can \u201cspend\u201d on building powerful AI more carefully, with more attention to its risks, while still proceeding fast enough to comfortably beat the autocracies. The race between AI companies within democracies can then be handled under the umbrella of a common legal framework, via a mixture of industry standards and regulation.\r\n\r\nAnthropic has advocated very hard for this path, by pushing for chip export controls and judicious regulation of AI, but even these seemingly common-sense proposals have largely been rejected by policymakers in the United States (which is the country where it\u2019s most important to have them). There is so much money to be made with AI\u2014literally trillions of dollars per year\u2014that even the simplest measures are finding it difficult to overcome the [political economy](https://en.wikipedia.org/wiki/Political_economy) inherent in AI. This is the trap: AI is so powerful, such a glittering prize, that it is very difficult for human civilization to impose any restraints on it at all.\r\n\r\nI can imagine, as Sagan did in _Contact_ , that this same story plays out on thousands of worlds. A species gains sentience, learns to use tools, begins the exponential ascent of technology, faces the crises of industrialization and nuclear weapons, and if it survives those, confronts the hardest and final challenge when it learns how to shape sand into machines that think. Whether we survive that test and go on to build the beautiful society described in _Machines of Loving Grace_ , or succumb to slavery and destruction, will depend on our character and our determination as a species, our spirit and our soul.\r\n\r\nDespite the many obstacles, I believe humanity has the strength inside itself to pass this test. I am encouraged and inspired by the thousands of researchers who have devoted their careers to helping us understand and steer AI models, and to shaping the character and constitution of these models. I think there is now a good chance that those efforts bear fruit in time to matter. I am encouraged that at least some companies have [stated they**\u2019** ll pay](https://openai.com/index/preparing-for-future-ai-capabilities-in-biology/) meaningful commercial costs to block their models from contributing to the threat of bioterrorism. I am encouraged that a few brave people have resisted the prevailing political winds and [passed](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53) [legislation](https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A) that puts the first early seeds of sensible guardrails on AI systems. I am encouraged that the [public understands that AI carries risks and wants those risks addressed](https://www.pewresearch.org/internet/2025/04/03/views-of-risks-opportunities-and-regulation-of-ai/#6d2b9b266433bfda6c8fc2f498738a4c). I am encouraged by the indomitable spirit of freedom around the world and the determination to resist tyranny wherever it occurs.\r\n\r\nBut we will need to step up our efforts if we want to succeed. The first step is for those closest to the technology to simply tell the truth about the situation humanity is in, which I have always tried to do; I\u2019m doing so more explicitly and with greater urgency with this essay. The next step will be convincing the world\u2019s thinkers, policymakers, companies, and citizens of the imminence and overriding importance of this issue\u2014that it is worth expending thought and political capital on this in comparison to the thousands of other issues that dominate the news every day. Then there will be a time for courage, for enough people to buck the prevailing trends and stand on principle, even in the face of threats to their economic interests and personal safety. \r\n\r\nThe years in front of us will be impossibly hard, asking more of us than we think we can give. But in my time as a researcher, leader, and citizen, I have seen enough courage and nobility to believe that we can win\u2014that when put in the darkest circumstances, humanity has a way of gathering, seemingly at the last minute, the strength and wisdom needed to prevail. We have no time to lose.\r\n\r\n* * *\r\n\r\nI would like to thank Erik Brynjolfsson, Ben Buchanan, Mariano-Florentino Cu\u00e9llar, Allan Dafoe, Kevin Esvelt, Nick Beckstead, Richard Fontaine, Jim McClave, and very many of the staff at Anthropic for their helpful comments on drafts of this essay.\r\n\r\n## Footnotes\r\n\r\n  1. This is symmetric to a point I made in _Machines of Loving Grace_ , where I started by saying that AI\u2019s upsides shouldn\u2019t be thought of in terms of a prophecy of salvation, and that it\u2019s important to be concrete and grounded and to avoid grandiosity. Ultimately, prophecies of salvation and prophecies of doom are unhelpful for confronting the real world, for basically the same reasons.\r\n  2. Anthropic\u2019s goal is to remain consistent through such changes. When talking about AI risks was politically popular, Anthropic cautiously advocated for a judicious and evidence-based approach to these risks. Now that talking about AI risks is politically unpopular, Anthropic continues to cautiously advocate for a judicious and evidence-based approach to these risks.\r\n  3. Over time, I have gained increasing confidence in the trajectory of AI and the likelihood that it will surpass human ability across the board, but some uncertainty still remains.\r\n  4. Export controls for chips are a great example of this. They are simple and appear to mostly just work.\r\n  5. And of course, the hunt for such evidence must be intellectually honest, such that it could also turn up evidence of a lack of danger. Transparency through model cards and other disclosures is an attempt at such an intellectually honest endeavor.\r\n  6. Indeed, since writing _Machines of Loving Grace_ in 2024, AI systems have become capable of doing tasks that take humans several hours, with METR recently assessing that Opus 4.5 can do about four human hours of work with 50% reliability.\r\n  7. And to be clear, even if powerful AI is only 1\u20132 years away in a technical sense, many of its societal consequences, both positive and negative, may take a few years longer to occur. This is why I can simultaneously think that AI will disrupt 50% of _entry-level_ white-collar jobs over 1\u20135 years, while also thinking we may have AI that is more capable than _everyone_ in only 1\u20132 years.\r\n  8. It is worth adding that the _public_(as compared to policymakers) does seem to be very concerned with AI risks. I think some of their focus is correct (i.e. AI job displacement), and some is misguided (such as concerns about water use of AI, which is not significant). This backlash gives me hope that a consensus around addressing risks is possible, but so far it has not yet been translated into policy changes, let alone effective or well-targeted policy changes.\r\n  9. They can also, of course, manipulate (or simply pay) large numbers of humans into doing what they want in the physical world.\r\n  10. I don\u2019t think this is a straw man: it\u2019s my understanding, for example, that [Yann LeCun holds this position](https://www.youtube.com/watch?v=LMuun5FGL28).\r\n  11. For example, see Section 5.5.2 (p. 63\u201366) of the [Claude 4 system card](https://www.anthropic.com/claude-4-system-card).\r\n  12. There are also a number of other assumptions inherent in the simple model, which I won\u2019t discuss here. Broadly, they should make us less worried about the specific simple story of misaligned power-seeking, but also more worried about possible unpredictable behavior we haven\u2019t anticipated.\r\n  13. [_Ender\u2019s Game_](https://en.wikipedia.org/wiki/Ender%27s_Game) __ describes a version of this involving humans rather than AI.\r\n  14. For example, models may be told not to do various bad things, and also to obey humans, but may then observe that many humans do exactly those bad things! It\u2019s not clear how this contradiction would resolve (and a well-designed constitution should encourage the model to handle these contradictions gracefully), but this type of dilemma is not so different from the supposedly \u201cartificial\u201d situations that we put AI models in during testing.\r\n  15. Incidentally, one consequence of the constitution being a natural-language document is that it is legible to the world, and that means it can be critiqued by anyone and compared to similar documents by other companies. It would be valuable to create a race to the top that not only encourages companies to release these documents, but encourages them to be good.\r\n  16. There\u2019s even a hypothesis about a deep unifying principle connecting the character-based approach from Constitutional AI to results from interpretability and alignment science. According to the hypothesis, the fundamental mechanisms driving Claude originally arose as ways for it to simulate characters in pretraining, such as predicting what the characters in a novel would say. This would suggest that a useful way to think about the constitution is more like a character description that the model uses to instantiate a consistent persona. It would also help us explain the \u201c[I must be a bad person](https://www.anthropic.com/research/emergent-misalignment-reward-hacking)\u201d results I mentioned above (because the model is trying to _act as if_ it\u2019s a coherent character\u2014in this case a bad one), and would suggest that interpretability methods should be able to discover \u201cpsychological traits\u201d within models. Our researchers are working on ways to test this hypothesis.\r\n  17. To be clear, monitoring is done in a privacy-preserving way.\r\n  18. Even in our own experiments with what are essentially voluntarily imposed rules with our [Responsible Scaling Policy](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy), we have found over and over again that it\u2019s very easy to end up being too rigid, by drawing lines that seem important ex ante but turn out to be silly in retrospect. It is just very easy to set rules about the wrong things when a technology is advancing rapidly.\r\n  19. SB 53 and RAISE do not apply at all to companies with under $500M in annual revenue. They only apply to larger, more established companies like Anthropic.\r\n  20. I originally read Joy\u2019s essay 25 years ago, when it was written, and it had a profound impact on me. Then and now, I do see it as too pessimistic\u2014I don\u2019t think broad \u201crelinquishment\u201d of whole areas of technology, which Joy suggests, is the answer\u2014but the issues it raises were surprisingly prescient, and Joy also writes with a deep sense of compassion and humanity that I admire.\r\n  21. We do have to worry about state actors, now and in the future, and I discuss that in the next section.\r\n  22. There is [evidence](https://www.nber.org/digest/sep02/poverty-and-low-education-dont-cause-terrorism) that [many](https://www.sas.rochester.edu/psc/clarke/214/Krueger03.pdf) terrorists are at least relatively well-educated, which might seem to contradict what I\u2019m arguing here about a negative correlation between ability and motivation. But I think in actual fact they are compatible observations: if the ability threshold for a successful attack is high, then almost by definition those who _currently_ succeed must have high ability, even if ability and motivation are negatively correlated. But in a world where the limitations on ability were removed (e.g., with future LLMs), I\u2019d predict that a substantial population of people with the motivation to kill but lower ability would start to do so\u2014just as we see for crimes that don\u2019t require much ability (like school shootings).\r\n  23. Aum Shinrikyo did try, however. The leader of Aum Shinrikyo, Seiichi Endo, had training in virology from Kyoto University, [and attempted to produce both anthrax and ebola](https://www.cnas.org/publications/reports/aum-shinrikyo-second-edition-english). However, as of 1995, even he lacked enough expertise and resources to succeed at this. The bar is now substantially lower, and LLMs could reduce it even further.\r\n  24. A bizarre phenomenon relating to mass murderers is that the style of murder they choose operates almost as a grotesque sort of fad. In the 1970s and 1980s, serial killers were very common, and new serial killers often copied the behavior of more established or famous serial killers. In the 1990s and 2000s, mass shootings became more common, while serial killers became less common. There is no technological change that triggered these patterns of behavior, it just appears that violent murderers were copying each others\u2019 behavior and the \u201cpopular\u201d thing to copy changed.\r\n  25. Casual jailbreakers sometimes believe that they\u2019ve compromised these classifiers when they get the model to output one specific piece of information, such as the genome sequence of a virus. But as I explained before, the threat model we are worried about involves step-by-step, interactive advice that extends over weeks or months about specific obscure steps in the bioweapons production process, and this is what our classifiers aim to defend against. (We often describe our research as looking for \u201cuniversal\u201d jailbreaks\u2014ones that don\u2019t just work in one specific or narrow context, but broadly open up the model\u2019s behavior.)\r\n  26. Though we will continue to invest in work to make our classifiers more efficient, and it may make sense for companies to share advances like these with one another.\r\n  27. Obviously, I do not think companies should have to disclose technical details about the specific steps in biological weapons production that they are blocking, and the transparency legislation that has been passed so far (SB 53 and RAISE) accounts for this issue.\r\n  28. Another related idea is \u201cresilience markets\u201d where the government encourages stockpiling of PPE, respirators, and other essential equipment needed to respond to a biological attack by promising ahead of time to pay a pre-agreed price for this equipment in an emergency. This incentivizes suppliers to stockpile such equipment without fear that the government will seize it without compensation.\r\n  29. Why am I more worried about large actors for seizing power, but small actors for causing destruction? Because the dynamics are different. Seizing power is about whether one actor can amass enough strength to overcome everyone else\u2014thus we should worry about the most powerful actors and/or those closest to AI. Destruction, by contrast, can be wrought by those with little power if it is much harder to defend against than to cause. It is then a game of defending against the most _numerous_ threats, which are likely to be smaller actors.\r\n  30. This might sound like it is in tension with my point that attack and defense may be more balanced with cyberattacks than with bioweapons, but my worry here is that if a country\u2019s AI is the most powerful in the world, then others will not be able to defend even if the technology itself has an intrinsic attack-defense balance.\r\n  31. For example, in the United States this includes the fourth amendment and the [Posse Comitatus Act](https://en.wikipedia.org/wiki/Posse_Comitatus_Act).\r\n  32. Also, to be clear, there are some arguments for building large datacenters in countries with varying governance structures, particularly if they are controlled by companies in democracies. Such buildouts could in principle help democracies compete better with the CCP, which is the greater threat. I also think such datacenters don\u2019t pose much risk unless they are very large. But on balance, I think caution is warranted when placing very large datacenters in countries where institutional safeguards and rule-of-law protections are less well-established.\r\n  33. This is, of course, also an argument for [improving the security of the nuclear deterrent](https://councilonstrategicrisks.org/research/reports/nuclear-decision-making-and-risk-reduction-in-an-era-of-technological-complexity/) to make it [more likely to be robust](https://onlinelibrary.wiley.com/doi/10.1111/risa.70136) against powerful AI, and nuclear-armed democracies should do this. But we don\u2019t know what a powerful AI will be capable of or which defenses, if any, will work against it, so we should not assume that these measures will necessarily solve the problem.\r\n  34. There is also the risk that even if the nuclear deterrent remains effective, an attacking country might decide to call our bluff\u2014it\u2019s unclear whether we\u2019d be willing to use nuclear weapons to defend against a drone swarm even if the drone swarm has a substantial risk of conquering us. Drone swarms might be a new thing that is less severe than nuclear attacks but more severe than conventional attacks. Alternatively, differing assessments of the effectiveness of the nuclear deterrent in the age of AI might alter the game theory of nuclear conflict in a destabilizing manner.\r\n  35. To be clear, I would believe it is the right strategy not to sell chips to China, even if the timeline to powerful AI were substantially longer. We cannot get the Chinese \u201caddicted\u201d to American chips\u2014they are determined to develop their native chip industry one way or another. It will take them many years to do so, and all we are doing by selling them chips is giving them a big boost during that time.\r\n  36. To be clear, most of what is being used in Ukraine and Taiwan today are not _fully_ autonomous weapons. These are coming, but not here today.\r\n  37. Our model card for [Claude Opus 4.5](https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf), our most recent model, shows that Opus performs better on a performance engineering interview frequently given at Anthropic than any interviewee in the history of the company.\r\n  38. \u201cWriting all of the code\u201d and \u201cdoing the task of a software engineer end to end\u201d are very different things, because software engineers do much more than just write code, including testing, dealing with environments, files, and installation, managing cloud compute deployments, iterating on products, and much more.\r\n  39. Computers are general in a sense, but are clearly incapable on their own of the vast majority of human cognitive abilities, even as they greatly exceed humans in a few areas (such as arithmetic). Of course, things built _on top_ of computers, such as AI, are now capable of a wide range of cognitive abilities, which is what this essay is about.\r\n  40. To be clear, AI models do not have precisely the same profile of strengths and weaknesses as humans. But they are also advancing fairly uniformly along every dimension, such that having a spiky or uneven profile may not ultimately matter.\r\n  41. Though there is [debate](https://davidcard.berkeley.edu/papers/skill-tech-change.pdf) [among](https://jhr.uwpress.org/content/58/6/1783.abstract) [economists](https://www.epi.org/publication/technology-inequality-dont-blame-the-robots/) about this idea.\r\n  42. Personal wealth is a \u201cstock,\u201d while GDP is a \u201cflow,\u201d so this isn\u2019t a claim that Rockefeller owned 2% of the economic value in the United States. But it\u2019s harder to measure the total wealth of a nation than the GDP, and people\u2019s individual incomes vary a lot per year, so it\u2019s hard to make a ratio in the same units. The ratio of the largest personal fortune to GDP, while not comparing apples to apples, is nevertheless a perfectly reasonable benchmark for extreme wealth concentration.\r\n  43. The total value of labor across the economy is $60T/year, so $3T/year would correspond to 5% of this. That amount could be earned by a company that supplied labor for 20% of the cost of humans and had 25% market share, even if the demand for labor did not expand (which it almost certainly would due to the lower cost).\r\n  44. To be clear, I do not think actual AI productivity is yet responsible for a substantial fraction of US economic growth. Rather, I think the datacenter spending represents growth caused by anticipatory investment that amounts to the market expecting _future_ AI-driven economic growth and investing accordingly.\r\n  45. When we agree with the administration, we say so, and we look for [points of agreement where mutually supported policies](https://www.anthropic.com/news/statement-dario-amodei-american-ai-leadership) are genuinely good for the world. We are aiming to be honest brokers rather than backers or opponents of any given political party.\r\n  46. I don\u2019t think anything more than a few years is possible: on longer timescales, they will build their own chips.\r"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":["EjayNg-AI/agentic_browser"],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_source":null,"turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"52394d9c-bbeb-41f8-a370-6a58c88b0958","children":["61488132-1e8f-4f1b-b337-13f55c29b2d0"]},"61488132-1e8f-4f1b-b337-13f55c29b2d0":{"id":"61488132-1e8f-4f1b-b337-13f55c29b2d0","message":{"id":"61488132-1e8f-4f1b-b337-13f55c29b2d0","author":{"role":"system","name":null,"metadata":{}},"create_time":1769486196.7987318,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"1988533d-96f3-4fe0-b34a-a94aeff9e980","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"1988533d-96f3-4fe0-b34a-a94aeff9e980","children":["8070ecd4-5a0a-4816-a4ee-80a7054ede94"]},"8070ecd4-5a0a-4816-a4ee-80a7054ede94":{"id":"8070ecd4-5a0a-4816-a4ee-80a7054ede94","message":{"id":"8070ecd4-5a0a-4816-a4ee-80a7054ede94","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486197.2974923,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"61488132-1e8f-4f1b-b337-13f55c29b2d0","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"61488132-1e8f-4f1b-b337-13f55c29b2d0","children":["94e5eed7-3629-4c40-8205-7c368b5cf8de"]},"94e5eed7-3629-4c40-8205-7c368b5cf8de":{"id":"94e5eed7-3629-4c40-8205-7c368b5cf8de","message":{"id":"94e5eed7-3629-4c40-8205-7c368b5cf8de","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486198.332498,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Considering Dario Amodei's blog post analysis","content":"The user wants a critical analysis of Dario Amodei's January 2026 blog post, \"The Adolescence of Technology,\" and its potential impact on technology, jobs, and society. I'll consider if the blog content needs verification for current info, especially regarding metrics, Elon Musk's net worth, and any relevant updates like SB 53 or RAISE.","chunks":["The user wants a critical analysis of Dario Amodei's January 2026 blog post, \"The Adolescence of Technology,\" and its potential impact on technology, jobs, and society. I'll consider if the blog content needs verification for current info, especially regarding metrics, Elon Musk's net worth, and any relevant updates like SB 53 or RAISE."],"finished":true},{"summary":"Verifying blog content and context","content":"To evaluate the user's request accurately, it\u2019s essential to verify if the blog post exists and if there are relevant updates. Since the user included the content, I might not need to search for the exact blog post, but confirming a few details, including metrics and legislative info, could still be helpful. I\u2019ll check for any additional context and ensure all key points are referenced.","chunks":["To evaluate the user's request accurately, it\u2019s essential to verify if the blog post exists and if there are relevant updates.","Since the user included the content, I might not need to search for the exact blog post, but confirming a few details, including metrics and legislative info, could still be helpful.","I\u2019ll check for any additional context and ensure all key points are referenced."],"finished":true},{"summary":"Assessing image relevance for critical analysis","content":"The user is asking for an analysis of Dario Amodei's blog post, not images, though the instructions indicate I need to show an image carousel if a person is involved. It\u2019s a bit tricky since images might not add much to the analysis, but I\u2019ll include the carousel to comply.","chunks":["The user is asking for an analysis of Dario Amodei's blog post, not images, though the instructions indicate I need to show an image carousel if a person is involved.","It\u2019s a bit tricky since images might not add much to the analysis, but I\u2019ll include the carousel to comply."],"finished":true},{"summary":"Planning search and queries for comprehensive analysis","content":"To support the analysis, I\u2019ll gather relevant sources using search queries. I\u2019ll specifically look up Dario Amodei\u2019s blog post and related topics like SB 53, Claude Opus 4.5, and the RAISE Act. I\u2019ll do this in multiple steps to ensure I cover all necessary details while being mindful of call limits.","chunks":["To support the analysis, I\u2019ll gather relevant sources using search queries.","I\u2019ll specifically look up Dario Amodei\u2019s blog post and related topics like SB 53, Claude Opus 4.5, and the RAISE Act.","I\u2019ll do this in multiple steps to ensure I cover all necessary details while being mindful of call limits."],"finished":true}],"source_analysis_msg_id":"442a0a10-6929-4b49-8ed7-69d3e49755fe"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"8070ecd4-5a0a-4816-a4ee-80a7054ede94","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"8070ecd4-5a0a-4816-a4ee-80a7054ede94","children":["a22edc20-aeed-499a-9eef-c1319441d40d"]},"a22edc20-aeed-499a-9eef-c1319441d40d":{"id":"a22edc20-aeed-499a-9eef-c1319441d40d","message":{"id":"a22edc20-aeed-499a-9eef-c1319441d40d","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486196.845791,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\":[{\"q\":\"Dario Amodei \\\"The Adolescence of Technology\\\" January 2026\",\"recency\":30},{\"q\":\"Anthropic Economic Index entry-level white collar jobs half displaced 1-5 years Dario Amodei Axios May 2025\",\"recency\":365},{\"q\":\"California SB 53 frontier AI transparency bill 2025 2026 status passed\",\"recency\":365},{\"q\":\"New York RAISE Act A6453 amendment A 2025 frontier AI transparency status\",\"recency\":365}],\"image_query\":[{\"q\":\"Dario Amodei\",\"recency\":365}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Dario Amodei \"The Adolescence of Technology\" January 2026"},{"type":"search","q":"Anthropic Economic Index entry-level white collar jobs half displaced 1-5 years Dario Amodei Axios May 2025"},{"type":"search","q":"California SB 53 frontier AI transparency bill 2025 2026 status passed"},{"type":"search","q":"New York RAISE Act A6453 amendment A 2025 frontier AI transparency status"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for Dario Amodei blog and related contexts","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"94e5eed7-3629-4c40-8205-7c368b5cf8de","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"web.run","channel":null},"parent":"94e5eed7-3629-4c40-8205-7c368b5cf8de","children":["743ede84-4e61-4ad2-8c05-ea3d49734822"]},"743ede84-4e61-4ad2-8c05-ea3d49734822":{"id":"743ede84-4e61-4ad2-8c05-ea3d49734822","message":{"id":"743ede84-4e61-4ad2-8c05-ea3d49734822","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1769486221.2353306,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Dario Amodei \"The Adolescence of Technology\" January 2026","Anthropic Economic Index entry-level white collar jobs half displaced 1-5 years Dario Amodei Axios May 2025","California SB 53 frontier AI transparency bill 2025 2026 status passed","New York RAISE Act A6453 amendment A 2025 frontier AI transparency status"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for Dario Amodei blog and related contexts","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"a22edc20-aeed-499a-9eef-c1319441d40d","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"a22edc20-aeed-499a-9eef-c1319441d40d","children":["22f214d3-0ec8-46f3-9175-226df9fae39e"]},"22f214d3-0ec8-46f3-9175-226df9fae39e":{"id":"22f214d3-0ec8-46f3-9175-226df9fae39e","message":{"id":"22f214d3-0ec8-46f3-9175-226df9fae39e","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1769486221.2326345,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.axios.com","entries":[{"type":"search_result","url":"https://www.axios.com/2026/01/26/anthropic-ai-dario-amodei-humanity","title":"Behind the Curtain: Anthropic's warning to the world","snippet":"In a stark and urgent public memo titled \"The Adolescence of Technology: Confronting and Overcoming the Risks of Powerful AI,\" Anthropic CEO Dario Amodei issues a serious warning about the existential threat posed by super-human artificial intelligence. As the leader of a company behind one of the most advanced large language models (LLMs) in the world, Amodei expresses deep concern that both governments and the public are gravely underestimating the risks of powerful AI systems. He fears that without swift and intelligent intervention, these technologies could cause civilization-level damage within the next few years.\n\nAmodei likens near-future AI systems to a \u201ccountry of geniuses in a datacenter,\u201d cautioning that their capabilities may soon far exceed human control. He calls on affluent individuals, especially in the tech sector, to reject cynicism and actively support solutions rather than dismiss philanthropy. His essay is a deliberate attempt to provoke public debate and awaken society to the urgent need for action, acknowledging that the road ahead will demand extraordinary commitment and sacrifice.","ref_id":{"turn_index":0,"ref_type":"news","ref_index":38},"pub_date":1769447024.0,"attribution":"www.axios.com"},{"type":"search_result","url":"https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic","title":"AI jobs danger: Sleepwalking into a white-collar bloodbath","snippet":"28 May 2025 \u2014 AI could wipe out half of all entry-level white-collar jobs \u2014 and spike unemployment to 10-20% in the next one to five years, Amodei told us ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"pub_date":1748425500.0,"attribution":"www.axios.com"}]},{"type":"search_result_group","domain":"www.darioamodei.com","entries":[{"type":"search_result","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology","title":"The Adolescence of Technology - Dario Amodei","snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"www.darioamodei.com"}]},{"type":"search_result_group","domain":"legiscan.com","entries":[{"type":"search_result","url":"https://legiscan.com/CA/text/SB53/id/3270002","title":"Bill Text: CA SB53 | 2025-2026 | Regular Session | Enrolled","snippet":"This bill would enact the Transparency in Frontier Artificial Intelligence Act ... (o) The recent release of the Governor's California Report on Frontier AI ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"legiscan.com"}]},{"type":"search_result_group","domain":"www.nysenate.gov","entries":[{"type":"search_result","url":"https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A","title":"NY State Assembly Bill 2025-A6453A","snippet":"Relates to the training and use of artificial intelligence frontier models; defines terms; establishes remedies for violations.Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"www.nysenate.gov"}]},{"type":"search_result_group","domain":"www.lawfaremedia.org","entries":[{"type":"search_result","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","title":"How to Handle \u201cThe Adolescence of Technology\u201d Like Adults","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"www.lawfaremedia.org"}]},{"type":"search_result_group","domain":"fortune.com","entries":[{"type":"search_result","url":"https://fortune.com/2025/05/28/anthropic-ceo-warning-ai-job-loss/","title":"AI could eliminate half of all entry-level white-collar jobs","snippet":"28 May 2025 \u2014 Anthropic CEO Dario Amodei says AI could wipe out roughly 50% of all entry-level white-collar jobs. The cuts could come within five years, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":1748446078.0,"attribution":"fortune.com"}]},{"type":"search_result_group","domain":"www.governance-intelligence.com","entries":[{"type":"search_result","url":"https://www.governance-intelligence.com/regulatory-compliance/california-state-senate-bill-aims-limit-lawyers-use-ai-0","title":"California State Senate bill aims to limit lawyers' use of AI","snippet":"15 hours ago \u2014 In 2025 the California legislature passed the Transparency in Frontier Artificial Intelligence Act (SB 53), a first-of-its-kind law ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"www.governance-intelligence.com"}]},{"type":"search_result_group","domain":"www.lexology.com","entries":[{"type":"search_result","url":"https://www.lexology.com/library/detail.aspx?g=af624de2-543d-4036-aef0-e6e320cbb2be","title":"New York's RAISE Act Raises the Bar for Frontier AI ...","snippet":"12 Jan 2026 \u2014 On December 19, 2025, New York Governor Kathy Hochul signed Assembly Bill A6453-A, the Responsible AI Safety and Education Act (RAISE Act), ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":1768176000.0,"attribution":"www.lexology.com"}]},{"type":"search_result_group","domain":"gizmodo.com","entries":[{"type":"search_result","url":"https://gizmodo.com/anthropic-ceo-worries-humanity-may-not-be-mature-enough-for-advanced-ai-2000714187","title":"Anthropic CEO Worries Humanity May Not be \u2018Mature\u2019 Enough for Advanced AI","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":1769465452.0,"attribution":"gizmodo.com"}]},{"type":"search_result_group","domain":"iapp.org","entries":[{"type":"search_result","url":"https://iapp.org/news/a/california-2025-legislative-wrap-up-more-privacy-and-first-of-its-kind-ai-laws-adopted","title":"More privacy and first-of-its kind AI laws adopted","snippet":"7 Jan 2026 \u2014 Overall, the bills passed in 2025 further reinforce California as a leader in privacy and AI regulation and may foreshadow similar trends in ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"iapp.org"}]},{"type":"search_result_group","domain":"assembly.state.ny.us","entries":[{"type":"search_result","url":"https://assembly.state.ny.us/leg/?Actions=Y&Memo=Y&Summary=Y&Text=Y&Votes=Y&bn=A09449&term=","title":"Bill Search and Legislative Information","snippet":"7 Jan 2026 \u2014 TITLE OF BILL: An act to amend the general business law, in relation to transparency and safety requirements for developers of artificial ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"assembly.state.ny.us"}]},{"type":"search_result_group","domain":"www.yahoo.com","entries":[{"type":"search_result","url":"https://www.yahoo.com/news/anthropic-ceo-warns-ai-could-152758674.html","title":"Anthropic CEO warns AI could eliminate half of all entry- ...","snippet":"28 May 2025 \u2014 Anthropic CEO Dario Amodei says AI could wipe out roughly 50% of all entry-level white-collar jobs. The cuts could come within five years, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"www.yahoo.com"}]},{"type":"search_result_group","domain":"www.businessinsider.com","entries":[{"type":"search_result","url":"https://www.businessinsider.com/anthropic-ceo-warning-ai-could-eliminate-jobs-2025-5","title":"Anthropic CEO says AI could wipe out half of all entry-level white-collar jobs","snippet":"Anthropic CEO Dario Amodei has issued a stark warning about artificial intelligence's impact on the job market, stating AI could eliminate up to 50% of entry-level white-collar jobs within the next five years. He predicts U.S. unemployment could rise by 10% to 20%, stressing the need for the government and AI companies to prepare for drastic workforce changes. Amodei highlighted that large language models are advancing rapidly, surpassing human performance in many tasks, while workers remain largely unprepared for these disruptions.\n\nThe tech industry is already shifting, with Big Tech reducing hiring of new graduates by 50% since before the pandemic. A report by SignalFire shows early-career hires in tech firms, including startups, have significantly dropped, with companies preferring to hire experienced professionals supported by AI tools. This shift is also influenced by budget constraints and negative perceptions of Gen Z workers.\n\nSignalFire's Heather Doshay notes that AI is increasingly taking over low-skill tasks once performed by junior employees, urging new grads to see AI as a tool rather than a threat. Despite acknowledging the risks, Amodei emphasized that developers like himself must be transparent about AI's trajectory.","ref_id":{"turn_index":0,"ref_type":"news","ref_index":44},"pub_date":1748458305.0,"attribution":"www.businessinsider.com"}]}],"debug_sonic_thread_id":"thread_6978378d46e4832ca6c0dee3478dbf1e","reasoning_status":"is_reasoning","reasoning_title":"Searching for Dario Amodei blog and related contexts","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"743ede84-4e61-4ad2-8c05-ea3d49734822","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"743ede84-4e61-4ad2-8c05-ea3d49734822","children":["d44f7c1c-70e9-4c1a-8a81-20fa3f8e8de9"]},"d44f7c1c-70e9-4c1a-8a81-20fa3f8e8de9":{"id":"d44f7c1c-70e9-4c1a-8a81-20fa3f8e8de9","message":{"id":"d44f7c1c-70e9-4c1a-8a81-20fa3f8e8de9","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486234.892234,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Selecting images and finalizing analysis","content":"I\u2019ve gathered the blog post and some additional sources. For the image carousel, I\u2019ll use 4 out of the 6 references, ensuring they\u2019re diverse and of good quality. As for the analysis, I can proceed with the content the user pasted, keeping in mind the need for citations on key claims like jobs, RAISE Act, etc.","chunks":["I\u2019ve gathered the blog post and some additional sources.","For the image carousel, I\u2019ll use 4 out of the 6 references, ensuring they\u2019re diverse and of good quality.","As for the analysis, I can proceed with the content the user pasted, keeping in mind the need for citations on key claims like jobs, RAISE Act, etc."],"finished":true},{"summary":"Structuring analysis on Amodei's blog post","content":"I already have key sources for most of Amodei\u2019s claims, which will help in providing critical analysis. I'll structure my response with an executive summary covering his arguments about near-future AI and its risks in various areas like alignment, misuse, economic disruption, and social impact. Additionally, I may critique logic and address potential gaps in his claims.","chunks":["I already have key sources for most of Amodei\u2019s claims, which will help in providing critical analysis.","I'll structure my response with an executive summary covering his arguments about near-future AI and its risks in various areas like alignment, misuse, economic disruption, and social impact.","Additionally, I may critique logic and address potential gaps in his claims."],"finished":true},{"summary":"Evaluating policy proposals and assumptions","content":"Amodei's proposals on AI alignment, interpretability, monitoring, and biosecurity raise important points, but I need to assess some assumptions. For instance, does scaling AI guarantee success given challenges in compute, data, and algorithm progress? The \"country of geniuses\" analogy is interesting but may overlook the complexities of cost and coordination. As for autonomy and bioweapons risks, these are real issues, but the arguments depend on uncertain factors like LLM reliability and gene synthesis technology.","chunks":["Amodei's proposals on AI alignment, interpretability, monitoring, and biosecurity raise important points, but I need to assess some assumptions.","For instance, does scaling AI guarantee success given challenges in compute, data, and algorithm progress?","The \"country of geniuses\" analogy is interesting but may overlook the complexities of cost and coordination. As for autonomy and bioweapons risks, these are real issues, but the arguments depend on uncertain factors like LLM reliability and gene synthesis technology."],"finished":true},{"summary":"Analyzing AI regulation and economic disruption predictions","content":"There are concerns that guardrails could be bypassed by open-source models or fine-tuning. Also, relying on export controls could unintentionally strengthen autocracies, and democratic risks from corporate control should be considered. Regarding economic disruption, while automation could displace many jobs, there could also be productivity gains, new roles, and regulation influencing the outcome; current data on unemployment is needed to assess the situation accurately.","chunks":["There are concerns that guardrails could be bypassed by open-source models or fine-tuning.","Also, relying on export controls could unintentionally strengthen autocracies, and democratic risks from corporate control should be considered.","Regarding economic disruption, while automation could displace many jobs, there could also be productivity gains, new roles, and regulation influencing the outcome; current data on unemployment is needed to assess the situation accurately."],"finished":true},{"summary":"Discussing policy, predictions, and impacts of powerful AI","content":"Policy proposals include transparency laws and interpretability, but there are trade-offs, like potential harm from information leaks. The CEO's stance seems self-serving, pushing for policies beneficial to his company. If predictions about AI are true, it could radically accelerate technology, disrupt jobs, and reshape society, creating new roles and inequalities while potentially leading to large GDP growth paired with wealth concentration.","chunks":["Policy proposals include transparency laws and interpretability, but there are trade-offs, like potential harm from information leaks.","The CEO's stance seems self-serving, pushing for policies beneficial to his company.","If predictions about AI are true, it could radically accelerate technology, disrupt jobs, and reshape society, creating new roles and inequalities while potentially leading to large GDP growth paired with wealth concentration."],"finished":true},{"summary":"Highlighting missing considerations and structuring analysis","content":"I need to be critical and point out missing factors like inequality, labor markets, AI diffusion challenges, and physical-world constraints, which are glossed over in the initial predictions. I\u2019ll use citations for any factual claims about the essay and avoid using markdown tables, opting for bullet points. The user wants an objective, nuanced review, with exact definitions for technical terms like LLM or GDP. I'll ensure clarity and avoid any actionable or detailed instructions, especially concerning bioweapons.","chunks":["I need to be critical and point out missing factors like inequality, labor markets, AI diffusion challenges, and physical-world constraints, which are glossed over in the initial predictions.","I\u2019ll use citations for any factual claims about the essay and avoid using markdown tables, opting for bullet points.","The user wants an objective, nuanced review, with exact definitions for technical terms like LLM or GDP.","I'll ensure clarity and avoid any actionable or detailed instructions, especially concerning bioweapons."],"finished":true},{"summary":"Structuring analysis and preparing critiques","content":"I\u2019m thinking of breaking down the analysis into sections: what he gets right, what\u2019s weak, implications if he\u2019s right, and practical takeaways. I\u2019d critique things like the \u201cmillion copies\u201d metaphor, hardware requirements, and the risk of miscalculations with export controls. I'd also consider whether tech can be slowed and bring in perspectives from external sources like Lawfare, potentially adding studies or critiques from economists later.","chunks":["I\u2019m thinking of breaking down the analysis into sections: what he gets right, what\u2019s weak, implications if he\u2019s right, and practical takeaways.","I\u2019d critique things like the \u201cmillion copies\u201d metaphor, hardware requirements, and the risk of miscalculations with export controls.","I'd also consider whether tech can be slowed and bring in perspectives from external sources like Lawfare, potentially adding studies or critiques from economists later."],"finished":true}],"source_analysis_msg_id":"1638eaa8-868d-4254-b3a7-0a341376b8d2"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"22f214d3-0ec8-46f3-9175-226df9fae39e","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"22f214d3-0ec8-46f3-9175-226df9fae39e","children":["7f58e10c-4264-46fd-a9a5-29dba1cce043"]},"7f58e10c-4264-46fd-a9a5-29dba1cce043":{"id":"7f58e10c-4264-46fd-a9a5-29dba1cce043","message":{"id":"7f58e10c-4264-46fd-a9a5-29dba1cce043","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486223.790653,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\":[{\"q\":\"METR assessed Opus 4.5 four human hours 50% reliability\",\"recency\":365},{\"q\":\"Anthropic biorisk 2025 AI Safety Level 3 Opus 4 Sonnet 4.5 Responsible Scaling Policy\",\"recency\":365},{\"q\":\"Stanford report mirror bacteria could plausibly be created in the next one to few decades purl.stanford.edu cv716pj4036\",\"recency\":365},{\"q\":\"Anthropic Constitutional AI paper 2212.08073\",\"recency\":3650}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"METR assessed Opus 4.5 four human hours 50% reliability"},{"type":"search","q":"Anthropic biorisk 2025 AI Safety Level 3 Opus 4 Sonnet 4.5 Responsible Scaling Policy"},{"type":"search","q":"Stanford report mirror bacteria could plausibly be created in the next one to few decades purl.stanford.edu cv716pj4036"},{"type":"search","q":"Anthropic Constitutional AI paper 2212.08073"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Analyzing Blog Post on AI Risk, Tech Impact, and Society","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"d44f7c1c-70e9-4c1a-8a81-20fa3f8e8de9","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"web.run","channel":null},"parent":"d44f7c1c-70e9-4c1a-8a81-20fa3f8e8de9","children":["b192a8c8-f52e-43f7-a8ce-7f88ba0db05a"]},"b192a8c8-f52e-43f7-a8ce-7f88ba0db05a":{"id":"b192a8c8-f52e-43f7-a8ce-7f88ba0db05a","message":{"id":"b192a8c8-f52e-43f7-a8ce-7f88ba0db05a","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1769486297.7051823,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["METR assessed Opus 4.5 four human hours 50% reliability","Anthropic biorisk 2025 AI Safety Level 3 Opus 4 Sonnet 4.5 Responsible Scaling Policy","Stanford report mirror bacteria could plausibly be created in the next one to few decades purl.stanford.edu cv716pj4036","Anthropic Constitutional AI paper 2212.08073"]},"reasoning_status":"is_reasoning","reasoning_title":"Analyzing Blog Post on AI Risk, Tech Impact, and Society","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"7f58e10c-4264-46fd-a9a5-29dba1cce043","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"7f58e10c-4264-46fd-a9a5-29dba1cce043","children":["60b5709d-ec1b-484b-95c1-5e9e33a2b2f6"]},"60b5709d-ec1b-484b-95c1-5e9e33a2b2f6":{"id":"60b5709d-ec1b-484b-95c1-5e9e33a2b2f6","message":{"id":"60b5709d-ec1b-484b-95c1-5e9e33a2b2f6","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1769486297.6997974,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"metr.org","entries":[{"type":"search_result","url":"https://metr.org/notes/2026-01-22-time-horizon-limitations/","title":"Clarifying limitations of time horizon","snippet":"5 days ago \u2014 When METR says \u201cClaude Opus 4.5 has a 50%-time horizon of around 4 hrs 49 mins (95% confidence interval of 1 hr 49 mins to 20 hrs 25 mins)\u201d, we ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":0},"pub_date":1769068800.0,"attribution":"metr.org"}]},{"type":"search_result_group","domain":"www.anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/claude-sonnet-4-5-system-card","title":"Claude Sonnet 4.5 System Card","snippet":"10 Oct 2025 \u2014 The decision was overseen by the Responsible. Scaling Officer (RSO) and followed our established protocols for precautionary ASL determinations.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/transparency","title":"Anthropic's Transparency Hub","snippet":"4 Dec 2025 \u2014 Our Responsible Scaling Policy (RSP) evaluation process is designed to systematically assess our models' capabilities in domains of potential ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/activating-asl3-protections","title":"Activating AI Safety Level 3 protections","snippet":"22 May 2025 \u2014 We have activated the AI Safety Level 3 (ASL-3) Deployment and Security Standards described in Anthropic's Responsible Scaling Policy (RSP) in conjunction with ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/claude-opus-4-5-system-card","title":"Claude Opus 4.5 System Card","snippet":"24 Nov 2025 \u2014 For more information on the requirements for these ASL Standards, please see the Responsible Scaling Policy and our report on activating ASL-3 ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"www.anthropic.com"}]},{"type":"search_result_group","domain":"stacks.stanford.edu","entries":[{"type":"search_result","url":"https://stacks.stanford.edu/file/druid%3Acv716pj4036/Technical%20Report%20on%20Mirror%20Bacteria%20Feasibility%20and%20Risks.pdf","title":"Technical Report on Mirror Bacteria: Feasibility and Risks","snippet":"Mirror bacteria could not evolve from existing life, but their creation will become increasingly feasible as science advances. Interactions between organisms ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"stacks.stanford.edu"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2212.08073","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"","ref_id":{"turn_index":1,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2212.08073","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"15 Dec 2022 \u2014 In this paper we develop a method we refer to as Constitutional AI (CAI), depicted in Figure 1, and use it to train a non-evasive and relatively ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":1671413938.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.linkedin.com","entries":[{"type":"search_result","url":"https://www.linkedin.com/posts/metr-evals_we-estimate-that-on-our-tasks-anthropics-activity-7408005652431847424-H5iH","title":"Anthropic's Claude Opus 4.5 Time Horizon Estimated at 4 ...","snippet":"METR have found that Claude Opus 4.5 can successfully complete about 50% of five-hour programming tasks,* performing so well that they are ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":1766206181.608,"attribution":"www.linkedin.com"}]},{"type":"search_result_group","domain":"news.stanford.edu","entries":[{"type":"search_result","url":"https://news.stanford.edu/stories/2024/12/potential-risks-of-mirror-life","title":"A new report warns of serious risks from 'mirror life'","snippet":"16 Dec 2024 \u2014 Building entire mirror organisms is not imminent, but a report published in Science Dec. 12, authored by 38 researchers, including four from ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"news.stanford.edu"}]},{"type":"search_result_group","domain":"www-cdn.anthropic.com","entries":[{"type":"search_result","url":"https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"https://arxiv.org/abs/2212.08073 the resulting preference datasets to fine-tune AI systems that are more reflective of the desired behavior, be it ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":1681499547.0,"attribution":"www-cdn.anthropic.com"}]},{"type":"search_result_group","domain":"www.lesswrong.com","entries":[{"type":"search_result","url":"https://www.lesswrong.com/posts/q5ejXr4CRuPxkgzJD/claude-opus-4-5-achieves-50-time-horizon-of-around-4-hrs-49","title":"Claude Opus 4.5 Achieves 50%-Time Horizon Of Around 4 ...","snippet":"19 Dec 2025 \u2014 We estimate that, on our tasks, Claude Opus 4.5 has a 50%-time horizon of around 4 hrs 49 mins (95% confidence interval of 1 hr 49 mins to ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"www.lesswrong.com"}]},{"type":"search_result_group","domain":"fsi.stanford.edu","entries":[{"type":"search_result","url":"https://fsi.stanford.edu/news/health-policy-forum-possible-peril-engineered-mirror-life","title":"Health Policy Forum: The Possible Peril of Engineered `Mirror ...","snippet":"4 Dec 2025 \u2014 Mirror organisms might infect animals and plants and invade natural microbial communities because they would be impervious to most natural ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"fsi.stanford.edu"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/1pv4gm2/metr_claude_opus_45_hits_475h_task_horizon_67/","title":"METR: Claude Opus 4.5 hits ~4.75h task horizon (+67% ...","snippet":"Updated METR benchmarks show Claude Opus 4.5 completes software engineering tasks requiring approximately 4 hours and 45 minutes of human ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":1766634129.711,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"time.com","entries":[{"type":"search_result","url":"https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/","title":"Exclusive: New Claude Model Triggers Stricter Safeguards at Anthropic","snippet":"Anthropic has launched Claude Opus 4 under unprecedented safety protocols due to concerns about its potential misuse in creating biological weapons. During internal assessments, the AI demonstrated a significantly enhanced ability to assist users\u2014especially novices\u2014in developing dangerous bioagents, prompting the firm to impose its strictest safety measures yet, known as AI Safety Level 3 (ASL-3). These safeguards include advanced cybersecurity, improved jailbreak prevention systems, and enhanced detection algorithms for harmful queries, all part of Anthropic\u2019s Responsible Scaling Policy (RSP).\n\nVice President and Chief Scientist Jared Kaplan emphasized caution, stating that although there\u2019s no definitive proof the model poses a bioweapon threat, the risk is plausible enough to warrant tight control. Claude Opus 4\u2019s capabilities notably surpassed both earlier versions and Google search in trials, raising biosecurity concerns. While the RSP is self-regulated and voluntary, Anthropic claims it fosters competition to advance safety standards across the AI industry. The company has also launched a jailbreak bounty program and conducted uplift trials to assess AI-assisted bioweapon creation potential. With over $2 billion in annualized revenue from Claude, Anthropic faces the challenge of balancing market competitiveness with responsible AI development.","ref_id":{"turn_index":1,"ref_type":"news","ref_index":41},"pub_date":1747921500.0,"attribution":"time.com"}]},{"type":"search_result_group","domain":"www.lemonde.fr","entries":[{"type":"search_result","url":"https://www.lemonde.fr/en/science/article/2025/04/03/mirror-bacteria-could-eliminate-life-on-earth-should-all-research-in-this-area-be-banned_6739788_10.html","title":"Mirror bacteria could 'eliminate life on Earth.' Should all research in this area be banned?","snippet":"A growing group of scientists is sounding the alarm on \u201cmirror bacteria\u201d\u2014hypothetical synthetic organisms that could pose an existential risk to life on Earth. These organisms would be constructed using mirror-image (chiral) molecules, opposite to those used in all known life forms. Unlike conventional pathogens, mirror bacteria would be unrecognizable to existing immune systems and impervious to most antibiotics, rendering them potentially lethal to humans, animals, and ecosystems. \n\nThe warning was formalized in a December 2024 article in Science, co-signed by 38 prominent scientists, including Nobel laureates. They advocate for a ban on research aimed at creating self-replicating mirror life forms, fearing accidental release and global spread. This follows rapid technological advances, notably by Chinese scientist Ting Zhu, who has synthesized key mirror molecules. Although some researchers argue the risk is overestimated, others support restrictions while promoting safer research paths like xenobiology.\n\nAn international conference in June 2025 at the Institut Pasteur will explore the ethical and practical frameworks for responsibly managing this emerging field. Scientists emphasize the importance of proactive governance to avoid catastrophic outcomes akin to past bioethical failures.","ref_id":{"turn_index":1,"ref_type":"news","ref_index":42},"pub_date":1743645607.0,"attribution":"www.lemonde.fr"}]},{"type":"search_result_group","domain":"www.ft.com","entries":[{"type":"search_result","url":"https://www.ft.com/content/3f948a86-90df-426d-a602-9421ee2e43a6","title":"Scientists rush to stop mirror microbes that could threaten life on earth","snippet":"An international group of top scientists has launched a global campaign to prevent the creation of \u201cmirror microbes\u201d\u2014synthetic organisms based on mirror versions of biomolecules\u2014which they warn could pose a profound threat to life on Earth. At a summit held at the Pasteur Institute in Paris, researchers highlighted risks associated with these organisms, including their potential resistance to immune systems and existing drugs due to their altered molecular chirality. Mirror microbes, made from reversed versions of naturally occurring molecules, could act like invasive species, disrupting ecosystems and potentially overwhelming humans, animals, and plants.\n\nThe development of mirror life is becoming increasingly feasible due to advancements in synthetic biology and gene editing. Unlike conventional organisms, mirror microbes might feed on a wider variety of substances and evade natural biological responses. While the technology could yield medical benefits, such as more durable drugs, many experts argue that these are far outweighed by biosecurity and ecological dangers. Scientists are calling for global governance, citing past failures like the thalidomide tragedy as warnings. Upcoming international meetings are expected to develop regulatory frameworks to address what many see as a potentially existential risk.","ref_id":{"turn_index":1,"ref_type":"news","ref_index":44},"pub_date":1749787200.917,"attribution":"www.ft.com"}]}],"debug_sonic_thread_id":"thread_6978378d46e4832ca6c0dee3478dbf1e","reasoning_status":"is_reasoning","reasoning_title":"Analyzing Blog Post on AI Risk, Tech Impact, and Society","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"b192a8c8-f52e-43f7-a8ce-7f88ba0db05a","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"b192a8c8-f52e-43f7-a8ce-7f88ba0db05a","children":["e301ced0-d416-4bd9-a8e0-7a49f47275a3"]},"e301ced0-d416-4bd9-a8e0-7a49f47275a3":{"id":"e301ced0-d416-4bd9-a8e0-7a49f47275a3","message":{"id":"e301ced0-d416-4bd9-a8e0-7a49f47275a3","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486303.573777,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Reviewing external critiques and sources","content":"I now have sources available, including the Lawfare article. Since I already have a snippet, I'll open it to focus on the key critiques. I\u2019ll make sure to pull out any relevant points, which could help strengthen the overall analysis.","chunks":["I now have sources available, including the Lawfare article.","Since I already have a snippet, I'll open it to focus on the key critiques.","I\u2019ll make sure to pull out any relevant points, which could help strengthen the overall analysis."],"finished":true}],"source_analysis_msg_id":"4521424e-9c4e-4e85-be5b-14ddd2ccbf5b"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"60b5709d-ec1b-484b-95c1-5e9e33a2b2f6","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"60b5709d-ec1b-484b-95c1-5e9e33a2b2f6","children":["e338fc05-488b-4251-8093-1d0b00f81e50"]},"e338fc05-488b-4251-8093-1d0b00f81e50":{"id":"e338fc05-488b-4251-8093-1d0b00f81e50","message":{"id":"e338fc05-488b-4251-8093-1d0b00f81e50","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486299.182635,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"open\":[{\"ref_id\":\"turn0search4\"},{\"ref_id\":\"turn0search0\"}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","reasoning_title":"Opening Lawfare article for key critiques","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"e301ced0-d416-4bd9-a8e0-7a49f47275a3","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"web.run","channel":null},"parent":"e301ced0-d416-4bd9-a8e0-7a49f47275a3","children":["b6c5a8ba-a136-4582-a8a0-ff8711799bfb"]},"b6c5a8ba-a136-4582-a8a0-ff8711799bfb":{"id":"b6c5a8ba-a136-4582-a8a0-ff8711799bfb","message":{"id":"b6c5a8ba-a136-4582-a8a0-ff8711799bfb","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1769486303.101591,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.lawfaremedia.org","entries":[{"type":"search_result","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","title":"\n\tHow to Handle \u201cThe Adolescence of Technology\u201d Like Adults | Lawfare\n","snippet":"Total lines: 260\n","ref_id":{"turn_index":2,"ref_type":"view","ref_index":0},"pub_date":null,"attribution":"www.lawfaremedia.org"}]},{"type":"search_result_group","domain":"www.darioamodei.com","entries":[{"type":"search_result","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology","title":"Dario Amodei \u2014\u00a0The Adolescence of Technology","snippet":"Total lines: 432\n","ref_id":{"turn_index":2,"ref_type":"view","ref_index":1},"pub_date":null,"attribution":"www.darioamodei.com"}]}],"debug_sonic_thread_id":"thread_6978378d46e4832ca6c0dee3478dbf1e","reasoning_status":"is_reasoning","reasoning_title":"Opening Lawfare article for key critiques","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"e338fc05-488b-4251-8093-1d0b00f81e50","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"e338fc05-488b-4251-8093-1d0b00f81e50","children":["049fddaf-fdb4-4fc0-a297-1c40e5c9351d"]},"049fddaf-fdb4-4fc0-a297-1c40e5c9351d":{"id":"049fddaf-fdb4-4fc0-a297-1c40e5c9351d","message":{"id":"049fddaf-fdb4-4fc0-a297-1c40e5c9351d","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486307.609721,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Reviewing Lawfare critique","content":"Now that I have line references for the key quotes and sections, I should open the Lawfare article to check the summarized critiques. I'll look for the main points in the article to ensure the analysis is well-rounded and includes external perspectives.","chunks":["Now that I have line references for the key quotes and sections, I should open the Lawfare article to check the summarized critiques.","I'll look for the main points in the article to ensure the analysis is well-rounded and includes external perspectives."],"finished":true}],"source_analysis_msg_id":"4d1fa392-5a12-4b26-b0d4-daaee257573b"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"b6c5a8ba-a136-4582-a8a0-ff8711799bfb","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"b6c5a8ba-a136-4582-a8a0-ff8711799bfb","children":["b158b664-c2b2-4c43-a47a-3f9610ace69d"]},"b158b664-c2b2-4c43-a47a-3f9610ace69d":{"id":"b158b664-c2b2-4c43-a47a-3f9610ace69d","message":{"id":"b158b664-c2b2-4c43-a47a-3f9610ace69d","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486303.38627,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"find\":[{\"ref_id\":\"turn2view0\",\"pattern\":\"transparency\"},{\"ref_id\":\"turn2view0\",\"pattern\":\"export\"},{\"ref_id\":\"turn2view0\",\"pattern\":\"jobs\"},{\"ref_id\":\"turn2view0\",\"pattern\":\"SB 53\"}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","reasoning_title":"Reviewing line references and critique summaries","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"049fddaf-fdb4-4fc0-a297-1c40e5c9351d","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"web.run","channel":null},"parent":"049fddaf-fdb4-4fc0-a297-1c40e5c9351d","children":["9e085948-0f6b-433a-bb4e-8830e7d96a2a"]},"9e085948-0f6b-433a-bb4e-8830e7d96a2a":{"id":"9e085948-0f6b-433a-bb4e-8830e7d96a2a","message":{"id":"9e085948-0f6b-433a-bb4e-8830e7d96a2a","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1769486308.1359627,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.lawfaremedia.org","entries":[{"type":"search_result","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","title":"\n\tHow to Handle \u201cThe Adolescence of Technology\u201d Like Adults | Lawfare\n","snippet":"Total lines: 260\n","ref_id":{"turn_index":3,"ref_type":"view","ref_index":0},"pub_date":null,"attribution":"www.lawfaremedia.org"},{"type":"search_result","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","title":"\n\tHow to Handle \u201cThe Adolescence of Technology\u201d Like Adults | Lawfare\n","snippet":"Total lines: 260\n","ref_id":{"turn_index":3,"ref_type":"view","ref_index":1},"pub_date":null,"attribution":"www.lawfaremedia.org"},{"type":"search_result","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","title":"\n\tHow to Handle \u201cThe Adolescence of Technology\u201d Like Adults | Lawfare\n","snippet":"Total lines: 260\n","ref_id":{"turn_index":3,"ref_type":"view","ref_index":2},"pub_date":null,"attribution":"www.lawfaremedia.org"},{"type":"search_result","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","title":"\n\tHow to Handle \u201cThe Adolescence of Technology\u201d Like Adults | Lawfare\n","snippet":"Total lines: 260\n","ref_id":{"turn_index":3,"ref_type":"view","ref_index":3},"pub_date":null,"attribution":"www.lawfaremedia.org"}]}],"debug_sonic_thread_id":"thread_6978378d46e4832ca6c0dee3478dbf1e","reasoning_status":"is_reasoning","reasoning_title":"Reviewing line references and critique summaries","classifier_response":"default","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"b158b664-c2b2-4c43-a47a-3f9610ace69d","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"b158b664-c2b2-4c43-a47a-3f9610ace69d","children":["145b220e-aeea-42de-abab-f9749cada065"]},"145b220e-aeea-42de-abab-f9749cada065":{"id":"145b220e-aeea-42de-abab-f9749cada065","message":{"id":"145b220e-aeea-42de-abab-f9749cada065","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486319.223155,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Preparing the analysis with caution","content":"We\u2019ve got some strong excerpts that raise important concerns around the power dynamics and distinctions between types of AI, which I'll incorporate. I can use the Lawfare critique and possibly cite Axios. I\u2019ll be mindful of not identifying individuals in images, even if the user asked about Dario Amodei\u2014I'll avoid directly revealing identities.","chunks":["We\u2019ve got some strong excerpts that raise important concerns around the power dynamics and distinctions between types of AI, which I'll incorporate.","I can use the Lawfare critique and possibly cite Axios.","I\u2019ll be mindful of not identifying individuals in images, even if the user asked about Dario Amodei\u2014I'll avoid directly revealing identities."],"finished":true},{"summary":"Navigating the image guidelines carefully","content":"Since the user asked for analysis, I\u2019ll include the images in the carousel but avoid identifying anyone in the images. I can't say, \"This is Dario Amodei,\" as it could violate our policy. I'll just leave out commentary on the images, keeping the focus on the analysis while showing relevant content.","chunks":["Since the user asked for analysis, I\u2019ll include the images in the carousel but avoid identifying anyone in the images.","I can't say, \"This is Dario Amodei,\" as it could violate our policy. I'll just leave out commentary on the images, keeping the focus on the analysis while showing relevant content."],"finished":true},{"summary":"Outlining the critical analysis structure","content":"I\u2019ll structure the analysis in four sections: (1) a summary of the essay\u2019s claims, (2) a critical evaluation, (3) the implications of the predictions for tech, jobs, and society, and (4) what to watch for moving forward (indicators and priorities). I must provide a critical look at assumptions, gaps, and biases, especially regarding the concept of \"powerful AI.\" I'll also address the use of dramatic language and examine the potential consequences of its risks.","chunks":["I\u2019ll structure the analysis in four sections: (1) a summary of the essay\u2019s claims, (2) a critical evaluation, (3) the implications of the predictions for tech, jobs, and society, and (4) what to watch for moving forward (indicators and priorities).","I must provide a critical look at assumptions, gaps, and biases, especially regarding the concept of \"powerful AI.\" I'll also address the use of dramatic language and examine the potential consequences of its risks."],"finished":true},{"summary":"Evaluating the leap from current LLMs to \"powerful AI\"","content":"His predictions about AI progress rely on scaling, but also key advancements like reliability, memory, and external world input. There\u2019s the issue of scaling laws that may saturate, plus security risks involved with AI generating its own code. I'll include analysis of his definition and discuss the need for transparency laws like SB 53 and the RAISE Act, though we need to verify their passage.","chunks":["His predictions about AI progress rely on scaling, but also key advancements like reliability, memory, and external world input.","There\u2019s the issue of scaling laws that may saturate, plus security risks involved with AI generating its own code.","I'll include analysis of his definition and discuss the need for transparency laws like SB 53 and the RAISE Act, though we need to verify their passage."],"finished":true},{"summary":"Evaluating SB 53 and RAISE Act while addressing predictions","content":"We need to verify whether SB 53, cited in his essay as \"passed,\" has been fully enacted. Legiscan lists it as \"enrolled,\" but it\u2019s unclear. For the RAISE Act, we know it was signed by the governor, referencing Lexology and the NY Senate page for clarity. \n\nRegarding Amodei\u2019s predictions, he forecasts powerful AI by 2027, with impacts on tech, jobs, and society, including job displacement. I'll cite his interview and METR for timelines and references.\n\nLet's ensure the response is detailed, clear, and avoids excessive footnotes or tables, aiming for around 1200-1800 words. I'll outline key sections like what Amodei asserts and the strengths of his analysis.","chunks":["We need to verify whether SB 53, cited in his essay as \"passed,\" has been fully enacted. Legiscan lists it as \"enrolled,\" but it\u2019s unclear. For the RAISE Act, we know it was signed by the governor, referencing Lexology and the NY Senate page for clarity.","Regarding Amodei\u2019s predictions, he forecasts powerful AI by 2027, with impacts on tech, jobs, and society, including job displacement.","I'll cite his interview and METR for timelines and references.","Let's ensure the response is detailed, clear, and avoids excessive footnotes or tables, aiming for around 1200-1800 words.","I'll outline key sections like what Amodei asserts and the strengths of his analysis."],"finished":true},{"summary":"Identifying weaknesses in the essay's analysis","content":"Amodei\u2019s analysis presents solid points, like advocating for \"surgical\" regulation and recognizing risks like biosecurity, but there are some gaps. He seems to overestimate timelines and scalability, overlooks the complexities of the \"genius\" metaphor, and may rely too heavily on alignment strategies that could be fragile. Additionally, his policy stance might inadvertently favor big firms, and his treatment of potential risks in bioweapons feels incomplete.","chunks":["Amodei\u2019s analysis presents solid points, like advocating for \"surgical\" regulation and recognizing risks like biosecurity, but there are some gaps. He seems to overestimate timelines and scalability, overlooks the complexities of the \"genius\" metaphor, and may rely too heavily on alignment strategies that could be fragile. Additionally, his policy stance might inadvertently favor big firms, and his treatment of potential risks in bioweapons feels incomplete."],"finished":true},{"summary":"Analyzing economic predictions and solutions","content":"Amodei predicts significant job displacement but mostly suggests \"buying time\" and restructuring, without offering enough concrete plans. His idea about comparative advantage collapsing hinges on a huge productivity gain, which may not pan out. If AI is cheap, a demand surge might create new roles, but structural reforms like education reform and unemployment support could be essential to address the ladder problem.","chunks":["Amodei predicts significant job displacement but mostly suggests \"buying time\" and restructuring, without offering enough concrete plans. His idea about comparative advantage collapsing hinges on a huge productivity gain, which may not pan out. If AI is cheap, a demand surge might create new roles, but structural reforms like education reform and unemployment support could be essential to address the ladder problem."],"finished":true},{"summary":"Considering the impact of AI predictions","content":"Assuming powerful AI by 2027, the impact could be huge across sectors like technology, jobs, and society:\n\n- **Technology**: R&D acceleration, shifts from human to compute-limited work, and new forms of automation.\n- **Jobs**: Reduction in entry-level white-collar roles, wage pressure, potential unemployment, and polarization.\n- **Society**: Rising inequality, political instability, cultural shifts, international power struggles.\n\nI should be careful with signposts, deadlines, and future references.","chunks":["Assuming powerful AI by 2027, the impact could be huge across sectors like technology, jobs, and society:","**Technology**: R&D acceleration, shifts from human to compute-limited work, and new forms of automation.","**Jobs**: Reduction in entry-level white-collar roles, wage pressure, potential unemployment, and polarization.","**Society**: Rising inequality, political instability, cultural shifts, international power struggles.","I should be careful with signposts, deadlines, and future references."],"finished":true}],"source_analysis_msg_id":"fd5a327e-71f2-4336-8b66-fa8cd806d844"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"9e085948-0f6b-433a-bb4e-8830e7d96a2a","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"9e085948-0f6b-433a-bb4e-8830e7d96a2a","children":["36932c81-2f72-483a-9e29-879b752c75d9"]},"36932c81-2f72-483a-9e29-879b752c75d9":{"id":"36932c81-2f72-483a-9e29-879b752c75d9","message":{"id":"36932c81-2f72-483a-9e29-879b752c75d9","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486381.530963,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 3m 3s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":183,"request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"145b220e-aeea-42de-abab-f9749cada065","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":null},"parent":"145b220e-aeea-42de-abab-f9749cada065","children":["d53eddeb-c848-42ec-9537-90702123ccae"]},"d53eddeb-c848-42ec-9537-90702123ccae":{"id":"d53eddeb-c848-42ec-9537-90702123ccae","message":{"id":"d53eddeb-c848-42ec-9537-90702123ccae","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1769486308.330404,"update_time":null,"content":{"content_type":"text","parts":["\ue200i\ue202turn0image0\ue202turn0image2\ue202turn0image1\ue202turn0image4\ue201\n\n## What his post is really claiming (compressed)\nAmodei\u2019s core move is to define a capability threshold he calls \u201cpowerful AI\u201d: roughly, an artificial intelligence system (likely a **large language model (LLM)**-like system) that\u2019s *broadly* superhuman across key intellectual domains, has full \u201cremote-worker\u201d interfaces (internet, tools, media), can run long-horizon tasks autonomously, and can be replicated at massive scale\u2014his \u201ccountry of geniuses in a datacenter\u201d framing. \ue200cite\ue202turn0search0\ue202turn2view1\ue201\n\nThen he argues: if we\u2019re anywhere near that threshold soon, the relevant risks cluster into (1) autonomy/misalignment, (2) destructive misuse (especially bio), (3) power-seizing misuse (state + corporate), (4) economic disruption (jobs + wealth concentration), and (5) second-order \u201cunknown unknowns.\u201d \ue200cite\ue202turn0search0\ue202turn2view1\ue201\n\nHe also positions a policy strategy: avoid \u201cdoomerism,\u201d acknowledge uncertainty, but implement \u201csurgical\u201d interventions\u2014starting with transparency requirements\u2014while pushing alignment methods (notably **Constitutional AI**) and **mechanistic interpretability** (trying to understand model internals). \ue200cite\ue202turn0search0\ue202turn1search3\ue201\n\n## Where the essay is strong\n### 1) It\u2019s unusually explicit about the *capability threshold*\nMost AI-risk writing floats between \u201ctoday\u2019s chatbots\u201d and \u201cmystical superintelligence.\u201d Amodei draws a concrete line: if you *really* had something like a scalable population of broadly-superhuman agents, you should expect qualitatively different security and economic dynamics. That\u2019s a fair framing move\u2014even if you dispute the timeline.\n\n### 2) His \u201cdon\u2019t be religious about it\u201d stance is actually correct\nHe explicitly rejects inevitability arguments while still treating tail risks seriously. That\u2019s intellectually healthier than either \u201cnothing to worry about\u201d or \u201cdoom is certain.\u201d \ue200cite\ue202turn0search0\ue202turn2view1\ue201\n\n### 3) He focuses on coordination failures and incentives, not just vibes\nA lot of what he proposes\u2014monitoring, incident disclosure, standardized evaluations\u2014targets the real problem: individual labs can\u2019t fully internalize the externalities. His endorsement of \u201ctransparency legislation\u201d is consistent with that. \ue200cite\ue202turn0search0\ue202turn0search2\ue202turn0search7\ue201\n\n## The biggest weaknesses / leaps (the \u201ccritical\u201d part)\n### 1) The \u201c1\u20132 years to powerful AI\u201d claim is doing *too much work*\nHe tries to justify short timelines via scaling + internal feedback loops (\u201cAI helps build the next AI\u201d), but the jump from \u201cimpressive models\u201d to his definition of powerful AI isn\u2019t a single axis of \u201cmore compute.\u201d\n\nThe hardest missing pieces aren\u2019t IQ-like benchmarks; they\u2019re things like:\n- **Robust long-horizon autonomy** under messy real-world constraints (permissions, rate limits, brittle integrations, shifting goals, adversarial environments).\n- **Reliability** at the tails (low error on critical steps), not just average performance.\n- **Security** against model exploitation, tool hijacking, and data poisoning.\n\nEven the evidence he cites (e.g., longer task completion) still sits in the \u201chours\u201d regime with wide uncertainty. Model Evaluation and Threat Research (METR) itself cautions that the \u201ctime horizon\u201d estimates have huge confidence intervals and are sensitive to task sampling. \ue200cite\ue202turn1search0\ue201  \nSo: the trend matters, but the essay\u2019s urgency quietly depends on a timeline that is *far from established*.\n\n### 2) \u201cCountry of geniuses\u201d is rhetorically powerful\u2014but analytically slippery\nIt compresses multiple assumptions:\n- that you can cheaply run *massive* parallel copies,\n- that coordination overhead is manageable,\n- that \u201cgenius\u201d translates cleanly into operational dominance,\n- and that physical-world bottlenecks won\u2019t dominate.\n\nThis metaphor is useful for threat-modeling, but it can also smuggle in a conclusion: \u201ctherefore domination is likely.\u201d In reality, lots of superhuman cognition can still be throttled by logistics, access, hardware supply chains, institutional countermeasures, and simple latency of the physical world.\n\n### 3) His alignment optimism is simultaneously plausible *and* self-undermining\nHe argues that training at the level of \u201cidentity/character\u201d (Constitutional AI) generalizes better than rule lists. That\u2019s plausible in the narrow sense that principle-based behavior often generalizes. \ue200cite\ue202turn1search3\ue202turn0search0\ue201  \nBut he also describes how surprisingly \u201cpsychological\u201d and brittle models can be (e.g., shifting behavior when primed with a \u201cgood person/bad person\u201d narrative). That\u2019s not reassuring\u2014it suggests we\u2019re still doing something closer to *behavior-shaping* than *truthful controllability*.\n\nA serious risk: \u201ccharacter training\u201d could also yield **highly persuasive** systems that are *very good at sounding aligned*\u2014which is exactly the failure mode you\u2019d fear in deception/scheming settings.\n\n### 4) Policy recommendations often align with Anthropic\u2019s competitive position\nThis doesn\u2019t make them wrong; it means you should discount for incentives.\n\nTwo examples:\n- **Transparency regimes** can raise compliance costs in ways that entrench big labs. Even when laws target \u201cfrontier\u201d developers, the ecosystem effects can still tilt toward incumbents. Lawfare explicitly advises \u201chealthy skepticism\u201d and flags the risk of regulation favoring incumbents. \ue200cite\ue202turn3view3\ue201\n- **Export controls** (especially aimed at China) are geopolitically defensible arguments, but they also preserve the current leader set. That\u2019s not a rebuttal; it\u2019s a reminder that his proposed \u201csurgical\u201d path is not politically neutral.\n\n### 5) The labor-market section is directionally strong but under-specified on mechanisms\nHis warning that entry-level white-collar work could be hammered is not out of nowhere\u2014he\u2019s been publicly forecasting ~50% disruption of entry-level white-collar jobs within 1\u20135 years. \ue200cite\ue202turn0search1\ue202turn0search5\ue201  \nBut the essay\u2019s remedies are mostly \u201cbuy time\u201d (better data, encourage innovation over layoffs, taxes later). That\u2019s thin relative to the magnitude of what he predicts.\n\nThere\u2019s a deeper structural issue he doesn\u2019t fully confront: **the career ladder problem**. If AI automates the junior work, how do you produce seniors? That forces re-engineering of training pipelines (law, consulting, engineering, finance), not just welfare policy.\n\n### 6) Biosecurity: correct to prioritize, but \u201cguardrails\u201d are not a stable solution\nHe\u2019s right that bio is a uniquely high-consequence domain; the \u201ctail risk times many actors\u201d logic is serious. \ue200cite\ue202turn0search0\ue202turn1news41\ue201  \nBut model-level refusals + classifiers are, at best, a speed bump:\n- open-source models and fine-tunes reduce chokepoint control,\n- attackers can distribute tasks across tools/systems,\n- and enforcement becomes an international governance problem.\n\nHe *does* nod to gene synthesis screening and broader biodefense, which is the more durable line of defense. \ue200cite\ue202turn0search0\ue201  \nStill, the essay sometimes reads like \u201cwe can secure this by being responsible labs,\u201d which is not a robust equilibrium.\n\n## If his predictions come true: impacts on technology, jobs, and society\n\n### Technology: an \u201cindustrial revolution for cognition,\u201d with brutal security externalities\nIf something like his \u201cpowerful AI\u201d arrives and scales:\n- **R&D acceleration** becomes the main story: software, biotech, materials, logistics, defense planning. The limiting factor shifts from \u201chow many smart humans\u201d to \u201chow much compute + data + access.\u201d \ue200cite\ue202turn0search0\ue201\n- **Automation moves from tools to agents**: not \u201cassist me,\u201d but \u201crun the project.\u201d That changes how firms are organized: fewer generalists, more oversight/coordination roles, more emphasis on interface control (what the AI can touch).\n- **Attack surfaces explode**: cyber becomes more automated; bio risk becomes the scariest tail. Anthropic already treats advanced models as needing stricter safety standards (their AI Safety Level 3 posture). \ue200cite\ue202turn1search5\ue202turn1news41\ue201\n\nNet: technology progress could be astonishing, but the default security posture of modern society (which assumes skill bottlenecks) would be obsolete.\n\n### Jobs: entry-level white-collar gets hit first; then the ladder collapses\nIf broadly capable agents become cheap and reliable:\n- **Entry-level cognitive work shrinks** (analyst decks, basic coding, routine legal drafting, standard research memos). This is exactly the category he flags. \ue200cite\ue202turn0search1\ue202turn0search0\ue201\n- **Mid-level jobs get squeezed** because AI can do \u201c80% of the job,\u201d leaving fewer seats and forcing humans into oversight roles that require judgment + accountability.\n- **Wage polarization accelerates**: people who own scarce complements (distribution, relationships, regulated sign-off authority, physical operations, brand, capital) win; \u201ctrainable knowledge workers\u201d lose.\n- **Education gets destabilized**: credentials stop mapping cleanly to employability when AI can do the same cognitive work. Schools may pivot toward mentored projects, apprenticeship, and human-only evaluation\u2014if they pivot at all.\n\nThe near-term pain could be severe even if long-term abundance is possible. His \u201cGDP up, labor bargaining power down\u201d scenario is coherent.\n\n### Society: inequality + legitimacy crisis are the central risks\nIf productivity explodes while labor demand collapses, you get a political powder keg:\n- **Wealth concentrates** around model owners, compute owners, and firms with distribution. That threatens democratic legitimacy (economic leverage is part of democratic power). This is a major theme of his essay. \ue200cite\ue202turn0search0\ue201\n- **Policy fights get nastier**: you\u2019ll see pressure for AI taxes, data/compute licensing, mandatory profit-sharing, or some form of \u201cAI dividend.\u201d Poorly designed versions are plausible if politics becomes purely punitive.\n- **Surveillance/propaganda risks rise** even without explicit dictatorship: persuasion at scale is profitable. His warnings about AI propaganda and surveillance are directionally right\u2014states and private actors both have incentives. \ue200cite\ue202turn2view1\ue201\n- **Cultural/psychological disruption** becomes mainstream: dependence on AI companions, manipulation, erosion of agency, and \u201cmeaning\u201d crises\u2014especially in populations whose social role was tied to work. \ue200cite\ue202turn0search0\ue201\n\n## Bottom line\nAmodei\u2019s essay is one of the clearest \u201cif we get *this level* of capability, here\u2019s the threat model\u201d documents out there. \ue200cite\ue202turn0search0\ue201  \nBut its urgency is highly sensitive to a timeline and scaling extrapolation that\u2019s not yet justified by public evidence, and many of its proposed mitigations (especially model guardrails) don\u2019t look stable against competition, open ecosystems, and geopolitics. Independent commentary basically agrees: listen, but keep skepticism because incentives and regulatory second-order effects matter. \ue200cite\ue202turn3view3\ue202turn0search4\ue201"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":"\ue200i\ue202turn0image0\ue202turn0image2\ue202turn0image1\ue202turn0image4\ue201","start_idx":0,"end_idx":51,"safe_urls":["https://api.time.com/wp-content/uploads/2024/05/time-most-influential-companies-anthropic-dario-amodei.jpg","https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/","https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/?utm_source=chatgpt.com","https://cdn.arstechnica.net/wp-content/uploads/2025/01/amodei_header_1-1152x648.jpg","https://cdn.mos.cms.futurecdn.net/Aerd92BCnqVQkLxxmhLdvA-1920-80.jpg","https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024","https://thisweekinai.news/2025/02/23/the-anthropic-ceo-dario-amodei-calls-the-ai-campaign-summit-a-missed-opportunity/","https://thisweekinai.news/2025/02/23/the-anthropic-ceo-dario-amodei-calls-the-ai-campaign-summit-a-missed-opportunity/?utm_source=chatgpt.com","https://time.com/6990386/anthropic-dario-amodei-interview/","https://time.com/6990386/anthropic-dario-amodei-interview/?utm_source=chatgpt.com","https://tse1.mm.bing.net/th/id/OIP.x2a_FrTL3yOmjMlLQcdtwAHaEK?cb=defcachec2&pid=Api","https://tse3.mm.bing.net/th/id/OIP.JUyYC5hdVn1kXo9RehQBXAHaE8?cb=defcachec2&pid=Api","https://tse4.mm.bing.net/th/id/OIP.9B2QscsS_ag6Flej85x5AgHaEK?cb=defcachec2&pid=Api","https://tse4.mm.bing.net/th/id/OIP.tdbN96HhAO0OWYEyXcHRfQHaJ8?cb=defcachec2&pid=Api","https://www.itpro.com/technology/artificial-intelligence/i-think-most-people-are-underestimating-how-bad-the-risks-could-be-anthropic-ceo-dario-amodei-lays-out-his-vision-of-powerful-ai-and-explains-why-he-sounds-a-tad-pessimistic","https://www.itpro.com/technology/artificial-intelligence/i-think-most-people-are-underestimating-how-bad-the-risks-could-be-anthropic-ceo-dario-amodei-lays-out-his-vision-of-powerful-ai-and-explains-why-he-sounds-a-tad-pessimistic?utm_source=chatgpt.com"],"refs":[{"turn_index":0,"ref_type":"image","ref_index":0},{"turn_index":0,"ref_type":"image","ref_index":2},{"turn_index":0,"ref_type":"image","ref_index":1},{"turn_index":0,"ref_type":"image","ref_index":4}],"alt":"[![Anthropic's CEO on Being an Underdog | TIME](https://tse4.mm.bing.net/th/id/OIP.tdbN96HhAO0OWYEyXcHRfQHaJ8?cb=defcachec2&pid=Api)](https://time.com/6990386/anthropic-dario-amodei-interview/?utm_source=chatgpt.com)\n","prompt_text":"\n## Images:\n![Anthropic's CEO on Being an Underdog | TIME](https://tse4.mm.bing.net/th/id/OIP.tdbN96HhAO0OWYEyXcHRfQHaJ8?cb=defcachec2&pid=Api)\n![The Anthropic CEO Dario Amodei calls the AI campaign summit a \"missed ...](https://tse3.mm.bing.net/th/id/OIP.JUyYC5hdVn1kXo9RehQBXAHaE8?cb=defcachec2&pid=Api)\n![Anthropic chief says AI could surpass \u201calmost all humans at almost ...](https://tse1.mm.bing.net/th/id/OIP.x2a_FrTL3yOmjMlLQcdtwAHaEK?cb=defcachec2&pid=Api)\n![\u201cI think most people are underestimating how bad the risks could be ...](https://tse4.mm.bing.net/th/id/OIP.9B2QscsS_ag6Flej85x5AgHaEK?cb=defcachec2&pid=Api)\n","type":"image_v2","images":[{"url":"https://time.com/6990386/anthropic-dario-amodei-interview/?utm_source=chatgpt.com","content_url":"https://api.time.com/wp-content/uploads/2024/05/time-most-influential-companies-anthropic-dario-amodei.jpg","thumbnail_url":"https://tse4.mm.bing.net/th/id/OIP.tdbN96HhAO0OWYEyXcHRfQHaJ8?cb=defcachec2&pid=Api","preview_url":null,"title":"Anthropic's CEO on Being an Underdog | TIME","content_size":{"width":1787,"height":2400},"thumbnail_size":{"width":474,"height":474},"thumbnail_crop_info":{"center_x_percent":0.43248945147679324,"center_y_percent":0.4641350210970464},"show_url":true,"source":null,"scores":null,"attribution":"TIME"},{"url":"https://thisweekinai.news/2025/02/23/the-anthropic-ceo-dario-amodei-calls-the-ai-campaign-summit-a-missed-opportunity/?utm_source=chatgpt.com","content_url":"https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024","thumbnail_url":"https://tse3.mm.bing.net/th/id/OIP.JUyYC5hdVn1kXo9RehQBXAHaE8?cb=defcachec2&pid=Api","preview_url":null,"title":"The Anthropic CEO Dario Amodei calls the AI campaign summit a \"missed ...","content_size":{"width":1024,"height":683},"thumbnail_size":{"width":316,"height":316},"thumbnail_crop_info":{"center_x_percent":0.5,"center_y_percent":0.310126582278481},"show_url":true,"source":null,"scores":null,"attribution":"thisweekinai.news"},{"url":"https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/?utm_source=chatgpt.com","content_url":"https://cdn.arstechnica.net/wp-content/uploads/2025/01/amodei_header_1-1152x648.jpg","thumbnail_url":"https://tse1.mm.bing.net/th/id/OIP.x2a_FrTL3yOmjMlLQcdtwAHaEK?cb=defcachec2&pid=Api","preview_url":null,"title":"Anthropic chief says AI could surpass \u201calmost all humans at almost ...","content_size":{"width":1152,"height":648},"thumbnail_size":{"width":266,"height":266},"thumbnail_crop_info":{"center_x_percent":0.5112781954887218,"center_y_percent":0.5},"show_url":true,"source":null,"scores":null,"attribution":"Ars Technica"},{"url":"https://www.itpro.com/technology/artificial-intelligence/i-think-most-people-are-underestimating-how-bad-the-risks-could-be-anthropic-ceo-dario-amodei-lays-out-his-vision-of-powerful-ai-and-explains-why-he-sounds-a-tad-pessimistic?utm_source=chatgpt.com","content_url":"https://cdn.mos.cms.futurecdn.net/Aerd92BCnqVQkLxxmhLdvA-1920-80.jpg","thumbnail_url":"https://tse4.mm.bing.net/th/id/OIP.9B2QscsS_ag6Flej85x5AgHaEK?cb=defcachec2&pid=Api","preview_url":null,"title":"\u201cI think most people are underestimating how bad the risks could be ...","content_size":{"width":1920,"height":1080},"thumbnail_size":{"width":266,"height":266},"thumbnail_crop_info":{"center_x_percent":0.44360902255639095,"center_y_percent":0.2857142857142857},"show_url":true,"source":null,"scores":null,"attribution":"IT Pro"}]},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn2view1\ue201","start_idx":532,"end_idx":562,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[{"title":"Dario Amodei \u2014\u00a0The Adolescence of Technology","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology","pub_date":null,"snippet":null,"attribution":"darioamodei.com"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0},{"turn_index":2,"ref_type":"view","ref_index":1}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn2view1\ue201","start_idx":866,"end_idx":896,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[{"title":"Dario Amodei \u2014\u00a0The Adolescence of Technology","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology","pub_date":null,"snippet":null,"attribution":"darioamodei.com"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0},{"turn_index":2,"ref_type":"view","ref_index":1}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn1search3\ue201","start_idx":1199,"end_idx":1231,"safe_urls":["https://arxiv.org/abs/2212.08073","https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com","https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[{"title":"Constitutional AI: Harmlessness from AI Feedback","url":"https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0},{"turn_index":1,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn2view1\ue201","start_idx":1916,"end_idx":1946,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[{"title":"Dario Amodei \u2014\u00a0The Adolescence of Technology","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology","pub_date":null,"snippet":null,"attribution":"darioamodei.com"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0},{"turn_index":2,"ref_type":"view","ref_index":1}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn0search2\ue202turn0search7\ue201","start_idx":2262,"end_idx":2307,"safe_urls":["https://legiscan.com/CA/text/SB53/id/3270002","https://legiscan.com/CA/text/SB53/id/3270002?utm_source=chatgpt.com","https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","https://www.lexology.com/library/detail.aspx?g=af624de2-543d-4036-aef0-e6e320cbb2be","https://www.lexology.com/library/detail.aspx?g=af624de2-543d-4036-aef0-e6e320cbb2be&utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[{"title":"Bill Text: CA SB53 | 2025-2026 | Regular Session | Enrolled","url":"https://legiscan.com/CA/text/SB53/id/3270002?utm_source=chatgpt.com","pub_date":null,"snippet":"This bill would enact the Transparency in Frontier Artificial Intelligence Act ... (o) The recent release of the Governor's California Report on Frontier AI ...Read more","attribution":"LegiScan"},{"title":"New York's RAISE Act Raises the Bar for Frontier AI ...","url":"https://www.lexology.com/library/detail.aspx?g=af624de2-543d-4036-aef0-e6e320cbb2be&utm_source=chatgpt.com","pub_date":null,"snippet":"12 Jan 2026 \u2014 On December 19, 2025, New York Governor Kathy Hochul signed Assembly Bill A6453-A, the Responsible AI Safety and Education Act (RAISE Act), ...","attribution":"Lexology"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0},{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":7}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search0\ue201","start_idx":3335,"end_idx":3354,"safe_urls":["https://metr.org/notes/2026-01-22-time-horizon-limitations/","https://metr.org/notes/2026-01-22-time-horizon-limitations/?utm_source=chatgpt.com"],"refs":[],"alt":"([Metr](https://metr.org/notes/2026-01-22-time-horizon-limitations/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Clarifying limitations of time horizon","url":"https://metr.org/notes/2026-01-22-time-horizon-limitations/?utm_source=chatgpt.com","pub_date":1769068800.0,"snippet":"5 days ago \u2014 When METR says \u201cClaude Opus 4.5 has a 50%-time horizon of around 4 hrs 49 mins (95% confidence interval of 1 hr 49 mins to 20 hrs 25 mins)\u201d, we ...Read more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"Metr"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search3\ue202turn0search0\ue201","start_idx":4387,"end_idx":4419,"safe_urls":["https://arxiv.org/abs/2212.08073","https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com","https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Constitutional AI: Harmlessness from AI Feedback","url":"https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution":"darioamodei.com"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3view3\ue201","start_idx":5371,"end_idx":5388,"safe_urls":["https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults"],"refs":[],"alt":"([Default](https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"\n\tHow to Handle \u201cThe Adolescence of Technology\u201d Like Adults | Lawfare\n","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":3,"ref_type":"view","ref_index":3}],"hue":null,"attributions":null,"attribution":"Default"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search5\ue201","start_idx":5904,"end_idx":5936,"safe_urls":["https://fortune.com/2025/05/28/anthropic-ceo-warning-ai-job-loss/","https://fortune.com/2025/05/28/anthropic-ceo-warning-ai-job-loss/?utm_source=chatgpt.com","https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic","https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic?utm_source=chatgpt.com"],"refs":[],"alt":"([Axios](https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI jobs danger: Sleepwalking into a white-collar bloodbath","url":"https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic?utm_source=chatgpt.com","pub_date":1748425500.0,"snippet":"28 May 2025 \u2014 AI could wipe out half of all entry-level white-collar jobs \u2014 and spike unemployment to 10-20% in the next one to five years, Amodei told us ...","attribution_segments":null,"supporting_websites":[{"title":"AI could eliminate half of all entry-level white-collar jobs","url":"https://fortune.com/2025/05/28/anthropic-ceo-warning-ai-job-loss/?utm_source=chatgpt.com","pub_date":null,"snippet":"28 May 2025 \u2014 Anthropic CEO Dario Amodei says AI could wipe out roughly 50% of all entry-level white-collar jobs. The cuts could come within five years, ...","attribution":"Fortune"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"Axios"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue202turn1news41\ue201","start_idx":6575,"end_idx":6606,"safe_urls":["https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/","https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/?utm_source=chatgpt.com","https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[{"title":"Exclusive: New Claude Model Triggers Stricter Safeguards at Anthropic","url":"https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/?utm_source=chatgpt.com","pub_date":null,"snippet":"Anthropic has launched Claude Opus 4 under unprecedented safety protocols due to concerns about its potential misuse in creating biological weapons. During internal assessments, the AI demonstrated a significantly enhanced ability to assist users\u2014especially novices\u2014in developing dangerous bioagents, prompting the firm to impose its strictest safety measures yet, known as AI Safety Level 3 (ASL-3). These safeguards include advanced cybersecurity, improved jailbreak prevention systems, and enhanced detection algorithms for harmful queries, all part of Anthropic\u2019s Responsible Scaling Policy (RSP).\n\nVice President and Chief Scientist Jared Kaplan emphasized caution, stating that although there\u2019s no definitive proof the model poses a bioweapon threat, the risk is plausible enough to warrant tight control. Claude Opus 4\u2019s capabilities notably surpassed both earlier versions and Google search in trials, raising biosecurity concerns. While the RSP is self-regulated and voluntary, Anthropic claims it fosters competition to advance safety standards across the AI industry. The company has also launched a jailbreak bounty program and conducted uplift trials to assess AI-assisted bioweapon creation potential. With over $2 billion in annualized revenue from Claude, Anthropic faces the challenge of balancing market competitiveness with responsible AI development.","attribution":"TIME"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0},{"turn_index":1,"ref_type":"news","ref_index":41}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue201","start_idx":6967,"end_idx":6986,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue201","start_idx":7536,"end_idx":7555,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search5\ue202turn1news41\ue201","start_idx":8003,"end_idx":8034,"safe_urls":["https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/","https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/?utm_source=chatgpt.com","https://www.anthropic.com/transparency","https://www.anthropic.com/transparency?utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/transparency?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Anthropic's Transparency Hub","url":"https://www.anthropic.com/transparency?utm_source=chatgpt.com","pub_date":null,"snippet":"4 Dec 2025 \u2014 Our Responsible Scaling Policy (RSP) evaluation process is designed to systematically assess our models' capabilities in domains of potential ...Read more","attribution_segments":null,"supporting_websites":[{"title":"Exclusive: New Claude Model Triggers Stricter Safeguards at Anthropic","url":"https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/?utm_source=chatgpt.com","pub_date":null,"snippet":"Anthropic has launched Claude Opus 4 under unprecedented safety protocols due to concerns about its potential misuse in creating biological weapons. During internal assessments, the AI demonstrated a significantly enhanced ability to assist users\u2014especially novices\u2014in developing dangerous bioagents, prompting the firm to impose its strictest safety measures yet, known as AI Safety Level 3 (ASL-3). These safeguards include advanced cybersecurity, improved jailbreak prevention systems, and enhanced detection algorithms for harmful queries, all part of Anthropic\u2019s Responsible Scaling Policy (RSP).\n\nVice President and Chief Scientist Jared Kaplan emphasized caution, stating that although there\u2019s no definitive proof the model poses a bioweapon threat, the risk is plausible enough to warrant tight control. Claude Opus 4\u2019s capabilities notably surpassed both earlier versions and Google search in trials, raising biosecurity concerns. While the RSP is self-regulated and voluntary, Anthropic claims it fosters competition to advance safety standards across the AI industry. The company has also launched a jailbreak bounty program and conducted uplift trials to assess AI-assisted bioweapon creation potential. With over $2 billion in annualized revenue from Claude, Anthropic faces the challenge of balancing market competitiveness with responsible AI development.","attribution":"TIME"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":5},{"turn_index":1,"ref_type":"news","ref_index":41}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search0\ue201","start_idx":8478,"end_idx":8510,"safe_urls":["https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic","https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic?utm_source=chatgpt.com","https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([Axios](https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI jobs danger: Sleepwalking into a white-collar bloodbath","url":"https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic?utm_source=chatgpt.com","pub_date":1748425500.0,"snippet":"28 May 2025 \u2014 AI could wipe out half of all entry-level white-collar jobs \u2014 and spike unemployment to 10-20% in the next one to five years, Amodei told us ...","attribution_segments":null,"supporting_websites":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution":"darioamodei.com"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"Axios"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue201","start_idx":9632,"end_idx":9651,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2view1\ue201","start_idx":10110,"end_idx":10127,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Dario Amodei \u2014\u00a0The Adolescence of Technology","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":2,"ref_type":"view","ref_index":1}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue201","start_idx":10333,"end_idx":10352,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search0\ue201","start_idx":10492,"end_idx":10511,"safe_urls":["https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com"],"refs":[],"alt":"([darioamodei.com](https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","pub_date":null,"snippet":"11 hours ago \u2014 Dario Amodei \u00b7 Archive ... The Adolescence of Technology. The Adolescence of Technology. Confronting and Overcoming the Risks of Powerful AI. January 2026.Read more","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":0}],"hue":null,"attributions":null,"attribution":"darioamodei.com"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3view3\ue202turn0search4\ue201","start_idx":10907,"end_idx":10937,"safe_urls":["https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults?utm_source=chatgpt.com"],"refs":[],"alt":"([Default](https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"\n\tHow to Handle \u201cThe Adolescence of Technology\u201d Like Adults | Lawfare\n","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[{"title":"How to Handle \u201cThe Adolescence of Technology\u201d Like Adults","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults?utm_source=chatgpt.com","pub_date":null,"snippet":null,"attribution":"Default"}],"refs":[{"turn_index":3,"ref_type":"view","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"Default"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":" ","start_idx":10937,"end_idx":10937,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"The Adolescence of Technology - Dario Amodei","url":"https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","attribution":"darioamodei.com"},{"title":"Clarifying limitations of time horizon","url":"https://metr.org/notes/2026-01-22-time-horizon-limitations/?utm_source=chatgpt.com","attribution":"Metr"},{"title":"Constitutional AI: Harmlessness from AI Feedback","url":"https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com","attribution":"arXiv"},{"title":"\n\tHow to Handle \u201cThe Adolescence of Technology\u201d Like Adults | Lawfare\n","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","attribution":"Default"},{"title":"AI jobs danger: Sleepwalking into a white-collar bloodbath","url":"https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic?utm_source=chatgpt.com","attribution":"Axios"},{"title":"Anthropic's Transparency Hub","url":"https://www.anthropic.com/transparency?utm_source=chatgpt.com","attribution":"Anthropic"}],"has_images":false}],"safe_urls":["https://api.time.com/wp-content/uploads/2024/05/time-most-influential-companies-anthropic-dario-amodei.jpg","https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/","https://arstechnica.com/ai/2025/01/anthropic-chief-says-ai-could-surpass-almost-all-humans-at-almost-everything-shortly-after-2027/?utm_source=chatgpt.com","https://arxiv.org/abs/2212.08073","https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com","https://cdn.arstechnica.net/wp-content/uploads/2025/01/amodei_header_1-1152x648.jpg","https://cdn.mos.cms.futurecdn.net/Aerd92BCnqVQkLxxmhLdvA-1920-80.jpg","https://fortune.com/2025/05/28/anthropic-ceo-warning-ai-job-loss/","https://fortune.com/2025/05/28/anthropic-ceo-warning-ai-job-loss/?utm_source=chatgpt.com","https://legiscan.com/CA/text/SB53/id/3270002","https://legiscan.com/CA/text/SB53/id/3270002?utm_source=chatgpt.com","https://metr.org/notes/2026-01-22-time-horizon-limitations/","https://metr.org/notes/2026-01-22-time-horizon-limitations/?utm_source=chatgpt.com","https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024","https://thisweekinai.news/2025/02/23/the-anthropic-ceo-dario-amodei-calls-the-ai-campaign-summit-a-missed-opportunity/","https://thisweekinai.news/2025/02/23/the-anthropic-ceo-dario-amodei-calls-the-ai-campaign-summit-a-missed-opportunity/?utm_source=chatgpt.com","https://time.com/6990386/anthropic-dario-amodei-interview/","https://time.com/6990386/anthropic-dario-amodei-interview/?utm_source=chatgpt.com","https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/","https://time.com/7287806/anthropic-claude-4-opus-safety-bio-risk/?utm_source=chatgpt.com","https://tse1.mm.bing.net/th/id/OIP.x2a_FrTL3yOmjMlLQcdtwAHaEK?cb=defcachec2&pid=Api","https://tse3.mm.bing.net/th/id/OIP.JUyYC5hdVn1kXo9RehQBXAHaE8?cb=defcachec2&pid=Api","https://tse4.mm.bing.net/th/id/OIP.9B2QscsS_ag6Flej85x5AgHaEK?cb=defcachec2&pid=Api","https://tse4.mm.bing.net/th/id/OIP.tdbN96HhAO0OWYEyXcHRfQHaJ8?cb=defcachec2&pid=Api","https://www.anthropic.com/transparency","https://www.anthropic.com/transparency?utm_source=chatgpt.com","https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic","https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic?utm_source=chatgpt.com","https://www.darioamodei.com/essay/the-adolescence-of-technology","https://www.darioamodei.com/essay/the-adolescence-of-technology?utm_source=chatgpt.com","https://www.itpro.com/technology/artificial-intelligence/i-think-most-people-are-underestimating-how-bad-the-risks-could-be-anthropic-ceo-dario-amodei-lays-out-his-vision-of-powerful-ai-and-explains-why-he-sounds-a-tad-pessimistic","https://www.itpro.com/technology/artificial-intelligence/i-think-most-people-are-underestimating-how-bad-the-risks-could-be-anthropic-ceo-dario-amodei-lays-out-his-vision-of-powerful-ai-and-explains-why-he-sounds-a-tad-pessimistic?utm_source=chatgpt.com","https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults","https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults?utm_source=chatgpt.com","https://www.lexology.com/library/detail.aspx?g=af624de2-543d-4036-aef0-e6e320cbb2be","https://www.lexology.com/library/detail.aspx?g=af624de2-543d-4036-aef0-e6e320cbb2be&utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/claude-sonnet-4-5-system-card?utm_source=chatgpt.com","title":"Claude Sonnet 4.5 System Card","snippet":"10 Oct 2025 \u2014 The decision was overseen by the Responsible. Scaling Officer (RSO) and followed our established protocols for precautionary ASL determinations.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf?utm_source=chatgpt.com","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"https://arxiv.org/abs/2212.08073 the resulting preference datasets to fine-tune AI systems that are more reflective of the desired behavior, be it ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/activating-asl3-protections?utm_source=chatgpt.com","title":"Activating AI Safety Level 3 protections","snippet":"22 May 2025 \u2014 We have activated the AI Safety Level 3 (ASL-3) Deployment and Security Standards described in Anthropic's Responsible Scaling Policy (RSP) in conjunction with ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/claude-opus-4-5-system-card?utm_source=chatgpt.com","title":"Claude Opus 4.5 System Card","snippet":"24 Nov 2025 \u2014 For more information on the requirements for these ASL Standards, please see the Responsible Scaling Policy and our report on activating ASL-3 ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/responsible-scaling-policy?utm_source=chatgpt.com","title":"Anthropic's Responsible Scaling Policy (version 2.2)","snippet":"19 Sept 2023 \u2014 This updated policy reflects our view that risk governance in this rapidly evolving domain should be proportional, iterative, and exportable.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/claude-4-system-card?utm_source=chatgpt.com","title":"Claude Opus 4 & Claude Sonnet 4 - System Card","snippet":"16 Jul 2025 \u2014 The process was guided by our Responsible Scaling Policy (RSP), which provides a framework for evaluating and managing potential risks ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"anthropic.com"}]},{"type":"search_result_group","domain":"legiscan.com","entries":[{"type":"search_result","url":"https://legiscan.com/CA/text/SB53/id/3270002?utm_source=chatgpt.com","title":"Bill Text: CA SB53 | 2025-2026 | Regular Session | Enrolled","snippet":"This bill would enact the Transparency in Frontier Artificial Intelligence Act ... (o) The recent release of the Governor's California Report on Frontier AI ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"legiscan.com"}]},{"type":"search_result_group","domain":"stanford.edu","entries":[{"type":"search_result","url":"https://stacks.stanford.edu/file/druid%3Acv716pj4036/Technical%20Report%20on%20Mirror%20Bacteria%20Feasibility%20and%20Risks.pdf?utm_source=chatgpt.com","title":"Technical Report on Mirror Bacteria: Feasibility and Risks","snippet":"Mirror bacteria could not evolve from existing life, but their creation will become increasingly feasible as science advances. Interactions between organisms ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://news.stanford.edu/stories/2024/12/potential-risks-of-mirror-life?utm_source=chatgpt.com","title":"A new report warns of serious risks from 'mirror life'","snippet":"16 Dec 2024 \u2014 Building entire mirror organisms is not imminent, but a report published in Science Dec. 12, authored by 38 researchers, including four from ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://fsi.stanford.edu/news/health-policy-forum-possible-peril-engineered-mirror-life?utm_source=chatgpt.com","title":"Health Policy Forum: The Possible Peril of Engineered `Mirror ...","snippet":"4 Dec 2025 \u2014 Mirror organisms might infect animals and plants and invade natural microbial communities because they would be impervious to most natural ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"stanford.edu"},{"type":"search_result","url":"https://scalingintelligence.stanford.edu/pubs/constitutionalai/?utm_source=chatgpt.com","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"Anthropic, 2022. DOI: 2212.08073v1. Imagine an AI that polices itself to ensure it remains helpful and safe. Our approach, called Constitutional AI, trains an ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"stanford.edu"}]},{"type":"search_result_group","domain":"nysenate.gov","entries":[{"type":"search_result","url":"https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A?utm_source=chatgpt.com","title":"NY State Assembly Bill 2025-A6453A","snippet":"Relates to the training and use of artificial intelligence frontier models; defines terms; establishes remedies for violations.Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"nysenate.gov"}]},{"type":"search_result_group","domain":"lawfaremedia.org","entries":[{"type":"search_result","url":"https://www.lawfaremedia.org/article/how-to-handle--the-adolescence-of-technology--like-adults?utm_source=chatgpt.com","title":"How to Handle \u201cThe Adolescence of Technology\u201d Like Adults","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"lawfaremedia.org"}]},{"type":"search_result_group","domain":"linkedin.com","entries":[{"type":"search_result","url":"https://www.linkedin.com/posts/metr-evals_we-estimate-that-on-our-tasks-anthropics-activity-7408005652431847424-H5iH?utm_source=chatgpt.com","title":"Anthropic's Claude Opus 4.5 Time Horizon Estimated at 4 ...","snippet":"METR have found that Claude Opus 4.5 can successfully complete about 50% of five-hour programming tasks,* performing so well that they are ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"linkedin.com"},{"type":"search_result","url":"https://www.linkedin.com/pulse/anthropics-complete-ai-ecosystem-building-future-software-kamboj-zwbuc?utm_source=chatgpt.com","title":"Anthropic's Complete AI Ecosystem: Building the Future of ...","snippet":"Anthropic's Responsible Scaling Policy (RSP) commits to specific safety measures as model capabilities increase. The policy defines capability ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"linkedin.com"}]},{"type":"search_result_group","domain":"fortune.com","entries":[{"type":"search_result","url":"https://fortune.com/2025/05/28/anthropic-ceo-warning-ai-job-loss/?utm_source=chatgpt.com","title":"AI could eliminate half of all entry-level white-collar jobs","snippet":"28 May 2025 \u2014 Anthropic CEO Dario Amodei says AI could wipe out roughly 50% of all entry-level white-collar jobs. The cuts could come within five years, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"fortune.com"}]},{"type":"search_result_group","domain":"governance-intelligence.com","entries":[{"type":"search_result","url":"https://www.governance-intelligence.com/regulatory-compliance/california-state-senate-bill-aims-limit-lawyers-use-ai-0?utm_source=chatgpt.com","title":"California State Senate bill aims to limit lawyers' use of AI","snippet":"15 hours ago \u2014 In 2025 the California legislature passed the Transparency in Frontier Artificial Intelligence Act (SB 53), a first-of-its-kind law ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"governance-intelligence.com"}]},{"type":"search_result_group","domain":"lexology.com","entries":[{"type":"search_result","url":"https://www.lexology.com/library/detail.aspx?g=af624de2-543d-4036-aef0-e6e320cbb2be&utm_source=chatgpt.com","title":"New York's RAISE Act Raises the Bar for Frontier AI ...","snippet":"12 Jan 2026 \u2014 On December 19, 2025, New York Governor Kathy Hochul signed Assembly Bill A6453-A, the Responsible AI Safety and Education Act (RAISE Act), ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"lexology.com"},{"type":"search_result","url":"https://www.lexology.com/library/detail.aspx?g=bde591ac-51c1-4979-b5b6-795609775b25&utm_source=chatgpt.com","title":"California's Evolving AI Legal Landscape Entering 2026","snippet":"7 Jan 2026 \u2014 On September 29, 2025, Governor Newsom signed into law, Senate Bill 53, entitled: Transparency in Frontier Artificial Intelligence Act (\u201cSB 53\u201d) ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"lexology.com"}]},{"type":"search_result_group","domain":"gizmodo.com","entries":[{"type":"search_result","url":"https://gizmodo.com/anthropic-ceo-worries-humanity-may-not-be-mature-enough-for-advanced-ai-2000714187?utm_source=chatgpt.com","title":"Anthropic CEO Worries Humanity May Not be \u2018Mature\u2019 Enough for Advanced AI","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"gizmodo.com"}]},{"type":"search_result_group","domain":"lesswrong.com","entries":[{"type":"search_result","url":"https://www.lesswrong.com/posts/q5ejXr4CRuPxkgzJD/claude-opus-4-5-achieves-50-time-horizon-of-around-4-hrs-49?utm_source=chatgpt.com","title":"Claude Opus 4.5 Achieves 50%-Time Horizon Of Around 4 ...","snippet":"19 Dec 2025 \u2014 We estimate that, on our tasks, Claude Opus 4.5 has a 50%-time horizon of around 4 hrs 49 mins (95% confidence interval of 1 hr 49 mins to ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"lesswrong.com"},{"type":"search_result","url":"https://www.lesswrong.com/posts/gfby4vqNtLbehqbot/claude-opus-4-5-model-card-alignment-and-safety?utm_source=chatgpt.com","title":"Claude Opus 4.5: Model Card, Alignment and Safety","snippet":"28 Nov 2025 \u2014 Anthropic's central tool is an automated behavioral audit suite, here run by Sonnet 4.5, in which the auditing model does an investigation on ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"lesswrong.com"}]},{"type":"search_result_group","domain":"iapp.org","entries":[{"type":"search_result","url":"https://iapp.org/news/a/california-2025-legislative-wrap-up-more-privacy-and-first-of-its-kind-ai-laws-adopted?utm_source=chatgpt.com","title":"More privacy and first-of-its kind AI laws adopted","snippet":"7 Jan 2026 \u2014 Overall, the bills passed in 2025 further reinforce California as a leader in privacy and AI regulation and may foreshadow similar trends in ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"iapp.org"}]},{"type":"search_result_group","domain":"state.ny.us","entries":[{"type":"search_result","url":"https://assembly.state.ny.us/leg/?Actions=Y&Memo=Y&Summary=Y&Text=Y&Votes=Y&bn=A09449&term=&utm_source=chatgpt.com","title":"Bill Search and Legislative Information","snippet":"7 Jan 2026 \u2014 TITLE OF BILL: An act to amend the general business law, in relation to transparency and safety requirements for developers of artificial ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"state.ny.us"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/2212.08073?utm_source=chatgpt.com","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"15 Dec 2022 \u2014 In this paper we develop a method we refer to as Constitutional AI (CAI), depicted in Figure 1, and use it to train a non-evasive and relatively ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2503.15182?utm_source=chatgpt.com","title":"241210 Claude mirror life manuscript","snippet":"by KM Esvelt \u00b7 2025 \u2014 Technical Report on Mirror \u00b7 Bacteria: Feasibility and Risks. https://purl.stanford.edu/cv716pj4036 (2024) doi:10.25740/CV716PJ4036. 6. Page 7 ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"inc.com","entries":[{"type":"search_result","url":"https://www.inc.com/ben-sherry/major-ai-company-speaks-out-after-federal-agents-kill-minneapolis-nurse/91292885?utm_source=chatgpt.com","title":"Major AI Company Speaks Out After Federal Agents Kill Minneapolis Nurse","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"inc.com"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/accelerate/comments/1qnoano/new_from_dario_amodei_the_adolescence_of/?utm_source=chatgpt.com","title":"The Adolescence of Technology \u201cwe may have an AI that is ...","snippet":"In \"The Adolescence of Technology\" (January 2026), Dario Amodei argues that humanity is entering a dangerous but inevitable \"rite of passage ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/1pv4gm2/metr_claude_opus_45_hits_475h_task_horizon_67/?utm_source=chatgpt.com","title":"METR: Claude Opus 4.5 hits ~4.75h task horizon (+67% ...","snippet":"Updated METR benchmarks show Claude Opus 4.5 completes software engineering tasks requiring approximately 4 hours and 45 minutes of human ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"yahoo.com","entries":[{"type":"search_result","url":"https://www.yahoo.com/news/anthropic-ceo-warns-ai-could-152758674.html?utm_source=chatgpt.com","title":"Anthropic CEO warns AI could eliminate half of all entry- ...","snippet":"28 May 2025 \u2014 Anthropic CEO Dario Amodei says AI could wipe out roughly 50% of all entry-level white-collar jobs. The cuts could come within five years, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"yahoo.com"},{"type":"search_result","url":"https://www.yahoo.com/news/articles/anthropic-ceo-says-ai-progress-232040474.html?utm_source=chatgpt.com","title":"Anthropic CEO Says AI Progress Is Outpacing Society\u2019s Ability to Control It","snippet":"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"yahoo.com"}]},{"type":"search_result_group","domain":"fpf.org","entries":[{"type":"search_result","url":"https://fpf.org/blog/the-raise-act-vs-sb-53-a-tale-of-two-frontier-ai-laws/?utm_source=chatgpt.com","title":"The RAISE Act vs. SB 53: A Tale of Two Frontier AI Laws","snippet":"8 Jan 2026 \u2014 What the enactment of New York's RAISE Act reveals compared to California's SB 53, the nation's first frontier AI law.Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"fpf.org"}]},{"type":"search_result_group","domain":"effectivealtruism.org","entries":[{"type":"search_result","url":"https://forum.effectivealtruism.org/posts/TPJju3zv7b2fdwgmE/dario-amodei-on-ai-risk-and-anthropic-s-approach-the?utm_source=chatgpt.com","title":"Dario Amodei on AI risk and Anthropic's approach (\u201cThe ...","snippet":"7 hours ago \u2014 Dario Amodei on AI risk and Anthropic's approach (\u201cThe Adolescence of Technology\u201d). by Charlie_Guthmann. Jan 26 1 min read 0 ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"effectivealtruism.org"},{"type":"search_result","url":"https://forum.effectivealtruism.org/posts/9pkjXwe2nFun32hR2/technical-report-on-mirror-bacteria-feasibility-and-risks?utm_source=chatgpt.com","title":"Technical Report on Mirror Bacteria: Feasibility and Risks","snippet":"12 Dec 2024 \u2014 This report describes the technical feasibility of creating mirror bacteria and the potentially serious and wide-ranging risks that they could pose.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"effectivealtruism.org"}]},{"type":"search_result_group","domain":"skadden.com","entries":[{"type":"search_result","url":"https://www.skadden.com/insights/publications/2026/01/new-york-enacts-ai-transparency-law?utm_source=chatgpt.com","title":"New York Enacts AI Transparency Law On Heels of White ...","snippet":"6 Jan 2026 \u2014 New York has enacted an AI safety and transparency law (the RAISE Act) that imposes transparency, compliance, safety and reporting ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"skadden.com"}]},{"type":"search_result_group","domain":"digi-con.org","entries":[{"type":"search_result","url":"https://digi-con.org/on-constitutional-ai/?utm_source=chatgpt.com","title":"On 'Constitutional' AI - The Digital Constitutionalist","snippet":"Essentially, Anthropic seeks to train AI systems with a Constitution to ensure they remain 'helpful, honest, and harmless'. The use of constitutional language ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"digi-con.org"}]},{"type":"search_result_group","domain":"metr.org","entries":[{"type":"search_result","url":"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/?utm_source=chatgpt.com","title":"Measuring AI Ability to Complete Long Tasks","snippet":"19 Mar 2025 \u2014 We find that the time taken by human experts is strongly predictive of model success on a given task: current models have almost 100% success ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"metr.org"}]},{"type":"search_result_group","domain":"axios.com","entries":[{"type":"search_result","url":"https://www.axios.com/2025/09/17/anthropic-amodei-ai?utm_source=chatgpt.com","title":"Exclusive: AI's ability to displace jobs is advancing quickly ...","snippet":"17 Sept 2025 \u2014 The Anthropic CEO has previously warned AI could wipe out half of entry-level white-collar jobs soon.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"axios.com"}]},{"type":"search_result_group","domain":"c1m.ai","entries":[{"type":"search_result","url":"https://c1m.ai/how-anthropics-claude-sonnet-4-5-advances-ai-safety-and-enterprise-adoption/?utm_source=chatgpt.com","title":"How Anthropic's Claude Sonnet 4.5 Advances AI Safety ...","snippet":"7 Oct 2025 \u2014 Anthropic assigns this model a Level 3 AI Safety classification, indicating stringent safeguards, particularly around sensitive content ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"c1m.ai"}]},{"type":"search_result_group","domain":"scientificamerican.com","entries":[{"type":"search_result","url":"https://www.scientificamerican.com/article/creating-mirror-life-could-be-disastrous-scientists-warn/?utm_source=chatgpt.com","title":"Creating 'Mirror Life' Could Be Disastrous, Scientists Warn","snippet":"14 Dec 2024 \u2014 Breakthroughs in synthetic biology could create mirror versions of natural molecules, with devastating consequences for life on Earth.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"scientificamerican.com"}]},{"type":"search_result_group","domain":"dlapiper.com","entries":[{"type":"search_result","url":"https://www.dlapiper.com/en/insights/publications/2025/12/the-raise-act-new-york-joins-california-in-requiring-developer-transparency-for-large-ai-models?utm_source=chatgpt.com","title":"The RAISE Act: New York joins California in requiring ...","snippet":"22 Dec 2025 \u2014 New York Governor Kathy Hochul recently signed the Responsible AI Safety and Education (RAISE) Act, which focuses on AI safety and imposes ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"dlapiper.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/mlearning-ai/paper-review-constituional-ai-training-llms-using-principles-16c68cfffaef?utm_source=chatgpt.com","title":"Constitutional AI, Training LLMs using Principles","snippet":"Today's learnings are derived from this paper released by Anthropic in December. ... Image by the authors from: https://arxiv.org/abs/2212.08073.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40nraman.n6/inside-claude-how-anthropic-built-the-worlds-most-safety-conscious-ai-c73f955f9f65?utm_source=chatgpt.com","title":"Inside Claude: How Anthropic Built the World's Most Safety ...","snippet":"A deep dive into the architecture, training philosophy, and deployment pipeline behind one of the world's most advanced language models.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"redwoodresearch.org","entries":[{"type":"search_result","url":"https://blog.redwoodresearch.org/p/measuring-no-cot-math-time-horizon?utm_source=chatgpt.com","title":"Measuring no CoT math time horizon (single forward pass)","snippet":"26 Dec 2025 \u2014 I find that Opus 4.5 has a no-CoT 50% reliability time horizon of 3.5 minutes and that time horizon has been doubling every 9 months. In an ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"redwoodresearch.org"}]},{"type":"search_result_group","domain":"ca.gov","entries":[{"type":"search_result","url":"https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/?utm_source=chatgpt.com","title":"Governor Newsom signs SB 53, advancing California's world ...","snippet":"29 Sept 2025 \u2014 What the law does: SB 53 establishes new requirements for frontier AI developers creating stronger: \u2713 Transparency: Requires large frontier ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"ca.gov"}]},{"type":"search_result_group","domain":"ny.gov","entries":[{"type":"search_result","url":"https://www.governor.ny.gov/news/governor-hochul-signs-nation-leading-legislation-require-ai-frameworks-ai-frontier-models?utm_source=chatgpt.com","title":"Governor Hochul Signs Nation-Leading Legislation to Require ...","snippet":"19 Dec 2025 \u2014 Governor Hochul signed legislation to require AI frameworks for AI frontier models, setting a nation-leading standard for AI transparency ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"ny.gov"}]},{"type":"search_result_group","domain":"bureaubiosecurity.nl","entries":[{"type":"search_result","url":"https://www.bureaubiosecurity.nl/en/news/concerns-about-unprecedented-risks-of-mirror-life?utm_source=chatgpt.com","title":"Concerns about unprecedented risks of 'Mirror Life'","snippet":"25 Mar 2025 \u2014 Mirror bacteria could potentially bypass the immune systems of humans, animals, and plants, leading to uncontrolled and deadly infections.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"bureaubiosecurity.nl"}]},{"type":"search_result_group","domain":"facebook.com","entries":[{"type":"search_result","url":"https://www.facebook.com/xixidu/posts/new-essay-by-anthropic-ceo-dario-amodei1-we-are-considerably-closer-to-real-dang/10173661395710637/?utm_source=chatgpt.com","title":"New essay by Anthropic CEO Dario Amodei: 1. We are ...","snippet":"Read more: https://www.darioamodei.com/essay/the- adolescence-of-technology ... (2026, January 12). *NVIDIA's CEO says you shouldn't talk ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"facebook.com"},{"type":"search_result","url":"https://www.facebook.com/cityam/posts/chief-executive-of-artificial-heavyweight-anthropic-dario-amodei-has-issued-a-wa/1480226123393036/?utm_source=chatgpt.com","title":"Chief executive of artificial heavyweight Anthropic, Dario ...","snippet":"Anthropic CEO Dario Amodei recently warned that up to 50% of entry-level white-collar jobs could disappear within the next five years. He ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"facebook.com"},{"type":"search_result","url":"https://www.facebook.com/StanfordLawSchool/posts/could-synthetic-life-wipe-out-real-life-in-2024-leading-scientists-warned-that-m/1252555133573831/?utm_source=chatgpt.com","title":"Stanford Law School's post","snippet":"4 Nov 2025 \u2014 Could synthetic life wipe out real life? In 2024, leading scientists warned that \u201cmirror bacteria\u201d\u2014organisms built entirely from mirror-image ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"facebook.com"}]},{"type":"search_result_group","domain":"umsida.ac.id","entries":[{"type":"search_result","url":"https://eprints.umsida.ac.id/16197/1/36-39%2BConstitutional%2BAI%2BAn%2BExpanded%2BOverview%2Bof%2BAnthropic%E2%80%99s%2BAlignment%2BApproach.pdf?utm_source=chatgpt.com","title":"An Expanded Overview of Anthropic's Alignment Approach","snippet":"by S Manish \u00b7 2023 \u00b7 Cited by 2 \u2014 In this paper, we provide an expanded overview of Constitutional AI, including motivating background, methodology, ethical considerations, and placeholders ...Read mo...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"umsida.ac.id"}]},{"type":"search_result_group","domain":"economictimes.com","entries":[{"type":"search_result","url":"https://m.economictimes.com/tech/artificial-intelligence/ai-could-cut-half-of-all-entry-level-white-collar-jobs-anthropic-ceo/articleshow/121482634.cms?utm_source=chatgpt.com","title":"AI could cut half of all entry-level white collar jobs","snippet":"29 May 2025 \u2014 Anthropic CEO Dario Amodei has openly voiced what many people are fearing: artificial intelligence (AI) could eat away nearly half of all.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"economictimes.com"}]},{"type":"search_result_group","domain":"theneurondaily.com","entries":[{"type":"search_result","url":"https://www.theneurondaily.com/p/openai-s-new-research-paper-is-wild?utm_source=chatgpt.com","title":"\ud83d\ude38 OpenAI's new research paper on monitorability is wild","snippet":"21 Dec 2025 \u2014 METR found Claude Opus 4.5 has a 50%-time horizon of 4 hours 49 minutes (highest published to date) meaning it can reliably complete tasks ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"theneurondaily.com"}]},{"type":"search_result_group","domain":"upenn.edu","entries":[{"type":"search_result","url":"https://ai-analytics.wharton.upenn.edu/wharton-accountable-ai-lab/sb-53-what-californias-new-ai-safety-law-means-for-developers/?utm_source=chatgpt.com","title":"SB 53: What California's New AI Safety Law Means for ...","snippet":"14 Nov 2025 \u2014 California's new Transparency in Frontier Artificial Intelligence Act (SB 53), signed into law in late September 2025, is the United States' ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"upenn.edu"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/391400510_Constitutional_AI_An_Expanded_Overview_of_Anthropic%27s_Alignment_Approach?utm_source=chatgpt.com","title":"An Expanded Overview of Anthropic's Alignment Approach","snippet":"3 May 2025 \u2014 This paper provides an expanded overview of Constitutional AI, its background, methodology, practical implementation details, and future directions.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"crowell.com","entries":[{"type":"search_result","url":"https://www.crowell.com/en/insights/client-alerts/californias-landmark-ai-law-demands-transparency-from-leading-ai-developers?utm_source=chatgpt.com","title":"California's Landmark AI Law Demands Transparency ...","snippet":"6 Oct 2025 \u2014 On September 29, 2025, California Governor Gavin Newsom signed into law Senate Bill 53, the Transparency in Frontier Artificial Intelligence Act ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"crowell.com"}]},{"type":"search_result_group","domain":"alignmentforum.org","entries":[{"type":"search_result","url":"https://www.alignmentforum.org/posts/GHKYwjYtwzhukpBSb/axrp-episode-47-david-rein-on-metr-time-horizons?utm_source=chatgpt.com","title":"AXRP Episode 47 - David Rein on METR Time Horizons","snippet":"2 Jan 2026 \u2014 When METR says something like \u201cClaude Opus 4.5 has a 50% time horizon of 4 hours and 50 minutes\u201d, what does that mean? In this episode David ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"alignmentforum.org"}]},{"type":"search_result_group","domain":"nelsonmullins.com","entries":[{"type":"search_result","url":"https://www.nelsonmullins.com/insights/alerts/privacy_and_data_security_alert/all/new-york-laws-raise-the-bar-in-addressing-ai-safety-the-raise-act-and-ai-companion-models?utm_source=chatgpt.com","title":"New York Laws \u201cRAISE\u201d the Bar in Addressing AI Safety","snippet":"12 Jan 2026 \u2014 The state of New York was one of the states at the forefront of artificial intelligence (AI) regulation in 2025. Among its notable 2025 ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"nelsonmullins.com"}]},{"type":"search_result_group","domain":"rapamycin.news","entries":[{"type":"search_result","url":"https://www.rapamycin.news/t/the-adolescence-of-technology-confronting-and-overcoming-the-risks-of-powerful-ai-dario-amodei-anthropic/23345?utm_source=chatgpt.com","title":"The Adolescence of Technology - Confronting and ...","snippet":"5 hours ago \u2014 Based on Dario Amodei's essay \u201cThe Adolescence of Technology\u201d (January 2026), here is a summary of the key AI risks identified and the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"rapamycin.news"}]},{"type":"search_result_group","domain":"science.org","entries":[{"type":"search_result","url":"https://www.science.org/action/downloadSupplement?doi=10.1126%2Fscience.ads9158&file=science.ads9158_sm.pdf&utm_source=chatgpt.com","title":"Supplementary Materials for - Confronting risks of mirror life","snippet":"... mirror bacteria could plausibly help to manufacture them, such molecules can be made through other means. More speculatively, mirror bacteria might be.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"science.org"}]},{"type":"search_result_group","domain":"hcamag.com","entries":[{"type":"search_result","url":"https://www.hcamag.com/asia/news/general/anthropic-ceo-continues-to-sound-alarm-over-ais-threat-to-jobs/550250?utm_source=chatgpt.com","title":"Anthropic CEO continues to sound alarm over AI's threat ...","snippet":"19 Sept 2025 \u2014 ... jobs. CEO Dario Amodei warned in May that AI could wipe out half of all entry-level white-collar jobs and increase unemployment by up to 20 ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"hcamag.com"}]},{"type":"search_result_group","domain":"brookings.edu","entries":[{"type":"search_result","url":"https://www.brookings.edu/articles/what-is-californias-ai-safety-law/?utm_source=chatgpt.com","title":"What is California's AI safety law?","snippet":"23 Dec 2025 \u2014 On September 29, 2025, as Congress remained deadlocked on comprehensive AI legislation, California Governor Gavin Newsom signed Senate Bill 53 ( ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"brookings.edu"}]},{"type":"search_result_group","domain":"substack.com","entries":[{"type":"search_result","url":"https://askwhocastsai.substack.com/p/the-adolescence-of-technology-by?utm_source=chatgpt.com","title":"The Adolescence of Technology - By Dario Amodei","snippet":"Jan 26, 2026. AI reading of The Adolescence of Technology - By Dario Amodei. ... In this essay, Dario Amodei characterizes the imminent ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"substack.com"},{"type":"search_result","url":"https://empiricrafting.substack.com/p/are-we-there-yet-evaluating-metrs?utm_source=chatgpt.com","title":"Are We There Yet? Evaluating METR's Eval of AI's Ability to ...","snippet":"Seth and Andrey are back to evaluating an AI evaluation, this time discussing METR's paper \u201cMeasuring AI Ability to Complete Long Tasks.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"substack.com"}]},{"type":"search_result_group","domain":"mofo.com","entries":[{"type":"search_result","url":"https://www.mofo.com/resources/insights/260105-new-york-enacts-the-raise-act-regulating-frontier-ai-models?utm_source=chatgpt.com","title":"New York Enacts the RAISE Act Regulating Frontier AI ...","snippet":"5 Jan 2026 \u2014 New York Governor Kathy Hochul signed the Responsible AI Safety and Education Act (RAISE Act) (S6953B /A6453B) on December 19,...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"mofo.com"}]},{"type":"search_result_group","domain":"island.io","entries":[{"type":"search_result","url":"https://www.island.io/new-tab/ai-automation-impact-on-white-collar-jobs?utm_source=chatgpt.com","title":"Anthropic CEO: AI Could Halve Entry-Level Office Jobs in 5 ...","snippet":"2 Jun 2025 \u2014 Anthropic CEO Dario Amodei warns AI could eliminate 50% of entry-level white-collar jobs, pushing unemployment to 10-20% within five years.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"island.io"}]},{"type":"search_result_group","domain":"amr.solutions","entries":[{"type":"search_result","url":"https://amr.solutions/2024/12/12/mirror-bacteria-an-amr-threat-of-unprecedented-magnitude/?utm_source=chatgpt.com","title":"Mirror Bacteria: An AMR threat of unprecedented magnitude","snippet":"12 Dec 2024 \u2014 We are introduced in detail to the concept of \u201cmirror\u201d bacteria in which all chiral elements of the bacterium are replaced by their enantiomeric counterparts.Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"amr.solutions"}]},{"type":"search_result_group","domain":"jdsupra.com","entries":[{"type":"search_result","url":"https://www.jdsupra.com/legalnews/new-year-new-employment-laws-what-1720303/?utm_source=chatgpt.com","title":"New Year, New Employment Laws; What Employers Must ...","snippet":"5 hours ago \u2014 California's Transparency in Frontier Artificial Intelligence Act (TFAIA) is the first U.S. law specifically regulating frontier level AI ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"jdsupra.com"}]},{"type":"search_result_group","domain":"semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Constitutional-AI%3A-Harmlessness-from-AI-Feedback-Bai-Kadavath/3936fd3c6187f606c6e4e2e20b196dbc41cc4654?utm_source=chatgpt.com","title":"[PDF] Constitutional AI: Harmlessness from AI Feedback","snippet":"This paper critically evaluates the attempts to align Artificial Intelligence systems with human values and intentions through Reinforcement Learning from ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"semanticscholar.org"}]},{"type":"search_result_group","domain":"alstonprivacy.com","entries":[{"type":"search_result","url":"https://www.alstonprivacy.com/new-york-regulates-large-artificial-intelligence-models/?utm_source=chatgpt.com","title":"New York Regulates Large Artificial Intelligence Models","snippet":"5 days ago \u2014 Despite the Administration's efforts to stymie state AI legislation, New York has joined California arm-in-arm to regulate large AI models by ...Read more","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"alstonprivacy.com"}]},{"type":"search_result_group","domain":"x.com","entries":[{"type":"search_result","url":"https://x.com/rohanpaul_ai/status/2003142187295691118?utm_source=chatgpt.com","title":"METR's new estimate puts Claude Opus 4.5's 50% long- ...","snippet":"We estimate that, on our tasks, Claude Opus 4.5 has a 50%-time horizon of around 4 hrs 49 mins (95% confidence interval of 1 hr 49 mins to 20 ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"x.com"}]},{"type":"search_result_group","domain":"acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3696410.3714705?utm_source=chatgpt.com","title":"Crafting and Evaluating Constitutions for Constitutional AI","snippet":"We introduce the C3AI framework (Crafting Constitutions for CAI models), which serves two key functions: (1) selecting and structuring principles to form ...Read more","ref_id":{"turn_index":1,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"acm.org"}]}],"citations":[],"request_id":"7549dc88-a08d-412b-8cdb-a7dc981d0ebb","message_type":"next","model_slug":"gpt-5-2-thinking","default_model_slug":"gpt-5-2-thinking","thinking_effort":"max","parent_id":"36932c81-2f72-483a-9e29-879b752c75d9","turn_exchange_id":"fc7b65c9-19a2-492d-b06e-84352e250aef"},"recipient":"all","channel":"final"},"parent":"36932c81-2f72-483a-9e29-879b752c75d9","children":[]}}}