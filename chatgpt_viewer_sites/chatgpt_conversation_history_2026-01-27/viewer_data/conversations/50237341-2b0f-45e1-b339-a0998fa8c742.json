{"id":"50237341-2b0f-45e1-b339-a0998fa8c742","title":"PyTorch support for KANs","create_time":1716257311.246244,"update_time":1716257323.566815,"current_node":"4343df6e-c3c0-483c-8df6-4dfac75ecaa2","mapping":{"aaa1d6c9-6791-4c0a-abb7-f62b3c0e27be":{"id":"aaa1d6c9-6791-4c0a-abb7-f62b3c0e27be","message":null,"parent":null,"children":["9a25ac5c-7846-4a09-976b-ad9391af6d4a"]},"9a25ac5c-7846-4a09-976b-ad9391af6d4a":{"id":"9a25ac5c-7846-4a09-976b-ad9391af6d4a","message":{"id":"9a25ac5c-7846-4a09-976b-ad9391af6d4a","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"aaa1d6c9-6791-4c0a-abb7-f62b3c0e27be","children":["fb049050-d6cd-4d49-baae-3448b96737c8"]},"fb049050-d6cd-4d49-baae-3448b96737c8":{"id":"fb049050-d6cd-4d49-baae-3448b96737c8","message":{"id":"fb049050-d6cd-4d49-baae-3448b96737c8","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_user_system_message":true,"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"About Myself: I have a very high level of mathematical knowledge up to the post-graduate level. \n\nThese are my main areas of interest:\n(1) coding: mainly Python\n(2) mathematics up to graduate level and research level\n(3) machine learning and artificial intelligence"}},"recipient":"all","channel":null},"parent":"9a25ac5c-7846-4a09-976b-ad9391af6d4a","children":["aaa26869-cae8-4b85-a2db-e12435602b44"]},"aaa26869-cae8-4b85-a2db-e12435602b44":{"id":"aaa26869-cae8-4b85-a2db-e12435602b44","message":{"id":"aaa26869-cae8-4b85-a2db-e12435602b44","author":{"role":"user","name":null,"metadata":{}},"create_time":1716257311.255878,"update_time":null,"content":{"content_type":"text","parts":["Do machine leaning libraries like PyTorch currently support Kolmogorov-Arnold Networks?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"887104dfbf1ba069-SIN","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"fb049050-d6cd-4d49-baae-3448b96737c8","children":["11b5550f-47b4-4457-a3ef-63139718015e"]},"11b5550f-47b4-4457-a3ef-63139718015e":{"id":"11b5550f-47b4-4457-a3ef-63139718015e","message":{"id":"11b5550f-47b4-4457-a3ef-63139718015e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716257322.60934,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"aaa26869-cae8-4b85-a2db-e12435602b44","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"aaa26869-cae8-4b85-a2db-e12435602b44","children":["f9239417-7de0-4656-a817-d8fa6dc0a245"]},"f9239417-7de0-4656-a817-d8fa6dc0a245":{"id":"f9239417-7de0-4656-a817-d8fa6dc0a245","message":{"id":"f9239417-7de0-4656-a817-d8fa6dc0a245","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716257322.609689,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"PyTorch support for Kolmogorov-Arnold Networks 2024\")"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200007]},"citations":[],"gizmo_id":null,"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"11b5550f-47b4-4457-a3ef-63139718015e","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"browser","channel":null},"parent":"11b5550f-47b4-4457-a3ef-63139718015e","children":["be0629cd-7199-4cd6-ab2d-add1f06c6b10"]},"be0629cd-7199-4cd6-ab2d-add1f06c6b10":{"id":"be0629cd-7199-4cd6-ab2d-add1f06c6b10","message":{"id":"be0629cd-7199-4cd6-ab2d-add1f06c6b10","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716257322.615405,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020GitHub - 1ssb/torchkan: An easy to use PyTorch implementation of the ...\u2020github.com\u3011\nThe KAN model has demonstrated promising outcomes across various Generative Additive Models (GAMs) since the 1980s. Inspired by a range of sources, this first implementation in KAN in torchkan.py achieves over 97% accuracy with an evaluation time of 0.6 seconds. The quantized model further reduces this to under 0.55 seconds on the MNIST dataset within 8 epochs, utilizing an Nvidia RTX 4090 on ...\n# \u30101\u2020Support for Kolmogorov-Arnold Networks (KANs) #125389 - GitHub\u2020github.com\u3011\nProposal for adding support for Kolmogorov-Arnold Networks (KANs) in PyTorch. KANs are a promising alternative to traditional Multi-Layer Perceptrons (MLPs) and offer potential advantages in terms of accuracy, scaling, and interpretability. KANs are inspired by the Kolmogorov-Arnold representation theorem and have a fundamentally different ...\n# \u30102\u2020Awesome KAN (Kolmogorov-Arnold Network) - GitHub\u2020github.com\u3011\npykan: Offical implementation for Kolmogorov Arnold Networks \uff5c ; efficient-kan: An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN). \uff5c ; FastKAN: Very Fast Calculation of Kolmogorov-Arnold Networks (KAN) \uff5c ; FasterKAN: FasterKAN = FastKAN + RSWAF bases functions and benchmarking with other KANs.Fastest KAN variation as of 5/13/2024, 2 times slower than MLP in ...\n# \u30103\u2020TorchKAN \u00b7 PyPI\u2020pypi.org\u3011\nHashes for TorchKAN-0.1.1-py3-none-any.whl; Algorithm Hash digest; SHA256: b5dc5b99c1fd0246c6431254e7b04cd5e0b6bc6079456e87c064a1d0d983b3e1: Copy : MD5\n# \u30104\u2020KAN: Kolmogorov\u2013Arnold Networks - arXiv.org\u2020arxiv.org\u3011\nAbstract. Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\u201cneurons\u201d), KANs have learnable activation functions on edges (\u201cweights\u201d). KANs have no linear weights at all ...\n# \u30105\u2020KAN: Kolmogorov-Arnold Networks | Papers With Code\u2020paperswithcode.com\u3011\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight ...\n# \u30106\u2020[2404.19756] KAN: Kolmogorov-Arnold Networks - arXiv.org\u2020arxiv.org\u3011\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function ...\n# \u30107\u2020Kolmogorov-Arnold Networks (KANs) and Lennard Jones\u2020kitchingroup.cheme.cmu.edu\u3011\n2. Create and train the model. We start by making the model. We are going to model a Lennard-Jones potential with one input, the distance between two atoms, and one output. We start with a width of 2 \"neurons\". model = KAN(width=[1, 2, 1]) Training is easy. You can even run this cell several times.\n# \u30108\u2020[2404.19756] KAN: Kolmogorov-Arnold Networks\u2020export.arxiv.org\u3011\nComputer Science > Machine Learning. Abstract: Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\n# \u30109\u2020GitHub - Blealtan/efficient-kan: An efficient pure-PyTorch ...\u2020github.com\u3011\nThis repository contains an efficient implementation of Kolmogorov-Arnold Network (KAN). The original implementation of KAN is available here . The performance issue of the original implementation is mostly because it needs to expand all intermediate variables to perform the different activation functions.\n# \u301010\u2020Kolmogorov-Arnold Networks are Radial Basis Function Networks - arXiv.org\u2020arxiv.org\u3011\nKolmogorov-Arnold Networks [ 4], also known as KANs, are a class of neural network architectures inspired by the seminal work of Andrey Kolmogorov and Vladimir Arnold on representing continuous functions using a sum of simple functions. KANs are designed to learn and approximate complex, high-dimensional functions efficiently by decomposing ...\n# \u301011\u2020Paper page - KAN: Kolmogorov-Arnold Networks - Hugging Face\u2020huggingface.co\u3011\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight ...\n# \u301012\u2020Kolmogorov\u2013Arnold Networks (KAN) in PyTorch - GitHub\u2020github.com\u3011\nPython library : torch_kan. Simple implementation of a KAN Layer following the below paper. We use an optimized version of B-Splines computation. Only grid extension feature is implemented, where you are able to arbitrary increase the number of control points to get less smooth splines curves. We try the simple MNIST classification task using ...\n# \u301013\u2020GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks\u2020github.com\u3011\nKolmogorov-Arnold Networks (KANs) are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while ...\n# \u301014\u2020Kolmogorov-Arnold Networks (KANs) for Time Series Analysis\u2020arxiv.org\u3011\nThis paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We ...\n# \u301015\u2020GitHub - riiswa/kanrl: Kolmogorov-Arnold Network for Reinforcement ...\u2020github.com\u3011\nExperimentation. The implementation of Kolmogorov-Arnold Q-Network (KAQN) offers a promising avenue in reinforcement learning. In this project, we replace the Multi-Layer Perceptron (MLP) component of Deep Q-Networks (DQN) with the Kolmogorov-Arnold Network. Furthermore, we employ the Double Deep Q-Network (DDQN) update rule to enhance ...\n# \u301016\u2020Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient ...\u2020arxiv.org\u3011\nAccurate approximation of complex nonlinear functions is a fundamental challenge across many scientific and engineering domains. Traditional neural network architectures often struggle to capture intricate patterns and irregularities present in high-dimensional functions. This paper introduces the Chebyshev Kolmogorov-Arnold Network (Chebyshev KAN), a novel approach that combines the ...\n# \u301017\u2020[2405.07344] TKAN: Temporal Kolmogorov-Arnold Networks - arXiv.org\u2020arxiv.org\u3011\nRecurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing. Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural ...\n# \u301018\u2020A Simplified Explanation Of The New Kolmogorov-Arnold Network ... - Medium\u2020medium.com\u3011\nKolmogorov-Arnold Networks: the latest advance in Neural Networks, simply explained The new type of network that is making waves in the ML world. \u00b7 9 min read \u00b7 May 12, 2024\n# \u301019\u2020Optimizing deeper networks with KFAC in PyTorch. - Medium\u2020yaroslavvb.medium.com\u3011\nFor a fully connected network, an existing optimizer can be augmented with KFAC preconditioning in just a few lines of PyTorch, see \u201cImplementation\u201d below. Consider the following autoencoder on MNIST. Optimizing this architecture using batch of size 10k, the advantage of KFAC is stark, 100x less iterations and 25x less wall-clock time than ...\nVisible: 0% - 100%","summary":null,"assets":[],"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"GitHub - 1ssb/torchkan: An easy to use PyTorch implementation of the ...","url":"https://github.com/1ssb/torchkan","text":"\nThe KAN model has demonstrated promising outcomes across various Generative Additive Models (GAMs) since the 1980s. Inspired by a range of sources, this first implementation in KAN in torchkan.py achieves over 97% accuracy with an evaluation time of 0.6 seconds. The quantized model further reduces this to under 0.55 seconds on the MNIST dataset within 8 epochs, utilizing an Nvidia RTX 4090 on ...\n","pub_date":"2024-05-19T05:36:00.0000000Z","extra":null},{"type":"webpage","title":"Support for Kolmogorov-Arnold Networks (KANs) #125389 - GitHub","url":"https://github.com/pytorch/pytorch/issues/125389","text":"\nProposal for adding support for Kolmogorov-Arnold Networks (KANs) in PyTorch. KANs are a promising alternative to traditional Multi-Layer Perceptrons (MLPs) and offer potential advantages in terms of accuracy, scaling, and interpretability. KANs are inspired by the Kolmogorov-Arnold representation theorem and have a fundamentally different ...\n","pub_date":"2024-05-18T17:30:00.0000000Z","extra":null},{"type":"webpage","title":"Awesome KAN (Kolmogorov-Arnold Network) - GitHub","url":"https://github.com/mintisan/awesome-kan","text":"\npykan: Offical implementation for Kolmogorov Arnold Networks \uff5c ; efficient-kan: An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN). \uff5c ; FastKAN: Very Fast Calculation of Kolmogorov-Arnold Networks (KAN) \uff5c ; FasterKAN: FasterKAN = FastKAN + RSWAF bases functions and benchmarking with other KANs.Fastest KAN variation as of 5/13/2024, 2 times slower than MLP in ...\n","pub_date":"2024-05-19T17:10:00.0000000Z","extra":null},{"type":"webpage","title":"TorchKAN \u00b7 PyPI","url":"https://pypi.org/project/TorchKAN/","text":"\nHashes for TorchKAN-0.1.1-py3-none-any.whl; Algorithm Hash digest; SHA256: b5dc5b99c1fd0246c6431254e7b04cd5e0b6bc6079456e87c064a1d0d983b3e1: Copy : MD5\n","pub_date":"2024-05-17T21:12:00.0000000Z","extra":null},{"type":"webpage","title":"KAN: Kolmogorov\u2013Arnold Networks - arXiv.org","url":"https://arxiv.org/html/2404.19756v1","text":"\nAbstract. Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\u201cneurons\u201d), KANs have learnable activation functions on edges (\u201cweights\u201d). KANs have no linear weights at all ...\n","pub_date":"2024-05-19T20:08:00.0000000Z","extra":null},{"type":"webpage","title":"KAN: Kolmogorov-Arnold Networks | Papers With Code","url":"https://paperswithcode.com/paper/kan-kolmogorov-arnold-networks","text":"\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight ...\n","pub_date":"2024-05-18T10:19:00.0000000Z","extra":null},{"type":"webpage","title":"[2404.19756] KAN: Kolmogorov-Arnold Networks - arXiv.org","url":"https://arxiv.org/abs/2404.19756","text":"\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function ...\n","pub_date":"2024-05-19T16:48:00.0000000Z","extra":null},{"type":"webpage","title":"Kolmogorov-Arnold Networks (KANs) and Lennard Jones","url":"https://kitchingroup.cheme.cmu.edu/blog/2024/05/05/Kolmogorov-Arnold-Networks-KANs-and-Lennard-Jones/","text":"\n2. Create and train the model. We start by making the model. We are going to model a Lennard-Jones potential with one input, the distance between two atoms, and one output. We start with a width of 2 \"neurons\". model = KAN(width=[1, 2, 1]) Training is easy. You can even run this cell several times.\n","pub_date":"2024-05-19T02:57:00.0000000Z","extra":null},{"type":"webpage","title":"[2404.19756] KAN: Kolmogorov-Arnold Networks","url":"http://export.arxiv.org/abs/2404.19756","text":"\nComputer Science > Machine Learning. Abstract: Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\n","pub_date":"2024-05-14T16:57:00.0000000Z","extra":null},{"type":"webpage","title":"GitHub - Blealtan/efficient-kan: An efficient pure-PyTorch ...","url":"https://github.com/Blealtan/efficient-kan","text":"\nThis repository contains an efficient implementation of Kolmogorov-Arnold Network (KAN). The original implementation of KAN is available here . The performance issue of the original implementation is mostly because it needs to expand all intermediate variables to perform the different activation functions.\n","pub_date":"2024-05-18T20:15:00.0000000Z","extra":null},{"type":"webpage","title":"Kolmogorov-Arnold Networks are Radial Basis Function Networks - arXiv.org","url":"https://arxiv.org/html/2405.06721v1","text":"\nKolmogorov-Arnold Networks [ 4], also known as KANs, are a class of neural network architectures inspired by the seminal work of Andrey Kolmogorov and Vladimir Arnold on representing continuous functions using a sum of simple functions. KANs are designed to learn and approximate complex, high-dimensional functions efficiently by decomposing ...\n","pub_date":"2024-05-19T23:51:00.0000000Z","extra":null},{"type":"webpage","title":"Paper page - KAN: Kolmogorov-Arnold Networks - Hugging Face","url":"https://huggingface.co/papers/2404.19756","text":"\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight ...\n","pub_date":"2024-05-19T23:41:00.0000000Z","extra":null},{"type":"webpage","title":"Kolmogorov\u2013Arnold Networks (KAN) in PyTorch - GitHub","url":"https://github.com/Simon-Bertrand/KAN-PyTorch","text":"\nPython library : torch_kan. Simple implementation of a KAN Layer following the below paper. We use an optimized version of B-Splines computation. Only grid extension feature is implemented, where you are able to arbitrary increase the number of control points to get less smooth splines curves. We try the simple MNIST classification task using ...\n","pub_date":"2024-05-18T17:27:00.0000000Z","extra":null},{"type":"webpage","title":"GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks","url":"https://github.com/KindXiaoming/pykan","text":"\nKolmogorov-Arnold Networks (KANs) are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while ...\n","pub_date":"2024-05-19T14:14:00.0000000Z","extra":null},{"type":"webpage","title":"Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","url":"https://arxiv.org/abs/2405.08790","text":"\nThis paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We ...\n","pub_date":"2024-05-18T08:57:00.0000000Z","extra":null},{"type":"webpage","title":"GitHub - riiswa/kanrl: Kolmogorov-Arnold Network for Reinforcement ...","url":"https://github.com/riiswa/kanrl","text":"\nExperimentation. The implementation of Kolmogorov-Arnold Q-Network (KAQN) offers a promising avenue in reinforcement learning. In this project, we replace the Multi-Layer Perceptron (MLP) component of Deep Q-Networks (DQN) with the Kolmogorov-Arnold Network. Furthermore, we employ the Double Deep Q-Network (DDQN) update rule to enhance ...\n","pub_date":"2024-05-19T09:19:00.0000000Z","extra":null},{"type":"webpage","title":"Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient ...","url":"https://arxiv.org/abs/2405.07200","text":"\nAccurate approximation of complex nonlinear functions is a fundamental challenge across many scientific and engineering domains. Traditional neural network architectures often struggle to capture intricate patterns and irregularities present in high-dimensional functions. This paper introduces the Chebyshev Kolmogorov-Arnold Network (Chebyshev KAN), a novel approach that combines the ...\n","pub_date":"2024-05-18T16:53:00.0000000Z","extra":null},{"type":"webpage","title":"[2405.07344] TKAN: Temporal Kolmogorov-Arnold Networks - arXiv.org","url":"https://arxiv.org/abs/2405.07344","text":"\nRecurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing. Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural ...\n","pub_date":"2024-05-19T16:38:00.0000000Z","extra":null},{"type":"webpage","title":"A Simplified Explanation Of The New Kolmogorov-Arnold Network ... - Medium","url":"https://medium.com/@isaakmwangi2018/a-simplified-explanation-of-the-new-kolmogorov-arnold-network-kan-from-mit-cbb59793a040","text":"\nKolmogorov-Arnold Networks: the latest advance in Neural Networks, simply explained The new type of network that is making waves in the ML world. \u00b7 9 min read \u00b7 May 12, 2024\n","pub_date":"2024-05-20T02:07:00.0000000Z","extra":null},{"type":"webpage","title":"Optimizing deeper networks with KFAC in PyTorch. - Medium","url":"https://yaroslavvb.medium.com/optimizing-deeper-networks-with-kfac-in-pytorch-4004adcba1b0","text":"\nFor a fully connected network, an existing optimizer can be augmented with KFAC preconditioning in just a few lines of PyTorch, see \u201cImplementation\u201d below. Consider the following autoencoder on MNIST. Optimizing this architecture using batch of size 10k, the advantage of KFAC is stark, 100x less iterations and 25x less wall-clock time than ...\nVisible: 0% - 100%","pub_date":"2024-05-16T02:47:00.0000000Z","extra":null},{"type":"webpage","title":"Search results for query: 'PyTorch support for Kolmogorov-Arnold Networks 2024'","url":"","text":"# \u30100\u2020GitHub - 1ssb/torchkan: An easy to use PyTorch implementation of the ...\u2020github.com\u3011\nThe KAN model has demonstrated promising outcomes across various Generative Additive Models (GAMs) since the 1980s. Inspired by a range of sources, this first implementation in KAN in torchkan.py achieves over 97% accuracy with an evaluation time of 0.6 seconds. The quantized model further reduces this to under 0.55 seconds on the MNIST dataset within 8 epochs, utilizing an Nvidia RTX 4090 on ...\n# \u30101\u2020Support for Kolmogorov-Arnold Networks (KANs) #125389 - GitHub\u2020github.com\u3011\nProposal for adding support for Kolmogorov-Arnold Networks (KANs) in PyTorch. KANs are a promising alternative to traditional Multi-Layer Perceptrons (MLPs) and offer potential advantages in terms of accuracy, scaling, and interpretability. KANs are inspired by the Kolmogorov-Arnold representation theorem and have a fundamentally different ...\n# \u30102\u2020Awesome KAN (Kolmogorov-Arnold Network) - GitHub\u2020github.com\u3011\npykan: Offical implementation for Kolmogorov Arnold Networks \uff5c ; efficient-kan: An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN). \uff5c ; FastKAN: Very Fast Calculation of Kolmogorov-Arnold Networks (KAN) \uff5c ; FasterKAN: FasterKAN = FastKAN + RSWAF bases functions and benchmarking with other KANs.Fastest KAN variation as of 5/13/2024, 2 times slower than MLP in ...\n# \u30103\u2020TorchKAN \u00b7 PyPI\u2020pypi.org\u3011\nHashes for TorchKAN-0.1.1-py3-none-any.whl; Algorithm Hash digest; SHA256: b5dc5b99c1fd0246c6431254e7b04cd5e0b6bc6079456e87c064a1d0d983b3e1: Copy : MD5\n# \u30104\u2020KAN: Kolmogorov\u2013Arnold Networks - arXiv.org\u2020arxiv.org\u3011\nAbstract. Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\u201cneurons\u201d), KANs have learnable activation functions on edges (\u201cweights\u201d). KANs have no linear weights at all ...\n# \u30105\u2020KAN: Kolmogorov-Arnold Networks | Papers With Code\u2020paperswithcode.com\u3011\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight ...\n# \u30106\u2020[2404.19756] KAN: Kolmogorov-Arnold Networks - arXiv.org\u2020arxiv.org\u3011\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function ...\n# \u30107\u2020Kolmogorov-Arnold Networks (KANs) and Lennard Jones\u2020kitchingroup.cheme.cmu.edu\u3011\n2. Create and train the model. We start by making the model. We are going to model a Lennard-Jones potential with one input, the distance between two atoms, and one output. We start with a width of 2 \"neurons\". model = KAN(width=[1, 2, 1]) Training is easy. You can even run this cell several times.\n# \u30108\u2020[2404.19756] KAN: Kolmogorov-Arnold Networks\u2020export.arxiv.org\u3011\nComputer Science > Machine Learning. Abstract: Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\n# \u30109\u2020GitHub - Blealtan/efficient-kan: An efficient pure-PyTorch ...\u2020github.com\u3011\nThis repository contains an efficient implementation of Kolmogorov-Arnold Network (KAN). The original implementation of KAN is available here . The performance issue of the original implementation is mostly because it needs to expand all intermediate variables to perform the different activation functions.\n# \u301010\u2020Kolmogorov-Arnold Networks are Radial Basis Function Networks - arXiv.org\u2020arxiv.org\u3011\nKolmogorov-Arnold Networks [ 4], also known as KANs, are a class of neural network architectures inspired by the seminal work of Andrey Kolmogorov and Vladimir Arnold on representing continuous functions using a sum of simple functions. KANs are designed to learn and approximate complex, high-dimensional functions efficiently by decomposing ...\n# \u301011\u2020Paper page - KAN: Kolmogorov-Arnold Networks - Hugging Face\u2020huggingface.co\u3011\nInspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight ...\n# \u301012\u2020Kolmogorov\u2013Arnold Networks (KAN) in PyTorch - GitHub\u2020github.com\u3011\nPython library : torch_kan. Simple implementation of a KAN Layer following the below paper. We use an optimized version of B-Splines computation. Only grid extension feature is implemented, where you are able to arbitrary increase the number of control points to get less smooth splines curves. We try the simple MNIST classification task using ...\n# \u301013\u2020GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks\u2020github.com\u3011\nKolmogorov-Arnold Networks (KANs) are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while ...\n# \u301014\u2020Kolmogorov-Arnold Networks (KANs) for Time Series Analysis\u2020arxiv.org\u3011\nThis paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We ...\n# \u301015\u2020GitHub - riiswa/kanrl: Kolmogorov-Arnold Network for Reinforcement ...\u2020github.com\u3011\nExperimentation. The implementation of Kolmogorov-Arnold Q-Network (KAQN) offers a promising avenue in reinforcement learning. In this project, we replace the Multi-Layer Perceptron (MLP) component of Deep Q-Networks (DQN) with the Kolmogorov-Arnold Network. Furthermore, we employ the Double Deep Q-Network (DDQN) update rule to enhance ...\n# \u301016\u2020Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient ...\u2020arxiv.org\u3011\nAccurate approximation of complex nonlinear functions is a fundamental challenge across many scientific and engineering domains. Traditional neural network architectures often struggle to capture intricate patterns and irregularities present in high-dimensional functions. This paper introduces the Chebyshev Kolmogorov-Arnold Network (Chebyshev KAN), a novel approach that combines the ...\n# \u301017\u2020[2405.07344] TKAN: Temporal Kolmogorov-Arnold Networks - arXiv.org\u2020arxiv.org\u3011\nRecurrent Neural Networks (RNNs) have revolutionized many areas of machine learning, particularly in natural language and data sequence processing. Long Short-Term Memory (LSTM) has demonstrated its ability to capture long-term dependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks (KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed a new neural ...\n# \u301018\u2020A Simplified Explanation Of The New Kolmogorov-Arnold Network ... - Medium\u2020medium.com\u3011\nKolmogorov-Arnold Networks: the latest advance in Neural Networks, simply explained The new type of network that is making waves in the ML world. \u00b7 9 min read \u00b7 May 12, 2024\n# \u301019\u2020Optimizing deeper networks with KFAC in PyTorch. - Medium\u2020yaroslavvb.medium.com\u3011\nFor a fully connected network, an existing optimizer can be augmented with KFAC preconditioning in just a few lines of PyTorch, see \u201cImplementation\u201d below. Consider the following autoencoder on MNIST. Optimizing this architecture using batch of size 10k, the advantage of KFAC is stark, 100x less iterations and 25x less wall-clock time than ...\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"search","args":["PyTorch support for Kolmogorov-Arnold Networks 2024"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"f9239417-7de0-4656-a817-d8fa6dc0a245","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"f9239417-7de0-4656-a817-d8fa6dc0a245","children":["a880530f-5cba-463a-b0ad-ede7bbec8e5f"]},"a880530f-5cba-463a-b0ad-ede7bbec8e5f":{"id":"a880530f-5cba-463a-b0ad-ede7bbec8e5f","message":{"id":"a880530f-5cba-463a-b0ad-ede7bbec8e5f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716257322.619852,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"mclick([\"0\", \"1\", \"2\", \"3\", \"12\"])"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200007]},"citations":[],"gizmo_id":null,"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"be0629cd-7199-4cd6-ab2d-add1f06c6b10","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"browser","channel":null},"parent":"be0629cd-7199-4cd6-ab2d-add1f06c6b10","children":["85885461-cfe1-4fdc-b619-8cf66177f881"]},"85885461-cfe1-4fdc-b619-8cf66177f881":{"id":"85885461-cfe1-4fdc-b619-8cf66177f881","message":{"id":"85885461-cfe1-4fdc-b619-8cf66177f881","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716257322.626996,"update_time":null,"content":{"content_type":"tether_quote","url":"https://github.com/1ssb/torchkan","domain":"github.com","text":"\n### \n\n\u301064\u2020polyvolver.py\u3011\n\n### \n\n\u301065\u2020requirements.txt\u3011\n\n### \n\n\u301065\u2020requirements.txt\u3011\n\n### \n\n\u301066\u2020setup.py\u3011\n\n### \n\n\u301066\u2020setup.py\u3011\n\n### \n\n\u301067\u2020to_gif.py\u3011\n\n### \n\n\u301067\u2020to_gif.py\u3011\n\n### \n\n\u301068\u2020torchkan.py\u3011\n\n### \n\n\u301068\u2020torchkan.py\u3011\n\n### \n\n\u301069\u2020wandb_report.pdf\u3011\n\n### \n\n\u301069\u2020wandb_report.pdf\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * MIT license \n\n# TorchKAN: Simplified KAN Model with Variations\n\nTorchKAN introduces a simplified KAN model and its variations, including KANvolver and KAL-Net, designed for high-performance image classification and leveraging polynomial transformations for enhanced feature detection.\n\n## Table of Contents\n\n  1. Overview of the Simplified KAN Model\n  2. KANvolver: Monomial Basis Functions for MNIST Image Classification\n  3. KAL-Net: Utilizing Legendre Polynomials in Kolmogorov Arnold Legendre Networks\n\nThis project showcases the training, validation, and quantization of the KAN model using PyTorch with CUDA acceleration. The `torchkan` model is evaluated on the MNIST dataset, demonstrating significant accuracy improvements.\n\n## Project Status: Under Development\n\nThe KAN model has demonstrated promising outcomes across various Generative Additive Models (GAMs) since the 1980s. Inspired by a range of sources, this first implementation in `KAN` in `torchkan.py` achieves over 97% accuracy with an evaluation time of 0.6 seconds. The quantized model further reduces this to under 0.55 seconds on the MNIST dataset within 8 epochs, utilizing an Nvidia RTX 4090 on Ubuntu 22.04.\n\nMy current understanding: I am not sure of the huge hype with KANs, it is imperative to understand that learning weights for activation functions (MLPs) and learning the activation function themselves are pretty old ideas, its unclear how much interpretability they offer. Its also unclear how scalable, quantisable or efficient they are; as such it seems that the quantisability is not an issue and quantised evaluation on the base model leads to only ~0.6% drop in test performance.\n\nNote: As the model is still being researched, further explorations into its full potential are ongoing. Contributions, questions, and critiques are welcome. I appreciate constructive feedback and contributions. Merge requests will be processed promptly, with a clear outline of the issue, the solution, and its effectiveness.\n\nNote: The PyPI pipeline is currently deprecated and will be stabilized following the release of Version 1.\n\n* * *\n\n### Introduction to the KANvolver Model\n\nThe `KANvolver` model is a specialized neural network designed for classifying images from the MNIST dataset. It achieves an at best accuracy of ~99.56% with a minimal error rate of 0.18%. This model combines convolutional neural networks (CNNs) with polynomial feature expansions, effectively capturing both simple and complex patterns.\n\nI am conducting large-scale analysis to investigate how KANs can be made more interpretable.\n\n### Model Architecture\n\nConvolutional Feature Extraction: The model begins with two convolutional layers, each paired with ReLU activation and max-pooling. The first layer employs 16 filters of size 3x3, while the second increases the feature maps to 32 channels.\n\nPolynomial Feature Transformation: Post feature extraction, the model applies polynomial transformations up to the n'th order to the flattened convolutional outputs, enhancing its ability to discern non-linear relationships.\n\nHow Monomials Work: In the context of this model, monomials are polynomial powers of the input features. By computing monomials up to a specified order, the model can capture non-linear interactions between the features, potentially leading to richer and more informative representations for downstream tasks.\n\nFor a given input image, the monomials of its flattened pixel values are computed, which are then used to adjust the output of linear layers before activation. This approach introduces an additional dimension of feature interaction, allowing the network to learn more complex patterns in the data.\n\n### Forward Propagation Process\n\n  1. Input Reshaping: Images are reshaped from vectors of 784 elements to 1x28x28 tensors for CNN processing.\n  2. Feature Extraction: Spatial features are extracted and pooled through convolutional layers.\n  3. Polynomial Expansion: Features undergo polynomial expansion to capture higher-order interactions.\n  4. Linear Processing: The expanded features are processed by linear layers with normalization and activation.\n  5. Output Generation: The network produces logits for each digit class in MNIST.\n\n### Performance and Conclusion\n\nThe `KANvolver` model's 99.5% accuracy on MNIST underscores its robustness in leveraging CNNs and polynomial expansions for effective digit classification. While showing significant potential, the model remains open for further adaptation and exploration in broader image processing challenges. Here are the results:\n\n### Integrated Gradients for Primary Explainability\n\n[Image 0: Validation Integrated Heatmaps] [Image 1: Validation Integrated Heatmaps] [Image 2: Validation Integrated Heatmaps] [Image 3: Validation Integrated Heatmaps] [Image 4: Validation Integrated Heatmaps]\n\nNote KANvolver uses polynomials which are distinct from the original KANs[1].\n\n* * *\n\n## Introducing KAL_Net\n\nThe `KAL_Net` represents the Kolmogorov Arnold Legendre Network (KAL-Net), a GAM architecture using Legendre polynomials to surpass traditional polynomial approximations like splines in KANs.\n\n### Key Features\n\n  * Polynomial Order: Utilizes Legendre polynomials up to a specific order for each input normalization, capturing nonlinear relationships more efficiently than sim","title":"GitHub - 1ssb/torchkan: An easy to use PyTorch  implementation of the Kolmogorov Arnold Network and a few novel variations","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"GitHub - 1ssb/torchkan: An easy to use PyTorch  implementation of the Kolmogorov Arnold Network and a few novel variations","url":"https://github.com/1ssb/torchkan","text":"\n### \n\n\u301064\u2020polyvolver.py\u3011\n\n### \n\n\u301065\u2020requirements.txt\u3011\n\n### \n\n\u301065\u2020requirements.txt\u3011\n\n### \n\n\u301066\u2020setup.py\u3011\n\n### \n\n\u301066\u2020setup.py\u3011\n\n### \n\n\u301067\u2020to_gif.py\u3011\n\n### \n\n\u301067\u2020to_gif.py\u3011\n\n### \n\n\u301068\u2020torchkan.py\u3011\n\n### \n\n\u301068\u2020torchkan.py\u3011\n\n### \n\n\u301069\u2020wandb_report.pdf\u3011\n\n### \n\n\u301069\u2020wandb_report.pdf\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * MIT license \n\n# TorchKAN: Simplified KAN Model with Variations\n\nTorchKAN introduces a simplified KAN model and its variations, including KANvolver and KAL-Net, designed for high-performance image classification and leveraging polynomial transformations for enhanced feature detection.\n\n## Table of Contents\n\n  1. Overview of the Simplified KAN Model\n  2. KANvolver: Monomial Basis Functions for MNIST Image Classification\n  3. KAL-Net: Utilizing Legendre Polynomials in Kolmogorov Arnold Legendre Networks\n\nThis project showcases the training, validation, and quantization of the KAN model using PyTorch with CUDA acceleration. The `torchkan` model is evaluated on the MNIST dataset, demonstrating significant accuracy improvements.\n\n## Project Status: Under Development\n\nThe KAN model has demonstrated promising outcomes across various Generative Additive Models (GAMs) since the 1980s. Inspired by a range of sources, this first implementation in `KAN` in `torchkan.py` achieves over 97% accuracy with an evaluation time of 0.6 seconds. The quantized model further reduces this to under 0.55 seconds on the MNIST dataset within 8 epochs, utilizing an Nvidia RTX 4090 on Ubuntu 22.04.\n\nMy current understanding: I am not sure of the huge hype with KANs, it is imperative to understand that learning weights for activation functions (MLPs) and learning the activation function themselves are pretty old ideas, its unclear how much interpretability they offer. Its also unclear how scalable, quantisable or efficient they are; as such it seems that the quantisability is not an issue and quantised evaluation on the base model leads to only ~0.6% drop in test performance.\n\nNote: As the model is still being researched, further explorations into its full potential are ongoing. Contributions, questions, and critiques are welcome. I appreciate constructive feedback and contributions. Merge requests will be processed promptly, with a clear outline of the issue, the solution, and its effectiveness.\n\nNote: The PyPI pipeline is currently deprecated and will be stabilized following the release of Version 1.\n\n* * *\n\n### Introduction to the KANvolver Model\n\nThe `KANvolver` model is a specialized neural network designed for classifying images from the MNIST dataset. It achieves an at best accuracy of ~99.56% with a minimal error rate of 0.18%. This model combines convolutional neural networks (CNNs) with polynomial feature expansions, effectively capturing both simple and complex patterns.\n\nI am conducting large-scale analysis to investigate how KANs can be made more interpretable.\n\n### Model Architecture\n\nConvolutional Feature Extraction: The model begins with two convolutional layers, each paired with ReLU activation and max-pooling. The first layer employs 16 filters of size 3x3, while the second increases the feature maps to 32 channels.\n\nPolynomial Feature Transformation: Post feature extraction, the model applies polynomial transformations up to the n'th order to the flattened convolutional outputs, enhancing its ability to discern non-linear relationships.\n\nHow Monomials Work: In the context of this model, monomials are polynomial powers of the input features. By computing monomials up to a specified order, the model can capture non-linear interactions between the features, potentially leading to richer and more informative representations for downstream tasks.\n\nFor a given input image, the monomials of its flattened pixel values are computed, which are then used to adjust the output of linear layers before activation. This approach introduces an additional dimension of feature interaction, allowing the network to learn more complex patterns in the data.\n\n### Forward Propagation Process\n\n  1. Input Reshaping: Images are reshaped from vectors of 784 elements to 1x28x28 tensors for CNN processing.\n  2. Feature Extraction: Spatial features are extracted and pooled through convolutional layers.\n  3. Polynomial Expansion: Features undergo polynomial expansion to capture higher-order interactions.\n  4. Linear Processing: The expanded features are processed by linear layers with normalization and activation.\n  5. Output Generation: The network produces logits for each digit class in MNIST.\n\n### Performance and Conclusion\n\nThe `KANvolver` model's 99.5% accuracy on MNIST underscores its robustness in leveraging CNNs and polynomial expansions for effective digit classification. While showing significant potential, the model remains open for further adaptation and exploration in broader image processing challenges. Here are the results:\n\n### Integrated Gradients for Primary Explainability\n\n[Image 0: Validation Integrated Heatmaps] [Image 1: Validation Integrated Heatmaps] [Image 2: Validation Integrated Heatmaps] [Image 3: Validation Integrated Heatmaps] [Image 4: Validation Integrated Heatmaps]\n\nNote KANvolver uses polynomials which are distinct from the original KANs[1].\n\n* * *\n\n## Introducing KAL_Net\n\nThe `KAL_Net` represents the Kolmogorov Arnold Legendre Network (KAL-Net), a GAM architecture using Legendre polynomials to surpass traditional polynomial approximations like splines in KANs.\n\n### Key Features\n\n  * Polynomial Order: Utilizes Legendre polynomials up to a specific order for each input normalization, capturing nonlinear relationships more efficiently than sim","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[\"0\", \"1\", \"2\", \"3\", \"12\"]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"a880530f-5cba-463a-b0ad-ede7bbec8e5f","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"a880530f-5cba-463a-b0ad-ede7bbec8e5f","children":["37ac9c5d-4bbf-4b1a-acb7-40a0dac71192"]},"37ac9c5d-4bbf-4b1a-acb7-40a0dac71192":{"id":"37ac9c5d-4bbf-4b1a-acb7-40a0dac71192","message":{"id":"37ac9c5d-4bbf-4b1a-acb7-40a0dac71192","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716257322.62732,"update_time":null,"content":{"content_type":"tether_quote","url":"https://github.com/pytorch/pytorch/issues/125389","domain":"github.com","text":"\n\u301032\u2020 Sign up \u3011 \n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.   Dismiss alert\n\n{{ message }}\n\n\u301033\u2020 pytorch \u3011  / \u301034\u2020pytorch\u3011  Public\n\n  * \u301035\u2020 Notifications \u3011 \n  * \u301035\u2020 Fork 21.2k \u3011 \n  * \u301035\u2020 Star 78.6k \u3011 \n\n  * \u301034\u2020 Code \u3011\n  * \u301036\u2020 Issues 5k+ \u3011\n  * \u301037\u2020 Pull requests 974 \u3011\n  * \u301038\u2020 Actions \u3011\n  * \u301039\u2020 Projects 31 \u3011\n  * \u301040\u2020 Wiki \u3011\n  * \u301041\u2020 Security \u3011\n  * \u301042\u2020 Insights \u3011\n\n  Additional navigation options  \n\n  * \u301034\u2020 Code \u3011 \n  * \u301036\u2020 Issues \u3011 \n  * \u301037\u2020 Pull requests \u3011 \n  * \u301038\u2020 Actions \u3011 \n  * \u301039\u2020 Projects \u3011 \n  * \u301040\u2020 Wiki \u3011 \n  * \u301041\u2020 Security \u3011 \n  * \u301042\u2020 Insights \u3011 \n\nNew issue \n\nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community. \n\n\u301043\u2020 Sign up for GitHub \u3011 \n\nBy clicking \u201cSign up for GitHub\u201d, you agree to our \u301044\u2020terms of service\u2020docs.github.com\u3011 and \u301045\u2020privacy statement\u2020docs.github.com\u3011. We\u2019ll occasionally send you account related emails.\n\nAlready on GitHub? \u301046\u2020Sign in\u3011 to your account \n\nJump to bottom \n\n#  Support for Kolmogorov-Arnold Networks (KANs) #125389\n\nOpen \n\n\u301047\u2020anshika-anand\u3011 opened this issue May 2, 2024 \u00b7 3 comments \n\nOpen \n\n#  Support for Kolmogorov-Arnold Networks (KANs) #125389\n\n\u301047\u2020anshika-anand\u3011 opened this issue May 2, 2024 \u00b7 3 comments \n\nLabels\n\n\u301048\u2020 module: nn \u3011 Related to torch.nn \u301049\u2020 triaged \u3011 This issue has been looked at a team member, and triaged and prioritized into an appropriate module\n\n## Comments\n\n[Image 0: @anshika-anand]\n\nCopy link \n\n### \n\n\u301047\u2020anshika-anand\u3011  commented \u301050\u2020May 2, 2024\u3011  \u2022\n\nedited by pytorch-bot bot\n\nProposal for adding support for Kolmogorov-Arnold Networks (KANs) in PyTorch. KANs are a promising alternative to traditional Multi-Layer Perceptrons (MLPs) and offer potential advantages in terms of accuracy, scaling, and interpretability.\n\nKANs are inspired by the Kolmogorov-Arnold representation theorem and have a fundamentally different structure compared to MLPs. Instead of having fixed activation functions on nodes (neurons) and linear weights, KANs have learnable activation functions on edges (weights), and every weight parameter is replaced by a univariate function parameterized as a spline.\n\n### Additional context\n\n\u301051\u2020KAN: Kolmogorov\u2013Arnold Networks\u2020arxiv.org\u3011, The paper was submitted on 30th April,2024 by researchers from Massachusetts Institute of Technology, California Institute of Technology, Northeastern University, The NSF Institute for Artificial Intelligence and Fundamental Interactions.\n\ncc \u301052\u2020@albanD\u3011 \u301053\u2020@mruberry\u3011 \u301054\u2020@jbschlosser\u3011 \u301055\u2020@walterddr\u3011 \u301056\u2020@mikaylagawarecki\u3011  \n\nThe text was updated successfully, but these errors were encountered: \n\n  14 demon2036, KindXiaoming, hvaara, Flecart, kabachuha, nimrodraifer, arghaffari, BulatVakhitov, dzaripov, quocanh010, and 4 more reacted with thumbs up emoji   6 hvaara, Pranavkak, Flecart, vadlemar, JannesBrands, and rafa-br34 reacted with eyes emoji\n\nAll reactions\n\n  * 14 reactions\n  * 6 reactions\n\n[Image 1: @hvaara]\n\nCopy link \n\nContributor\n\n### \n\n\u301057\u2020hvaara\u3011  commented \u301058\u2020May 2, 2024\u3011 \n\nRef \u301059\u2020https://github.com/KindXiaoming/pykan\u3011  \ncc \u301060\u2020@KindXiaoming\u3011  \n\n  5 Pranavkak, Flecart, kabachuha, Charlyo, and nimrodraifer reacted with thumbs up emoji\n\nAll reactions\n\n  * 5 reactions\n\nSorry, something went wrong. \n\n[Image 2: @drisspg] \u301061\u2020drisspg\u3011 added the \u301062\u2020 triage review \u3011 label \u301063\u2020May 3, 2024\u3011 \n\n[Image 3: @jbschlosser] \u301054\u2020jbschlosser\u3011 added \u301048\u2020 module: nn \u3011 Related to torch.nn \u301049\u2020 triaged \u3011 This issue has been looked at a team member, and triaged and prioritized into an appropriate module and removed \u301062\u2020 triage review \u3011 labels \u301064\u2020May 6, 2024\u3011 \n\n[Image 4: @jbschlosser]\n\nCopy link \n\nContributor\n\n### \n\n\u301054\u2020jbschlosser\u3011  commented \u301065\u2020May 6, 2024\u3011  \u2022\n\nedited \n\nHey \u301047\u2020@anshika-a","title":"Support for Kolmogorov-Arnold Networks (KANs) \u00b7 Issue #125389 \u00b7 pytorch/pytorch \u00b7 GitHub","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"Support for Kolmogorov-Arnold Networks (KANs) \u00b7 Issue #125389 \u00b7 pytorch/pytorch \u00b7 GitHub","url":"https://github.com/pytorch/pytorch/issues/125389","text":"\n\u301032\u2020 Sign up \u3011 \n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.   Dismiss alert\n\n{{ message }}\n\n\u301033\u2020 pytorch \u3011  / \u301034\u2020pytorch\u3011  Public\n\n  * \u301035\u2020 Notifications \u3011 \n  * \u301035\u2020 Fork 21.2k \u3011 \n  * \u301035\u2020 Star 78.6k \u3011 \n\n  * \u301034\u2020 Code \u3011\n  * \u301036\u2020 Issues 5k+ \u3011\n  * \u301037\u2020 Pull requests 974 \u3011\n  * \u301038\u2020 Actions \u3011\n  * \u301039\u2020 Projects 31 \u3011\n  * \u301040\u2020 Wiki \u3011\n  * \u301041\u2020 Security \u3011\n  * \u301042\u2020 Insights \u3011\n\n  Additional navigation options  \n\n  * \u301034\u2020 Code \u3011 \n  * \u301036\u2020 Issues \u3011 \n  * \u301037\u2020 Pull requests \u3011 \n  * \u301038\u2020 Actions \u3011 \n  * \u301039\u2020 Projects \u3011 \n  * \u301040\u2020 Wiki \u3011 \n  * \u301041\u2020 Security \u3011 \n  * \u301042\u2020 Insights \u3011 \n\nNew issue \n\nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community. \n\n\u301043\u2020 Sign up for GitHub \u3011 \n\nBy clicking \u201cSign up for GitHub\u201d, you agree to our \u301044\u2020terms of service\u2020docs.github.com\u3011 and \u301045\u2020privacy statement\u2020docs.github.com\u3011. We\u2019ll occasionally send you account related emails.\n\nAlready on GitHub? \u301046\u2020Sign in\u3011 to your account \n\nJump to bottom \n\n#  Support for Kolmogorov-Arnold Networks (KANs) #125389\n\nOpen \n\n\u301047\u2020anshika-anand\u3011 opened this issue May 2, 2024 \u00b7 3 comments \n\nOpen \n\n#  Support for Kolmogorov-Arnold Networks (KANs) #125389\n\n\u301047\u2020anshika-anand\u3011 opened this issue May 2, 2024 \u00b7 3 comments \n\nLabels\n\n\u301048\u2020 module: nn \u3011 Related to torch.nn \u301049\u2020 triaged \u3011 This issue has been looked at a team member, and triaged and prioritized into an appropriate module\n\n## Comments\n\n[Image 0: @anshika-anand]\n\nCopy link \n\n### \n\n\u301047\u2020anshika-anand\u3011  commented \u301050\u2020May 2, 2024\u3011  \u2022\n\nedited by pytorch-bot bot\n\nProposal for adding support for Kolmogorov-Arnold Networks (KANs) in PyTorch. KANs are a promising alternative to traditional Multi-Layer Perceptrons (MLPs) and offer potential advantages in terms of accuracy, scaling, and interpretability.\n\nKANs are inspired by the Kolmogorov-Arnold representation theorem and have a fundamentally different structure compared to MLPs. Instead of having fixed activation functions on nodes (neurons) and linear weights, KANs have learnable activation functions on edges (weights), and every weight parameter is replaced by a univariate function parameterized as a spline.\n\n### Additional context\n\n\u301051\u2020KAN: Kolmogorov\u2013Arnold Networks\u2020arxiv.org\u3011, The paper was submitted on 30th April,2024 by researchers from Massachusetts Institute of Technology, California Institute of Technology, Northeastern University, The NSF Institute for Artificial Intelligence and Fundamental Interactions.\n\ncc \u301052\u2020@albanD\u3011 \u301053\u2020@mruberry\u3011 \u301054\u2020@jbschlosser\u3011 \u301055\u2020@walterddr\u3011 \u301056\u2020@mikaylagawarecki\u3011  \n\nThe text was updated successfully, but these errors were encountered: \n\n  14 demon2036, KindXiaoming, hvaara, Flecart, kabachuha, nimrodraifer, arghaffari, BulatVakhitov, dzaripov, quocanh010, and 4 more reacted with thumbs up emoji   6 hvaara, Pranavkak, Flecart, vadlemar, JannesBrands, and rafa-br34 reacted with eyes emoji\n\nAll reactions\n\n  * 14 reactions\n  * 6 reactions\n\n[Image 1: @hvaara]\n\nCopy link \n\nContributor\n\n### \n\n\u301057\u2020hvaara\u3011  commented \u301058\u2020May 2, 2024\u3011 \n\nRef \u301059\u2020https://github.com/KindXiaoming/pykan\u3011  \ncc \u301060\u2020@KindXiaoming\u3011  \n\n  5 Pranavkak, Flecart, kabachuha, Charlyo, and nimrodraifer reacted with thumbs up emoji\n\nAll reactions\n\n  * 5 reactions\n\nSorry, something went wrong. \n\n[Image 2: @drisspg] \u301061\u2020drisspg\u3011 added the \u301062\u2020 triage review \u3011 label \u301063\u2020May 3, 2024\u3011 \n\n[Image 3: @jbschlosser] \u301054\u2020jbschlosser\u3011 added \u301048\u2020 module: nn \u3011 Related to torch.nn \u301049\u2020 triaged \u3011 This issue has been looked at a team member, and triaged and prioritized into an appropriate module and removed \u301062\u2020 triage review \u3011 labels \u301064\u2020May 6, 2024\u3011 \n\n[Image 4: @jbschlosser]\n\nCopy link \n\nContributor\n\n### \n\n\u301054\u2020jbschlosser\u3011  commented \u301065\u2020May 6, 2024\u3011  \u2022\n\nedited \n\nHey \u301047\u2020@anshika-a","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[\"0\", \"1\", \"2\", \"3\", \"12\"]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"85885461-cfe1-4fdc-b619-8cf66177f881","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"85885461-cfe1-4fdc-b619-8cf66177f881","children":["a91a5825-3843-441e-bf2f-386bbaf13ec3"]},"a91a5825-3843-441e-bf2f-386bbaf13ec3":{"id":"a91a5825-3843-441e-bf2f-386bbaf13ec3","message":{"id":"a91a5825-3843-441e-bf2f-386bbaf13ec3","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716257322.627616,"update_time":null,"content":{"content_type":"tether_quote","url":"https://github.com/mintisan/awesome-kan","domain":"github.com","text":"  * \u301060\u2020FasterKAN\u3011 : FasterKAN = FastKAN + RSWAF bases functions and benchmarking with other KANs. Fastest KAN variation as of 5/13/2024, 2 times slower than MLP in backward speed. \uff5c [Image 9: Github stars]\n  * \u301061\u2020TorchKAN\u3011 : Simplified KAN Model Using Legendre approximations and Monomial basis functions for Image Classification for MNIST. Achieves 99.5% on MNIST using Conv+LegendreKAN. \uff5c [Image 10: Github stars]\n  * \u301062\u2020FourierKAN\u3011 : Pytorch Layer for FourierKAN. It is a layer intended to be a substitution for Linear + non-linear activation | [Image 11: Github stars]\n  * \u301063\u2020ChebyKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using Chebyshev polynomials instead of B-splines. \uff5c [Image 12: Github stars]\n  * \u301064\u2020GraphKAN\u3011 : Implementation of Graph Neural Network version of Kolmogorov Arnold Networks (GraphKAN) \uff5c [Image 13: Github stars]\n  * \u301065\u2020FCN-KAN\u3011 : Kolmogorov\u2013Arnold Networks with modified activation (using fully connected network to represent the activation) \uff5c [Image 14: Github stars]\n  * \u301066\u2020X-KANeRF\u3011 : KAN based NeRF with various basis functions like B-Splines, Fourier, Radial Basis Functions, Polynomials, etc \uff5c [Image 15: Github stars]\n  * \u301067\u2020Large Kolmogorov-Arnold Networks\u3011 : Variations of Kolmogorov-Arnold Networks (including CUDA-supported KAN convolutions) \uff5c [Image 16: Github stars]\n  * \u301059\u2020FastKAN\u3011 : Very Fast Calculation of Kolmogorov-Arnold Networks (KAN) \uff5c [Image 17: Github stars]\n  * \u301068\u2020xKAN\u3011 : Kolmogorov-Arnold Networks with various basis functions like B-Splines, Fourier, Chebyshev, Wavelets etc \uff5c [Image 18: Github stars]\n  * \u301069\u2020JacobiKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using Jacobi polynomials instead of B-splines. \uff5c [Image 19: Github stars]\n  * \u301064\u2020GraphKAN\u3011 : Implementation of Graph Neural Network version of Kolmogorov Arnold Networks (GraphKAN) \uff5c [Image 20: Github stars]\n  * \u301070\u2020OrthogPolyKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using orthogonal polynomials instead of B-splines. \uff5c [Image 21: Github stars]\n  * \u301071\u2020kansformers\u3011 : Kansformers: Transformers using KANs | [Image 22: Github stars]\n  * \u301072\u2020Deep-KAN\u3011: Better implementation of Kolmogorov Arnold Network | [Image 23: Github stars]\n  * \u301073\u2020RBF-KAN\u3011: RBF-KAN is a PyTorch module that implements a Radial Basis Function Kolmogorov-Arnold Network | [Image 24: Github stars]\n  * \u301074\u2020KANX\u3011 : Fast Implementation (Approximation) of Kolmogorov-Arnold Network in JAX | [Image 25: Github stars]\n\n### ConvKANs\n\n  * \u301075\u2020Convolutional-KANs\u3011 : This project extends the idea of the innovative architecture of Kolmogorov-Arnold Networks (KAN) to the Convolutional Layers, changing the classic linear transformation of the convolution to non linear activations in each pixel. \uff5c [Image 26: Github stars]\n  * \u301076\u2020Conv-KAN\u3011 : This repository implements Convolutional Kolmogorov-Arnold Layers with various basis functions. \uff5c [Image 27: Github stars]\n  * \u301077\u2020convkan\u3011 : Implementation of convolutional layer version of KAN (drop-in replacement of Conv2d) \uff5c [Image 28: Github stars]\n  * \u301078\u2020KA-Conv\u3011 : Kolmogorov-Arnold Convolutional Networks with Various Basis Functions (Optimization for Efficiency and GPU memory usage) | [Image 29: Github stars]\n\n### Benchmark\n\n  * \u301079\u2020KAN-benchmarking\u3011 : Benchmark for efficiency in memory and time of different KAN implementations. | [Image 30: Github stars]\n\n### Non-Python\n\n  * \u301080\u2020KolmogorovArnold.jl\u3011 : Very fast Julia implementation of KANs with RBF and RSWAF basis. Extra speedup is gained by writing custom gradients to share work between forward and backward pass. \uff5c [Image 31: Github stars]\n  * \u301081\u2020kan-polar\u3011 : Kolmogorov-Arnold Networks in MATLAB \uff5c [Image 32: Github stars]\n\n### Alternative\n\n  * \u301082\u2020high-order-layers-torch\u3011 : High order piecewise polynomial neural networks using Chebyshev polynomials at Gauss Lobatto nodes (lagrange polynomials). Includes convolutional layers as well HP refinement for non convolutional layers, linear initialization and various applications in the linked repos with varrying levels of success. Euler equations of fluid dynamics, nlp, implicit representation and more | [Image 33: Github stars]\n\n## Project\n\n  * \u301083\u2020KAN-GPT\u3011 : The PyTorch implementation of Generative Pre-trained Tran","title":"GitHub - mintisan/awesome-kan: A comprehensive collection of KAN(Kolmogorov-Arnold Network)-related resources, including libraries, projects, tutorials, papers, and more, for researchers and developers in the Kolmogorov-Arnold Network field.","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"GitHub - mintisan/awesome-kan: A comprehensive collection of KAN(Kolmogorov-Arnold Network)-related resources, including libraries, projects, tutorials, papers, and more, for researchers and developers in the Kolmogorov-Arnold Network field.","url":"https://github.com/mintisan/awesome-kan","text":"  * \u301060\u2020FasterKAN\u3011 : FasterKAN = FastKAN + RSWAF bases functions and benchmarking with other KANs. Fastest KAN variation as of 5/13/2024, 2 times slower than MLP in backward speed. \uff5c [Image 9: Github stars]\n  * \u301061\u2020TorchKAN\u3011 : Simplified KAN Model Using Legendre approximations and Monomial basis functions for Image Classification for MNIST. Achieves 99.5% on MNIST using Conv+LegendreKAN. \uff5c [Image 10: Github stars]\n  * \u301062\u2020FourierKAN\u3011 : Pytorch Layer for FourierKAN. It is a layer intended to be a substitution for Linear + non-linear activation | [Image 11: Github stars]\n  * \u301063\u2020ChebyKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using Chebyshev polynomials instead of B-splines. \uff5c [Image 12: Github stars]\n  * \u301064\u2020GraphKAN\u3011 : Implementation of Graph Neural Network version of Kolmogorov Arnold Networks (GraphKAN) \uff5c [Image 13: Github stars]\n  * \u301065\u2020FCN-KAN\u3011 : Kolmogorov\u2013Arnold Networks with modified activation (using fully connected network to represent the activation) \uff5c [Image 14: Github stars]\n  * \u301066\u2020X-KANeRF\u3011 : KAN based NeRF with various basis functions like B-Splines, Fourier, Radial Basis Functions, Polynomials, etc \uff5c [Image 15: Github stars]\n  * \u301067\u2020Large Kolmogorov-Arnold Networks\u3011 : Variations of Kolmogorov-Arnold Networks (including CUDA-supported KAN convolutions) \uff5c [Image 16: Github stars]\n  * \u301059\u2020FastKAN\u3011 : Very Fast Calculation of Kolmogorov-Arnold Networks (KAN) \uff5c [Image 17: Github stars]\n  * \u301068\u2020xKAN\u3011 : Kolmogorov-Arnold Networks with various basis functions like B-Splines, Fourier, Chebyshev, Wavelets etc \uff5c [Image 18: Github stars]\n  * \u301069\u2020JacobiKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using Jacobi polynomials instead of B-splines. \uff5c [Image 19: Github stars]\n  * \u301064\u2020GraphKAN\u3011 : Implementation of Graph Neural Network version of Kolmogorov Arnold Networks (GraphKAN) \uff5c [Image 20: Github stars]\n  * \u301070\u2020OrthogPolyKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using orthogonal polynomials instead of B-splines. \uff5c [Image 21: Github stars]\n  * \u301071\u2020kansformers\u3011 : Kansformers: Transformers using KANs | [Image 22: Github stars]\n  * \u301072\u2020Deep-KAN\u3011: Better implementation of Kolmogorov Arnold Network | [Image 23: Github stars]\n  * \u301073\u2020RBF-KAN\u3011: RBF-KAN is a PyTorch module that implements a Radial Basis Function Kolmogorov-Arnold Network | [Image 24: Github stars]\n  * \u301074\u2020KANX\u3011 : Fast Implementation (Approximation) of Kolmogorov-Arnold Network in JAX | [Image 25: Github stars]\n\n### ConvKANs\n\n  * \u301075\u2020Convolutional-KANs\u3011 : This project extends the idea of the innovative architecture of Kolmogorov-Arnold Networks (KAN) to the Convolutional Layers, changing the classic linear transformation of the convolution to non linear activations in each pixel. \uff5c [Image 26: Github stars]\n  * \u301076\u2020Conv-KAN\u3011 : This repository implements Convolutional Kolmogorov-Arnold Layers with various basis functions. \uff5c [Image 27: Github stars]\n  * \u301077\u2020convkan\u3011 : Implementation of convolutional layer version of KAN (drop-in replacement of Conv2d) \uff5c [Image 28: Github stars]\n  * \u301078\u2020KA-Conv\u3011 : Kolmogorov-Arnold Convolutional Networks with Various Basis Functions (Optimization for Efficiency and GPU memory usage) | [Image 29: Github stars]\n\n### Benchmark\n\n  * \u301079\u2020KAN-benchmarking\u3011 : Benchmark for efficiency in memory and time of different KAN implementations. | [Image 30: Github stars]\n\n### Non-Python\n\n  * \u301080\u2020KolmogorovArnold.jl\u3011 : Very fast Julia implementation of KANs with RBF and RSWAF basis. Extra speedup is gained by writing custom gradients to share work between forward and backward pass. \uff5c [Image 31: Github stars]\n  * \u301081\u2020kan-polar\u3011 : Kolmogorov-Arnold Networks in MATLAB \uff5c [Image 32: Github stars]\n\n### Alternative\n\n  * \u301082\u2020high-order-layers-torch\u3011 : High order piecewise polynomial neural networks using Chebyshev polynomials at Gauss Lobatto nodes (lagrange polynomials). Includes convolutional layers as well HP refinement for non convolutional layers, linear initialization and various applications in the linked repos with varrying levels of success. Euler equations of fluid dynamics, nlp, implicit representation and more | [Image 33: Github stars]\n\n## Project\n\n  * \u301083\u2020KAN-GPT\u3011 : The PyTorch implementation of Generative Pre-trained Tran","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[\"0\", \"1\", \"2\", \"3\", \"12\"]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"37ac9c5d-4bbf-4b1a-acb7-40a0dac71192","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"37ac9c5d-4bbf-4b1a-acb7-40a0dac71192","children":["4e4f673a-d652-406a-b959-cf261f1ed438"]},"4e4f673a-d652-406a-b959-cf261f1ed438":{"id":"4e4f673a-d652-406a-b959-cf261f1ed438","message":{"id":"4e4f673a-d652-406a-b959-cf261f1ed438","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716257322.62791,"update_time":null,"content":{"content_type":"tether_quote","url":"https://pypi.org/project/TorchKAN/","domain":"pypi.org","text":"\n### 1. Clone the Repository\n\nClone the `torchkan` repository and set up the project environment:\n\n    git clone https://github.com/1ssb/torchkan.git\n    cd torchkan\n    pip install -r requirements.txt\n\nAlternately PyPi install as:\n\n    pip install torchKAN\n\n### 2. Configure CUDA environment variables if they are not already set:\n\n    export PATH=/usr/local/cuda/bin:$PATH\n    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n### 3. Configure Weights & Biases (wandb)\n\nTo track experiments and model performance with wandb:\n\n  1. Set Up wandb Account:\n\n  * Sign up or log in at \u301023\u2020Weights & Biases\u2020wandb.ai\u3011.\n  * Find your API key in your account settings.\n\n  2. Initialize wandb in Your Project:\n\nBefore running the training script, initialize wandb:\n\n    wandb login\n\nWhen prompted, enter your API key. This will link your script executions to your wandb account.\n\n  3. Make sure to change the Entity name in the `mnist.py` file to your username instead of `1ssb`\n\nThis script will train the model, validate it, and log performance metrics using wandb.\n\n## Contact\n\nFor any inquiries or support, contact: Subhransu.Bhattacharjee@anu.edu.au\n\n## Cite this Project\n\nIf you use this project in your research or wish to refer to the baseline results, please use the following BibTeX entry.\n\n    @misc{torchkan,\n      author = {Subhransu S. Bhattacharjee},\n      title = {TorchKAN},\n      year = {2024},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      howpublished = {\\url{https://github.com/1ssb/torchkan/}}\n    }\n\n## Contributions\n\nContributions are welcome. Please raise issues as necessary after commit \"Fin.\", scheduled end-June, 2024. The code is licensed under the MIT License.\n\n## References\n\n  * [0] Ziming Liu et al., \"KAN: Kolmogorov-Arnold Networks\", 2024, arXiv. \u301024\u2020https://arxiv.org/abs/2404.19756\u2020arxiv.org\u3011\n  * [1] \u301025\u2020https://github.com/KindXiaoming/pykan\u2020github.com\u3011\n  * [2] \u301026\u2020https://github.com/Blealtan/efficient-kan\u2020github.com\u3011\n\n## Project details\n\n### Verified details\n\nThese details have been verified by PyPI\n\n###### Maintainers\n\n\u301010\u2020 1ssb \u3011 \n\n### Unverified details\n\nThese details have not been verified by PyPI\n\n###### Project links\n\n  * \u301011\u2020 Homepage \u2020github.com\u3011 \n\n###### GitHub Statistics\n\n  * Stars:\n  * Forks:\n  * Open issues:\n  * Open PRs:\n\nView statistics for this project via \u301012\u2020Libraries.io\u2020libraries.io\u3011, or by using \u301013\u2020our public dataset on Google BigQuery\u2020packaging.python.org\u3011 \n\n###### Meta\n\nLicense: MIT License\n\nAuthor: Subhransu Bhattacharjee\n\n###### Classifiers\n\n  * Development Status\n    * \u301014\u2020 3 - Alpha \u3011 \n  * Intended Audience\n    * \u301015\u2020 Developers \u3011 \n  * License\n    * \u301016\u2020 OSI Approved :: MIT License \u3011 \n  * Programming Language\n    * \u301017\u2020 Python :: 3 \u3011 \n    * \u301018\u2020 Python :: 3.9 \u3011 \n    * \u301019\u2020 Python :: 3.10 \u3011 \n    * \u301020\u2020 Python :: 3.11 \u3011 \n  * Topic\n    * \u301021\u2020 Software Development :: Libraries \u3011 \n\n##  Release history \u301027\u2020Release notifications\u3011 | \u301028\u2020RSS feed \u3011 \n\nThis version\n\n[Image 1] \n\n\u301029\u2020 0.1.1 May 10, 2024 \u3011 \n\n[Image 2] \n\n\u301030\u2020 0.1.0 yanked May 10, 2024 Reason this release was yanked: Updated with a newer model \u3011 \n\n## Download files\n\nDownload the file for your platform. If you're not sure which to choose, learn more about \u301031\u2020installing packages\u2020packaging.python.org\u3011.\n\n###  Source Distribution \n\n\u301032\u2020 TorchKAN-0.1.1.tar.gz \u2020files.pythonhosted.org\u3011 (3.6 kB \u301033\u2020view hashes\u3011) \n\nUploaded  May 10, 2024  `Source`\n\n###  Built Distribution \n\n\u301034\u2020 TorchKAN-0.1.1-py3-none-any.whl \u2020files.pythonhosted.org\u3011 (3.6 kB \u301035\u2020view hashes\u3011) \n\nUploaded  May 10, 2024  `Python 3`\n\nClose \n\n###  \u301036\u2020Hashes\u2020pip.pypa.io\u3011 for TorchKAN-0.1.1.tar.gz \n\nHashes for TorchKAN-0.1.1.tar.gz Algorithm Hash digest  \nSHA256 `06e3529df4e15feca598bd6dcf25de1b6a75e03e897b182026ceb49c64e743af` Copy   \nMD5 `ec2647cc35819df50c346621ce5fe38b` Copy   \nBLAKE2b-256 `1b176e4e99e41a0bdee061a8814b3dc65eb046df83044429c1461d71a3609779` Copy   \n\nClose \n\nClose \n\n###  \u301036\u2020Hashes\u2020pip.pypa.io\u3011 for TorchKAN-0.1","title":"TorchKAN \u00b7 PyPI","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"TorchKAN \u00b7 PyPI","url":"https://pypi.org/project/TorchKAN/","text":"\n### 1. Clone the Repository\n\nClone the `torchkan` repository and set up the project environment:\n\n    git clone https://github.com/1ssb/torchkan.git\n    cd torchkan\n    pip install -r requirements.txt\n\nAlternately PyPi install as:\n\n    pip install torchKAN\n\n### 2. Configure CUDA environment variables if they are not already set:\n\n    export PATH=/usr/local/cuda/bin:$PATH\n    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n### 3. Configure Weights & Biases (wandb)\n\nTo track experiments and model performance with wandb:\n\n  1. Set Up wandb Account:\n\n  * Sign up or log in at \u301023\u2020Weights & Biases\u2020wandb.ai\u3011.\n  * Find your API key in your account settings.\n\n  2. Initialize wandb in Your Project:\n\nBefore running the training script, initialize wandb:\n\n    wandb login\n\nWhen prompted, enter your API key. This will link your script executions to your wandb account.\n\n  3. Make sure to change the Entity name in the `mnist.py` file to your username instead of `1ssb`\n\nThis script will train the model, validate it, and log performance metrics using wandb.\n\n## Contact\n\nFor any inquiries or support, contact: Subhransu.Bhattacharjee@anu.edu.au\n\n## Cite this Project\n\nIf you use this project in your research or wish to refer to the baseline results, please use the following BibTeX entry.\n\n    @misc{torchkan,\n      author = {Subhransu S. Bhattacharjee},\n      title = {TorchKAN},\n      year = {2024},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      howpublished = {\\url{https://github.com/1ssb/torchkan/}}\n    }\n\n## Contributions\n\nContributions are welcome. Please raise issues as necessary after commit \"Fin.\", scheduled end-June, 2024. The code is licensed under the MIT License.\n\n## References\n\n  * [0] Ziming Liu et al., \"KAN: Kolmogorov-Arnold Networks\", 2024, arXiv. \u301024\u2020https://arxiv.org/abs/2404.19756\u2020arxiv.org\u3011\n  * [1] \u301025\u2020https://github.com/KindXiaoming/pykan\u2020github.com\u3011\n  * [2] \u301026\u2020https://github.com/Blealtan/efficient-kan\u2020github.com\u3011\n\n## Project details\n\n### Verified details\n\nThese details have been verified by PyPI\n\n###### Maintainers\n\n\u301010\u2020 1ssb \u3011 \n\n### Unverified details\n\nThese details have not been verified by PyPI\n\n###### Project links\n\n  * \u301011\u2020 Homepage \u2020github.com\u3011 \n\n###### GitHub Statistics\n\n  * Stars:\n  * Forks:\n  * Open issues:\n  * Open PRs:\n\nView statistics for this project via \u301012\u2020Libraries.io\u2020libraries.io\u3011, or by using \u301013\u2020our public dataset on Google BigQuery\u2020packaging.python.org\u3011 \n\n###### Meta\n\nLicense: MIT License\n\nAuthor: Subhransu Bhattacharjee\n\n###### Classifiers\n\n  * Development Status\n    * \u301014\u2020 3 - Alpha \u3011 \n  * Intended Audience\n    * \u301015\u2020 Developers \u3011 \n  * License\n    * \u301016\u2020 OSI Approved :: MIT License \u3011 \n  * Programming Language\n    * \u301017\u2020 Python :: 3 \u3011 \n    * \u301018\u2020 Python :: 3.9 \u3011 \n    * \u301019\u2020 Python :: 3.10 \u3011 \n    * \u301020\u2020 Python :: 3.11 \u3011 \n  * Topic\n    * \u301021\u2020 Software Development :: Libraries \u3011 \n\n##  Release history \u301027\u2020Release notifications\u3011 | \u301028\u2020RSS feed \u3011 \n\nThis version\n\n[Image 1] \n\n\u301029\u2020 0.1.1 May 10, 2024 \u3011 \n\n[Image 2] \n\n\u301030\u2020 0.1.0 yanked May 10, 2024 Reason this release was yanked: Updated with a newer model \u3011 \n\n## Download files\n\nDownload the file for your platform. If you're not sure which to choose, learn more about \u301031\u2020installing packages\u2020packaging.python.org\u3011.\n\n###  Source Distribution \n\n\u301032\u2020 TorchKAN-0.1.1.tar.gz \u2020files.pythonhosted.org\u3011 (3.6 kB \u301033\u2020view hashes\u3011) \n\nUploaded  May 10, 2024  `Source`\n\n###  Built Distribution \n\n\u301034\u2020 TorchKAN-0.1.1-py3-none-any.whl \u2020files.pythonhosted.org\u3011 (3.6 kB \u301035\u2020view hashes\u3011) \n\nUploaded  May 10, 2024  `Python 3`\n\nClose \n\n###  \u301036\u2020Hashes\u2020pip.pypa.io\u3011 for TorchKAN-0.1.1.tar.gz \n\nHashes for TorchKAN-0.1.1.tar.gz Algorithm Hash digest  \nSHA256 `06e3529df4e15feca598bd6dcf25de1b6a75e03e897b182026ceb49c64e743af` Copy   \nMD5 `ec2647cc35819df50c346621ce5fe38b` Copy   \nBLAKE2b-256 `1b176e4e99e41a0bdee061a8814b3dc65eb046df83044429c1461d71a3609779` Copy   \n\nClose \n\nClose \n\n###  \u301036\u2020Hashes\u2020pip.pypa.io\u3011 for TorchKAN-0.1","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[\"0\", \"1\", \"2\", \"3\", \"12\"]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"a91a5825-3843-441e-bf2f-386bbaf13ec3","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"a91a5825-3843-441e-bf2f-386bbaf13ec3","children":["e36924ad-af9c-4493-863b-ef6ad87cdacc"]},"e36924ad-af9c-4493-863b-ef6ad87cdacc":{"id":"e36924ad-af9c-4493-863b-ef6ad87cdacc","message":{"id":"e36924ad-af9c-4493-863b-ef6ad87cdacc","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716257322.628198,"update_time":null,"content":{"content_type":"tether_quote","url":"https://github.com/Simon-Bertrand/KAN-PyTorch","domain":"github.com","text":"    * \u301017\u2020 Education \u2020education.github.com\u3011\n\nBy Solution\n    * \u301018\u2020 CI/CD & Automation \u3011\n    * \u301019\u2020 DevOps \u3011\n    * \u301020\u2020 DevSecOps \u2020resources.github.com\u3011\n\nResources\n    * \u301021\u2020 Learning Pathways \u2020resources.github.com\u3011\n    * \u301022\u2020 White papers, Ebooks, Webinars \u2020resources.github.com\u3011\n    * \u301023\u2020 Customer Stories \u3011\n    * \u301024\u2020 Partners \u2020partner.github.com\u3011\n\n  * Open Source \n\n    * \u301025\u2020 GitHub Sponsors Fund open source developers \u3011\n\n    * \u301026\u2020 The ReadME Project GitHub community articles \u3011\n\nRepositories\n    * \u301027\u2020 Topics \u3011\n    * \u301028\u2020 Trending \u3011\n    * \u301029\u2020 Collections \u3011\n\n  * \u301030\u2020Pricing\u3011 \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\n Search \n\nClear  \n\n\u301031\u2020 Search syntax tips \u2020docs.github.com\u3011 \n\n#  Provide feedback \n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback \n\n#  Saved searches \n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our \u301031\u2020documentation\u2020docs.github.com\u3011. \n\nCancel  Create saved search \n\n\u30101\u2020 Sign in \u3011 \n\n\u301032\u2020 Sign up \u3011 \n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.   Dismiss alert\n\n{{ message }}\n\n\u301033\u2020 Simon-Bertrand \u3011  / \u301034\u2020KAN-PyTorch\u3011  Public\n\n  * \u301035\u2020 Notifications \u3011 \n  * \u301035\u2020 Fork 0 \u3011 \n  * \u301035\u2020 Star 8 \u3011 \n\n  * \n\nKolmogorov\u2013Arnold Networks (KAN) in PyTorch \n\n### License\n\n\u301036\u2020 GPL-3.0 license \u3011 \n\n\u301037\u2020 8 stars \u3011 \u301038\u2020 0 forks \u3011 \u301039\u2020 Branches \u3011 \u301040\u2020 Tags \u3011 \u301041\u2020 Activity \u3011 \n\n\u301035\u2020 Star \u3011 \n\n\u301035\u2020 Notifications \u3011 \n\n  * \u301034\u2020 Code \u3011\n  * \u301042\u2020 Issues 0 \u3011\n  * \u301043\u2020 Pull requests 0 \u3011\n  * \u301044\u2020 Actions \u3011\n  * \u301045\u2020 Projects 0 \u3011\n  * \u301046\u2020 Security \u3011\n  * \u301047\u2020 Insights \u3011\n\n  Additional navigation options  \n\n  * \u301034\u2020 Code \u3011 \n  * \u301042\u2020 Issues \u3011 \n  * \u301043\u2020 Pull requests \u3011 \n  * \u301044\u2020 Actions \u3011 \n  * \u301045\u2020 Projects \u3011 \n  * \u301046\u2020 Security \u3011 \n  * \u301047\u2020 Insights \u3011 \n\n# Simon-Bertrand/KAN-PyTorch\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\n\u00a0main\n\n\u301039\u2020Branches\u3011\u301040\u2020Tags\u3011\n\nGo to file \n\nCode\n\n## Folders and files \n\nName Name \n\nLast commit message \n\nLast commit date   \n\n## Latest commit\n\n## History\n\n\u301048\u202011 Commits\u3011  \n\n### \n\n\u301049\u2020.vscode\u3011\n\n### \n\n\u301049\u2020.vscode\u3011\n\n### \n\n\u301050\u2020examples\u3011\n\n### \n\n\u301050\u2020examples\u3011\n\n### \n\n\u301051\u2020torch_kan\u3011\n\n### \n\n\u301051\u2020torch_kan\u3011\n\n### \n\n\u301052\u2020.gitignore\u3011\n\n### \n\n\u301052\u2020.gitignore\u3011\n\n### \n\n\u301036\u2020LICENSE\u3011\n\n### \n\n\u301036\u2020LICENSE\u3011\n\n### \n\n\u301053\u2020README.md\u3011\n\n### \n\n\u301053\u2020README.md\u3011\n\n### \n\n\u301054\u2020pyproject.toml\u3011\n\n### \n\n\u301054\u2020pyproject.toml\u3011\n\n### \n\n\u301055\u2020requirements.txt\u3011\n\n### \n\n\u301055\u2020requirements.txt\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * GPL-3.0 license \n\n# Python library : torch_kan\n\nSimple implementation of a KAN Layer following the below paper.\n\nWe use an optimized version of B-Splines computation. Only grid extension feature is implemented, where you are able to arbitrary increase the number of control points to get less smooth splines curves.\n\nWe try the simple MNIST classification task using KAN in the example/sb folder.\n\nWe are open for PR concerning library improvements, adding missing features such as automatic pruning and fixing symbolics and correcting wrong interpretation of the original paper.   \n\nIn this library, the paper from Ziming Liu et al. has been implemented :   \n\n# References :\n\n  * Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Solja\u010di\u0107, M., Hou, T. Y., & Tegmark, M. (2024, April 30). KAN: Kolmogorov-Arnold Networks. arXiv.org. \u301056\u2020https://arxiv.org/abs/2404.19756v1\u2020arxiv.org\u3011\n\n* * *\n\n# Install library\n\n    pip install https://github.com/Simon-Bertrand/KAN-PyTorch/archive/main.zip\n\n# Import library\n\n    import torch_kan\n\n## About\n\nKolmogorov\u2013Arnold Networks (KAN) in PyTorch \n\n### Topics\n\n\u301057\u2020 deep-learning \u3011 \u301058\u2020 pytorch \u3011 \u301059\u2020 kan \u3011 \u301060\u2020 kolmogorov-arnold-representation \u3011 \n\n### Resources\n\nReadme \n\n","title":"GitHub - Simon-Bertrand/KAN-PyTorch: Kolmogorov\u2013Arnold Networks (KAN) in PyTorch","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"GitHub - Simon-Bertrand/KAN-PyTorch: Kolmogorov\u2013Arnold Networks (KAN) in PyTorch","url":"https://github.com/Simon-Bertrand/KAN-PyTorch","text":"    * \u301017\u2020 Education \u2020education.github.com\u3011\n\nBy Solution\n    * \u301018\u2020 CI/CD & Automation \u3011\n    * \u301019\u2020 DevOps \u3011\n    * \u301020\u2020 DevSecOps \u2020resources.github.com\u3011\n\nResources\n    * \u301021\u2020 Learning Pathways \u2020resources.github.com\u3011\n    * \u301022\u2020 White papers, Ebooks, Webinars \u2020resources.github.com\u3011\n    * \u301023\u2020 Customer Stories \u3011\n    * \u301024\u2020 Partners \u2020partner.github.com\u3011\n\n  * Open Source \n\n    * \u301025\u2020 GitHub Sponsors Fund open source developers \u3011\n\n    * \u301026\u2020 The ReadME Project GitHub community articles \u3011\n\nRepositories\n    * \u301027\u2020 Topics \u3011\n    * \u301028\u2020 Trending \u3011\n    * \u301029\u2020 Collections \u3011\n\n  * \u301030\u2020Pricing\u3011 \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\n Search \n\nClear  \n\n\u301031\u2020 Search syntax tips \u2020docs.github.com\u3011 \n\n#  Provide feedback \n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback \n\n#  Saved searches \n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our \u301031\u2020documentation\u2020docs.github.com\u3011. \n\nCancel  Create saved search \n\n\u30101\u2020 Sign in \u3011 \n\n\u301032\u2020 Sign up \u3011 \n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.   Dismiss alert\n\n{{ message }}\n\n\u301033\u2020 Simon-Bertrand \u3011  / \u301034\u2020KAN-PyTorch\u3011  Public\n\n  * \u301035\u2020 Notifications \u3011 \n  * \u301035\u2020 Fork 0 \u3011 \n  * \u301035\u2020 Star 8 \u3011 \n\n  * \n\nKolmogorov\u2013Arnold Networks (KAN) in PyTorch \n\n### License\n\n\u301036\u2020 GPL-3.0 license \u3011 \n\n\u301037\u2020 8 stars \u3011 \u301038\u2020 0 forks \u3011 \u301039\u2020 Branches \u3011 \u301040\u2020 Tags \u3011 \u301041\u2020 Activity \u3011 \n\n\u301035\u2020 Star \u3011 \n\n\u301035\u2020 Notifications \u3011 \n\n  * \u301034\u2020 Code \u3011\n  * \u301042\u2020 Issues 0 \u3011\n  * \u301043\u2020 Pull requests 0 \u3011\n  * \u301044\u2020 Actions \u3011\n  * \u301045\u2020 Projects 0 \u3011\n  * \u301046\u2020 Security \u3011\n  * \u301047\u2020 Insights \u3011\n\n  Additional navigation options  \n\n  * \u301034\u2020 Code \u3011 \n  * \u301042\u2020 Issues \u3011 \n  * \u301043\u2020 Pull requests \u3011 \n  * \u301044\u2020 Actions \u3011 \n  * \u301045\u2020 Projects \u3011 \n  * \u301046\u2020 Security \u3011 \n  * \u301047\u2020 Insights \u3011 \n\n# Simon-Bertrand/KAN-PyTorch\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\n\u00a0main\n\n\u301039\u2020Branches\u3011\u301040\u2020Tags\u3011\n\nGo to file \n\nCode\n\n## Folders and files \n\nName Name \n\nLast commit message \n\nLast commit date   \n\n## Latest commit\n\n## History\n\n\u301048\u202011 Commits\u3011  \n\n### \n\n\u301049\u2020.vscode\u3011\n\n### \n\n\u301049\u2020.vscode\u3011\n\n### \n\n\u301050\u2020examples\u3011\n\n### \n\n\u301050\u2020examples\u3011\n\n### \n\n\u301051\u2020torch_kan\u3011\n\n### \n\n\u301051\u2020torch_kan\u3011\n\n### \n\n\u301052\u2020.gitignore\u3011\n\n### \n\n\u301052\u2020.gitignore\u3011\n\n### \n\n\u301036\u2020LICENSE\u3011\n\n### \n\n\u301036\u2020LICENSE\u3011\n\n### \n\n\u301053\u2020README.md\u3011\n\n### \n\n\u301053\u2020README.md\u3011\n\n### \n\n\u301054\u2020pyproject.toml\u3011\n\n### \n\n\u301054\u2020pyproject.toml\u3011\n\n### \n\n\u301055\u2020requirements.txt\u3011\n\n### \n\n\u301055\u2020requirements.txt\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * GPL-3.0 license \n\n# Python library : torch_kan\n\nSimple implementation of a KAN Layer following the below paper.\n\nWe use an optimized version of B-Splines computation. Only grid extension feature is implemented, where you are able to arbitrary increase the number of control points to get less smooth splines curves.\n\nWe try the simple MNIST classification task using KAN in the example/sb folder.\n\nWe are open for PR concerning library improvements, adding missing features such as automatic pruning and fixing symbolics and correcting wrong interpretation of the original paper.   \n\nIn this library, the paper from Ziming Liu et al. has been implemented :   \n\n# References :\n\n  * Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Solja\u010di\u0107, M., Hou, T. Y., & Tegmark, M. (2024, April 30). KAN: Kolmogorov-Arnold Networks. arXiv.org. \u301056\u2020https://arxiv.org/abs/2404.19756v1\u2020arxiv.org\u3011\n\n* * *\n\n# Install library\n\n    pip install https://github.com/Simon-Bertrand/KAN-PyTorch/archive/main.zip\n\n# Import library\n\n    import torch_kan\n\n## About\n\nKolmogorov\u2013Arnold Networks (KAN) in PyTorch \n\n### Topics\n\n\u301057\u2020 deep-learning \u3011 \u301058\u2020 pytorch \u3011 \u301059\u2020 kan \u3011 \u301060\u2020 kolmogorov-arnold-representation \u3011 \n\n### Resources\n\nReadme \n\n","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[\"0\", \"1\", \"2\", \"3\", \"12\"]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"4e4f673a-d652-406a-b959-cf261f1ed438","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"4e4f673a-d652-406a-b959-cf261f1ed438","children":["4343df6e-c3c0-483c-8df6-4dfac75ecaa2"]},"4343df6e-c3c0-483c-8df6-4dfac75ecaa2":{"id":"4343df6e-c3c0-483c-8df6-4dfac75ecaa2","message":{"id":"4343df6e-c3c0-483c-8df6-4dfac75ecaa2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716257322.628482,"update_time":null,"content":{"content_type":"text","parts":["Yes, PyTorch currently supports Kolmogorov-Arnold Networks (KANs) through various implementations and libraries available on GitHub and PyPI. \n\nOne notable implementation is the `torchkan` library, which provides a simplified KAN model along with some variations like KANvolver and KAL-Net. These models are designed for high-performance image classification and leverage polynomial transformations for enhanced feature detection. The `torchkan` library has shown promising results, achieving over 97% accuracy on the MNIST dataset using CUDA acceleration\u30107\u2020source\u3011\u301010\u2020source\u3011.\n\nAnother implementation is available in the `pykan` library, which offers efficient KAN models using different basis functions such as B-splines. This library supports advanced features like grid extension for smoother spline curves and is aimed at researchers looking to explore the full potential of KANs\u30109\u2020source\u3011\u301011\u2020source\u3011.\n\nAdditionally, there is ongoing discussion and development to integrate KANs more formally into PyTorch's ecosystem, as evidenced by proposals and issues raised on PyTorch's GitHub repository\u30108\u2020source\u3011.\n\nFor more details, you can explore the following resources:\n- [TorchKAN GitHub Repository](https://github.com/1ssb/torchkan)\n- [PyKAN GitHub Repository](https://github.com/KindXiaoming/pykan)\n- [TorchKAN on PyPI](https://pypi.org/project/torchKAN)\n\nThese implementations provide a robust starting point for utilizing KANs in your machine learning projects with PyTorch."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"citations":[{"start_ix":555,"end_ix":565,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"GitHub - 1ssb/torchkan: An easy to use PyTorch  implementation of the Kolmogorov Arnold Network and a few novel variations","url":"https://github.com/1ssb/torchkan","text":"\n### \n\n\u301064\u2020polyvolver.py\u3011\n\n### \n\n\u301065\u2020requirements.txt\u3011\n\n### \n\n\u301065\u2020requirements.txt\u3011\n\n### \n\n\u301066\u2020setup.py\u3011\n\n### \n\n\u301066\u2020setup.py\u3011\n\n### \n\n\u301067\u2020to_gif.py\u3011\n\n### \n\n\u301067\u2020to_gif.py\u3011\n\n### \n\n\u301068\u2020torchkan.py\u3011\n\n### \n\n\u301068\u2020torchkan.py\u3011\n\n### \n\n\u301069\u2020wandb_report.pdf\u3011\n\n### \n\n\u301069\u2020wandb_report.pdf\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * MIT license \n\n# TorchKAN: Simplified KAN Model with Variations\n\nTorchKAN introduces a simplified KAN model and its variations, including KANvolver and KAL-Net, designed for high-performance image classification and leveraging polynomial transformations for enhanced feature detection.\n\n## Table of Contents\n\n  1. Overview of the Simplified KAN Model\n  2. KANvolver: Monomial Basis Functions for MNIST Image Classification\n  3. KAL-Net: Utilizing Legendre Polynomials in Kolmogorov Arnold Legendre Networks\n\nThis project showcases the training, validation, and quantization of the KAN model using PyTorch with CUDA acceleration. The `torchkan` model is evaluated on the MNIST dataset, demonstrating significant accuracy improvements.\n\n## Project Status: Under Development\n\nThe KAN model has demonstrated promising outcomes across various Generative Additive Models (GAMs) since the 1980s. Inspired by a range of sources, this first implementation in `KAN` in `torchkan.py` achieves over 97% accuracy with an evaluation time of 0.6 seconds. The quantized model further reduces this to under 0.55 seconds on the MNIST dataset within 8 epochs, utilizing an Nvidia RTX 4090 on Ubuntu 22.04.\n\nMy current understanding: I am not sure of the huge hype with KANs, it is imperative to understand that learning weights for activation functions (MLPs) and learning the activation function themselves are pretty old ideas, its unclear how much interpretability they offer. Its also unclear how scalable, quantisable or efficient they are; as such it seems that the quantisability is not an issue and quantised evaluation on the base model leads to only ~0.6% drop in test performance.\n\nNote: As the model is still being researched, further explorations into its full potential are ongoing. Contributions, questions, and critiques are welcome. I appreciate constructive feedback and contributions. Merge requests will be processed promptly, with a clear outline of the issue, the solution, and its effectiveness.\n\nNote: The PyPI pipeline is currently deprecated and will be stabilized following the release of Version 1.\n\n* * *\n\n### Introduction to the KANvolver Model\n\nThe `KANvolver` model is a specialized neural network designed for classifying images from the MNIST dataset. It achieves an at best accuracy of ~99.56% with a minimal error rate of 0.18%. This model combines convolutional neural networks (CNNs) with polynomial feature expansions, effectively capturing both simple and complex patterns.\n\nI am conducting large-scale analysis to investigate how KANs can be made more interpretable.\n\n### Model Architecture\n\nConvolutional Feature Extraction: The model begins with two convolutional layers, each paired with ReLU activation and max-pooling. The first layer employs 16 filters of size 3x3, while the second increases the feature maps to 32 channels.\n\nPolynomial Feature Transformation: Post feature extraction, the model applies polynomial transformations up to the n'th order to the flattened convolutional outputs, enhancing its ability to discern non-linear relationships.\n\nHow Monomials Work: In the context of this model, monomials are polynomial powers of the input features. By computing monomials up to a specified order, the model can capture non-linear interactions between the features, potentially leading to richer and more informative representations for downstream tasks.\n\nFor a given input image, the monomials of its flattened pixel values are computed, which are then used to adjust the output of linear layers before activation. This approach introduces an additional dimension of feature interaction, allowing the network to learn more complex patterns in the data.\n\n### Forward Propagation Process\n\n  1. Input Reshaping: Images are reshaped from vectors of 784 elements to 1x28x28 tensors for CNN processing.\n  2. Feature Extraction: Spatial features are extracted and pooled through convolutional layers.\n  3. Polynomial Expansion: Features undergo polynomial expansion to capture higher-order interactions.\n  4. Linear Processing: The expanded features are processed by linear layers with normalization and activation.\n  5. Output Generation: The network produces logits for each digit class in MNIST.\n\n### Performance and Conclusion\n\nThe `KANvolver` model's 99.5% accuracy on MNIST underscores its robustness in leveraging CNNs and polynomial expansions for effective digit classification. While showing significant potential, the model remains open for further adaptation and exploration in broader image processing challenges. Here are the results:\n\n### Integrated Gradients for Primary Explainability\n\n[Image 0: Validation Integrated Heatmaps] [Image 1: Validation Integrated Heatmaps] [Image 2: Validation Integrated Heatmaps] [Image 3: Validation Integrated Heatmaps] [Image 4: Validation Integrated Heatmaps]\n\nNote KANvolver uses polynomials which are distinct from the original KANs[1].\n\n* * *\n\n## Introducing KAL_Net\n\nThe `KAL_Net` represents the Kolmogorov Arnold Legendre Network (KAL-Net), a GAM architecture using Legendre polynomials to surpass traditional polynomial approximations like splines in KANs.\n\n### Key Features\n\n  * Polynomial Order: Utilizes Legendre polynomials up to a specific order for each input normalization, capturing nonlinear relationships more efficiently than sim","pub_date":null,"extra":{"cited_message_idx":7,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":565,"end_ix":576,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"TorchKAN \u00b7 PyPI","url":"https://pypi.org/project/TorchKAN/","text":"\n### 1. Clone the Repository\n\nClone the `torchkan` repository and set up the project environment:\n\n    git clone https://github.com/1ssb/torchkan.git\n    cd torchkan\n    pip install -r requirements.txt\n\nAlternately PyPi install as:\n\n    pip install torchKAN\n\n### 2. Configure CUDA environment variables if they are not already set:\n\n    export PATH=/usr/local/cuda/bin:$PATH\n    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n### 3. Configure Weights & Biases (wandb)\n\nTo track experiments and model performance with wandb:\n\n  1. Set Up wandb Account:\n\n  * Sign up or log in at \u301023\u2020Weights & Biases\u2020wandb.ai\u3011.\n  * Find your API key in your account settings.\n\n  2. Initialize wandb in Your Project:\n\nBefore running the training script, initialize wandb:\n\n    wandb login\n\nWhen prompted, enter your API key. This will link your script executions to your wandb account.\n\n  3. Make sure to change the Entity name in the `mnist.py` file to your username instead of `1ssb`\n\nThis script will train the model, validate it, and log performance metrics using wandb.\n\n## Contact\n\nFor any inquiries or support, contact: Subhransu.Bhattacharjee@anu.edu.au\n\n## Cite this Project\n\nIf you use this project in your research or wish to refer to the baseline results, please use the following BibTeX entry.\n\n    @misc{torchkan,\n      author = {Subhransu S. Bhattacharjee},\n      title = {TorchKAN},\n      year = {2024},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      howpublished = {\\url{https://github.com/1ssb/torchkan/}}\n    }\n\n## Contributions\n\nContributions are welcome. Please raise issues as necessary after commit \"Fin.\", scheduled end-June, 2024. The code is licensed under the MIT License.\n\n## References\n\n  * [0] Ziming Liu et al., \"KAN: Kolmogorov-Arnold Networks\", 2024, arXiv. \u301024\u2020https://arxiv.org/abs/2404.19756\u2020arxiv.org\u3011\n  * [1] \u301025\u2020https://github.com/KindXiaoming/pykan\u2020github.com\u3011\n  * [2] \u301026\u2020https://github.com/Blealtan/efficient-kan\u2020github.com\u3011\n\n## Project details\n\n### Verified details\n\nThese details have been verified by PyPI\n\n###### Maintainers\n\n\u301010\u2020 1ssb \u3011 \n\n### Unverified details\n\nThese details have not been verified by PyPI\n\n###### Project links\n\n  * \u301011\u2020 Homepage \u2020github.com\u3011 \n\n###### GitHub Statistics\n\n  * Stars:\n  * Forks:\n  * Open issues:\n  * Open PRs:\n\nView statistics for this project via \u301012\u2020Libraries.io\u2020libraries.io\u3011, or by using \u301013\u2020our public dataset on Google BigQuery\u2020packaging.python.org\u3011 \n\n###### Meta\n\nLicense: MIT License\n\nAuthor: Subhransu Bhattacharjee\n\n###### Classifiers\n\n  * Development Status\n    * \u301014\u2020 3 - Alpha \u3011 \n  * Intended Audience\n    * \u301015\u2020 Developers \u3011 \n  * License\n    * \u301016\u2020 OSI Approved :: MIT License \u3011 \n  * Programming Language\n    * \u301017\u2020 Python :: 3 \u3011 \n    * \u301018\u2020 Python :: 3.9 \u3011 \n    * \u301019\u2020 Python :: 3.10 \u3011 \n    * \u301020\u2020 Python :: 3.11 \u3011 \n  * Topic\n    * \u301021\u2020 Software Development :: Libraries \u3011 \n\n##  Release history \u301027\u2020Release notifications\u3011 | \u301028\u2020RSS feed \u3011 \n\nThis version\n\n[Image 1] \n\n\u301029\u2020 0.1.1 May 10, 2024 \u3011 \n\n[Image 2] \n\n\u301030\u2020 0.1.0 yanked May 10, 2024 Reason this release was yanked: Updated with a newer model \u3011 \n\n## Download files\n\nDownload the file for your platform. If you're not sure which to choose, learn more about \u301031\u2020installing packages\u2020packaging.python.org\u3011.\n\n###  Source Distribution \n\n\u301032\u2020 TorchKAN-0.1.1.tar.gz \u2020files.pythonhosted.org\u3011 (3.6 kB \u301033\u2020view hashes\u3011) \n\nUploaded  May 10, 2024  `Source`\n\n###  Built Distribution \n\n\u301034\u2020 TorchKAN-0.1.1-py3-none-any.whl \u2020files.pythonhosted.org\u3011 (3.6 kB \u301035\u2020view hashes\u3011) \n\nUploaded  May 10, 2024  `Python 3`\n\nClose \n\n###  \u301036\u2020Hashes\u2020pip.pypa.io\u3011 for TorchKAN-0.1.1.tar.gz \n\nHashes for TorchKAN-0.1.1.tar.gz Algorithm Hash digest  \nSHA256 `06e3529df4e15feca598bd6dcf25de1b6a75e03e897b182026ceb49c64e743af` Copy   \nMD5 `ec2647cc35819df50c346621ce5fe38b` Copy   \nBLAKE2b-256 `1b176e4e99e41a0bdee061a8814b3dc65eb046df83044429c1461d71a3609779` Copy   \n\nClose \n\nClose \n\n###  \u301036\u2020Hashes\u2020pip.pypa.io\u3011 for TorchKAN-0.1","pub_date":null,"extra":{"cited_message_idx":10,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":884,"end_ix":894,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"GitHub - mintisan/awesome-kan: A comprehensive collection of KAN(Kolmogorov-Arnold Network)-related resources, including libraries, projects, tutorials, papers, and more, for researchers and developers in the Kolmogorov-Arnold Network field.","url":"https://github.com/mintisan/awesome-kan","text":"  * \u301060\u2020FasterKAN\u3011 : FasterKAN = FastKAN + RSWAF bases functions and benchmarking with other KANs. Fastest KAN variation as of 5/13/2024, 2 times slower than MLP in backward speed. \uff5c [Image 9: Github stars]\n  * \u301061\u2020TorchKAN\u3011 : Simplified KAN Model Using Legendre approximations and Monomial basis functions for Image Classification for MNIST. Achieves 99.5% on MNIST using Conv+LegendreKAN. \uff5c [Image 10: Github stars]\n  * \u301062\u2020FourierKAN\u3011 : Pytorch Layer for FourierKAN. It is a layer intended to be a substitution for Linear + non-linear activation | [Image 11: Github stars]\n  * \u301063\u2020ChebyKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using Chebyshev polynomials instead of B-splines. \uff5c [Image 12: Github stars]\n  * \u301064\u2020GraphKAN\u3011 : Implementation of Graph Neural Network version of Kolmogorov Arnold Networks (GraphKAN) \uff5c [Image 13: Github stars]\n  * \u301065\u2020FCN-KAN\u3011 : Kolmogorov\u2013Arnold Networks with modified activation (using fully connected network to represent the activation) \uff5c [Image 14: Github stars]\n  * \u301066\u2020X-KANeRF\u3011 : KAN based NeRF with various basis functions like B-Splines, Fourier, Radial Basis Functions, Polynomials, etc \uff5c [Image 15: Github stars]\n  * \u301067\u2020Large Kolmogorov-Arnold Networks\u3011 : Variations of Kolmogorov-Arnold Networks (including CUDA-supported KAN convolutions) \uff5c [Image 16: Github stars]\n  * \u301059\u2020FastKAN\u3011 : Very Fast Calculation of Kolmogorov-Arnold Networks (KAN) \uff5c [Image 17: Github stars]\n  * \u301068\u2020xKAN\u3011 : Kolmogorov-Arnold Networks with various basis functions like B-Splines, Fourier, Chebyshev, Wavelets etc \uff5c [Image 18: Github stars]\n  * \u301069\u2020JacobiKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using Jacobi polynomials instead of B-splines. \uff5c [Image 19: Github stars]\n  * \u301064\u2020GraphKAN\u3011 : Implementation of Graph Neural Network version of Kolmogorov Arnold Networks (GraphKAN) \uff5c [Image 20: Github stars]\n  * \u301070\u2020OrthogPolyKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using orthogonal polynomials instead of B-splines. \uff5c [Image 21: Github stars]\n  * \u301071\u2020kansformers\u3011 : Kansformers: Transformers using KANs | [Image 22: Github stars]\n  * \u301072\u2020Deep-KAN\u3011: Better implementation of Kolmogorov Arnold Network | [Image 23: Github stars]\n  * \u301073\u2020RBF-KAN\u3011: RBF-KAN is a PyTorch module that implements a Radial Basis Function Kolmogorov-Arnold Network | [Image 24: Github stars]\n  * \u301074\u2020KANX\u3011 : Fast Implementation (Approximation) of Kolmogorov-Arnold Network in JAX | [Image 25: Github stars]\n\n### ConvKANs\n\n  * \u301075\u2020Convolutional-KANs\u3011 : This project extends the idea of the innovative architecture of Kolmogorov-Arnold Networks (KAN) to the Convolutional Layers, changing the classic linear transformation of the convolution to non linear activations in each pixel. \uff5c [Image 26: Github stars]\n  * \u301076\u2020Conv-KAN\u3011 : This repository implements Convolutional Kolmogorov-Arnold Layers with various basis functions. \uff5c [Image 27: Github stars]\n  * \u301077\u2020convkan\u3011 : Implementation of convolutional layer version of KAN (drop-in replacement of Conv2d) \uff5c [Image 28: Github stars]\n  * \u301078\u2020KA-Conv\u3011 : Kolmogorov-Arnold Convolutional Networks with Various Basis Functions (Optimization for Efficiency and GPU memory usage) | [Image 29: Github stars]\n\n### Benchmark\n\n  * \u301079\u2020KAN-benchmarking\u3011 : Benchmark for efficiency in memory and time of different KAN implementations. | [Image 30: Github stars]\n\n### Non-Python\n\n  * \u301080\u2020KolmogorovArnold.jl\u3011 : Very fast Julia implementation of KANs with RBF and RSWAF basis. Extra speedup is gained by writing custom gradients to share work between forward and backward pass. \uff5c [Image 31: Github stars]\n  * \u301081\u2020kan-polar\u3011 : Kolmogorov-Arnold Networks in MATLAB \uff5c [Image 32: Github stars]\n\n### Alternative\n\n  * \u301082\u2020high-order-layers-torch\u3011 : High order piecewise polynomial neural networks using Chebyshev polynomials at Gauss Lobatto nodes (lagrange polynomials). Includes convolutional layers as well HP refinement for non convolutional layers, linear initialization and various applications in the linked repos with varrying levels of success. Euler equations of fluid dynamics, nlp, implicit representation and more | [Image 33: Github stars]\n\n## Project\n\n  * \u301083\u2020KAN-GPT\u3011 : The PyTorch implementation of Generative Pre-trained Tran","pub_date":null,"extra":{"cited_message_idx":9,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":894,"end_ix":905,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"GitHub - Simon-Bertrand/KAN-PyTorch: Kolmogorov\u2013Arnold Networks (KAN) in PyTorch","url":"https://github.com/Simon-Bertrand/KAN-PyTorch","text":"    * \u301017\u2020 Education \u2020education.github.com\u3011\n\nBy Solution\n    * \u301018\u2020 CI/CD & Automation \u3011\n    * \u301019\u2020 DevOps \u3011\n    * \u301020\u2020 DevSecOps \u2020resources.github.com\u3011\n\nResources\n    * \u301021\u2020 Learning Pathways \u2020resources.github.com\u3011\n    * \u301022\u2020 White papers, Ebooks, Webinars \u2020resources.github.com\u3011\n    * \u301023\u2020 Customer Stories \u3011\n    * \u301024\u2020 Partners \u2020partner.github.com\u3011\n\n  * Open Source \n\n    * \u301025\u2020 GitHub Sponsors Fund open source developers \u3011\n\n    * \u301026\u2020 The ReadME Project GitHub community articles \u3011\n\nRepositories\n    * \u301027\u2020 Topics \u3011\n    * \u301028\u2020 Trending \u3011\n    * \u301029\u2020 Collections \u3011\n\n  * \u301030\u2020Pricing\u3011 \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\n Search \n\nClear  \n\n\u301031\u2020 Search syntax tips \u2020docs.github.com\u3011 \n\n#  Provide feedback \n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback \n\n#  Saved searches \n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our \u301031\u2020documentation\u2020docs.github.com\u3011. \n\nCancel  Create saved search \n\n\u30101\u2020 Sign in \u3011 \n\n\u301032\u2020 Sign up \u3011 \n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.   Dismiss alert\n\n{{ message }}\n\n\u301033\u2020 Simon-Bertrand \u3011  / \u301034\u2020KAN-PyTorch\u3011  Public\n\n  * \u301035\u2020 Notifications \u3011 \n  * \u301035\u2020 Fork 0 \u3011 \n  * \u301035\u2020 Star 8 \u3011 \n\n  * \n\nKolmogorov\u2013Arnold Networks (KAN) in PyTorch \n\n### License\n\n\u301036\u2020 GPL-3.0 license \u3011 \n\n\u301037\u2020 8 stars \u3011 \u301038\u2020 0 forks \u3011 \u301039\u2020 Branches \u3011 \u301040\u2020 Tags \u3011 \u301041\u2020 Activity \u3011 \n\n\u301035\u2020 Star \u3011 \n\n\u301035\u2020 Notifications \u3011 \n\n  * \u301034\u2020 Code \u3011\n  * \u301042\u2020 Issues 0 \u3011\n  * \u301043\u2020 Pull requests 0 \u3011\n  * \u301044\u2020 Actions \u3011\n  * \u301045\u2020 Projects 0 \u3011\n  * \u301046\u2020 Security \u3011\n  * \u301047\u2020 Insights \u3011\n\n  Additional navigation options  \n\n  * \u301034\u2020 Code \u3011 \n  * \u301042\u2020 Issues \u3011 \n  * \u301043\u2020 Pull requests \u3011 \n  * \u301044\u2020 Actions \u3011 \n  * \u301045\u2020 Projects \u3011 \n  * \u301046\u2020 Security \u3011 \n  * \u301047\u2020 Insights \u3011 \n\n# Simon-Bertrand/KAN-PyTorch\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\n\u00a0main\n\n\u301039\u2020Branches\u3011\u301040\u2020Tags\u3011\n\nGo to file \n\nCode\n\n## Folders and files \n\nName Name \n\nLast commit message \n\nLast commit date   \n\n## Latest commit\n\n## History\n\n\u301048\u202011 Commits\u3011  \n\n### \n\n\u301049\u2020.vscode\u3011\n\n### \n\n\u301049\u2020.vscode\u3011\n\n### \n\n\u301050\u2020examples\u3011\n\n### \n\n\u301050\u2020examples\u3011\n\n### \n\n\u301051\u2020torch_kan\u3011\n\n### \n\n\u301051\u2020torch_kan\u3011\n\n### \n\n\u301052\u2020.gitignore\u3011\n\n### \n\n\u301052\u2020.gitignore\u3011\n\n### \n\n\u301036\u2020LICENSE\u3011\n\n### \n\n\u301036\u2020LICENSE\u3011\n\n### \n\n\u301053\u2020README.md\u3011\n\n### \n\n\u301053\u2020README.md\u3011\n\n### \n\n\u301054\u2020pyproject.toml\u3011\n\n### \n\n\u301054\u2020pyproject.toml\u3011\n\n### \n\n\u301055\u2020requirements.txt\u3011\n\n### \n\n\u301055\u2020requirements.txt\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * GPL-3.0 license \n\n# Python library : torch_kan\n\nSimple implementation of a KAN Layer following the below paper.\n\nWe use an optimized version of B-Splines computation. Only grid extension feature is implemented, where you are able to arbitrary increase the number of control points to get less smooth splines curves.\n\nWe try the simple MNIST classification task using KAN in the example/sb folder.\n\nWe are open for PR concerning library improvements, adding missing features such as automatic pruning and fixing symbolics and correcting wrong interpretation of the original paper.   \n\nIn this library, the paper from Ziming Liu et al. has been implemented :   \n\n# References :\n\n  * Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Solja\u010di\u0107, M., Hou, T. Y., & Tegmark, M. (2024, April 30). KAN: Kolmogorov-Arnold Networks. arXiv.org. \u301056\u2020https://arxiv.org/abs/2404.19756v1\u2020arxiv.org\u3011\n\n* * *\n\n# Install library\n\n    pip install https://github.com/Simon-Bertrand/KAN-PyTorch/archive/main.zip\n\n# Import library\n\n    import torch_kan\n\n## About\n\nKolmogorov\u2013Arnold Networks (KAN) in PyTorch \n\n### Topics\n\n\u301057\u2020 deep-learning \u3011 \u301058\u2020 pytorch \u3011 \u301059\u2020 kan \u3011 \u301060\u2020 kolmogorov-arnold-representation \u3011 \n\n### Resources\n\nReadme \n\n","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":1098,"end_ix":1108,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Support for Kolmogorov-Arnold Networks (KANs) \u00b7 Issue #125389 \u00b7 pytorch/pytorch \u00b7 GitHub","url":"https://github.com/pytorch/pytorch/issues/125389","text":"\n\u301032\u2020 Sign up \u3011 \n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.   Dismiss alert\n\n{{ message }}\n\n\u301033\u2020 pytorch \u3011  / \u301034\u2020pytorch\u3011  Public\n\n  * \u301035\u2020 Notifications \u3011 \n  * \u301035\u2020 Fork 21.2k \u3011 \n  * \u301035\u2020 Star 78.6k \u3011 \n\n  * \u301034\u2020 Code \u3011\n  * \u301036\u2020 Issues 5k+ \u3011\n  * \u301037\u2020 Pull requests 974 \u3011\n  * \u301038\u2020 Actions \u3011\n  * \u301039\u2020 Projects 31 \u3011\n  * \u301040\u2020 Wiki \u3011\n  * \u301041\u2020 Security \u3011\n  * \u301042\u2020 Insights \u3011\n\n  Additional navigation options  \n\n  * \u301034\u2020 Code \u3011 \n  * \u301036\u2020 Issues \u3011 \n  * \u301037\u2020 Pull requests \u3011 \n  * \u301038\u2020 Actions \u3011 \n  * \u301039\u2020 Projects \u3011 \n  * \u301040\u2020 Wiki \u3011 \n  * \u301041\u2020 Security \u3011 \n  * \u301042\u2020 Insights \u3011 \n\nNew issue \n\nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community. \n\n\u301043\u2020 Sign up for GitHub \u3011 \n\nBy clicking \u201cSign up for GitHub\u201d, you agree to our \u301044\u2020terms of service\u2020docs.github.com\u3011 and \u301045\u2020privacy statement\u2020docs.github.com\u3011. We\u2019ll occasionally send you account related emails.\n\nAlready on GitHub? \u301046\u2020Sign in\u3011 to your account \n\nJump to bottom \n\n#  Support for Kolmogorov-Arnold Networks (KANs) #125389\n\nOpen \n\n\u301047\u2020anshika-anand\u3011 opened this issue May 2, 2024 \u00b7 3 comments \n\nOpen \n\n#  Support for Kolmogorov-Arnold Networks (KANs) #125389\n\n\u301047\u2020anshika-anand\u3011 opened this issue May 2, 2024 \u00b7 3 comments \n\nLabels\n\n\u301048\u2020 module: nn \u3011 Related to torch.nn \u301049\u2020 triaged \u3011 This issue has been looked at a team member, and triaged and prioritized into an appropriate module\n\n## Comments\n\n[Image 0: @anshika-anand]\n\nCopy link \n\n### \n\n\u301047\u2020anshika-anand\u3011  commented \u301050\u2020May 2, 2024\u3011  \u2022\n\nedited by pytorch-bot bot\n\nProposal for adding support for Kolmogorov-Arnold Networks (KANs) in PyTorch. KANs are a promising alternative to traditional Multi-Layer Perceptrons (MLPs) and offer potential advantages in terms of accuracy, scaling, and interpretability.\n\nKANs are inspired by the Kolmogorov-Arnold representation theorem and have a fundamentally different structure compared to MLPs. Instead of having fixed activation functions on nodes (neurons) and linear weights, KANs have learnable activation functions on edges (weights), and every weight parameter is replaced by a univariate function parameterized as a spline.\n\n### Additional context\n\n\u301051\u2020KAN: Kolmogorov\u2013Arnold Networks\u2020arxiv.org\u3011, The paper was submitted on 30th April,2024 by researchers from Massachusetts Institute of Technology, California Institute of Technology, Northeastern University, The NSF Institute for Artificial Intelligence and Fundamental Interactions.\n\ncc \u301052\u2020@albanD\u3011 \u301053\u2020@mruberry\u3011 \u301054\u2020@jbschlosser\u3011 \u301055\u2020@walterddr\u3011 \u301056\u2020@mikaylagawarecki\u3011  \n\nThe text was updated successfully, but these errors were encountered: \n\n  14 demon2036, KindXiaoming, hvaara, Flecart, kabachuha, nimrodraifer, arghaffari, BulatVakhitov, dzaripov, quocanh010, and 4 more reacted with thumbs up emoji   6 hvaara, Pranavkak, Flecart, vadlemar, JannesBrands, and rafa-br34 reacted with eyes emoji\n\nAll reactions\n\n  * 14 reactions\n  * 6 reactions\n\n[Image 1: @hvaara]\n\nCopy link \n\nContributor\n\n### \n\n\u301057\u2020hvaara\u3011  commented \u301058\u2020May 2, 2024\u3011 \n\nRef \u301059\u2020https://github.com/KindXiaoming/pykan\u3011  \ncc \u301060\u2020@KindXiaoming\u3011  \n\n  5 Pranavkak, Flecart, kabachuha, Charlyo, and nimrodraifer reacted with thumbs up emoji\n\nAll reactions\n\n  * 5 reactions\n\nSorry, something went wrong. \n\n[Image 2: @drisspg] \u301061\u2020drisspg\u3011 added the \u301062\u2020 triage review \u3011 label \u301063\u2020May 3, 2024\u3011 \n\n[Image 3: @jbschlosser] \u301054\u2020jbschlosser\u3011 added \u301048\u2020 module: nn \u3011 Related to torch.nn \u301049\u2020 triaged \u3011 This issue has been looked at a team member, and triaged and prioritized into an appropriate module and removed \u301062\u2020 triage review \u3011 labels \u301064\u2020May 6, 2024\u3011 \n\n[Image 4: @jbschlosser]\n\nCopy link \n\nContributor\n\n### \n\n\u301054\u2020jbschlosser\u3011  commented \u301065\u2020May 6, 2024\u3011  \u2022\n\nedited \n\nHey \u301047\u2020@anshika-a","pub_date":null,"extra":{"cited_message_idx":8,"search_result_idx":null,"evidence_text":"source"}}}],"gizmo_id":null,"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","pad":"AAAAAAAA","parent_id":"e36924ad-af9c-4493-863b-ef6ad87cdacc","request_id":"887104dfbf1ba069-SIN","timestamp_":"absolute","content_references":[{"matched_text":"\u30107\u2020source\u3011","start_idx":555,"end_idx":565,"alt":null,"type":"webpage_extended","title":"GitHub - 1ssb/torchkan: An easy to use PyTorch  implementation of the Kolmogorov Arnold Network and a few novel variations","url":"https://github.com/1ssb/torchkan","pub_date":null,"snippet":"\n### \n\n\u301064\u2020polyvolver.py\u3011\n\n### \n\n\u301065\u2020requirements.txt\u3011\n\n### \n\n\u301065\u2020requirements.txt\u3011\n\n### \n\n\u301066\u2020setup.py\u3011\n\n### \n\n\u301066\u2020setup.py\u3011\n\n### \n\n\u301067\u2020to_gif.py\u3011\n\n### \n\n\u301067\u2020to_gif.py\u3011\n\n### \n\n\u301068\u2020torchkan.py\u3011\n\n### \n\n\u301068\u2020torchkan.py\u3011\n\n### \n\n\u301069\u2020wandb_report.pdf\u3011\n\n### \n\n\u301069\u2020wandb_report.pdf\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * MIT license \n\n# TorchKAN: Simplified KAN Model with Variations\n\nTorchKAN introduces a simplified KAN model and its variations, including KANvolver and KAL-Net, designed for high-performance image classification and leveraging polynomial transformations for enhanced feature detection.\n\n## Table of Contents\n\n  1. Overview of the Simplified KAN Model\n  2. KANvolver: Monomial Basis Functions for MNIST Image Classification\n  3. KAL-Net: Utilizing Legendre Polynomials in Kolmogorov Arnold Legendre Networks\n\nThis project showcases the training, validation, and quantization of the KAN model using PyTorch with CUDA acceleration. The `torchkan` model is evaluated on the MNIST dataset, demonstrating significant accuracy improvements.\n\n## Project Status: Under Development\n\nThe KAN model has demonstrated promising outcomes across various Generative Additive Models (GAMs) since the 1980s. Inspired by a range of sources, this first implementation in `KAN` in `torchkan.py` achieves over 97% accuracy with an evaluation time of 0.6 seconds. The quantized model further reduces this to under 0.55 seconds on the MNIST dataset within 8 epochs, utilizing an Nvidia RTX 4090 on Ubuntu 22.04.\n\nMy current understanding: I am not sure of the huge hype with KANs, it is imperative to understand that learning weights for activation functions (MLPs) and learning the activation function themselves are pretty old ideas, its unclear how much interpretability they offer. Its also unclear how scalable, quantisable or efficient they are; as such it seems that the quantisability is not an issue and quantised evaluation on the base model leads to only ~0.6% drop in test performance.\n\nNote: As the model is still being researched, further explorations into its full potential are ongoing. Contributions, questions, and critiques are welcome. I appreciate constructive feedback and contributions. Merge requests will be processed promptly, with a clear outline of the issue, the solution, and its effectiveness.\n\nNote: The PyPI pipeline is currently deprecated and will be stabilized following the release of Version 1.\n\n* * *\n\n### Introduction to the KANvolver Model\n\nThe `KANvolver` model is a specialized neural network designed for classifying images from the MNIST dataset. It achieves an at best accuracy of ~99.56% with a minimal error rate of 0.18%. This model combines convolutional neural networks (CNNs) with polynomial feature expansions, effectively capturing both simple and complex patterns.\n\nI am conducting large-scale analysis to investigate how KANs can be made more interpretable.\n\n### Model Architecture\n\nConvolutional Feature Extraction: The model begins with two convolutional layers, each paired with ReLU activation and max-pooling. The first layer employs 16 filters of size 3x3, while the second increases the feature maps to 32 channels.\n\nPolynomial Feature Transformation: Post feature extraction, the model applies polynomial transformations up to the n'th order to the flattened convolutional outputs, enhancing its ability to discern non-linear relationships.\n\nHow Monomials Work: In the context of this model, monomials are polynomial powers of the input features. By computing monomials up to a specified order, the model can capture non-linear interactions between the features, potentially leading to richer and more informative representations for downstream tasks.\n\nFor a given input image, the monomials of its flattened pixel values are computed, which are then used to adjust the output of linear layers before activation. This approach introduces an additional dimension of feature interaction, allowing the network to learn more complex patterns in the data.\n\n### Forward Propagation Process\n\n  1. Input Reshaping: Images are reshaped from vectors of 784 elements to 1x28x28 tensors for CNN processing.\n  2. Feature Extraction: Spatial features are extracted and pooled through convolutional layers.\n  3. Polynomial Expansion: Features undergo polynomial expansion to capture higher-order interactions.\n  4. Linear Processing: The expanded features are processed by linear layers with normalization and activation.\n  5. Output Generation: The network produces logits for each digit class in MNIST.\n\n### Performance and Conclusion\n\nThe `KANvolver` model's 99.5% accuracy on MNIST underscores its robustness in leveraging CNNs and polynomial expansions for effective digit classification. While showing significant potential, the model remains open for further adaptation and exploration in broader image processing challenges. Here are the results:\n\n### Integrated Gradients for Primary Explainability\n\n[Image 0: Validation Integrated Heatmaps] [Image 1: Validation Integrated Heatmaps] [Image 2: Validation Integrated Heatmaps] [Image 3: Validation Integrated Heatmaps] [Image 4: Validation Integrated Heatmaps]\n\nNote KANvolver uses polynomials which are distinct from the original KANs[1].\n\n* * *\n\n## Introducing KAL_Net\n\nThe `KAL_Net` represents the Kolmogorov Arnold Legendre Network (KAL-Net), a GAM architecture using Legendre polynomials to surpass traditional polynomial approximations like splines in KANs.\n\n### Key Features\n\n  * Polynomial Order: Utilizes Legendre polynomials up to a specific order for each input normalization, capturing nonlinear relationships more efficiently than sim","attribution":"github.com","icon_type":null},{"matched_text":"\u301010\u2020source\u3011","start_idx":565,"end_idx":576,"alt":null,"type":"webpage_extended","title":"TorchKAN \u00b7 PyPI","url":"https://pypi.org/project/TorchKAN/","pub_date":null,"snippet":"\n### 1. Clone the Repository\n\nClone the `torchkan` repository and set up the project environment:\n\n    git clone https://github.com/1ssb/torchkan.git\n    cd torchkan\n    pip install -r requirements.txt\n\nAlternately PyPi install as:\n\n    pip install torchKAN\n\n### 2. Configure CUDA environment variables if they are not already set:\n\n    export PATH=/usr/local/cuda/bin:$PATH\n    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n\n### 3. Configure Weights & Biases (wandb)\n\nTo track experiments and model performance with wandb:\n\n  1. Set Up wandb Account:\n\n  * Sign up or log in at \u301023\u2020Weights & Biases\u2020wandb.ai\u3011.\n  * Find your API key in your account settings.\n\n  2. Initialize wandb in Your Project:\n\nBefore running the training script, initialize wandb:\n\n    wandb login\n\nWhen prompted, enter your API key. This will link your script executions to your wandb account.\n\n  3. Make sure to change the Entity name in the `mnist.py` file to your username instead of `1ssb`\n\nThis script will train the model, validate it, and log performance metrics using wandb.\n\n## Contact\n\nFor any inquiries or support, contact: Subhransu.Bhattacharjee@anu.edu.au\n\n## Cite this Project\n\nIf you use this project in your research or wish to refer to the baseline results, please use the following BibTeX entry.\n\n    @misc{torchkan,\n      author = {Subhransu S. Bhattacharjee},\n      title = {TorchKAN},\n      year = {2024},\n      publisher = {GitHub},\n      journal = {GitHub repository},\n      howpublished = {\\url{https://github.com/1ssb/torchkan/}}\n    }\n\n## Contributions\n\nContributions are welcome. Please raise issues as necessary after commit \"Fin.\", scheduled end-June, 2024. The code is licensed under the MIT License.\n\n## References\n\n  * [0] Ziming Liu et al., \"KAN: Kolmogorov-Arnold Networks\", 2024, arXiv. \u301024\u2020https://arxiv.org/abs/2404.19756\u2020arxiv.org\u3011\n  * [1] \u301025\u2020https://github.com/KindXiaoming/pykan\u2020github.com\u3011\n  * [2] \u301026\u2020https://github.com/Blealtan/efficient-kan\u2020github.com\u3011\n\n## Project details\n\n### Verified details\n\nThese details have been verified by PyPI\n\n###### Maintainers\n\n\u301010\u2020 1ssb \u3011 \n\n### Unverified details\n\nThese details have not been verified by PyPI\n\n###### Project links\n\n  * \u301011\u2020 Homepage \u2020github.com\u3011 \n\n###### GitHub Statistics\n\n  * Stars:\n  * Forks:\n  * Open issues:\n  * Open PRs:\n\nView statistics for this project via \u301012\u2020Libraries.io\u2020libraries.io\u3011, or by using \u301013\u2020our public dataset on Google BigQuery\u2020packaging.python.org\u3011 \n\n###### Meta\n\nLicense: MIT License\n\nAuthor: Subhransu Bhattacharjee\n\n###### Classifiers\n\n  * Development Status\n    * \u301014\u2020 3 - Alpha \u3011 \n  * Intended Audience\n    * \u301015\u2020 Developers \u3011 \n  * License\n    * \u301016\u2020 OSI Approved :: MIT License \u3011 \n  * Programming Language\n    * \u301017\u2020 Python :: 3 \u3011 \n    * \u301018\u2020 Python :: 3.9 \u3011 \n    * \u301019\u2020 Python :: 3.10 \u3011 \n    * \u301020\u2020 Python :: 3.11 \u3011 \n  * Topic\n    * \u301021\u2020 Software Development :: Libraries \u3011 \n\n##  Release history \u301027\u2020Release notifications\u3011 | \u301028\u2020RSS feed \u3011 \n\nThis version\n\n[Image 1] \n\n\u301029\u2020 0.1.1 May 10, 2024 \u3011 \n\n[Image 2] \n\n\u301030\u2020 0.1.0 yanked May 10, 2024 Reason this release was yanked: Updated with a newer model \u3011 \n\n## Download files\n\nDownload the file for your platform. If you're not sure which to choose, learn more about \u301031\u2020installing packages\u2020packaging.python.org\u3011.\n\n###  Source Distribution \n\n\u301032\u2020 TorchKAN-0.1.1.tar.gz \u2020files.pythonhosted.org\u3011 (3.6 kB \u301033\u2020view hashes\u3011) \n\nUploaded  May 10, 2024  `Source`\n\n###  Built Distribution \n\n\u301034\u2020 TorchKAN-0.1.1-py3-none-any.whl \u2020files.pythonhosted.org\u3011 (3.6 kB \u301035\u2020view hashes\u3011) \n\nUploaded  May 10, 2024  `Python 3`\n\nClose \n\n###  \u301036\u2020Hashes\u2020pip.pypa.io\u3011 for TorchKAN-0.1.1.tar.gz \n\nHashes for TorchKAN-0.1.1.tar.gz Algorithm Hash digest  \nSHA256 `06e3529df4e15feca598bd6dcf25de1b6a75e03e897b182026ceb49c64e743af` Copy   \nMD5 `ec2647cc35819df50c346621ce5fe38b` Copy   \nBLAKE2b-256 `1b176e4e99e41a0bdee061a8814b3dc65eb046df83044429c1461d71a3609779` Copy   \n\nClose \n\nClose \n\n###  \u301036\u2020Hashes\u2020pip.pypa.io\u3011 for TorchKAN-0.1","attribution":"pypi.org","icon_type":null},{"matched_text":"\u30109\u2020source\u3011","start_idx":884,"end_idx":894,"alt":null,"type":"webpage_extended","title":"GitHub - mintisan/awesome-kan: A comprehensive collection of KAN(Kolmogorov-Arnold Network)-related resources, including libraries, projects, tutorials, papers, and more, for researchers and developers in the Kolmogorov-Arnold Network field.","url":"https://github.com/mintisan/awesome-kan","pub_date":null,"snippet":"  * \u301060\u2020FasterKAN\u3011 : FasterKAN = FastKAN + RSWAF bases functions and benchmarking with other KANs. Fastest KAN variation as of 5/13/2024, 2 times slower than MLP in backward speed. \uff5c [Image 9: Github stars]\n  * \u301061\u2020TorchKAN\u3011 : Simplified KAN Model Using Legendre approximations and Monomial basis functions for Image Classification for MNIST. Achieves 99.5% on MNIST using Conv+LegendreKAN. \uff5c [Image 10: Github stars]\n  * \u301062\u2020FourierKAN\u3011 : Pytorch Layer for FourierKAN. It is a layer intended to be a substitution for Linear + non-linear activation | [Image 11: Github stars]\n  * \u301063\u2020ChebyKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using Chebyshev polynomials instead of B-splines. \uff5c [Image 12: Github stars]\n  * \u301064\u2020GraphKAN\u3011 : Implementation of Graph Neural Network version of Kolmogorov Arnold Networks (GraphKAN) \uff5c [Image 13: Github stars]\n  * \u301065\u2020FCN-KAN\u3011 : Kolmogorov\u2013Arnold Networks with modified activation (using fully connected network to represent the activation) \uff5c [Image 14: Github stars]\n  * \u301066\u2020X-KANeRF\u3011 : KAN based NeRF with various basis functions like B-Splines, Fourier, Radial Basis Functions, Polynomials, etc \uff5c [Image 15: Github stars]\n  * \u301067\u2020Large Kolmogorov-Arnold Networks\u3011 : Variations of Kolmogorov-Arnold Networks (including CUDA-supported KAN convolutions) \uff5c [Image 16: Github stars]\n  * \u301059\u2020FastKAN\u3011 : Very Fast Calculation of Kolmogorov-Arnold Networks (KAN) \uff5c [Image 17: Github stars]\n  * \u301068\u2020xKAN\u3011 : Kolmogorov-Arnold Networks with various basis functions like B-Splines, Fourier, Chebyshev, Wavelets etc \uff5c [Image 18: Github stars]\n  * \u301069\u2020JacobiKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using Jacobi polynomials instead of B-splines. \uff5c [Image 19: Github stars]\n  * \u301064\u2020GraphKAN\u3011 : Implementation of Graph Neural Network version of Kolmogorov Arnold Networks (GraphKAN) \uff5c [Image 20: Github stars]\n  * \u301070\u2020OrthogPolyKAN\u3011 : Kolmogorov-Arnold Networks (KAN) using orthogonal polynomials instead of B-splines. \uff5c [Image 21: Github stars]\n  * \u301071\u2020kansformers\u3011 : Kansformers: Transformers using KANs | [Image 22: Github stars]\n  * \u301072\u2020Deep-KAN\u3011: Better implementation of Kolmogorov Arnold Network | [Image 23: Github stars]\n  * \u301073\u2020RBF-KAN\u3011: RBF-KAN is a PyTorch module that implements a Radial Basis Function Kolmogorov-Arnold Network | [Image 24: Github stars]\n  * \u301074\u2020KANX\u3011 : Fast Implementation (Approximation) of Kolmogorov-Arnold Network in JAX | [Image 25: Github stars]\n\n### ConvKANs\n\n  * \u301075\u2020Convolutional-KANs\u3011 : This project extends the idea of the innovative architecture of Kolmogorov-Arnold Networks (KAN) to the Convolutional Layers, changing the classic linear transformation of the convolution to non linear activations in each pixel. \uff5c [Image 26: Github stars]\n  * \u301076\u2020Conv-KAN\u3011 : This repository implements Convolutional Kolmogorov-Arnold Layers with various basis functions. \uff5c [Image 27: Github stars]\n  * \u301077\u2020convkan\u3011 : Implementation of convolutional layer version of KAN (drop-in replacement of Conv2d) \uff5c [Image 28: Github stars]\n  * \u301078\u2020KA-Conv\u3011 : Kolmogorov-Arnold Convolutional Networks with Various Basis Functions (Optimization for Efficiency and GPU memory usage) | [Image 29: Github stars]\n\n### Benchmark\n\n  * \u301079\u2020KAN-benchmarking\u3011 : Benchmark for efficiency in memory and time of different KAN implementations. | [Image 30: Github stars]\n\n### Non-Python\n\n  * \u301080\u2020KolmogorovArnold.jl\u3011 : Very fast Julia implementation of KANs with RBF and RSWAF basis. Extra speedup is gained by writing custom gradients to share work between forward and backward pass. \uff5c [Image 31: Github stars]\n  * \u301081\u2020kan-polar\u3011 : Kolmogorov-Arnold Networks in MATLAB \uff5c [Image 32: Github stars]\n\n### Alternative\n\n  * \u301082\u2020high-order-layers-torch\u3011 : High order piecewise polynomial neural networks using Chebyshev polynomials at Gauss Lobatto nodes (lagrange polynomials). Includes convolutional layers as well HP refinement for non convolutional layers, linear initialization and various applications in the linked repos with varrying levels of success. Euler equations of fluid dynamics, nlp, implicit representation and more | [Image 33: Github stars]\n\n## Project\n\n  * \u301083\u2020KAN-GPT\u3011 : The PyTorch implementation of Generative Pre-trained Tran","attribution":"github.com","icon_type":null},{"matched_text":"\u301011\u2020source\u3011","start_idx":894,"end_idx":905,"alt":null,"type":"webpage_extended","title":"GitHub - Simon-Bertrand/KAN-PyTorch: Kolmogorov\u2013Arnold Networks (KAN) in PyTorch","url":"https://github.com/Simon-Bertrand/KAN-PyTorch","pub_date":null,"snippet":"    * \u301017\u2020 Education \u2020education.github.com\u3011\n\nBy Solution\n    * \u301018\u2020 CI/CD & Automation \u3011\n    * \u301019\u2020 DevOps \u3011\n    * \u301020\u2020 DevSecOps \u2020resources.github.com\u3011\n\nResources\n    * \u301021\u2020 Learning Pathways \u2020resources.github.com\u3011\n    * \u301022\u2020 White papers, Ebooks, Webinars \u2020resources.github.com\u3011\n    * \u301023\u2020 Customer Stories \u3011\n    * \u301024\u2020 Partners \u2020partner.github.com\u3011\n\n  * Open Source \n\n    * \u301025\u2020 GitHub Sponsors Fund open source developers \u3011\n\n    * \u301026\u2020 The ReadME Project GitHub community articles \u3011\n\nRepositories\n    * \u301027\u2020 Topics \u3011\n    * \u301028\u2020 Trending \u3011\n    * \u301029\u2020 Collections \u3011\n\n  * \u301030\u2020Pricing\u3011 \n\nSearch or jump to...\n\n# Search code, repositories, users, issues, pull requests...\n\n Search \n\nClear  \n\n\u301031\u2020 Search syntax tips \u2020docs.github.com\u3011 \n\n#  Provide feedback \n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted\n\nCancel  Submit feedback \n\n#  Saved searches \n\n## Use saved searches to filter your results more quickly\n\nName\n\nQuery\n\nTo see all available qualifiers, see our \u301031\u2020documentation\u2020docs.github.com\u3011. \n\nCancel  Create saved search \n\n\u30101\u2020 Sign in \u3011 \n\n\u301032\u2020 Sign up \u3011 \n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.   Dismiss alert\n\n{{ message }}\n\n\u301033\u2020 Simon-Bertrand \u3011  / \u301034\u2020KAN-PyTorch\u3011  Public\n\n  * \u301035\u2020 Notifications \u3011 \n  * \u301035\u2020 Fork 0 \u3011 \n  * \u301035\u2020 Star 8 \u3011 \n\n  * \n\nKolmogorov\u2013Arnold Networks (KAN) in PyTorch \n\n### License\n\n\u301036\u2020 GPL-3.0 license \u3011 \n\n\u301037\u2020 8 stars \u3011 \u301038\u2020 0 forks \u3011 \u301039\u2020 Branches \u3011 \u301040\u2020 Tags \u3011 \u301041\u2020 Activity \u3011 \n\n\u301035\u2020 Star \u3011 \n\n\u301035\u2020 Notifications \u3011 \n\n  * \u301034\u2020 Code \u3011\n  * \u301042\u2020 Issues 0 \u3011\n  * \u301043\u2020 Pull requests 0 \u3011\n  * \u301044\u2020 Actions \u3011\n  * \u301045\u2020 Projects 0 \u3011\n  * \u301046\u2020 Security \u3011\n  * \u301047\u2020 Insights \u3011\n\n  Additional navigation options  \n\n  * \u301034\u2020 Code \u3011 \n  * \u301042\u2020 Issues \u3011 \n  * \u301043\u2020 Pull requests \u3011 \n  * \u301044\u2020 Actions \u3011 \n  * \u301045\u2020 Projects \u3011 \n  * \u301046\u2020 Security \u3011 \n  * \u301047\u2020 Insights \u3011 \n\n# Simon-Bertrand/KAN-PyTorch\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\n\u00a0main\n\n\u301039\u2020Branches\u3011\u301040\u2020Tags\u3011\n\nGo to file \n\nCode\n\n## Folders and files \n\nName Name \n\nLast commit message \n\nLast commit date   \n\n## Latest commit\n\n## History\n\n\u301048\u202011 Commits\u3011  \n\n### \n\n\u301049\u2020.vscode\u3011\n\n### \n\n\u301049\u2020.vscode\u3011\n\n### \n\n\u301050\u2020examples\u3011\n\n### \n\n\u301050\u2020examples\u3011\n\n### \n\n\u301051\u2020torch_kan\u3011\n\n### \n\n\u301051\u2020torch_kan\u3011\n\n### \n\n\u301052\u2020.gitignore\u3011\n\n### \n\n\u301052\u2020.gitignore\u3011\n\n### \n\n\u301036\u2020LICENSE\u3011\n\n### \n\n\u301036\u2020LICENSE\u3011\n\n### \n\n\u301053\u2020README.md\u3011\n\n### \n\n\u301053\u2020README.md\u3011\n\n### \n\n\u301054\u2020pyproject.toml\u3011\n\n### \n\n\u301054\u2020pyproject.toml\u3011\n\n### \n\n\u301055\u2020requirements.txt\u3011\n\n### \n\n\u301055\u2020requirements.txt\u3011\n\nView all files   \n\n## Repository files navigation \n\n  * README \n  * GPL-3.0 license \n\n# Python library : torch_kan\n\nSimple implementation of a KAN Layer following the below paper.\n\nWe use an optimized version of B-Splines computation. Only grid extension feature is implemented, where you are able to arbitrary increase the number of control points to get less smooth splines curves.\n\nWe try the simple MNIST classification task using KAN in the example/sb folder.\n\nWe are open for PR concerning library improvements, adding missing features such as automatic pruning and fixing symbolics and correcting wrong interpretation of the original paper.   \n\nIn this library, the paper from Ziming Liu et al. has been implemented :   \n\n# References :\n\n  * Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Solja\u010di\u0107, M., Hou, T. Y., & Tegmark, M. (2024, April 30). KAN: Kolmogorov-Arnold Networks. arXiv.org. \u301056\u2020https://arxiv.org/abs/2404.19756v1\u2020arxiv.org\u3011\n\n* * *\n\n# Install library\n\n    pip install https://github.com/Simon-Bertrand/KAN-PyTorch/archive/main.zip\n\n# Import library\n\n    import torch_kan\n\n## About\n\nKolmogorov\u2013Arnold Networks (KAN) in PyTorch \n\n### Topics\n\n\u301057\u2020 deep-learning \u3011 \u301058\u2020 pytorch \u3011 \u301059\u2020 kan \u3011 \u301060\u2020 kolmogorov-arnold-representation \u3011 \n\n### Resources\n\nReadme \n\n","attribution":"github.com","icon_type":null},{"matched_text":"\u30108\u2020source\u3011","start_idx":1098,"end_idx":1108,"alt":null,"type":"webpage_extended","title":"Support for Kolmogorov-Arnold Networks (KANs) \u00b7 Issue #125389 \u00b7 pytorch/pytorch \u00b7 GitHub","url":"https://github.com/pytorch/pytorch/issues/125389","pub_date":null,"snippet":"\n\u301032\u2020 Sign up \u3011 \n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.   Dismiss alert\n\n{{ message }}\n\n\u301033\u2020 pytorch \u3011  / \u301034\u2020pytorch\u3011  Public\n\n  * \u301035\u2020 Notifications \u3011 \n  * \u301035\u2020 Fork 21.2k \u3011 \n  * \u301035\u2020 Star 78.6k \u3011 \n\n  * \u301034\u2020 Code \u3011\n  * \u301036\u2020 Issues 5k+ \u3011\n  * \u301037\u2020 Pull requests 974 \u3011\n  * \u301038\u2020 Actions \u3011\n  * \u301039\u2020 Projects 31 \u3011\n  * \u301040\u2020 Wiki \u3011\n  * \u301041\u2020 Security \u3011\n  * \u301042\u2020 Insights \u3011\n\n  Additional navigation options  \n\n  * \u301034\u2020 Code \u3011 \n  * \u301036\u2020 Issues \u3011 \n  * \u301037\u2020 Pull requests \u3011 \n  * \u301038\u2020 Actions \u3011 \n  * \u301039\u2020 Projects \u3011 \n  * \u301040\u2020 Wiki \u3011 \n  * \u301041\u2020 Security \u3011 \n  * \u301042\u2020 Insights \u3011 \n\nNew issue \n\nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community. \n\n\u301043\u2020 Sign up for GitHub \u3011 \n\nBy clicking \u201cSign up for GitHub\u201d, you agree to our \u301044\u2020terms of service\u2020docs.github.com\u3011 and \u301045\u2020privacy statement\u2020docs.github.com\u3011. We\u2019ll occasionally send you account related emails.\n\nAlready on GitHub? \u301046\u2020Sign in\u3011 to your account \n\nJump to bottom \n\n#  Support for Kolmogorov-Arnold Networks (KANs) #125389\n\nOpen \n\n\u301047\u2020anshika-anand\u3011 opened this issue May 2, 2024 \u00b7 3 comments \n\nOpen \n\n#  Support for Kolmogorov-Arnold Networks (KANs) #125389\n\n\u301047\u2020anshika-anand\u3011 opened this issue May 2, 2024 \u00b7 3 comments \n\nLabels\n\n\u301048\u2020 module: nn \u3011 Related to torch.nn \u301049\u2020 triaged \u3011 This issue has been looked at a team member, and triaged and prioritized into an appropriate module\n\n## Comments\n\n[Image 0: @anshika-anand]\n\nCopy link \n\n### \n\n\u301047\u2020anshika-anand\u3011  commented \u301050\u2020May 2, 2024\u3011  \u2022\n\nedited by pytorch-bot bot\n\nProposal for adding support for Kolmogorov-Arnold Networks (KANs) in PyTorch. KANs are a promising alternative to traditional Multi-Layer Perceptrons (MLPs) and offer potential advantages in terms of accuracy, scaling, and interpretability.\n\nKANs are inspired by the Kolmogorov-Arnold representation theorem and have a fundamentally different structure compared to MLPs. Instead of having fixed activation functions on nodes (neurons) and linear weights, KANs have learnable activation functions on edges (weights), and every weight parameter is replaced by a univariate function parameterized as a spline.\n\n### Additional context\n\n\u301051\u2020KAN: Kolmogorov\u2013Arnold Networks\u2020arxiv.org\u3011, The paper was submitted on 30th April,2024 by researchers from Massachusetts Institute of Technology, California Institute of Technology, Northeastern University, The NSF Institute for Artificial Intelligence and Fundamental Interactions.\n\ncc \u301052\u2020@albanD\u3011 \u301053\u2020@mruberry\u3011 \u301054\u2020@jbschlosser\u3011 \u301055\u2020@walterddr\u3011 \u301056\u2020@mikaylagawarecki\u3011  \n\nThe text was updated successfully, but these errors were encountered: \n\n  14 demon2036, KindXiaoming, hvaara, Flecart, kabachuha, nimrodraifer, arghaffari, BulatVakhitov, dzaripov, quocanh010, and 4 more reacted with thumbs up emoji   6 hvaara, Pranavkak, Flecart, vadlemar, JannesBrands, and rafa-br34 reacted with eyes emoji\n\nAll reactions\n\n  * 14 reactions\n  * 6 reactions\n\n[Image 1: @hvaara]\n\nCopy link \n\nContributor\n\n### \n\n\u301057\u2020hvaara\u3011  commented \u301058\u2020May 2, 2024\u3011 \n\nRef \u301059\u2020https://github.com/KindXiaoming/pykan\u3011  \ncc \u301060\u2020@KindXiaoming\u3011  \n\n  5 Pranavkak, Flecart, kabachuha, Charlyo, and nimrodraifer reacted with thumbs up emoji\n\nAll reactions\n\n  * 5 reactions\n\nSorry, something went wrong. \n\n[Image 2: @drisspg] \u301061\u2020drisspg\u3011 added the \u301062\u2020 triage review \u3011 label \u301063\u2020May 3, 2024\u3011 \n\n[Image 3: @jbschlosser] \u301054\u2020jbschlosser\u3011 added \u301048\u2020 module: nn \u3011 Related to torch.nn \u301049\u2020 triaged \u3011 This issue has been looked at a team member, and triaged and prioritized into an appropriate module and removed \u301062\u2020 triage review \u3011 labels \u301064\u2020May 6, 2024\u3011 \n\n[Image 4: @jbschlosser]\n\nCopy link \n\nContributor\n\n### \n\n\u301054\u2020jbschlosser\u3011  commented \u301065\u2020May 6, 2024\u3011  \u2022\n\nedited \n\nHey \u301047\u2020@anshika-a","attribution":"github.com","icon_type":null}]},"recipient":"all","channel":null},"parent":"e36924ad-af9c-4493-863b-ef6ad87cdacc","children":[]}}}