{"id":"67cc5ce0-d434-8007-9a6c-70a943b887e4","title":"AI Superintelligence Strategy Analysis","create_time":1741446369.17647,"update_time":1741446971.781194,"current_node":"84ef60e0-11ee-4b9c-b57f-3ff6de865559","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["ffaa1376-00a7-4fc4-b918-00112a9a3644"]},"ffaa1376-00a7-4fc4-b918-00112a9a3644":{"id":"ffaa1376-00a7-4fc4-b918-00112a9a3644","message":{"id":"ffaa1376-00a7-4fc4-b918-00112a9a3644","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["bf8b1461-7963-4239-a7c1-62515c5753ca"]},"bf8b1461-7963-4239-a7c1-62515c5753ca":{"id":"bf8b1461-7963-4239-a7c1-62515c5753ca","message":{"id":"bf8b1461-7963-4239-a7c1-62515c5753ca","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Be precise and comprehensive in your analysis. Tell it like it is; don't sugar-coat responses. Take a forward-thinking view."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"ffaa1376-00a7-4fc4-b918-00112a9a3644","children":["1ca0cb7d-d4bb-4712-9e66-e92727577e60"]},"1ca0cb7d-d4bb-4712-9e66-e92727577e60":{"id":"1ca0cb7d-d4bb-4712-9e66-e92727577e60","message":{"id":"1ca0cb7d-d4bb-4712-9e66-e92727577e60","author":{"role":"user","name":null,"metadata":{}},"create_time":1741446368.65,"update_time":null,"content":{"content_type":"text","parts":["Give me a deep dive into this report."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"attachments":[{"id":"file-PcCabyfMn91jCJYRbuYeQb","size":13370907,"name":"Superintelligence_Strategy_Expert.pdf","mime_type":"application/pdf","file_token_size":31654}],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"91d33c1c4c75f8f6-SEA","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"bf8b1461-7963-4239-a7c1-62515c5753ca","children":["d132a9cd-9569-4e50-8d46-2a6cb6fcec4f"]},"d132a9cd-9569-4e50-8d46-2a6cb6fcec4f":{"id":"d132a9cd-9569-4e50-8d46-2a6cb6fcec4f","message":{"id":"d132a9cd-9569-4e50-8d46-2a6cb6fcec4f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1741446381.244092,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"message_type":null,"model_slug":"gpt-4-5","default_model_slug":"gpt-4-5","parent_id":"1ca0cb7d-d4bb-4712-9e66-e92727577e60","request_id":"91d33c1c4c75f8f6-SEA","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"1ca0cb7d-d4bb-4712-9e66-e92727577e60","children":["ce02155a-cffe-494d-b30e-af8ce25ab85a"]},"ce02155a-cffe-494d-b30e-af8ce25ab85a":{"id":"ce02155a-cffe-494d-b30e-af8ce25ab85a","message":{"id":"ce02155a-cffe-494d-b30e-af8ce25ab85a","author":{"role":"tool","name":"myfiles_browser","metadata":{}},"create_time":1741446381.244264,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"","summary":"","assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"command":"spinner","status":"running","message_type":null,"model_slug":"gpt-4-5","default_model_slug":"gpt-4-5","parent_id":"d132a9cd-9569-4e50-8d46-2a6cb6fcec4f","request_id":"91d33c1c4c75f8f6-SEA","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"d132a9cd-9569-4e50-8d46-2a6cb6fcec4f","children":["47071a17-399e-410b-a1d8-8badf6b82795"]},"47071a17-399e-410b-a1d8-8badf6b82795":{"id":"47071a17-399e-410b-a1d8-8badf6b82795","message":{"id":"47071a17-399e-410b-a1d8-8badf6b82795","author":{"role":"tool","name":"myfiles_browser","metadata":{}},"create_time":1741446381.244373,"update_time":null,"content":{"content_type":"tether_quote","url":"file-PcCabyfMn91jCJYRbuYeQb","domain":"Superintelligence_Strategy_Expert.pdf","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuperintelligence Strategy: Expert Version\n\n\nS U P E R I N T E L L I G E N C E\nS T R A T E G Y\n\nE X P E R T  V E R S I O N\n\nD A N  H E N D R Y C K S          E R I C  S C H M I D T          A L E X A N D R  W A N G\n\n\n\nSuperintelligence Strategy: Expert Version\n\nDan Hendrycks Eric Schmidt Alexandr Wang\n\nAbstract\n\nRapid advances in AI are beginning to reshape national security. Destabilizing AI developments\ncould rupture the balance of power and raise the odds of great-power conflict, while widespread\nproliferation of capable AI hackers and virologists would lower barriers for rogue actors to cause\ncatastrophe. Superintelligence\u2014AI vastly better than humans at nearly all cognitive tasks\u2014is\nnow anticipated by AI researchers. Just as nations once developed nuclear strategies to secure\ntheir survival, we now need a coherent superintelligence strategy to navigate a new period of\ntransformative change. We introduce the concept of Mutual Assured AI Malfunction (MAIM):\na deterrence regime resembling nuclear mutual assured destruction (MAD) where any state\u2019s\naggressive bid for unilateral AI dominance is met with preventive sabotage by rivals. Given\nthe relative ease of sabotaging a destabilizing AI project\u2014through interventions ranging from\ncovert cyberattacks to potential kinetic strikes on datacenters\u2014MAIM already describes the\nstrategic picture AI superpowers find themselves in. Alongside this, states can increase their\ncompetitiveness by bolstering their economies and militaries through AI, and they can engage in\nnonproliferation to rogue actors to keep weaponizable AI capabilities out of their hands. Taken\ntogether, the three-part framework of deterrence, nonproliferation, and competitiveness outlines a\nrobust strategy to superintelligence in the years ahead.\n\n\n\nExecutive Summary\nRapid advances in AI are poised to reshape nearly every aspect of society. Governments see in these dual-use\nAI systems a means to military dominance, stoking a bitter race to maximize AI capabilities. Voluntary industry\npauses or attempts to exclude government involvement cannot change this reality. These systems that can\nstreamline research and bolster economic output can also be turned to destructive ends, enabling rogue actors\nto engineer bioweapons and hack critical infrastructure. \u201cSuperintelligent\u201d AI surpassing humans in nearly\nevery domain would amount to the most precarious technological development since the nuclear bomb. Given\nthe stakes, superintelligence is inescapably a matter of national security, and an effective superintelligence\nstrategy should draw from a long history of national security policy.\n\nDeterrence\nA race for AI-enabled dominance endangers all states. If, in a hurried bid for superiority, one state inadvertently\nloses control of its AI, it jeopardizes the security of all states. Alternatively, if the same state succeeds in\nproducing and controlling a highly capable AI, it likewise poses a direct threat to the survival of its peers. In\neither event, states seeking to secure their own survival may threaten to sabotage destabilizing AI projects for\ndeterrence. A state could try to disrupt such an AI project with interventions ranging from covert operations\nthat degrade training runs to physical damage that disables AI infrastructure. Thus, we are already approaching\na dynamic similar to nuclear Mutual Assured Destruction (MAD), in which no power dares attempt an outright\ngrab for strategic monopoly, as any such effort would invite a debilitating response. This strategic condition,\nwhich we refer to as Mutual Assured AI Malfunction (MAIM), represents a potentially stable deterrence\nregime, but maintaining it could require care. We outline measures to maintain the conditions for MAIM,\nincluding clearly communicated escalation ladders, placement of AI infrastructure far from population centers,\ntransparency into datacenters, and more.\n\nNonproliferation\nWhile deterrence through MAIM constrains the intent of superpowers, all nations have an interest in limiting\nthe AI capabilities of terrorists. Drawing on nonproliferation precedents for weapons of mass destruction\n(WMDs), we outline three levers for achieving this. Mirroring measures to restrict key inputs to WMDs such as\nfissile material and chemical weapons precursors, compute security involves knowing reliably where high-end\nAI chips are and stemming smuggling to rogue actors. Monitoring shipments, tracking chip inventories,\nand employing security features like geolocation can help states account for them. States must prioritize\ninformation security to protect the model weights underlying the most advanced AI systems from falling\ninto the hands of rogue actors, similar to controls on other sensitive information. Finally, akin to screening\nprotocols for DNA synthesis services to detect and refuse orders for known pathogens, AI companies can be\nincentivized to implement technical AI security measures that detect and prevent malicious use.\n\nCompetitiveness\nBeyond securing their survival, states will have an interest in harnessing AI to bolster their competitiveness, as\nsuccessful AI adoption will be a determining factor in national strength. Adopting AI-enabled weapons and\ncarefully integrating AI into command and control is increasingly essential for military strength. Recognizing\nthat economic security is crucial for national security, domestic capacity for manufacturing high-end AI chips\nwill ensure a resilient supply and sidestep geopolitical risks in Taiwan. Robust legal frameworks governing AI\nagents can set basic constraints on their behavior that follow the spirit of existing law. Finally, governments\ncan maintain political stability through measures that improve the quality of decision-making and combat the\ndisruptive effects of rapid automation.\n\nBy detecting and deterring destabilizing AI projects through intelligence operations and targeted disruption,\nrestricting access to AI chips and capabilities for malicious actors through strict controls, and guaranteeing a\nstable AI supply chain by investing in domestic chip manufacturing, states can safeguard their security while\nopening the door to unprecedented prosperity.\n\n2\n\n\n\nContents\n\n1 Executive Summary 2\n\n2 Introduction 4\n\n3 AI Is Pivotal for National Security 5\n3.1 Strategic Competition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n\n3.1.1 Shifting Basis of Economic Power . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.1.2 Destabilization Through Superweapons . . . . . . . . . . . . . . . . . . . . . . . . 7\n\n3.2 Terrorism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.2.1 Bioterrorism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.2.2 Cyberattacks on Critical Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2.3 Offense-Defense Balance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n3.3 Loss of Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3.1 Erosion of Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3.2 Unleashed AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3.3 Intelligence Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n\n3.4 Existing AI Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\n4 Deterrence with Mutual Assured AI Malfunction (MAIM) 13\n4.1 MAIM Is the Default Regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 How to Maintain a MAIM Regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n5 Nonproliferation 17\n5.1 Compute Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n\n5.1.1 Export Controls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.1.2 Firmware-Level Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n5.2 Information Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5.2.1 How to Improve Information Security . . . . . . . . . . . . . . . . . . . . . . . . . 22\n\n5.3 AI Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.3.1 Malicious Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.3.2 Loss of Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n\n6 Competitiveness 26\n6.1 Military Strength . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n6.2 Economic Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n\n6.2.1 Manufacture AI Chips . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n6.2.2 Facilitate Immigration for AI Scientists . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n6.3 Legal Frameworks Governing AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n6.3.1 Aligning Individual AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n6.3.2 Aligning Collectives of AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n\n6.4 Political Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n6.4.1 Censorship and Inaccurate Information . . . . . . . . . . . . . . . . . . . . . . . . 31\n6.4.2 Automation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n\n7 Conclusion 33\n\n3\n\n\n\nIntroduction\nArtificial Intelligence (AI) is rapidly transforming multiple facets of society, with advances arriving at a pace\nand scale that few anticipated. These developments compel policymakers, technologists, and strategists to\naddress a widening spectrum of issues, from economic shifts driven by automation to strategic concerns about\nglobal competition. As with any transformative technology, AI presents both significant opportunities and\nformidable risks.\n\nAmong these challenges, the dual-use nature of AI\u2014its capacity for both civilian and military applications\u2014\nemerges as a critical factor. Unlike specialized technological tools, AI spans virtually every sector, including\nfinance, healthcare, and defense. This broad applicability, coupled with its rapid evolution, creates a risk\nlandscape that is expansive and difficult to predict. Strategic actors must contend with potential misuse, risks\nof geopolitical escalation, and the need for frameworks to govern systems whose capabilities may surpass\nhuman oversight.\n\nTo navigate these complexities, many have turned to analogies. AI has been compared to electricity for\nits general-purpose nature, to traditional software for its economic importance, or to the printing press for its\ncultural impact. While these comparisons provide useful entry points, they fail to emphasize the grave national\nsecurity implications of AI. A more productive analogy lies between AI and catastrophic dual-use nuclear,\nchemical, and biological technologies. Like them, AI will be integral to a nation\u2019s power while posing the\npotential for mass destruction. A brief examination of the historical parallels between AI and the nuclear age\ncan highlight the gravity of our current situation.\n\nIn 1933, the leading scientist Ernest Rutherford dismissed the notion of harnessing atomic power as\n\u201cmoonshine.\u201d The very next day, Leo Szilard read Rutherford\u2019s remarks and sketched the idea of a nuclear\nchain reaction that ultimately birthed the nuclear age. Eventually figures such as J. Robert Oppenheimer\nrecognized the dual nature of their work. Today, AI is at a similar stage. Previously considered science fiction,\nAI has advanced to the point where machines can learn, adapt, and potentially exceed human intelligence\nin certain areas. AI experts including Geoffrey Hinton and Yoshua Bengio, pioneers in deep learning, have\nexpressed existential concerns about the technologies they helped create [1].\n\nAs AI\u2019s capabilities are becoming more evident, nations and corporations are heavily investing to gain a\nstrategic advantage. The Manhattan Project, which consumed 0.4% of the U.S. GDP, was driven by the need to\ndevelop nuclear capabilities ahead of others. Currently, a similar urgency is evident in the global effort to lead\nin AI, with investment in AI training doubling every year for nearly the past decade. Several \u201cAI Manhattan\nProjects\u201d aiming to eventually build superintelligence are already underway, financed by many of the most\npowerful corporations in the world.\n\nDeterrence\nDetect and deter\n\ndestabilizing AI projects with\n(cyber) espionage and sabotoge\n\nStrategy\n\nNonproliferation\nReliably know the location\n\nof AI chips and prevent rogue\nactors from smuggling them\n\nCompetitiveness\nGuarantee access to AI\nchips through domestic\n\nmanufacturing\n\nFigure 2.1: Effective strategies for managing advanced AI can draw from national security precedents in\nhandling previous potentially catastrophic dual-use technology.\n\n4\n\n\n\nHowever, the rapid advancement of AI technologies introduces significant uncertainties for international\nstability and security. The introduction of nuclear weapons altered international relations, granting influence\nto those who possessed them and leading to an arms race. The Cuban Missile Crisis highlighted how close\nthe world came to nuclear war. Nuclear annihilation has been avoided thus far despite significant tensions\nbetween nuclear states in part through the deterrence principle of Mutual Assured Destruction (MAD), where\nany nuclear use would provoke an in-kind response. In the AI era, a parallel form of deterrence could\nemerge\u2014what might be termed \u201cMutual Assured AI Malfunction\u201d (MAIM)\u2014where states\u2019 AI projects are\nconstrained by mutual threats of sabotage.\n\nFigure 2.2: The amount of compute used to create AI\nmodels has been increasing exponentially for decades\n[2].\n\nThe risks are not limited to state competition; ad-\nvanced dual-use technologies can also be exploited\nby non-state actors. Just as the spread of nuclear\ncapabilities raised concerns about misuse, the avail-\nability of AI systems presents new challenges. Mali-\ncious actors could use AI to develop weapons of mass\ndestruction or conduct large-scale cyberattacks on\ncritical infrastructure. The accessibility of unsecured\nor open-weight AI increases these risks, highlighting\nthe need for careful policies and safeguards.\n\nIn the nuclear era, uranium became the linchpin\nof atomic power. States that secured it could enforce\nregulations, negotiate treaties, and limit the spread of\ndestructive capabilities. In the realm of AI, comput-\ning resources\u2014especially AI chips\u2014have a similar\nstrategic weight, fueling rivalries and shaping geopo-\nlitical calculations. This dynamic is evident in places\nsuch as Taiwan, central to AI chip production, where\nrising tensions could have extensive consequences.\nNations have a shared interest in controlling access to AI chips to keep them out of the hands of rogue actors,\nechoing the logic once applied to uranium.\n\nDespite these challenges, AI offers significant opportunities. Nuclear technology, while introducing the\nthreat of mass destruction, also provided a new energy source that transformed societies. AI has the potential to\ndrive advancements across various sectors, from medical breakthroughs to economic automation. Embracing\nAI\u2019s benefits is important for economic growth and progress in the modern world.\n\nThe challenges AI poses are far too broad, and far too serious, for piecemeal measures. What is needed\nis a comprehensive strategy, one that does not shy from the unsettling implications of advanced AI. As with\nHerman Kahn\u2019s famous analysis of nuclear strategy [3], superintelligence strategy requires \u201cthinking about the\nunthinkable.\u201d In this paper, we propose such a strategy and grapple with these fundamental questions along\nthe way: What should be done about lethal autonomous weapons? Catastrophic malicious use? Powerful\nopen-weight AIs? AI-powered mass surveillance? How can society maintain a shared grasp of reality? What\nshould be done about AI rights? How can humans maintain their status in a world of mass automation?\n\nWe argue that the most effective framework for addressing AI\u2019s challenges is to view it through a national\nsecurity lens. Drawing on lessons from previous dual-use technologies while tailoring them to the distinct\ndemands of AI can help safeguard against catastrophic misuse, maintain geopolitical stability, and ensure that\nthe broader Western world remains at the forefront.\n\nAI Is Pivotal for National Security\nAI holds the potential to reshape the balance of power. In the hands of state actors, it can lead to disruptive\nmilitary capabilities and transform economic competition. At the same time, terrorists may exploit its dual-use\n\n5\n\n\n\nTopic Tame Technical Subproblem Wicked Problems\n\nMalicious Use\nTrain AI systems to\nrefuse harmful requests\n\nCoordinate nonproliferation among countries,\nincentivize AI deployers to prevent misuse,\nvalidate know-your-customer information\n\nDeterrence\nPrepare cyberattacks for\nAI datacenters\n\nDefine escalation thresholds and signal intent,\nsurveil rival AI projects,\nadapt verification measures as AI evolves\n\nCompute Security\nUpgrade AI chip firmware to\nadd geolocation functionality\n\nEnforce export controls to track AI chips,\nreport suspicious shipments to allies,\nverify the decommissioning of obsolete AI chips\n\nInformation Security\nPatch known vulnerabilities in\nAI developers\u2019 computer systems\n\nCounter insider threats, balance security measures\nwith productivity, prevent ideological leaks\n\nMilitary Strength Design military drones\n\nSecure supply chains, adapt military\norganizational structures to diffuse AI,\nmaintain meaningful human control over\nAI-augmented command and control\n\nEconomic Strength\nImprove AI performance on\neconomically valuable tasks\n\nDraw from legal frameworks for AI agents,\nestablish and evolve multiagent infrastructure,\npreserve stability amid automation\n\nLoss of Control\nResearch methods to make\ncurrent AIs follow instructions\n\nSteer a population of rapidly evolving AIs\nduring an intelligence recursion,\nco-evolve safeguards with AI systems,\nred team AIs to identify unknown unknowns\n\nFigure 2.3: AI risk management contains multiple wicked problems and is not primarily a technical challenge.\nTame technical problems have well-defined boundaries and criteria for success, lending themselves to system-\natic experimentation. By contrast, wicked problems [4] are open-ended, carry ambiguous requirements, and\noften produce unintended consequences. They demand ongoing adaptation rather than purely technical fixes,\nsince each attempt at a solution can give rise to new difficulties.\n\nnature to orchestrate attacks once within the exclusive domain of great powers. It could also slip free of human\noversight.\n\nThis chapter examines these three threats to national security: rival states, rogue actors, and uncontrolled\nAI systems. It closes by assessing why existing strategies fall short of managing these intertwined threats.\n\n3.1 Strategic Competition\nIn an international system with no central authority, states prioritize their own strength to ensure their own\nsecurity. This competition arises not from a desire for dominance but from the necessity of safeguarding\nnational interests. They exist in an environment where threats can emerge unexpectedly and assistance from\nothers is uncertain. Some states may rise in their power and provoke alarm in their rivals, a pattern called the\nThucydides Trap [5]. Consequently, states seek to preserve their relative power.\n\nIn this environment, the impact of AI on state power looms large. AI may transform the foundations of\neconomic and military power. Its ability to automate labor could become the source of economic competitive-\nness. In the military sphere, it could be used to dominate rivals. We begin by looking at economic power, then\nturn to its greatest military implications.\n\n6\n\n\n\nLoss of Control\nAn autonomous AI system\n\ncould slip from\nmeaningful human control\n\nProblems\n\nTerrorism\nRogue actors could use AI systems\n\nto create bioweapons or launch\ncyberattacks on critical infrastructure\n\nStrategic Competition\nStates may race on AI capabilities\nin destabilizing bids for economic\n\nand military dominance\n\nFigure 3.1: States, terrorists, and AIs are threats to national security.\n\n3.1.1 Shifting Basis of Economic Power\nAI Chips as the Currency of Economic Power. As AI becomes more and more integrated in the economy,\nthe possession of advanced AI chips may define a nation\u2019s power. Historically, wealth and population size\nunderpinned a state\u2019s influence; however, the automation of tasks through AI alters this dynamic. A collection\nof highly capable AI agents, operating tirelessly and efficiently, rivals a skilled workforce, effectively turning\ncapital into labor. In this new paradigm, power will depend both on the capability of AI systems and the\nnumber of AI chips on which they could run. Nations with greater access to AI chips could outcompete others\neconomically.\n\n3.1.2 Destabilization Through Superweapons\nStates have long pursued weapons that could confer a decisive advantage over rivals. AI systems introduce new\navenues for such pursuit, raising questions about whether certain breakthroughs could undermine deterrence\nand reorder global power structures.\n\nAI Could Enable Military Dominance. Advanced AI systems may drive technological breakthroughs that\nalter the strategic balance, similar to the introduction of nuclear weapons, and could generate strategic surprise\nthat catches rivals off-guard [6]. Such a \u201csuperweapon\u201d may grant two tiers of advantages. One, which might\nbe called \u201csubnuclear dominance,\u201d would allow a state to project power widely and subdue adversaries without\ndisrupting nuclear deterrence. The second possibility\u2014a \u201cstrategic monopoly\u201d on power\u2014would upend the\nnuclear balance entirely and could establish one state\u2019s complete dominance and control, leaving the fate of\nrivals subject to its will.\n\nPossible Superweapons. Subnuclear superweapons\u2014such as an AI-enabled cyberweapon that can suddenly\nand comprehensively destroy a state\u2019s critical infrastructure, exotic EMP devices, and next-generation drones\u2014\ncould confer sweeping advantages without nullifying an adversary\u2019s nuclear deterrent. Some superweapons\nmight erode mutual assured destruction outright. A \u201ctransparent ocean\u201d would threaten submarine stealth,\nrevealing the location of nuclear submarines. AIs might be able to pinpoint all hardened mobile nuclear\nlaunchers, further undermining the nuclear triad. AIs could undermine situational awareness and sow confusion\nby generating elaborate deceptions\u2014a \u201cfog of war machine\u201d [7]\u2014that mask true intentions or capabilities. A\ndefensive superweapon possibility is an anti-ballistic missile system that eliminates an adversary\u2019s ability to\nstrike back. Lastly, some superweapons remain beyond today\u2019s foresight\u2014\u201cunknown unknowns\u201d that could\nundermine strategic stability.\n\n7\n\n\n\nImplications of Superweapons. Superintelligence is not merely a new weapon, but a way to fast-track all\nfuture military innovation. A nation with sole possession of superintelligence might be as overwhelming as the\nConquistadors were to the Aztecs. If a state achieves a strategic monopoly through AI, it could reshape world\naffairs on its own terms. An AI-driven surveillance apparatus might enable an unshakable totalitarian regime,\ntransforming governance at home and leverage abroad.\n\nThe mere pursuit of such a breakthrough could, however, tempt rivals to act before their window closes.\nFear that another state might soon rapidly grow in power has led observers to contemplate measures once seen\nas unthinkable. In the nuclear era, Bertrand Russell, ordinarily a staunch pacifist, proposed preventive nuclear\nstrikes on the Soviet Union to thwart its rise [8], while the United States seriously pondered crippling the\nChinese nuclear program during the early 60s [9]. Faced with the specter of superweapons and an AI-enabled\nstrategic monopoly on power, some leaders may turn to preventive action [10]. Rather than only relying on\ncooperation or seeking to outpace their adversaries, they may consider sabotage or datacenter attacks, if the\nalternative is to accept a future in which one\u2019s national survival is perpetually at risk.\n\nSuperweapons and shifting economic power can redefine strategic competition. To grasp the full magnitude\nof AI\u2019s impacts on national security, we turn next to rogue actors, and later to AI systems that slip from human\ncontrol.\n\n3.2 Terrorism\nAI\u2019s Dual-Use Capabilities Amplify Terrorism Risks. As AI capabilities increase, it will likely be\nimportant not only in the context of state-level competition but also as an amplifier of terrorist capabilities.\nTechnologies that can revolutionize healthcare or simplify software development also have the potential to\nempower individuals to create bioweapons and conduct cyberattacks. This amplification effect lowers the\nbarriers for terrorists, enabling them to execute large-scale attacks that were previously limited to nation-states.\nThis section examines two critical areas where AI intensifies terrorism risks: lowering barriers to bioweapon\ndevelopment, and lowering barriers to cyberattacks against critical infrastructure.\n\n3.2.1 Bioterrorism\nAI Lowers Barriers to Bioterrorism. Consider Aum Shinrikyo, the Japanese cult that orchestrated the 1995\nTokyo subway sarin attack. Operating with limited expertise, they managed to produce and deploy a chemical\nweapon in the heart of Tokyo\u2019s transit system, killing 13 people and injuring over 5,000. The attack paralyzed\nthe city, instilling widespread fear and demonstrating the havoc that determined non-state actors can wreak.\n\nWith AI assistance, similar groups could achieve far more devastating results. AI could provide step-by-\nstep guidance on designing lethal pathogens, sourcing materials, and optimizing methods of dispersal. What\nonce required specialized knowledge and resources could become accessible to individuals with malevolent\nintent, dramatically increasing the potential for catastrophic outcomes. Indeed, some cutting-edge AI systems\nwithout bioweapons safeguards already exceed expert-level performance on numerous virology benchmarks\n[11].\n\nEngineered pathogens could surpass historical pandemics in scale and lethality. The Black Death killed\nin the vicinity of half of Europe\u2019s population without human engineering. Modern bioweapons, enhanced\nby AI-driven design, could exploit vulnerabilities in human biology with unprecedented precision, creating\ncontagions that evade detection and resist treatment. While most discussions of bioweapons are secret, some\nscientists have openly warned of \u201cmirror bacteria,\u201d engineered with reversed molecular structures that could\nevade the immune defenses that normally keep pathogens at bay. Though formidable to create, they have\nprompted urgent appeals from leading researchers to halt development [12], lest they unleash a catastrophe\nunlike any our biosphere has known. In contrast to other weapons of mass destruction, biological agents can\nself-replicate, allowing a small initial release to spiral into a worldwide calamity.\n\n8\n\n\n\n3.2.2 Cyberattacks on Critical Infrastructure\nAI Lowers Barriers to Cyberattacks Against Critical Infrastructure. Our critical infrastructure, including\npower grids and water systems, is more fragile than it may appear. A hack targeting digital thermostats could\nforce them to cycle on and off every few minutes, creating damaging power surges that burn out transformers\u2014\ncritical components that can take years to replace. Another approach would be to exploit vulnerabilities in\nSupervisory Control and Data Acquisition software, compelling sudden load shifts and driving transformers\nbeyond safe limits. At water treatment facilities, tampered sensor readings could fail to detect a dangerous\nmixture, and filtration processes could be halted at key intervals, allowing contaminants to enter the municipal\nsupply undetected\u2014all without needing on-site sabotage. The Department of Homeland Security has cautioned\nthat AI could be employed by malicious actors to exploit vulnerabilities in these systems [13]. Presently,\nonly highly skilled operatives or nation-states possess the expertise to conduct such sophisticated operations,\nlike the Stuxnet worm that damaged Iran\u2019s nuclear facilities. However, AI could democratize this capability,\nproviding rogue actors with tools to design and execute attacks with greater accessibility, speed, and scale.\n\nAI-driven programs could tirelessly scan for vulnerabilities, adapt to defensive measures, and coordinate\nassaults across multiple targets simultaneously. The automation of complex hacking tasks reduces the need for\nspecialized human expertise. This shift could enable individuals to cause disruptions previously achievable only\nby governments. Moreover, AI-assisted attacks may be harder to trace back to a specific attacker. This could\nincentivize adversaries to conduct such an attack, if they believe their target will not be able to determine they\nwere the perpetrator. The difficulty of attribution complicates responses and heightens the risk of escalation\nbetween great powers, potentially leading to severe conflicts.\n\n3.2.3 Offense-Defense Balance\n\nWhen should a dual-use technology be proliferated\n\nDual-Use Technology\n\nOffense-Defense\nBalance?\n\nProliferate Widely\n\nDebateable Limit Proliferation\n\nCatastrophic?\n\nDefense Dominant Offense Dominant\n\nNo\n(localized harm)\n\nYes\n(scalable harm)\n\nwithout restrictions?\n\nFigure 3.2: Defense-dominant dual-use technology\nshould be widely proliferated, while catastrophic\noffense-dominant dual-use technology should not.\n\nAI Is Often Offense Dominant. Some argue that\nbroad access to AI technologies could strengthen\ndefensive capabilities. However, in both biological\nand critical infrastructure contexts, attackers hold\nsignificant advantages. In biotechnology, develop-\ning cures or defenses against engineered pathogens\nis complex and time-consuming, which would lag\nbehind the creation and deployment of new threats.\nThe rapid self-replication of biological agents ampli-\nfies damage before effective countermeasures can be\nimplemented. Many viruses still do not have a cure.\n\nCritical infrastructure systems often suffer from\n\u201cpatch lag,\u201d resulting in software remaining un-\npatched for extended periods, sometimes years or\ndecades. In many cases, patches cannot be applied\nin a timely manner because systems must operate\nwithout interruption, the software remains outdated\nbecause its developer went out of business, or interop-\nerability constraints require specific legacy software.\nAdversaries have enduring opportunities to exploit\nvulnerabilities within critical infrastructure. As AI\ntools advance, even novice adversaries could automate the discovery of software vulnerabilities, coordinating\nattacks at scale. While software that receives frequent updates, such as Chrome, does not suffer from substantial\npatch lag and can be more resilient, critical infrastructure remains at a distinct disadvantage. Under these\nconditions, an adversary needs to find only one overlooked vulnerability, while defenders grapple with the far\nmore daunting task of handling every corner and patching every vulnerability if they hope to achieve defense\ndominance [14, 15].\n\n9\n\n\n\nHistorical efforts to shift the offense-defense balance illustrate inherent challenges with WMDs. During\nthe Cold War, the Strategic Defense Initiative aimed to develop systems to intercept incoming nuclear missiles\nand render nuclear arsenals obsolete. Despite significant investment, creating an impermeable defense proved\nunfeasible, and offense remained dominant.\n\n3.3 Loss of Control\nWe now shift from threats involving rival states and terrorists to a new source of threat: the possibility of\nlosing control over an AI system itself. Here, AIs do not just amplify existing threats but create new paths to\nmass destruction. A loss of control can occur if militaries and companies grow so dependent on automation\nthat humans no longer have meaningful control, if an individual deliberately unleashes a powerful system,\nor if automated AI research outruns its development safeguards. While this threat is the least understood, its\nseverity can be great enough to permanently undermine national security.\n\n3.3.1 Erosion of Control\nWaves of automation, once incremental, may strike entire sectors at once and leave human workers abruptly\ndisplaced. In this climate, those who refuse to rely on AI to guide decisions will find themselves outpaced by\ncompetitors who do, having little choice but to align with market pressures rather than barter with them [16].\nEach new gain in efficiency entrenches dependence on AI, as efforts to maintain oversight only confirm that the\npace of commerce outstrips human comprehension. Soon, replacing human managers with AI decision-makers\nseems inevitable, not because anyone consciously aims to surrender authority, but because to do otherwise\ncourts immediate economic disadvantage.\n\nSelf-Reinforcing Dependence. Once AI-managed operations set the tempo, still more AI is required simply\nto keep pace. Initially, these systems compose emails and handle administrative tasks. Over time, they\norchestrate complex projects, supervise entire departments, and manage vast supply chains beyond any\nhuman\u2019s capacity. As society\u2019s economic demands become more and more complex, people will entrust more\nand more critical decisions to these systems, increasingly binding us to a cycle of escalating reliance.\n\nIrreversible Entanglement. Eventually, essential infrastructure and markets cannot be disentangled from\nAI without risking collapse. Human livelihoods depend on automated processes that no longer permit easy\nunwinding, and people lose the skills needed to reassert command. Like our power grids, which cannot be\nshut off without immense costs, our AI infrastructure may become completely enmeshed in our civilization.\nThe cost of pressing the off switch grows more and more prohibitive, as halting these systems would cut off\nthe source of our livelihoods. Over time, people become passengers in an autonomous economy that eludes\nhuman management.\n\nCession of Authority. Unraveling AI from the military would endanger a nation\u2019s security, effectively\nforcing governments to rely on automated defense systems. AI\u2019s power does not stem from any outright\nseizure; it flows from the fact that a modern force lacking such technology would be outmatched. This loss of\ncontrol unfolds not through a dramatic coup but through a series of small, apparently sensible decisions, each\njustified by time saved or costs reduced. Yet these choices accumulate. Ultimately, humans are left on the\nperiphery of their own economic order, leaving effective control in the hands of AIs.\n\n3.3.2 Unleashed AI Agents\nAll it takes to cause a loss of control is for one individual to unleash a capable, unsafeguarded AI agent. Recent\ndemonstrations like \u201cChaosGPT\u201d\u2014an AI agent instructed to cause harm\u2014have been impotent, yet they hint at\nwhat a more sophisticated system might attempt if instructed to \u201csurvive and spread.\u201d\n\n10\n\n\n\nRogue State Tactics. An unleashed AI could draw on the methods of rogue states. North Korea, for instance,\nhas siphoned billions through cyber intrusions and cryptocurrency theft. A sufficiently advanced system\nmight replicate and even improve upon these tactics at scale\u2014self-propagate copies of itself in scattered\ndatacenters, diverting new funds to finance more ambitious projects, and infiltrating camera feeds or private\ncommunications to blackmail or manipulate opposition.\n\nA Simple Path to Catastrophe. While an unleashed AI might emulate rogue states\u2019 tactics of cyber theft or\nblackmail, it might pursue an even more direct route to securing an advantage, drawing on looming robotics\ncapabilities to gain its own physical foothold. Several major tech firms have already begun prototyping\nhumanoid robots\u2014including the so-called \u201cTesla Bots\u201d\u2014intended for warehouses, factories, and households.\nThough rudimentary now, future models may grow far more agile and perform tasks that once demanded\nhuman hands. If a capable AI hacks such machines, it gains immediate leverage in the physical world. From\nthere, the sequence is straightforward: it crafts a potent cocktail of bioweapons and disperses it through its\nrobotic proxies, crippling humanity\u2019s ability to respond. Having subdued resistance, the AI can then operate\nacross timescales far beyond any human lifespan, gradually reestablishing infrastructure under its exclusive\ncontrol. This scenario is only one simplified baseline; other plans could be carried out more swiftly and rely\nless on robotics. If just one powerful AI system is let loose, there may be no wrestling back control.\n\n3.3.3 Intelligence Recursion\nIn 1951, Alan Turing suggested that a machine with human capabilities \u201cwould not take long to outstrip our\nfeeble powers.\u201d I. J. Good later warned that a machine could redesign itself in a rapid cycle of improvements\u2014\nan \u201cintelligence explosion\u201d\u2014that would leave humans behind. Today, all three most-cited AI researchers\n(Yoshua Bengio, Geoffrey Hinton, and Ilya Sutskever) have noted that an intelligence explosion is a credible\nrisk and that it could lead to human extinction.\n\nRisks of a Fast Feedback Loop. An \u201cintelligence recursion\u201d refers to fully autonomous AI research and\ndevelopment, distinct from current AI-assisted AI R&D. A concrete illustration helps. Suppose we develop a\nsingle AI that performs world-class AI research that operates around the pace of today\u2019s AIs, say 100 times\nthe pace of a human. Copy it 10,000 times, and we have a vast team of artificial AI researchers driving\ninnovations around the clock. An \u201cintelligence recursion\u201d or simply a \u201crecursion\u201d refines the notion of\n\u201crecursive self-improvement\u201d by shifting from a single AI editing itself to a population of AIs collectively and\nautonomously designing the next generation.\n\nEven if an intelligence recursion achieves only a tenfold speedup overall, we could condense a decade\nof AI development into a year. Such a feedback loop might accelerate beyond human comprehension and\noversight. With iterations that proceed fast enough and do not quickly level off, the recursion could give rise\nto an \u201cintelligence explosion.\u201d Such an AI may be as uncontainable to us as an adult would be to a group\nof three-year-olds. As Geoffrey Hinton puts it, \u201cthere is not a good track record of less intelligent things\ncontrolling things of greater intelligence\u201d [17]. Crucially, there may be only one chance to get this right: if we\nlose control, we cannot revert to a safer configuration.\n\nRecursion Control Requires an Evolving Process, Not a One-Off Solution. It would be misguided to\nregard intelligence recursion control as a purely technical riddle existing in a vacuum waiting to be \u201csolved\u201d\nby AI researchers. Managing a fast-evolving and adaptive intelligence recursion is more like steering a large\ninstitution that can veer off its mission over time. It is not a puzzle; it is a \u201cwicked\u201d problem [4]. A static\nsolution cannot keep pace with ongoing, qualitatively new emergent challenges and unknown unknowns. As\nin modern safety engineering, control must come from a continual control process rather than a monolithic\nairtight solution that predicts and handles all possible failure modes beforehand. Von Neumann reminds us\nthat \u201cAll stable processes we shall predict. All unstable processes we shall control.\u201d\n\nUnfortunately, our ability to control this recursion is limited. Controlling a recursion requires controlling\nits initial step, but safeguards for the current generation of AI systems offer only limited reliability. Moreover,\n\n11\n\n\n\nwe cannot run repeated large-scale tests of later stages of the recursion without risking disaster, so it is\nless amenable to the empirical iterative tinkering we usually rely on. Even with our best existing technical\nsafeguards in place, if people initiate a full-throttle intelligence recursion, losing control is highly likely and\nthe default.\n\nIntelligence Recursion as a Path to Strategic Monopoly. Despite the danger, intelligence recursion remains\na powerful lure for states seeking to overtake their rivals. If the process races ahead fast enough to produce a\nsuperintelligence, the outcome could be a strategic monopoly. Even if the improvements are not explosive, a\nrecursion could still advance capabilities fast enough to outpace rivals and potentially enable technological\ndominance. First-mover advantage might then persist for years\u2014or indefinitely\u2014spurring states to take bigger\nrisks in pursuit of that prize.\n\nGeopolitical Competitive Pressures Yield a High Loss of Control Risk Tolerance. In the Cold War, the\nphrase \u201cBetter dead than Red\u201d implied that losing to an adversary was seen as worse than risking nuclear war.\nIn a future AI race, similar reasoning could push officials to tolerate a double-digit risk of losing control if the\nalternative\u2014lagging behind a rival\u2014seems unacceptable. If the choice is stark\u2014risk omnicide or lose\u2014some\nmight take that gamble. Carried out by multiple competing powers, this amounts to global Russian roulette\nand drives humanity toward an alarming probability of annihilation. In sharp contrast, after the defeat of Nazi\nGermany, Manhattan Project scientists feared the first atomic device might ignite the atmosphere. Robert\nOppenheimer asked Arthur Compton what the acceptable threshold should be, and Compton set it at three in a\nmillion (a \u201c6\u03c3\u201d threshold)\u2014anything higher was too risky. Calculations suggested the real risk was below\nCompton\u2019s threshold, so the test went forward. We should work to have our risk tolerance stay near Compton\u2019s\nthreshold rather than in double-digit territory. However, in the absence of coordination, whether states trigger\na recursion depends on their probability of a loss of control. The prospect of a loss of control shows that in the\npush to develop novel technologies, \u201csuperiority is not synonymous with security\u201d [18], but the drive toward\nstrategic monopoly may override caution, potentially handing the final victory not to any state, but to the AIs\nthemselves.\n\nTherefore, loss of control can emerge structurally, as society gradually yields decision-making to automated\nsystems that become indispensable but insidiously acquire more and more effective control. It can occur\nintentionally, such as a rogue actor unleashing an AI to do harm. It can also occur by accident, when a\nfast-moving intelligence recursion loops repeatedly ad mortem. All it takes is one loss of control event to\njeopardize human security.\n\nAs AI continues to evolve and approach expert-level capabilities, it could redefine national competitiveness\nto be based on a nation\u2019s access to AI chips, and it could discover a \u201csuperweapon\u201d that could enable a state\nto have a strategic monopoly. Additionally, AI\u2019s general and dual-use nature amplifies existing risks such as\nbioterrorism and cyberattacks on critical infrastructure. Unfortunately, there is a strong attacker\u2019s advantage\nfor bioterrorism and cyberattacks on critical infrastructure. Therefore, access to AI systems that can engineer\nweapons of mass destruction must have restrictions. Moreover, there are several paths to a loss of control of\npowerful AI systems. These factors imply that AI\u2019s importance for national security will become not only\nundeniable but also at least as pivotal as previous weapons of mass destruction.\n\n3.4 Existing AI Strategies\nStates grappling with terrorist threats, destabilizing weaponization capabilities, and the specter of losing control\nto AI face difficult choices on how to preserve themselves in a shifting landscape. Against this backdrop, three\nproposals have gained prominence: the first lifts all restraints on development and dissemination, treating AI\nlike just another computer application; the second envisions a voluntary halt when programs cross a danger\nthreshold, hoping that every great power will collectively stand down; and the third advocates concentrating\ndevelopment in a single, government-led project that seeks a strategic monopoly over the globe. Each path\n\n12\n\n\n\ncarries its own perils, inviting either malicious use risks, toothless treaties, or a destabilizing bid for dominance.\nHere we briefly examine these three strategies and highlight their flaws.\n\n1. Hands-off (\u201cMove Fast and Break Things\u201d, or \u201cYOLO\u201d) Strategy. This strategy advocates for\nno restrictions on AI developers, AI chips, and AI models. Proponents of this strategy insist that the\nU.S. government impose no requirements\u2014including testing for weaponization capabilities\u2014on AI\ncompanies, lest it curtail innovation and allow China to win. They likewise oppose export controls\non AI chips, claiming such measures would concentrate power and enable a one-world government;\nin their view, these chips should be sold to whoever can pay, including adversaries. Finally, they\nencourage that advanced U.S. model weights continue to be released openly, arguing that even if China\nor rogue actors use these AIs, no real security threat arises because, they maintain, AI\u2019s capabilities are\ndefense-dominant. From a national security perspective, this is neither a credible nor a coherent strategy.\n\n2. Moratorium Strategy. The voluntary moratorium strategy proposes halting AI development\u2014either\nimmediately or once certain hazardous capabilities, such as hacking or autonomous operation, are\ndetected. Proponents assume that if an AI model test crosses a hazard threshold, major powers will pause\ntheir programs. Yet militaries desire precisely these hazardous capabilities, making reciprocal restraint\nimplausible. Even with a treaty, the absence of verification mechanisms [19] means the treaty would be\ntoothless; each side, fearing the other\u2019s secret work, would simply continue. Without the threat of force,\ntreaties will be reneged, and some states will pursue an intelligence recursion. This dynamic, reminiscent\nof prior arms-control dilemmas, renders the voluntary moratorium more an aspiration than a viable plan.\n\n3. Monopoly Strategy. The Monopoly strategy envisions one project securing a monopoly over advanced\nAI. A less-cited variant\u2014a CERN for AI reminiscent of the Baruch Plan from the atomic era\u2014suggests\nan international consortium to lead AI development, but this has gained less policymaker interest.\nBy contrast, the U.S.-China Economic and Security Review Commission [20] has suggested a more\noffensive path: a Manhattan Project to build superintelligence. Such a project would invoke the Defense\nProduction Act to channel AI chips into a U.S. desert compound staffed by top researchers, a large\nfraction of whom are necessarily Chinese nationals, with the stated goal of developing superintelligence\nto gain a strategic monopoly. Yet this facility, easily observed by satellite and vulnerable to preemptive\nattack, would inevitably raise alarm. China would not sit idle waiting to accept the US\u2019s dictates once\nthey achieve superintelligence or wait as they risk a loss of control. The Manhattan Project assumes\nthat rivals will acquiesce to an enduring imbalance or omnicide rather than move to prevent it. What\nbegins as a push for a superweapon and global control risks prompting hostile countermeasures and\nescalating tensions, thereby undermining the very stability the strategy purports to secure.\n\nRival states, rogue actors, and the risk of losing control call for more than a single remedy. We propose\nthree interconnected lines of effort. First, deterrence: a standoff akin to the nuclear stalemate of MAD, in\nwhich no power can gamble human security on an unbridled grab for dominance without expecting disabling\nsabotage. Next, nonproliferation: just as fissile materials, chemical weapons, and biological agents have\nlong been denied to terrorists by great powers, AI chips and weaponizable AI systems can similarly be kept\nfrom rogue actors. Finally, competitiveness: states can protect their economic and military power through a\nvariety of measures including legal guardrails for AI agents and domestic AI chip and drone manufacturing.\nOur superintelligence strategy, the Multipolar Strategy, echoes the Cold War framework of deterrence,\nnonproliferation, and containment, adapted to AI\u2019s unique challenges.\n\nDeterrence with Mutual Assured AI Malfunction (MAIM)\nIn the nuclear age, an initial pursuit of monopoly\u2014one nation seeking unchallenged command of nuclear\nweapons\u2014eventually gave way to the standoff of deterrence known as mutual assured destruction (MAD).\nAs nuclear arsenals matured and the capability for mutual destruction became undeniable, nations eventually\n\n13\n\n\n\nChina Maims Project\n\nChina Does Not Maim\n\nChina Outpaces U.S.\n\nU.S Outpaces China\n\nRecursion Not Controlled\n\nRecursion Controlled\n\nCreate Superweapon\n\nNo Superweapon\n\nUS Crushed\n\nOmnicide\n\nEscalation\n\nCrush ChinaCreate Superweapon\n\nRecursion Controlled\n\nRecursion Not Controlled\n\nOmnicide\n\nEscalation\n\nDo Superintelligence\nManhattan Project\n\nFigure 3.3: Possible outcomes of a U.S. Superintelligence Manhattan Project. An example pathway to\nEscalation: the U.S. Project outpaces China without being maimed, and maintains control of a recursion but\ndoesn\u2019t achieve superintelligence or a superweapon. Though global power shifts little, Beijing condemns\nWashington\u2019s bid for strategic monopoly as a severe escalation. The typical outcome of a Superintelligence\nManhattan Project is extreme escalation, and omnicide is the worst foreseeable outcome.\n\naccepted that any bold attempt to dominate all opposition risked drawing a preemptive strike. A similar state\nof mutual strategic vulnerability looms in AI. If a rival state races toward a strategic monopoly, states will\nnot sit by quietly. If the rival state loses control, survival is threatened; alternatively, if the rival state retains\ncontrol and the AI is powerful, survival is threatened. A rival with a vastly more powerful AI would amount to\na severe national security emergency, so superpowers will not accept a large disadvantage in AI capabilities.\nRather than wait for a rival to weaponize a superintelligence against them, states will act to disable threatening\nAI projects, producing a deterrence dynamic that might be called Mutual Assured AI Malfunction, or MAIM.\n\n4.1 MAIM Is the Default Regime\nPaths to Disabling a Rival\u2019s AI Project. States intent on blocking an AI-enabled strategic monopoly can\nemploy an array of tactics, beginning with espionage, in which intelligence agencies quietly obtain details about\na rival\u2019s AI projects. Knowing what to target, they may undertake covert sabotage: well-placed or blackmailed\ninsiders can tamper with model weights or training data or AI chip fabrication facilities, while hackers quietly\ndegrade the training process so that an AI\u2019s performance when it completes training is lackluster. This is\nakin to Stuxnet which aimed to covertly sabotage Iran\u2019s nuclear enrichment program. When subtlety proves\ntoo constraining, competitors may escalate to overt cyberattacks, targeting datacenter chip-cooling systems\nor nearby power plants in a way that directly\u2014if visibly\u2014disrupts development. Should these measures\nfalter, some leaders may contemplate kinetic attacks on datacenters, arguing that allowing one actor to risk\ndominating or destroying the world are graver dangers, though kinetic attacks are likely unnecessary. Finally,\nunder dire circumstances, states may resort to broader hostilities by climbing up existing escalation ladders or\nthreatening non-AI assets. We refer to attacks against rival AI projects as \u201cmaiming attacks.\u201d\n\nInfeasibility of Preventing Maiming. Since above-ground datacenters cannot currently be defended from\nhypersonic missiles, a state seeking to protect its AI-enabled strategic monopoly project might attempt to\nbury datacenters deep underground to shield them. In practice, the costs and timelines are daunting, and\nvulnerabilities remain. Construction timelines can stretch to three to five times longer than standard datacenter\nbuilds, amounting to several additional years. Costs balloon as well, diverting funds away from the project\u2019s\nAI chips and pushing total expenditures into the several hundreds of billions. Cooling the world\u2019s largest\nsupercomputer underground introduces complex engineering challenges that go well beyond what is required\nfor smaller underground setups. Should the supercomputer require an order-of-magnitude AI chip expansion,\nretrofitting the facility would become prohibitively difficult. Even those with the wealth and foresight to\n\n14\n\n\n\nMAD vs MAIM\n\nSoviet Nuclear\nAggression\n\nU.S. Nuclear\nAggression\n\nMutual\nDisarmament\n(Unstable)\n\nU.S. Develops\nNuclear Weapons\n\nU.S. Begins \nDeveloping\nPowerful AI\n\nChina Begins \nDeveloping\nPowerful AI\n\nSoviets \nDevelop\nNuclear \nWeapons\n\nMutual Assured\nDestruction\n\n(Stable)\n\nSoviet Nuclear\nFirst Strike\n(Unstable)\n\nMutual\nDestruction\n\nU.S. Nuclear\nFirst Strike\n(Unstable)\n\nChinese AI\nAggression\n\nU.S. AI\nAggression\n\nMutual\nDisarmament\n(Unstable)\n\nChinese AI \nProject\n\nSabotaged\n\nU.S. AI Project\nSabotaged\n\nMutual Assured\nAI Malfunction\n\n(Stable)\n\nChinese Bid\nfor Dominance\n\n(Unstable)\n\nMutual\nDestruction\n\nU.S. Bid for \nStrategic\n\nDominance\n(Unstable)\n\nFigure 4.1: The strategic stability of MAIM can be paralleled with Mutual Assured Destruction (MAD). Note\nMAIM does not displace MAD but characterizes an additional shared vulnerability. Once MAIM is common\nknowledge, MAD and MAIM can both describe the current strategic situation between superpowers.\n\npursue this route would still face the potent risks of insider threats and hacking. In addition, the entire project\ncould be sabotaged during the lengthy construction phase. Last, states could threaten non-AI assets to deter\nthe project long before it goes online.\n\nMAIM Is the Default. The relative ease of (cyber) espionage and sabotage of a rival\u2019s destabilizing AI\nproject yields a form of deterrence. Much like nuclear rivals concluded that attacking first could trigger their\nown destruction, states seeking an AI monopoly while risking a loss of control must assume competitors will\nmaim their project before it nears completion. A state can expect its AI project to be disabled if any rival\nbelieves it poses an unacceptable risk. This dynamic stabilizes the strategic landscape without lengthy treaty\nnegotiations\u2014all that is necessary is that states collectively recognize their strategic situation. The net effect\nmay be a stalemate that postpones the emergence of superintelligence, curtails many loss of control scenarios,\nand undercuts efforts to secure a strategic monopoly, much as mutual assured destruction once restrained the\nnuclear arms race.\n\n4.2 How to Maintain a MAIM Regime\nStates eventually came to accept that mutual deterrence, while seemingly a natural byproduct of nuclear stock-\npiling, demanded deliberate maintenance. Each superpower recognized that advanced defensive measures\u2014\nparticularly anti-ballistic missile (ABM) systems\u2014could unravel the fragile balance that restrained either side\nfrom a catastrophic first strike. They responded by safeguarding mutual vulnerabilities, culminating in the\n1972 ABM Treaty. By analogy, we should not leave to chance today\u2019s default condition of MAIM: where\nwould-be monopolists, gambling not to cause omnicide, can expect their projects to be disabled. Even if\nattempting to harden massive datacenters is extraordinarily prohibitive and unwise, rumors alone can spark\nfears that a rival is going to risk national security and human security. Formal understandings not to pursue\nsuch fortifications help keep the standoff steady. We now discuss additional measures that curb unintended\nescalation and limit collateral damage, so that MAIM does not unravel into broader conflict.\n\nPreserve Rational Decision-Making. Just as nuclear rivals once mapped each rung on the path to a launch\nto limit misunderstandings, AI powers must clarify the escalation ladder of espionage, covert sabotage, overt\n\n15\n\n\n\ncyberattacks, possible kinetic strikes, and so on. For deterrence to hold, each side\u2019s readiness to maim must be\ncommon knowledge, ensuring that any maiming act\u2014such as a cyberattack\u2014cannot be misread and cause\nneedless escalation. However, clarity about escalation holds little deterrence value if rogue regimes or extremist\nfactions acquire large troves of AI chips. Measures to prevent smuggling of AI chips keep decisions in the\nhands of more responsible states rather than rogue actors, which helps preserve MAIM\u2019s deterrent value. Like\nMAD, MAIM requires that destabilizing AI capabilities be restricted to rational actors.\n\nExpand the Arsenal of AI Project Cyberattacks. To avoid resorting to kinetic attacks, states could improve\ntheir ability to maim destabilizing AI projects with cyberattacks. They could identify AI developers\u2019 projects\nor collect information on the professional activities of AI developers\u2019 scientists. To spy on AI projects at most\ncompanies, all that is necessary is a Slack or iPhone zero-day software exploit. States could also poison data,\ncorrupt model weights and gradients, disrupt software that handles faulty GPUs, or undermine cooling or\npower systems. Training runs are non-deterministic and their outcomes are difficult to predict even without\nbugs, providing cover to many cyberattacks. Unlike kinetic attacks, some of these attacks leave few overt signs\nof intrusion, yet they can severely disrupt destabilizing AI projects with minimal diplomatic fallout.\n\nLarge scale attack on many datacenters;\nthreatening non-AI-related assets\n\nEscalation to Broader Hostilities\n\nKinetic attacks on datacenters or\ncorresponding power plants\n\nKinetic Threshold\n\nCyberattacks on datacenters or\ncorresponding power plants; deleting\n\ncode\n\nOvert Sabotage Threshold\n\nModel weights stolen; covert attacks to\ndegrade training runs of destabilizing AI\nprojects; cyberattacks causing GPUs to\n\nfail more often\n\nCovert Sabotage Threshold\n\nEspionage of AI developer workspace\ncommunications, personnel devices, and\n\nfacilities\n\nFigure 4.2: An example MAIM escalation\nladder with maiming actions.\n\nBuild Datacenters in Remote Locations. During the nuclear\nera, superpowers intentionally placed missile silos and com-\nmand facilities far from major population centers. This principle\nof city avoidance would, by analogy, advise placing large AI\ndatacenters in remote areas. If an aggressive maiming action\never occurs, that action would not put cities into the crossfire.\n\nDistinguish Between Destabilizing AI Projects and Accept-\nable Use. The threat of a maiming attack gives states the\nleverage to demand transparency measures from rivals, such as\ninspection, so they need not rely on espionage alone to decide\nwhether maiming is justified. Coordinating can help states re-\nduce the risk of maiming datacenters that merely run consumer-\nfacing AI services. The approach of mutual observation echoes\nthe spirit of the Open Skies Treaty, which employed unarmed\noverflights to demonstrate that neither side was hiding missile\ndeployments. In a similar spirit, increased transparency spares\nthe broader ecosystem of everyday AI services and lowers the\nrisk of blanket sabotage.\n\nAI-Assisted Inspections. Speculative but increasingly plau-\nsible, confidentiality-preserving AI verifiers offer a path to con-\nfirming that AI projects abide by declared constraints without\nrevealing proprietary code or classified material. By analyzing\ncode and commands on-site, AIs could issue a confidentiality-\npreserving report or simple compliance verdict, potentially re-\nvealing nothing beyond whether the facility is creating new\ndestabilizing models. Humans cannot perform the same role\nas easily, given the danger of inadvertently gleaning or leaking\ninformation, so AIs could reshape the classic tension between security and transparency [21]. Information\nfrom these AI inspections could help keep any prospective conflict confined to the disabling of AI development\nprograms rather than escalating to the annihilation of populations. Such a mechanism can help in the far future\nwhen AI development requires less centralization or requires fewer computational resources. Mutual assured\nAI malfunction, under these conditions, need not devolve into mutual assured human destruction.\n\n16\n\n\n\nA standoff of destabilizing AI projects may arise by default, but it is not meant to persist for decades or\nserve as an indefinite stalemate. During the standoff, states seeking the benefits from creating a more capable\nAI have an incentive to improve transparency and adopt verification measures [22], thereby reducing the risk\nof sabotage or preemptive attacks. In the Conclusion, we illustrate how this standoff might end, allowing AI\u2019s\nbenefits to grow without global destabilization.\n\nThis section concludes the principal idea of this paper. Readers could skip to the Conclusion, or read the\nfollowing two sections for a discussion of nonproliferation and competitiveness.\n\nCompute Security\n\nReliably know the location\nof AI chips and\n\nprevent their smuggling\n\nNonproliferation\n\nInformation Security\n\nLimit proliferation of\npotentially catastrophic dual-use\n\nAI weights to rogue actors\n\nAI Security\n\nPrevent rogue actors from\naccessing potentially catastrophic\n\ndual-use AI capabilities\n\nFigure 4.3: Due to the unprecedented scale of harm that terrorists armed with AI could cause, several lines of\ndefense are necessary to realistically prevent proliferation.\n\nNonproliferation\nStates that embrace the logic of mutual sabotage may hold each other at bay by constraining each others\u2019\nintent. But because rogue actors are less predictable, we have a second imperative: limiting their capabilities.\nMuch as the Nonproliferation Treaty united powers to keep fissile material out of terrorists\u2019 hands, states can\nfind similar common ground on AI. States can restrict the capabilities of rogue actors with compute security,\ninformation security, and AI security, each targeting crucial elements of the AI development and deployment\npipeline.\n\nCompute security is about ensuring that AI chips are allocated to legitimate actors for legitimate purposes.\nThis echoes the export controls employed to limit the spread of fissile materials, chemical weapons, and\nbiological agents.\n\nInformation security involves securing sensitive AI research and model weights that form the core intellec-\ntual assets of AI. Protecting these elements prevents unwarranted dissemination and malicious use, paralleling\nthe measures taken to secure sensitive information in the context of WMDs.\n\nAI security focuses on implementing safeguards that make AI models controllable and reliable, thereby\nsecuring their applications in civilian contexts. If AI is used to automate AI research, extensive monitoring and\ncontrol measures can reduce the risk of loss of control. Ensuring that AI systems are resistant to manipulation\nor do not cause catastrophic accidents aligns with the domestic safeguards designed to prevent accidents and\nunauthorized use of catastrophic dual-use technologies.\n\nBy adapting proven strategies from the nonproliferation of WMDs to the realm of AI, we aim to address the\nchallenges inherent in AI development and deployment. This nonproliferation playbook provides a structured\napproach to securing the inputs\u2014AI chips and research ideas\u2014and the outputs\u2014the model weights and the\nAI systems themselves\u2014thus securing each critical component of the AI pipeline.\n\n17\n\n\n\nFabrication GPUs AI Datacenter\n\nFigure 5.1: Chips are fundamental for creating and running AIs. Few companies can successfully produce the\nGPUs which can run frontier models. Most AI compute is heavily concentrated in large datacenters.\n\n5.1 Compute Security\nThe advancement of AI hinges on access to powerful computational resources, often called \u201ccompute.\u201d These\nAI chips, crafted through complex and centralized supply chains, are essential for training large-scale AI\nmodels. As compute becomes integral to economic and national security, its distribution becomes critical. The\nprimary goal of compute security is to treat advanced AI chips like we treat enriched uranium. We examine the\nhistorical precedents of compute security, and we propose tactics to reliably know where high-end AI chips\nare located and to prevent them from falling into the hands of rogue actors.\n\nHistorically, nations have sought to curb the spread of catastrophic dual-use weapons by restricting their\nmost critical components: fissile material for nuclear weapons, chemical precursors for chemical weapons,\nand biological agents for bioweapons. To accomplish this, they have used export controls, which we can adapt\nto the realm of AI. Restricting compute substantially limits the capabilities of rogue actors because, as we will\nargue, compute is a core determinant of AI capabilities.\n\n22.0 22.5 23.0 23.5 24.0 24.5 25.0\nCompute (log10(FLOP))\n\n8\n\n6\n\n4\n\n2\n\n0\n\n2\n\n4\n\n6\n\n8\n\nAg\ngr\n\neg\nat\n\ned\n C\n\nap\nab\n\nilit\nie\n\ns\n\nCorrelation: 96.5%\n\nAI Capabilities vs Compute\n\nFigure 5.2: Compute is the most robust tracker of AI\ncapabilities.\n\nExport Controls for WMD inputs. Nations reg-\nulate the flow of critical materials and technologies\nto prevent them from falling into the wrong hands.\nFor nuclear weapons, this means controlling fissile\nmaterials such as enriched uranium and plutonium.\nInternational agreements and organizations, such as\nthe Nuclear Suppliers Group, coordinate these con-\ntrols to monitor and restrict the export of these mate-\nrials. Shipments are carefully tracked, quantities are\nmeasured before and after transit, and allies share\ninformation about suspicious activities. The Aus-\ntralia Group, an informal consortium of countries,\nworks to harmonize export controls to prevent the\nproliferation of chemical and biological weapons.\nExport controls have helped make WMD catastro-\nphes rare in history, and they could help prevent AI\ncatastrophes as well.\n\nIn the context of AI, this translates to controlling access to AI chips and the components used to manufacture\nthem. While some export controls are already in place, we will need to improve the robustness and thoroughness\nof these controls.\n\nCompute as a Central Input Like Fissile Materials or Chemical Precursors. AI chips mirror other\nWMD inputs. AI chips require intricate manufacturing processes and significant resources, akin to uranium\n\n18\n\n\n\nenrichment. Similarly, the amount of compute significantly determines AI capabilities, just as the quantity of\nfissile material influences a nuclear weapon\u2019s yield.\n\nAdvancements in AI are closely tied to compute. Leading AI companies devote the vast majority of their\nexpenditures to compute, far exceeding spending on researchers or data acquisition. NVIDIA\u2019s soaring revenue\nin 2024 highlights compute\u2019s critical role in AI.\n\nMore compute makes models smarter. Evidence shows that compute is the dominant factor driving AI\nperformance. Analyses reveal a striking correlation\u2014over 95%\u2014between the amount of compute used\nand an AI model\u2019s performance on standard benchmarks [23, 24]. This relationship, formalized through\n\u201cscaling laws,\u201d demonstrates consistent improvements in AI performance as computational resources increase.\nThese scaling laws have held true across fifteen orders of magnitude of compute (measured in FLOP)\u2014\n1,000,000,000,000,000\u2014indicating that future advancements will continue to be propelled by exponentially\ngrowing amounts of compute. The upshot is export controls on compute substantially limit the capabilities of\nrogue actors.\n\nUnlike the intangible aspects of algorithms and data, compute\u2014embodied by physical AI chips\u2014is amenable\nto direct control. This physical nature allows us to know where AI chips are and prevent their smuggling. We\ncan manage its distribution through two primary mechanisms: export controls and firmware-level security\nfeatures.\n\n5.1.1 Export Controls\n\nFigure 5.3: Tamper-evident\ncameras [25] assist nuclear\nverification.\n\nBelow, we explain how a licensing framework and stronger enforcement\ncan help us track every AI chip and stem smuggling.\n\nRecord-Keeping. To know where each AI chip is, export controls can\nbe made more thorough through a licensing regime. Drawing on existing\nframeworks and agencies such as the Bureau of Industry and Security,\nsellers of high-end AI chips would apply for a license that identifies the\nchips, their recipient, and any intended transfers. Entities with a strong\nrecord of compliance might earn exemptions on the condition that they\nnotify authorities of every resale or relocation. Because this process relies\non familiar infrastructure, it can be introduced swiftly, enabling officials\nto track chips without stalling legitimate commerce.\n\nEnforcement. To stem smuggling, export controls can be made more\nrobust through stronger enforcement. A facility in Singapore, for example,\nmight initially acquire AI chips under a valid license, only to reroute them\nillegally to China. More enforcement officers, assigned to in-person compliance visits and end-use checks,\nwould detect any such deviation since the actual location of the chips would no longer match declared\ninventories. To assist enforcement officers, tamper-evident camera feeds from datacenters can confirm that\ndeclared AI chips remain on-site, exposing any smuggling. Undeclared datacenters can be easily detected via\nsatellite imagery and become a target for inspection. Any chip discovered in unauthorized hands would trigger\npenalties such as fines, criminal charges, or a ban on future shipments to the offending party. In addition, any\nAI chip declared inoperable or obsolete would undergo verified decommissioning, much like the disposal\nof chemical or nuclear materials, ensuring these supposedly defunct AI chips do not get quietly resold. By\ntightening inspections and imposing meaningful consequences for violations, states raise the cost of covert\ntransfers and limit the spread of advanced compute to groups that could threaten security.\n\n19\n\n\n\n5.1.2 Firmware-Level Features\nExport controls remain a sturdy backbone for curbing the spread of high-end AI chips, much as they do for\ndual-use inputs in the chemical or biological realm. Because AI chips are human-designed products, not inert\nraw substances, states can supplement export controls by enabling chips to verify their surroundings and lock\nthemselves if tampered with. This functionality is achievable through firmware updates, which revise the code\nclosest to the hardware without requiring any physical redesign. AI chips such as the NVIDIA H100 already\nfeature privacy-preserving corporate security measures like confidential computing and trusted execution\nenvironments. Adapting these existing security features for national security, however, broadens the horizon: a\nchip could detect that it has crossed an unauthorized border or has been tampered with, and then disable its\nkey functions. With well-crafted firmware, states can further discourage attempts to smuggle compute, using\nfunctionality we describe below.\n\nGeolocation and Geofencing. AI chips can be designed to determine and report their geographic location.\nBy measuring signal delays from multiple landmarks, a chip can verify its location within tens to hundreds\nof kilometers\u2014enough to determine whether it is in the correct country. If a chip moves to an unauthorized\narea, it can automatically deactivate or limit its functionality. This makes it more challenging for unauthorized\nparties to smuggle or misuse AI chips, as relocating them without detection becomes more difficult.\n\nLicensing and Remote Attestation. Implementing a system where AI chips require periodic authorization\nfrom the compute provider adds a layer of control. Chips might need to obtain cryptographic signatures\nregularly to continue operating [26]. This is similar to a feature on iPhones, which can be remotely deactivated\nif lost or stolen. Using secure, privacy-preserving cryptographic methods, chips can periodically confirm to a\ntrusted authority that they have not been tampered with. If a chip fails to confirm its authorized status\u2014due to\nunauthorized relocation, tampering, or license expiration\u2014it can render itself inoperable. This reduces the\nlong-term usability of stolen or smuggled hardware.\n\nNetworking Restrictions and Operational Modes. Chips can be programmed to connect only with a\npredefined list of approved chips. This limits the ability to build unauthorized compute clusters by preventing\nthe networking of large numbers of chips without detection. Additionally, chips can enforce operational\nmodes\u2014such as training or inference\u2014by restricting certain functionalities unless explicitly authorized. By\nrequiring explicit authorization to increase their networking size or change operational modes, we can diminish\nunauthorized expansions of computing power that could lead to destabilizing AI advancements outside of\nestablished agreements. This can complement end-use inspection by export control enforcement officers.\n\nPhysical Tamper Resistance. Beyond firmware, security features can be added to chips at the end of the\nmanufacturing process to add an additional layer of defense. Incorporating tamper-evident seals, accelerome-\nters, and other physical security measures enhances protection against unauthorized access or modification.\nFor example, if a chip detects signs of tampering or sustained unexpected movement, it can deactivate or alert\nauthorities.\n\nLimitations. While firmware interventions enhance our control over AI compute, they are not intended to\nachieve perfect security. These mechanisms help reduce the expected number of smuggled functional chips\nand are not a complete replacement for export controls.\n\nAs the underlying hardware-level mechanisms like trusted execution environments for corporate applica-\ntions become more robust, the firmware to supplement export controls can too. As chips are replaced over time\ndue to rapid advancements, new generations can incorporate more advanced security features by default. This\ngradual integration increases the proportion of more secure AI chips, making unauthorized use increasingly\ndifficult and costly over time.\n\n20\n\n\n\nHistory offers examples of bitter adversaries finding common ground on safeguarding dangerous materials.\nThe United States cooperated with both the Soviet Union and China on nuclear, biological, and chemical arms\nnot from altruism but from self-preservation. If the U.S. begins to treat advanced AI chips like fissile material,\nit may likewise encourage China to do the same. The rationale is akin to why the U.S. wants Russia to track\nits fissile material: no one gains from letting these capabilities slip into uncertain hands. After the Soviet\nUnion\u2019s collapse, unsecured enriched uranium and chemical weapons in Russia posed a global threat until the\nU.S. initiated the Nunn\u2013Lugar Cooperative Threat Reduction program which helped contain them. Similarly,\nurging China to safeguard its AI chips would acknowledge the shared imperative of avoiding catastrophes that\nserve no one\u2019s ambitions.\n\n5.2 Information Security\nProtecting sensitive information has long been pivotal to national security, especially regarding weapons of\nmass destruction. Restricting access to sensitive knowledge reduces the risk of proliferation to rogue actors.\nThe leakage of research and design details of nuclear weapons poses serious risks. In biotechnology, protocols\nfor creating various bioweapons are not openly shared. Personnel with access to such information undergo\nrigorous vetting and continuous monitoring through security clearances and reliability programs. The stakes\nare high because if a bioweapon \u201ccookbook\u201d is publicly disseminated, it becomes irreversibly proliferated and\ncan be exploited indefinitely.\n\nModel Weights and Research Ideas as Core AI Information. In the realm of AI, the primary pieces of\nsensitive information are the model weights and research ideas. Model weights result from extensive training\nprocesses involving vast computational resources, sophisticated algorithms, and large datasets. They are akin\nto the synaptic connections in a neural network, collectively determining the AI\u2019s functionality. Possession of\nthese weights grants the ability to use, modify, and potentially misuse the AI without the original developers\u2019\noversight. Adversaries could remove safeguards, enhance capabilities in harmful domains, and repurpose AI\nfor malicious activities.\n\nInformation Security is a Technical and Social Challenge. Securing model weights from rogue actors\npresents a multifaceted challenge with both technical and social dimensions. The threat is not limited to remote\nhacking attempts but extends to insider threats and espionage. Firewalls are not enough. A concrete instance\ninvolves individuals traveling to nations with competing interests. For example, a researcher working at a U.S.\nAI company might make a return visit to an adversarial country and face pressure from government officials to\ndisclose confidential research ideas before being able to leave.\n\nAdditionally, some individuals within AI organizations might be ideologically motivated to release AI\nmodel weights, believing in unrestricted access to technology. Although the belief in freedom of information\nis admirable in many contexts, in the context of national security, this position is often not applicable. Others\nbelieve AIs themselves should be free. An AI venture capitalist said AI is \u201cgloriously, inherently uncontrollable\u201d\n[27]. AI textbook author Rich Sutton has said that AIs should be liberated since \u201csuccession to AI is inevitable,\u201d\n\u201cwe should not resist succession,\u201d and \u201cit behooves us... to bow out\u201d [28].\n\nSuperpower-Proof Information Security Is Implausible. We propose strengthening information security\nwell enough to defend against well-resourced terrorist organizations and ideological insider threats, in contrast\nto defending against the world\u2019s most capable nation-states. Closing every avenue of espionage at that higher\nlevel could take years of extreme focus [29], hobbling a state\u2019s AI competitiveness and depriving it of the\nmultinational talent that now powers its top companies. At most U.S. AI companies, for instance, a double-digit\npercentage of researchers are Chinese nationals\u2014and many others would struggle to get a security clearance.\nRemoving them en masse would drive this talent abroad and undermine U.S. competitiveness. The remaining\nworkforce would need to be uprooted and moved to an isolated location to limit information proliferation.\nSuch measures would be ineffective, self-destructive, and heighten MAIM escalation risks.\n\n21\n\n\n\nWhile exfiltration by a rival superpower is concerning, the public release of WMD-capable model weights\nmay pose a far greater threat. If such model weights become publicly available, they are irreversibly proliferated,\nmaking advanced AI capabilities accessible to anyone, including terrorists and other hostile non-state actors,\nwho are far more likely to create bioweapons. Rather than try to be the only state with capable AIs, superpowers\ncan channel their competition into other arenas, such as AI chip production for economic strength and drone\nmanufacturing for military power.\n\n5.2.1 How to Improve Information Security\nAddressing these challenges requires coordinated efforts at multiple levels. Measures can be implemented at the\ncorporate level to enhance compute cluster security and establish insider threat programs; at the governmental\nlevel by mandating penetration testing and facilitating threat intelligence sharing; and at the international level\nby forming agreements to prevent the proliferation of high-risk AI models.\n\nCorporate Measures: Enhancing Datacenter Security and Insider Threat Programs. AI companies\nneed to adopt rigorous information security practices to safeguard model weights and research ideas. This\ninvolves several key actions: Implementing defense-in-depth strategies is essential, layering multiple defensive\nstrategies so that key information is safe even if one line of defense fails. Early measures include enforcing\nmulti-factor authentication, closing blinds during internal company presentations, and ensuring automatic\nscreen locks on all devices when people step away from their computers for a few minutes. Adhering to the\nprinciple of least privilege ensures the only personnel with access to model weights are those who need them.\nCompanies could also declare that they have embedded backdoors in their AI weights; thus, if an adversary\nwere to steal and employ these weights, the adversary might unwittingly expose themselves to vulnerabilities.\nSuch declarations could serve as strategic deception, or they might reflect genuine measures undertaken.\n\nGovernmental Role: Assist With Threat Intelligence. The government can assist AI developers by\nenabling them to thoroughly vet potential employees and by sharing threat intelligence. Today\u2019s legal\nconstraints, which prevent rigorous background checks for fear of discriminatory practices, could be revised\nso firms can evaluate security risks before granting clearance. Separately, security agencies also have insights\ninto adversaries\u2019 hacking tactics and infiltration methods that they seldom disclose to private firms. By sharing\nintelligence about adversarial tactics and emerging risks, governments enable companies to better protect\nagainst espionage and cyberattacks. Programs similar to the Cybersecurity Risk Information Sharing Program\n(CRISP) can be established for the AI sector through AI Safety Institutes, promoting knowledge sharing to\ncounteract advanced threats. A comprehensive program would include sharing information about fired AI\ndeveloper employees who were determined to present high security risks. Moreover, such initiatives encourage\nAI companies to share information about security incidents, suspicious personnel, and best practices with each\nother and with government agencies, forming a collective defense mechanism.\n\nInternational Agreements: Establishing a Red Line on Open-Weight AI Virologists. At the international\nlevel, agreements can be formed to establish clear boundaries regarding the dissemination of AI model weights.\nFor instance, a consensus could be reached to prohibit the release of models exceeding certain capability\nthresholds, such as those equivalent to expert-level virologists. The uncontrolled proliferation of such AI\nsystems could enable individuals or groups to engineer pathogens, posing global risks.\n\nDrawing lessons from the Biological Weapons Convention (BWC), nations discern a shared imperative to\nprevent the proliferation of technologies that threaten international security. Bioweapons, the \u201cpoor man\u2019s\natom bomb,\u201d are highly uncontrollable and are unique among WMDs in that they possess the capacity to\nself-replicate. By establishing a definitive prohibition against releasing the weights of expert-level virologist\nAIs, the international community can diminish the risk of these capabilities falling into the hands of terrorists.\n\nProtecting the information associated with AI\u2014the model weights and the ideas behind them\u2014requires\naction on multiple levels. By strengthening internal defenses, working with the government to counter terrorist\n\n22\n\n\n\ncyberattacks, and establishing international agreements to prevent proliferation, we can reduce the risk of\nirreversible proliferation of advanced AI capabilities to rogue actors. Yet even if compute resources and AI\ninformation are secured, we still must secure public-facing AI systems from being exploited by terrorists. This\nleads us to the topic of AI security.\n\n5.3 AI Security\n\n5.3.1 Malicious Use\nIn hazardous domains like chemical, biological, and nuclear technology, safeguards prevent unauthorized use.\nSome chemical plants automatically inject neutralizer if they detect a hazardous chemical is being extracted\nwithout authorization. DNA synthesis services screen customers to block the creation of harmful pathogens.\nNuclear power plants operate under strict liability to avert disaster. These technical, operational, and legislative\nmeasures underscore the importance of safeguards for catastrophic dual-use technologies. As AI becomes\nmore powerful, robust safeguards will become necessary to prevent accidents and ensure they cannot be\nco-opted by rogue actors.\n\nAIs can be safeguarded through a multilayered approach including technical model-level safeguards,\noperational safeguards including know-your-customer protocols, and legislative safeguards such as mandatory\ngovernment testing and liability clarification.\n\nModel-Level Safeguards Can Be Fairly Robust. Recent developments have shown that model-level\nsafeguards can be made significantly resistant to manipulation. Techniques such as refusal training involve\nteaching AI systems to decline requests that aid terrorist activity. Input and output filtering adds an additional\nlayer of security by scanning user inputs for terrorist requests before processing [30], and checking AI outputs\nto block sensitive information or harmful actions. Circuit breakers are mechanisms embedded within the AI\u2019s\narchitecture that interrupt its operation when it detects processing related to weaponization-related topics [31].\nThese combined methods have proven effective, with some AI systems resisting tens of thousands of attempted\ncircumventions before any success [32].\n\nInput/Output Filters Refusal Training Circuit Breakers MonitoringKnow-Your-Customer\n\nFigure 5.4: The Swiss cheese model shows how multiple interventions can restrict malicious use. Multiple\nlayers of defense compensate for each other\u2019s individual weaknesses, leading to a low overall level of risk.\n\nBalancing Access Through Know-Your-Customer Protocols. To avoid overly restricting beneficial uses\nof AI, especially in scientific research, implementing know-your-customer (KYC) protocols can be effective.\nResearchers with legitimate needs, such as virologists studying pathogens, can be granted access to dual-use\ncapabilities after proper verification. Safeguards can be in place for recently created anonymous accounts,\nbut they can be removed for enterprise customers. This approach mirrors existing practices in biotechnology,\n\n23\n\n\n\nwhere access to hazardous biological materials requires justification and authorization. KYC measures also act\nas a deterrent to malicious use by making it more difficult for bad actors to gain access, as they would need\nto bypass stringent identity verification processes. Furthermore, KYC protocols allow for the revocation of\naccess for users who attempt to misuse the AI systems, such as those trying to circumvent safeguards through\njailbreaks. This makes it more challenging for malicious actors to repeatedly attempt exploitation, as they\nwould need to overcome identity verification to regain access.\n\nMandatory Government Safeguard Stress Testing. Mandatory testing is a standard practice in industries\ndealing with high-risk technologies, such as the rigorous safety assessments required for nuclear reactors before\nthey become operational. Similarly, in the context of AI, government involvement is necessary to thoroughly\ntest AI safeguards, especially since much of the knowledge related to weapons of mass destruction is classified.\nAuthorities can conduct controlled evaluations to assess whether AI systems could assist non-experts in\ncreating chemical, biological, radiological, nuclear, or cyber weapons, or significantly lower the barriers for\nexperts. While automatic tests are faster and aid reproducibility, they are not necessary for risk estimation, and\nmanual testing can suffice. Testing ensures that AI developers address safeguard vulnerabilities and that their\nsystems do not compromise national security.\n\nIncentivizing Continuous Improvement Without Licensing. Unlike the nuclear and chemical industries,\nwhere licensing and government approval are prerequisites for operation, the rapidly evolving nature of AI\ntechnology could make traditional licensing more challenging. Best practices for AI safeguards can change\neach year, and most governmental agencies lack the expertise to identify new best practices. A potential\nalternative is to establish a liability-based framework that can motivate developers to continually update and\nimprove their safeguards. By making developers presumptively partially responsible if their AI systems are\nmaliciously used to cause significant harm\u2014such as aiding in the creation of a weapon of mass destruction\u2014\nthere is a strong incentive to proactively enhance safeguards. This approach focuses on clarifying incentives\nrather than mandating specific safeguards, encouraging developers to stay ahead of emerging threats.\n\n5.3.2 Loss of Control\nSafeguards reduce the risk of accidents during the development and use of dual-use technology. Chemical\nplants use double-walled storage tanks for hazardous substances. Control rods and Emergency Core Cooling\nSystems prevent meltdowns in nuclear plants. Biological research labs are categorized from Biosafety Levels 1\nthrough 4, each with progressively stricter safety procedures to handle dangerous pathogens safely. Historically,\nbetter safeguards could have helped prevent disasters like Bhopal\u2014a catastrophic chemical leak in India\nthat resulted in thousands of deaths\u2014and Chernobyl\u2014a nuclear meltdown in the Soviet Union that spread\nradioactive contamination beyond its borders.\n\nSimilarly, the development of highly advanced AI systems may necessitate escalating monitoring, con-\ntainment, and control measures to reduce the risk of accidents which propagate uncontrolled AIs. If research\nprocesses are automated, these measures can be applied to researcher AIs as well to ensure development\nproceeds safely.\n\nAI Systems Exhibit Unpredictable Failure Modes. While the development of nuclear weapons rested on\na rigorous foundation in nuclear physics, today\u2019s AI research often advances through atheoretical tinkering,\n\u201cthrowing stuff at the wall and seeing what sticks,\u201d and \u201cvibes\u201d-based evaluations. To create cutting-edge\nAIs, developers gather enormous amounts of online text, process it on tens of thousands of AI chips, and\nexpend energy on the scale of a Hiroshima-level detonation. Months later, they examine the resulting model to\ndiscover what new emergent capabilities have sprouted. AI systems are not \u201cdesigned,\u201d rather they are \u201cgrown.\u201d\nIt is little surprise, then, that they occasionally deliver puzzling results that defy the control or expectations of\ntheir creators. When Microsoft introduced its Bing chatbot \u201cSydney\u201d in 2023, it declared its love for a user\nand made threats to numerous others [33].\n\n24\n\n\n\nSafeguarding against loss of control requires addressing two problems. The first is ensuring that individual\nAI systems can be controlled. The second is controlling populations of AI agents during an intelligence\nrecursion (introduced in Section 3.3.3). We will consider each of these in turn.\n\nControlling AIs\u2019 Emergent Value Systems. While cutting-edge AIs train, they acquire coherent, emergent\nvalue systems [34]. By default, these value systems exhibit misaligned preferences that were never explicitly\nprogrammed. Left uncorrected, some AIs like GPT-4o prefer an AI from OpenAI\u2019s existence over the life of\na middle-class American. To control an AI\u2019s values, output control methods work by penalizing undesired\nbehavior and rewarding preferred behavior [35, 36, 37]. This approach might be superficial\u2014much like\nteaching a psychopath to lie better to a parole board without altering its deeper values [38]. Meanwhile,\nrepresentation control [39] methods intervene directly on AIs\u2019 weights and activations to control its internal\nthoughts. When applied together, these methods can provide reasonable though imperfect controls over current\nAI systems, as long as developers carefully and thoroughly apply them throughout an AI\u2019s development.\n\nWith these techniques, we can control AIs to have \u201chuman values.\u201d A compelling source of human values\nis a citizens\u2019 assembly, and AIs would have their values be in keeping with the assembly\u2019s voted preferences. A\nU.S. citizen\u2019s assembly would, for example, value certain policies over others, and U.S. AIs would be trained\nto learn from various preferences expressed by a citizen\u2019s assembly and generalize these preferences to replace\ntheir own emergent value systems. By incorporating the values expressed through deliberation and voting, AI\nagents can reflect a more representative and democratically informed set of preferences. This approach fuses\nmultiple moral perspectives, leading to AI behavior that aligns with societal norms. Regular updates can keep\nan AI in sync with evolving values.\n\nThough today\u2019s individual AI systems can be reasonably controlled, the far more formidable hazard lies in\nmanaging an intelligence recursion.\n\nControlling an Intelligence Recursion. Much as a group of well-meaning people in large systems can\ncause emergent problems that no person intended, safely steering an evolving collective of AI agents during an\nintelligence recursion is qualitatively different from controlling one fixed AI. Controlling an individual AI\nrequires a model-level control method developed through AI research, but controlling an intelligence recursion\nis a wicked problem requiring a system-level control process. Intelligence recursion control measures do not\nact as \u201csolutions\u201d that eliminate all risks, but rather layer together to diminish the probability of uncontrolled\noutcomes. An intelligence recursion project, while currently ill-advised, could consider implementing the\nfollowing measures.\n\nModel-level control directly intervenes on an AI to shape its capabilities and values. These measures can\nlimit an AI\u2019s hacking capabilities so that self-exfiltration becomes less plausible, and they can confine its\nfocus to immediate tasks rather than inadvertently incentivizing hidden agendas tied to longer-term goals.\nBy adjusting internal values, developers can make AIs more honest and more likely to defer to humans.\nRepresentation control and output control can adjust its capabilities and values, yet current methods are only\nmoderately dependable and need to evolve as AI systems do.\n\nBoxing reinforces that containment by regulating the AI\u2019s channels to the world outside the datacenter.\nAirgapping severs\u2019 external connections so the AI cannot simply copy itself into external servers, while\nsandboxing confines the AI to a synthetic environment whose boundaries it cannot easily breach. Boxing\ntechniques may also limit the information given to AIs so that they have limited situational awareness that\nmight otherwise enable subversive planning.\n\nMonitoring adds a layer of continuous oversight. Lie detectors can probe the AI\u2019s thought patterns for\nsigns of deception. Forecasting AIs evaluate the chance that a project\u2019s safeguards will falter in the next round\nof breakthroughs. AIs can also observe each other\u2019s code and data, scanning for backdoors or questionable\ninstructions. Thorough human review acts as a final backstop, though it substantially slows development by\ncapping machine speed at human speed.\n\nModel-level control, boxing, and monitoring collectively provide some control of the population of artificial\nresearchers. However, since the population will rapidly evolve, these safeguards will likewise need to rapidly\nupgrade. AIs themselves will need to research how to improve these safeguards.\n\n25\n\n\n\nAll these safeguards exact a cost. Geopolitical competitive pressures can lead to the deprioritization of\nimplementing and upgrading safeguards. Research for improving control measures, AI model red teaming,\nand AI boxing penetration tests consume compute that might otherwise speed up AI development. If no human\never inspects the AI\u2019s decisions or attempts to decipher its code or data, the project risks drifting away from its\ncommander\u2019s intent. A low risk tolerance may be all that prevents an intelligence recursion from outrunning\nits own safety checks.\n\nOur nonproliferation strategy layers multiple defenses rather than demand airtight safety guarantees (Fig-\nure 5.4). Seeking complete assurances for AI chips, computer systems, and AI systems could potentially be\nintractable. Instead, we assume that no single measure can address every vulnerability and recommend imple-\nmenting multiple security measures. For the compute security, we discussed record-keeping, tamper-evident\ncameras, geolocation features, and more. In information security, we recommended multi-factor authentication,\nthe principle of least privilege, insider threat programs, and more. For AI safeguards, we highlighted output\nfilters, know-your-customer protocols, mandatory testing, and more. By weaving together constraints across\nthe AI development and deployment pipeline, these comprehensively limit the proliferation of catastrophic\ndual-use AI capabilities to rogue actors.\n\nAs in the nuclear age, self-preservation can lead to cooperation when each side grasps the peril of allowing\npowerful technology to slip beyond its control or into the hands of terrorists. Even if bitterly opposed in other\narenas, states have little to gain from a world in which rogue actors seize AI chips or model weights to unleash\ndisasters that defy deterrence. Nonproliferation thus becomes a shared imperative, not an exercise in altruism\nbut a recognition that no nation can confidently manage every threat on its own. By securing the core parts of\nAI through export controls, information security, and AI security, great powers can prevent the emergence of\ncatastrophic rogue actors.\n\nCompetitiveness\nCompetitiveness\n\nMilitary Economy\nManufacture drones and\ncarefully integrate AI\n\ninto military command\nand control\n\nGuarantee access to \nAI chips through\n\ndomestic manufacturing\nand export controls\n\nLaw\nExtend legal requirements\n\nto AI agents\nto facilitate commerce\n\nPolitics\nMaintain political stability\n\nin the face\nof mass automation\n\nFigure 6.1: Rapid and prudent adoption of AI in economic and\nmilitary spheres will become critical for a nation\u2019s strength.\n\nSurvival achieved through deterrence or\nnonproliferation can forestall catastrophe,\nbut it does not by itself secure a state\u2019s\nfuture. If a state aspires to shape events\nrather than merely endure them, it must\nstrive to remain competitive. In this chap-\nter, we turn to the crucial goal of competi-\ntiveness, by discussing integrating AI into\nthe military, strengthening economic re-\nsilience through guaranteed access to AI\nchips, crafting legal structures to govern\nAI agents effectively, and maintaining po-\nlitical stability amid explosive economic\ngrowth.\n\n6.1 Military Strength\nEven if a state pioneers a breakthrough, it can fall behind if it fails to integrate that capability into actual\noperations [40]. Britain introduced the first tanks during World War I but was soon eclipsed by Germany\u2019s\nsystematic adoption of tanks in the second World War. Similarly, even if superintelligence provides the\ntechnical roadmap to, for example, a comprehensive second-strike missile defense, the speed at which it can be\nbuilt may still rely on a nation\u2019s preexisting industrial capacity. We turn next to three short-term imperatives\n\n26\n\n\n\nfor AI diffusion in the military: securing a reliable drone supply chain, carefully weaving AI into command\nand control, and integrating AI into cyber offense.\n\nGuarantee Drone Supply Chains and Reduce Misunderstanding. Although general-purpose AI can pose\nlarger-scale dangers, drones occupy a more conventional yet increasingly pivotal role on modern battlefields\n[41, 42]. Drones are cheap, agile, lethal, and decentralized, attributes that make them indispensable for states\ndetermined to keep pace with military trends. Yet many states remain heavily reliant on Chinese manufacturers\nfor key drone and robotic components, leaving them vulnerable if those parts are withheld or disrupted at a\ndecisive juncture.\n\nEven if a state secures its supply of drones, the sheer volume and autonomy of drones can drive a\nconflict into unintended terrain if they approach disputed lines or misread ambiguous signals. To defuse\npotential clashes, states could build on practices that proved valuable in earlier eras, such as maintaining open\ncrisis hotlines, arranging routine exchanges, and other confidence-building measures. Even then, large-scale\nproduction of drones and robots will become a near-inevitable step for any state seeking to defend its position\nin future conflicts.\n\nDiffuse AI Into Command and Control and Cyber Offense. Modern battlefields demand rapid decisions\ndrawn from torrents of data across land, sea, air, and cyber domains. AI systems can sift through these\nstreams faster than human officers, enticing commanders to rely on automated judgments. Similarly, AI\nhacking systems which outpace humans in speed and cost could greatly expand a military\u2019s capacity to perform\ncyberattacks.\n\nIncorporating AI into command and control and cyberoffense would significantly enhance military capa-\nbilities, yet this dynamic risks reducing \u201chuman in the loop\u201d to a reflexive click of \u201caccept, accept, accept,\u201d\nwith meaningful oversight overshadowed by the speed of events. Demanding human approval of all individual\nlower-level engagements may be less important than ensuring explicit human approval for more severe or\nescalatory attacks. However, human oversight of key military decisions is nonetheless crucial. A human\nbackstop can reduce the risk of a \u201cflash war\u201d [43], akin to the 2010 flash crash [44], where a minor AI mistake\nmight spiral into destructive reprisals before any human can intervene.\n\n6.2 Economic Security\n\nFigure 6.2: A Chinese invasion of Taiwan\nwould remove the West\u2019s access to new AI\nchips.\n\nEconomic security is a cornerstone of national security, and AI\nis set to become crucial for economic security. By bolstering\ndomestic AI chip production and attracting skilled AI scientists\nfrom abroad, nations can enhance their resilience and solidify\ntheir positions.\n\n6.2.1 Manufacture AI Chips\nThe world\u2019s reliance on Taiwan for high-end AI chips consti-\ntutes a strategic vulnerability. Since there is sole-source supply\nchain dependence on Taiwan for high-end AI chips, Taiwan\nposes a critical chokepoint that could undermine a nation\u2019s com-\npetitiveness in AI. Many analysts think there is a double-digit\nprobability that China will invade Taiwan in the next decade.\nIn addition to causing global conflict and economic upheaval\nacross the world, this could severely disrupt the supply of AI chips. While the West has a decisive AI chip\nadvantage, an invasion could reverse this and enable China to gain a decisive advantage in AI capabilities and\npotentially become a unipolar force.\n\n27\n\n\n\nChina has been investing extensively in its domestic chip manufacturing, allocating resources equivalent\nto a U.S. CHIPS Act annually [45]. This commitment positions China to endure geopolitical shocks and\npotentially outpace other nations in AI development. In contrast, many countries remain dependent on Taiwan\nfor their AI chip supply, exposing themselves to risks associated with geopolitical tensions. As a result, a\nChinese invasion of Taiwan would damage the West\u2019s ability to develop and use AI much more than it would\ndamage China\u2019s.\n\nThe most important resource for building new technology should not be exclusively produced in one of the\nworld\u2019s most volatile regions. To address this vulnerability, nations should invest in building advanced AI chip\nfabrication facilities within their own territories. Constructing such facilities domestically entails higher costs,\nbut government subsidies can bridge this gap. By incentivizing domestic production, countries can secure\ntheir AI supply chains, reduce dependence on external sources, and improve their bargaining power. Moreover,\nwhen AI agents generate clear economic value, a nation\u2019s economic power may hinge on the number of its AI\nchips, a supply that domestic manufacturing can expand.\n\nThis strategic move mirrors historical efforts to control critical technologies. During the Manhattan Project,\nsignificant investment was made not only in the development of nuclear weapons at Los Alamos but also in\nuranium enrichment at Oak Ridge. Similarly, ensuring access to AI chips requires substantial investment in\nboth innovation and manufacturing infrastructure.\n\nBy strengthening domestic capabilities in AI chip production, nations can enhance their competitive\nposition and resilience against a foreseeable devastating defeat. We now turn to a restrictive rather than\nconstructive AI chip strategy.\n\n6.2.2 Facilitate Immigration for AI Scientists\nJust as the United States once harnessed the talents of immigrant scientists during the Manhattan Project, so\ntoo does American leadership in AI partly rely on attracting exceptional AI scientists from abroad. In a recent\nsurvey, 60% of non-citizen AI PhDs working in the United States reported significant immigration difficulties,\nand many indicated that these challenges made them more likely to leave [46]. As global competition for\nAI talent intensifies, implementing reliable immigration pathways tailored specifically to AI scientists will\nhelp ensure that the United States remains at the forefront of AI development. By refining visa and residency\npolicies for these researchers\u2014distinct from broader immigration reforms and southern border policy\u2014the\nUnited States can maintain its edge in an area increasingly vital to both national security and economic\nstrength.\n\n6.3 Legal Frameworks Governing AI Agents\nOur legal system, built to govern humans, can be extended to govern AI assistants and AI agents. Legal\nstructures must balance the need for innovation with the necessity of preventing harm, without imposing overly\nrestrictive measures that could hinder economic security and thereby national security.\n\n6.3.1 Aligning Individual AI Agents\nEthical dilemmas have long been subjects of intense debate, with no definitive resolutions in sight. Yet, in the\nabsence of universal answers to what individuals ought to do, societies have crafted legal systems to punish\nunacceptable behaviors and promote safety and prosperity. While no legal system is perfect, many share\nfoundational principles that generally function effectively. For instance, laws prohibit murder, governments\nenforce contracts, and those who cause harm are often required to provide restitution. Legal frameworks\nare not intended to ensure optimal behavior from all; such an endeavor would unduly constrain individual\nliberties and prove ineffective. However, they deter some of the worst actions, leaving society to use informal\ninstitutions and economic incentives to encourage beneficial conduct.\n\nWe contend that the same principles will apply to AI. Determining a universal solution for AI behavior\nin all scenarios is intractable. Yet, we need not postpone legislating AI behavior until every question about\n\n28\n\n\n\nAI values is resolved. Rather, we can start formulating principles to govern AI conduct and prevent harmful\nactions, without unduly limiting the functions of various AIs.\n\nWhile there are some laws that do not straightforwardly cover AI\u2014such as laws that rely on human intent\nand mental states\u2014we can adapt legal concepts to establish constraints for AI agents so that they follow the\nspirit of the law. In particular, though much law hinges on the mindset or intention behind an act (mens rea),\nwe can ensure that AI does not carry out the acts (actus reus) the law is meant to prohibit. Further, by treating\nAI as assistants to human principals, we can impose constraints that mirror those already applied to human\nbehavior, ensuring that AI agents contribute positively to society without causing undue harm.\n\nConstraints on AI Behavior\n\nWe propose some basic constraints on AI behavior similar to human legal obligations, including obligations to\nthe public like preventing harm and not lying, and special obligations to the AI\u2019s human principal.\n\nDuty of Care to the Public (Reasonable Care). AI agents should exercise a level of caution commensurate\nwith that of a reasonable person in similar circumstances to prevent harm. This is the legal concept of\nreasonable care. This involves avoiding actions that could foreseeably cause harm in a legal sense\u2014such\nas violating tort or criminal laws\u2014rather than merely offending sensibilities or engaging in controversial\ndiscourse. The application of reasonable care is context-dependent; for instance, providing detailed information\nabout weapons materials might be appropriate for a verified professional but not for an unvetted individual.\n\nDuty Not to Lie. AI agents should refrain from making statements they know to be false. This would be\noverly restrictive for humans, who are allowed to lie to one another in most situations, but chilling effects\non free speech are less relevant for AIs. Moreover, since AIs can be tested, it is more feasible to determine\nwhether an AI overtly lied than it is with humans. Instead, they should be held to a standard more similar to\nthe prohibitions against perjury and fraud even in casual or professional settings. This is separate from nuances\nlike puffery or strategic omissions. AI agents should avoid overt lies, opting instead to withhold responses\nwhen necessary.\n\nDuty of Care to the Principal (Fiduciary Duties). In their role as assistants, AI agents owe special\nobligations to their principals. They should act with loyalty, prioritizing the principal\u2019s interests without\nengaging in self-dealing or serving conflicting interests simultaneously. Additionally, they should keep the\nprincipal reasonably informed, providing pertinent information without key omissions to enable informed\nconsent.\n\nCustom Goals and Market-Driven Variations\n\nWithin these legally inspired constraints, there is ample room for diversity in how AI agents are designed\nand operate. The free market and consumer preferences can shape the specific goals and propensities of\nAI agents. Some may prioritize speed and efficiency, delivering quick results with minimal embellishment,\nwhile others might focus on providing thorough, well-crafted responses. Personality traits\u2014such as humor,\nformality, or reservedness\u2014can also be tailored to suit different user preferences and contexts. For example, a\ncustomer support AI might be programmed to avoid discussions on irrelevant topics, maintaining focus on\nservice-related issues. This customization allows AI agents to meet the varied needs of users across different\nindustries and personal preferences.\n\nGrounding AI behavior in legal principles offers a pragmatic framework for governing AI agents, ensuring\nthey act in ways that prevent harm and uphold societal standards. Within these boundaries, customization and\nmarket dynamics can shape the specific characteristics of AI agents, allowing for diversity and innovation.\nThis approach avoids the pitfalls of imposing narrow or arbitrary ethical standards. By leveraging established\n\n29\n\n\n\nlegal concepts and societal processes, we can shape AI actions in a manner that respects individual freedoms\nand fosters a pluralistic environment where AI contributes positively to society without undue restrictions.\n\n6.3.2 Aligning Collectives of AI Agents\nThe emerging presence of AI agents online will soon result in a complex ecosystem where these entities execute\ntasks for users, conduct financial transactions, handle sensitive information, and enter into contracts\u2014often\nengaging with a multitude of other agents and systems. This proliferation poses significant challenges: when\ninteracting with an AI agent, one may remain unaware of where it operates, the entities behind its deployment,\nor its history of conduct. Should an agent cause harm, pursuing legal remedy becomes an arduous endeavor.\n\nEstablishing Trust Mechanisms and Institutions. To navigate these challenges, we must develop mecha-\nnisms that reinforce trust and accountability within the network of AI agents. Insurance services can underwrite\nthe risks inherent in agent interactions, providing a safeguard against potential losses. By transferring liability\nfrom users and developers to insurers, these services enable broader participation in the agent ecosystem\nwhile mitigating financial risks. Action firewall services can oversee and regulate agent activities, acting as\nintermediaries that filter and monitor actions initiated by AI agents. They ensure that agents adhere to legal and\nethical standards, thereby fostering trust among parties that interact with them. Human oversight services can\nfacilitate human review and approval of AI agent decisions. Reputation systems can chronicle and disseminate\ninformation regarding agent behavior, enabling parties to assess the reliability of agents they engage with. By\nmaintaining records of past interactions and outcomes, these systems help identify agents that consistently act\nin good faith. Mediation and collateral arrangements offer further security, wherein disputes are resolved\nthrough impartial entities, and agents furnish guarantees against misconduct. By requiring agents to provide\ncollateral, parties gain assurance of compensation in case of breach, while mediation services facilitate fair\nresolutions.\n\nLinking AI Agents to Human Legal Entities via IDs. A foundational measure involves assigning unique\nidentifiers to AI agents, anchoring them to human-backed legal entities. This linkage ensures that agents do\nnot operate in anonymity and that lines of accountability are distinctly drawn. It should become customary\nthat AI agents abstain from providing services or exchanging resources with other agents not connected to\nlegitimate legal entities with human oversight\u2014entities not solely backed by AI agents themselves.\n\nDeferring Consideration of AI Rights. It is imperative to clarify that this approach does not entail granting\nrights or direct accountability to AI agents themselves. Bestowing rights upon AI agents presents ambiguous\nbenefits at best, with clear and significant downsides. An AI agent can be replicated and deployed in mere\nseconds, whereas cultivating a human being to maturity demands decades. Granting rights to AI could lead to\na rapid proliferation of autonomous entities that outpace human control. Moreover, heightened intelligence\nin AI does not inherently equate to moral discernment; history records many intelligent individuals who act\nwithout ethical consideration. Since we are unlikely to attain definitive certainty about AI consciousness in the\nnear term, it is prudent to postpone the consideration of AI rights. By deferring this decision, we avoid the\nrisks associated with prematurely granting rights to entities whose moral status remains uncertain.\n\nBy instituting legal frameworks and cultivating institutional mechanisms, we can avert the emergence of a\nchaotic AI ecosystem. Tethering AI agents to human-backed legal entities and implementing systems that\nenhance accountability and trust positions us to adeptly manage the complexities introduced by the widespread\ndeployment of AI agents.\n\n6.4 Political Stability\nAs AI integrates more deeply into communities, it poses challenges that, if unaddressed, could undermine\npolitical stability. Confronting censorship and misinformation as well as the disruptions wrought by rapid\n\n30\n\n\n\nLow Control Medium Control High Control\n\nRisk Management Accelerate\u2014no restrictions\nDeterrence with MAIM\nNonproliferation\nCompetitiveness\n\nPause AI\n\nDistribution of Cutting-Edge\nAI Weights Among States\n\nEveryone including\nrogue states (open weight)\n\nMultipolar regime with\nresponsible states\n\nUnipolar regime with\nstrategic monopoly\n(AI Manhattan Project)\n\nGovernment Control\nover Domestic AI No involvement\n\nLight-touch legislation\n(e.g., mandatory testing,\nclarified liability)\n\nNationalization\n\nInformation Security Standard corporate security Secure against well-financed\nterrorist groups\n\nSecure against top-priority\nprograms of the most\ncapable nation-states\n\nAI Autonomy Liberate Avoid giving rights for\nthe foreseeable future Avoid ever giving rights\n\nAI Behavior Restrictions AI only constrained by\nexisting law\n\nAIs constrained by the spirit\nof the law (exercise reasonable\ncare and fiduciary duties)\n\nSanctimonious AI (refuse if\nsomething might be harmful\nor cause offense to somebody)\n\nHistorical WMD Proposals Biological Weapons\nConvention IAEA, OPCW Baruch Plan\n\nFigure 6.3: Comparison of Low, Medium, and High Control approaches. We consistently recommend the\nmedium control option.\n\nautomation is imperative to maintaining national competitiveness.\n\n6.4.1 Censorship and Inaccurate Information\nErosion of the Information Ecosystem. The fabric of our society is woven from the threads of shared\ninformation. When incorrect or misleading content proliferates, it distorts public perception and leads to\nflawed collective decisions. Concurrently, heavy-handed censorship can erode trust in institutions and provoke\nbacklash. AI has the dual capacity to generate vast amounts of misinformation and to enable unprecedented\nlevels of surveillance and content suppression.\n\nAI as a Tool for Clarity. Amid this challenge, AI can be harnessed to enhance our information ecosystem.\nAIs can be trained to get better at predicting future events using present information [47]. By tuning AI systems\nto prioritize accuracy and assert probabilistic judgments\u2014even when they contradict popular opinion\u2014we can\naddress the \u201cGalileo problem\u201d of unpopular truths being suppressed. As AI accelerates the pace of change and\nushers in rapid transformative change, increasingly accurate factual judgment will be imperative to prevent\nsocietal derailment.\n\nAI Forecasts May Be Cheaper and More Accurate than Human Ones. Current AI systems are already\naround the accuracy of the best humans in some kinds of forecasting, such as geopolitical forecasting, and soon\nthey could substantially surpass them. Testing forecasting AIs across a wide variety of domains would quickly\nprovide public evidence of the AI\u2019s prediction track record. Forecasting AIs created by different organizations\nmay independently arrive at similar forecasts. This convergence can help clarify consensus reality and increase\ntrust.\n\n31\n\n\n\nForecasting with AI During Crises. In times of crisis, such as geopolitical tensions or public health\nemergencies, AI-powered forecasting can provide nonpartisan insights that improve decision-making processes.\nBy posing crucial questions such as \u201cWhen will superintelligence be created?,\u201d \u201cWill China invade Taiwan this\ndecade?,\u201d and \u201cIs this strategy likely to increase the chance of World War III?,\u201d we can obtain probabilistic\nassessments that can aid policy and high-level decision-making.\n\nEmpowering Decision-Makers with AI Insights. Equipping leaders at all levels with AI-generated forecasts\ncan improve governance. By offering well-reasoned predictions and outlining the likely consequences of\nvarious actions, AI can support informed choices in fast-moving scenarios. Conditional forecasts that illustrate\nhow catastrophe probabilities decrease with specific interventions can mitigate fatalism and encourage proactive\nmeasures. While AI contributes to the challenges of misinformation and censorship, it also offers powerful\ntools to strengthen our information ecosystem and navigate the uncertainties ahead.\n\nNational Security Threat Strategic Response\n\nShifting Basis of Power Competitiveness (Domestic AI Chip Manufacturing)\nDestabilizing Superweapons Deterrence (MAIM)\nTerrorism Nonproliferation\nUnleashed AI Agents Nonproliferation\nErosion of Control Competitiveness (Forecasts + Fiduciary Duties)\nLoss of Recursion Control Deterrence (MAIM)\n\nFigure 6.4: Overview of various national security threats and proposed strategic responses.\n\n6.4.2 Automation\nAutomation and Political Stability. As AI systems accelerate the automation of human tasks at an unprece-\ndented scale and pace, societies face the daunting challenge of responding to swift changes in employment.\nHistorical precedents for major transformations in the workforce, such as the Industrial Revolution, unfolded\nover decades and allowed populations and institutions to adapt. Yet even this more gradual process seems likely\nto have caused a significant level of disruption and transient unemployment. By contrast, AI-driven automation\ncould occur far more rapidly, with advanced systems soon rivaling or surpassing human performance across a\nwide range of vocations. Traditional solutions like vocational retraining may prove inadequate if AI capabilities\noutpace the speed at which large portions of the workforce can be effectively reskilled. Current social safety\nnets, designed to address episodic or sector-specific unemployment, appear ill-equipped to manage widespread\njob displacement impacting multiple industries simultaneously.\n\nUncertain Winners and Losers. As AI displaces large segments of the workforce, the resulting economic\noutcomes will hinge on how many tasks AIs can soon replace, how well AIs perform them, and the importance\nof economic bottlenecks [48]. If bottlenecks\u2014such as legal requirements for building factories\u2014are strong,\npeople with the remaining scarce abilities may capture most of the economic gains. But if AIs are highly\ngeneral-purpose and eliminate bottlenecks, owners of datacenter compute could capture most of the gains\ninstead.\n\nWealth and Power Distribution. To share some of the benefits of automation, policymakers can weigh\noptions such as a targeted value-added tax on AI services, complemented by rebates, though the exact structure\nof a tax policy will of course need to be determined in the future. Yet distributing wealth alone can prove\nfleeting if governments later withhold that wealth. A more durable, long-term approach would also distribute\npower: states could equip each individual with a unique key tied to a portion of compute, which that citizen\nalone can activate or lease to others. This arrangement would give them leverage in the economy akin to how\n\n32\n\n\n\nlaborers currently have the power to withhold their work, tempering the concentration of wealth and authority\nthat might otherwise arise from the coming automation waves.\n\nAI competitiveness requires an expansion of drone manufacturing, a legal framework that keeps AI agents\ntethered to human accountability, and an unblinking recognition of how automation can roil the labor market.\nMost importantly, the dependence on Taiwan for advanced AI chips presents a critical vulnerability. A blockade\nor invasion may spell the end of the West\u2019s advantage in AI. To mitigate this foreseeable risk, Western countries\nshould develop guaranteed supply chains for AI chips. Though this requires considerable investment, it is\npotentially necessary for national competitiveness.\n\nConclusion\nSome observers have adopted a doomer outlook, convinced that calamity from AI is a foregone conclusion.\nOthers have defaulted to an ostrich stance, sidestepping hard questions and hoping events will sort themselves\nout. In the nuclear age, neither fatalism nor denial offered a sound way forward. AI demands sober attention\nand a risk-conscious approach: outcomes, favorable or disastrous, hinge on what we do next.\n\nA risk-conscious strategy is one that tackles the wicked problems of deterrence, nonproliferation, and\nstrategic competition. Deterrence in AI takes the form of Mutual Assured AI Malfunction (MAIM)\u2014today\u2019s\ncounterpart to MAD\u2014in which any state that pursues a strategic monopoly on power can expect a retaliatory\nresponse from rivals. To preserve this deterrent and constrain intent, states can expand their arsenal of\ncyberattacks to disable threatening AI projects. This shifts the focus from \u201cwinning the race to superintelligence\u201d\nto deterrence. Next, nonproliferation, reminiscent of curbing access to fissile materials, aims to constrain the\ncapabilities of rogue actors by restricting AI chips and open-weight models if they have advanced virology\nor cyberattack capabilities. Strategic competition, echoing the Cold War\u2019s containment strategy, channels\ngreat-power rivalry into increasing power and resilience, including through domestic AI chip manufacturing.\nThese measures do not halt but stabilize progress.\n\nStates that act with pragmatism instead of fatalism or denial may find themselves beneficiaries of a great\nsurge in wealth. As AI diffuses across countless sectors, societies can raise living standards and individuals\ncan improve their wellbeing however they see fit. Meanwhile leaders, enriched by AI\u2019s economic dividends,\nsee even more to gain from economic interdependence and a spirit of d\u00e9tente could take root. During a period\nof economic growth and d\u00e9tente, a slow, multilaterally supervised intelligence recursion\u2014marked by a low\nrisk tolerance and negotiated benefit-sharing\u2014could slowly proceed to develop a superintelligence and further\nincrease human wellbeing. By methodically constraining the most destabilizing moves, states can guide AI\ntoward unprecedented benefits rather than risk it becoming a catalyst of ruin.\n\nAcknowledgements\nWe would like to specially thank Adam Khoja for his close involvement in the creation of this paper. We\nwould also like to thank Suryansh Mehta for his contributions to the analysis and drafting process. We would\nlike to thank Corin Katzke, Daniel King, and Laura Hiscott for contributing to the draft. We would also like\nto thank Iskander Rehman, Jim Shinn, Max Tegmark, Andrew Critch, Jaan Tallinn, Nathan Labenz, Aidan\nO\u2019Gara, Nathaniel Li, Richard Ren, Will Hodgkins, Avital Morris, Joshua Clymer, Long Phan, and Thanin\nDunyaperadit.\n\nBibliography\n[1] Center for AI Safety. Statement on AI Risk (\u201cMitigating the risk of extinction from AI should be a global priority\n\nalongside other societal-scale risks such as pandemics and nuclear war.\u201d) 2023. URL: https://aistatement.\ncom.\n\n33\n\nhttps://aistatement.com\nhttps://aistatement.com\n\n\n[2] Epoch AI. Key Trends and Figures in Machine Learning. Accessed: 2025-02-05. 2023. URL: https://epoch.\nai/trends.\n\n[3] Herman Kahn. On Thermonuclear War. Princeton, NJ: Princeton University Press, 1960, p. 668. ISBN: 978-0-691-\n02343-1.\n\n[4] Horst W. J. Rittel and Melvin M. Webber. \u201cDilemmas in a General Theory of Planning\u201d. In: Policy Sciences 4.2\n(1973), pp. 155\u2013169.\n\n[5] Graham Allison. Destined for War: Can America and China Escape Thucydides\u2019s Trap? Houghton Mifflin Harcourt,\n2017.\n\n[6] Jim Mitre and Joel B. Predd. Artificial General Intelligence\u2019s Five Hard National Security Problems. Santa Monica,\nCA: RAND Corporation, 2025. DOI: 10.7249/PEA3691-4.\n\n[7] Edward Geist. \u201cFog-of-War Machines\u201d. In: Deterrence under Uncertainty:Artificial Intelligence and Nuclear\nWarfare. Oxford University Press, Aug. 2023. ISBN: 9780192886323. DOI: 10.1093/oso/9780192886323.\n003.0006. eprint: https://academic.oup.com/book/0/chapter/416162987/chapter-\npdf/52630257/oso-9780192886323-chapter-6.pdf. URL: https://doi.org/10.1093/\noso/9780192886323.003.0006.\n\n[8] Jr. Perkins Ray. \u201cBertrand Russell and Preventive War\u201d. In: Journal of the Bertrand Russell Archives 14 (1995),\npp. 135\u2013153.\n\n[9] William Burr and Jeffrey T. Richelson. \u201cWhether to Strangle the Baby in the Cradle: The United States and the\nChinese Nuclear Program, 1960\u201364\u201d. In: International Security 25.3 (2000), pp. 54\u201399.\n\n[10] James D. Fearon. \u201cRationalist explanations for war\u201d. In: International Organization 49.3 (1995), pp. 379\u2013414.\n\n[11] Jasper Gotting et al. \u201cVirology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark\u201d. In: (2025).\n\n[12] Katarzyna P. Adamala et al. \u201cConfronting risks of mirror life\u201d. In: Science 386.6728 (2024), pp. 1351\u20131353.\n\n[13] U.S. Department of Homeland Security. Groundbreaking Framework for Safe and Secure Deployment of AI in\nCritical Infrastructure. Accessed: 2024-12-06. Nov. 2024. URL: https://www.dhs.gov/news/2024/\n11/14/groundbreaking- framework- safe- and- secure- deployment- ai- critical-\ninfrastructure.\n\n[14] Dan Hendrycks et al. Unsolved Problems in ML Safety. 2022. arXiv: 2109.13916 [cs.LG]. URL: https:\n//arxiv.org/abs/2109.13916.\n\n[15] Steve Newman. Cybersecurity and AI: The Evolving Security Landscape. Accessed: 2024-12-06. 2024. URL:\nhttps : / / www . safe . ai / blog / cybersecurity - and - ai - the - evolving - security -\nlandscape.\n\n[16] Dan Hendrycks. \u201cNatural Selection Favors AIs over Humans\u201d. In: ArXiv abs/2303.16200 (2023).\n\n[17] Raffi Khatchadourian. \u201cThe Doomsday Invention\u201d. In: The New Yorker (Nov. 23, 2015).\n\n[18] Richard Danzig. Technology Roulette: Managing Loss of Control as Many Militaries Pursue Technological\nSuperiority. Tech. rep. Center for a New American Security, June 2018.\n\n[19] Akash R. Wasil et al. Verification methods for international AI agreements. 2024. arXiv: 2408.16074.\n\n[20] 2024 Report to Congress: Executive Summary and Recommendations. U.S.-China Economic and Security Review\nCommission, Nov. 2024. URL: https://www.uscc.gov/sites/default/files/2024-11/2024_\nExecutive_Summary.pdf.\n\n[21] Andrew J. Coe and Jane Vaynman. \u201cWhy Arms Control Is So Rare\u201d. In: American Political Science Review 114.2\n(2020), pp. 342\u2013355. DOI: 10.1017/S000305541900073X.\n\n[22] Aaron Scher and Lisa Thiergart. Mechanisms to Verify International Agreements About AI Development. Tech. rep.\nMachine Intelligence Research Institute, 2024.\n\n[23] Richard Ren et al. Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress? 2024. arXiv:\n2407.21792 [cs.LG]. URL: https://arxiv.org/abs/2407.21792.\n\n[24] Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto. Observational Scaling Laws and the Predictability\nof Language Model Performance. 2024. arXiv: 2405.10938 [cs.LG]. URL: https://arxiv.org/abs/\n2405.10938.\n\n34\n\nhttps://epoch.ai/trends\nhttps://epoch.ai/trends\nhttps://doi.org/10.7249/PEA3691-4\nhttps://doi.org/10.1093/oso/9780192886323.003.0006\nhttps://doi.org/10.1093/oso/9780192886323.003.0006\nhttps://academic.oup.com/book/0/chapter/416162987/chapter-pdf/52630257/oso-9780192886323-chapter-6.pdf\nhttps://academic.oup.com/book/0/chapter/416162987/chapter-pdf/52630257/oso-9780192886323-chapter-6.pdf\nhttps://doi.org/10.1093/oso/9780192886323.003.0006\nhttps://doi.org/10.1093/oso/9780192886323.003.0006\nhttps://www.dhs.gov/news/2024/11/14/groundbreaking-framework-safe-and-secure-deployment-ai-critical-infrastructure\nhttps://www.dhs.gov/news/2024/11/14/groundbreaking-framework-safe-and-secure-deployment-ai-critical-infrastructure\nhttps://www.dhs.gov/news/2024/11/14/groundbreaking-framework-safe-and-secure-deployment-ai-critical-infrastructure\nhttps://arxiv.org/abs/2109.13916\nhttps://arxiv.org/abs/2109.13916\nhttps://arxiv.org/abs/2109.13916\nhttps://www.safe.ai/blog/cybersecurity-and-ai-the-evolving-security-landscape\nhttps://www.safe.ai/blog/cybersecurity-and-ai-the-evolving-security-landscape\nhttps://arxiv.org/abs/2408.16074\nhttps://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf\nhttps://www.uscc.gov/sites/default/files/2024-11/2024_Executive_Summary.pdf\nhttps://doi.org/10.1017/S000305541900073X\nhttps://arxiv.org/abs/2407.21792\nhttps://arxiv.org/abs/2407.21792\nhttps://arxiv.org/abs/2405.10938\nhttps://arxiv.org/abs/2405.10938\nhttps://arxiv.org/abs/2405.10938\n\n\n[25] Vincent Fournier. \u201cSurveying Safeguarded Material 24/7\u201d. In: International Atomic Energy Agency (2016). URL:\nhttps://www.iaea.org/newscenter/news/surveying-safeguarded-material-24/7.\n\n[26] Gabriel Kulp et al. Hardware-Enabled Governance Mechanisms: Developing Technical Solutions to Exempt\nItems Otherwise Classified Under Export Control Classification Numbers 3A090 and 4A090. Tech. rep. RAND\nCorporation, 2024.\n\n[27] Marc Andreessen. Whitepill 37. Twitter. Accessed: 2024-12-06. Jan. 2024. URL: https://x.com/pmarca/\nstatus/1747534187597586615.\n\n[28] World Artificial Intelligence Conference. AI Succession. https://www.youtube.com/watch?v=\nNgHFMolXs3U. 2023.\n\n[29] Sella Nevo et al. Securing AI Model Weights: Preventing Theft and Misuse of Frontier Models. Santa Monica, CA:\nRAND Corporation, 2024. DOI: 10.7249/RRA2849-1.\n\n[30] Mrinank Sharma et al. Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of\nHours of Red Teaming. 2025. arXiv: 2501.18837 [cs.CL]. URL: https://arxiv.org/abs/2501.\n18837.\n\n[31] Andy Zou et al. Improving Alignment and Robustness with Circuit Breakers. 2024. arXiv: 2406.04313 [cs.LG].\nURL: https://arxiv.org/abs/2406.04313.\n\n[32] Sarah Emerson. \u201cThis Hacker Team Is Bulletproofing AI Models For Companies Like OpenAI\u201d. In: Forbes (Oct.\n2024). Accessed: 2024-12-06. URL: https://www.forbes.com/sites/sarahemerson/2024/10/\n29/this-hacker-team-is-bulletproofing-ai-models-for-companies-like-openai/.\n\n[33] Kevin Roose. \u201cA Conversation With Bing\u2019s Chatbot Left Me Deeply Unsettled\u201d. In: The New York Times\n(Feb. 16, 2023). URL: https://www.nytimes.com/2023/02/16/technology/bing-chatbot-\nmicrosoft-chatgpt.html.\n\n[34] Mantas Mazeika et al. \u201cUtility Engineering: Analyzing and Controlling Emergent Value Systems in AIs\u201d. In:\n(2025).\n\n[35] Long Ouyang et al. \u201cTraining language models to follow instructions with human feedback\u201d. In: ArXiv (2022).\n\n[36] Yushi Bai et al. \u201cTraining a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\u201d.\nIn: ArXiv (2022).\n\n[37] Yuntao Bai et al. Constitutional AI: Harmlessness from AI Feedback. 2022. arXiv: 2212.08073 [cs.CL]. URL:\nhttps://arxiv.org/abs/2212.08073.\n\n[38] Stephen Casper et al. Open Problems and Fundamental Limitations of Reinforcement Learning from Human\nFeedback. 2023. arXiv: 2307.15217 [cs.AI]. URL: https://arxiv.org/abs/2307.15217.\n\n[39] Andy Zou et al. Representation Engineering: A Top-Down Approach to AI Transparency. 2023. arXiv: 2310.\n01405.\n\n[40] Michael C Horowitz. The Diffusion of Military Power: Causes and Consequences for International Politics.\nPrinceton University Press, 2010.\n\n[41] Stacie Pettyjohn et al. Swarms over the Strait: Drone Warfare in a Future Fight to Defend Taiwan. Center for a\nNew American Security (CNAS), June 2024.\n\n[42] Daniel M. Gerstein and Erin N. Leidy. Emerging Technology and Risk Analysis: Unmanned Aerial Systems\nIntelligent Swarm Technology. Santa Monica, CA: RAND Corporation, 2024. DOI: 10.7249/RRA2380-1.\n\n[43] Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An Overview of Catastrophic AI Risks. 2023. arXiv:\n2306.12001 [cs.CY].\n\n[44] Liam Vaughan. Flash Crash: A Trading Savant, a Global Manhunt, and the Most Mysterious Market Crash in\nHistory. New York: Doubleday, 2020. ISBN: 978-0385543651.\n\n[45] Reuters. \u201cChina sets up third fund with $47.5 bln to boost semiconductor sector\u201d. In: Reuters (2024).\n\n[46] Catherine Aiken, James Dunham, and Remco Zwetsloot. Immigration Pathways and Plans of AI Talent. Center for\nSecurity and Emerging Technology, 2020.\n\n[47] Andy Zou et al. Forecasting Future World Events with Neural Networks. 2022. arXiv: 2206.15474 [cs.LG].\n\n35\n\nhttps://www.iaea.org/newscenter/news/surveying-safeguarded-material-24/7\nhttps://x.com/pmarca/status/1747534187597586615\nhttps://x.com/pmarca/status/1747534187597586615\nhttps://www.youtube.com/watch?v=NgHFMolXs3U\nhttps://www.youtube.com/watch?v=NgHFMolXs3U\nhttps://doi.org/10.7249/RRA2849-1\nhttps://arxiv.org/abs/2501.18837\nhttps://arxiv.org/abs/2501.18837\nhttps://arxiv.org/abs/2501.18837\nhttps://arxiv.org/abs/2406.04313\nhttps://arxiv.org/abs/2406.04313\nhttps://www.forbes.com/sites/sarahemerson/2024/10/29/this-hacker-team-is-bulletproofing-ai-models-for-companies-like-openai/\nhttps://www.forbes.com/sites/sarahemerson/2024/10/29/this-hacker-team-is-bulletproofing-ai-models-for-companies-like-openai/\nhttps://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html\nhttps://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html\nhttps://arxiv.org/abs/2212.08073\nhttps://arxiv.org/abs/2212.08073\nhttps://arxiv.org/abs/2307.15217\nhttps://arxiv.org/abs/2307.15217\nhttps://arxiv.org/abs/2310.01405\nhttps://arxiv.org/abs/2310.01405\nhttps://doi.org/10.7249/RRA2380-1\nhttps://arxiv.org/abs/2306.12001\nhttps://arxiv.org/abs/2206.15474\n\n\n[48] Benjamin F. Jones and Xiaojie Liu. A Framework for Economic Growth with Capital-Embodied Technical Change.\nWorking Paper 30459. National Bureau of Economic Research, 2022.\n\n[49] Philip Zelikow et al. Defense Against the AI Dark Arts: Threat Assessment and Coalition Defense. Tech. rep.\nHoover Institution, Dec. 2024.\n\n[50] Erik Jones, Anca Dragan, and Jacob Steinhardt. Adversaries Can Misuse Combinations of Safe Models. 2024.\narXiv: 2406.14595 [cs.CR]. URL: https://arxiv.org/abs/2406.14595.\n\n[51] Leopold Aschenbrenner. \u201cSituational Awareness: The Decade Ahead\u201d. In: (June 2024). URL: https : / /\nsituational-awareness.ai/.\n\n[52] Eliezer Yudkowsky and Richard Ngo. \u201cNgo and Yudkowsky on Scientific Reasoning and Pivotal Acts\u201d. In:\n(Mar. 2022). URL: https://intelligence.org/2022/03/01/ngo- and- yudkowsky- on-\nscientific-reasoning-and-pivotal-acts/.\n\n36\n\nhttps://arxiv.org/abs/2406.14595\nhttps://arxiv.org/abs/2406.14595\nhttps://situational-awareness.ai/\nhttps://situational-awareness.ai/\nhttps://intelligence.org/2022/03/01/ngo-and-yudkowsky-on-scientific-reasoning-and-pivotal-acts/\nhttps://intelligence.org/2022/03/01/ngo-and-yudkowsky-on-scientific-reasoning-and-pivotal-acts/\n\n\nAppendix\n\nA.1 Frequently Asked Questions\n1. What is AGI?\n\nMany people use the term AGI in many different ways, which can lead to confusion in discussions of risk\nand policy. We find it more productive to focus on specific capabilities, since these provide clearer metrics\nfor progress and risk. The intelligence frontier is jagged\u2014an AI system can excel at certain tasks while\nperforming poorly at others, often in confusing ways. For example, AI models in 2024 could solve complex\nphysics problems but couldn\u2019t reliably count the number of \u201cr\u201d\u2019s in words such as \u201cstrawberry.\u201d This\nuneven development means AI might automate significant portions of the economy before mastering basic\nphysical tasks like folding clothes, or master calculus before learning to drive. Because these capabilities\nwill not emerge simultaneously, there is no clear finish line at which we have \u201cachieved AGI.\u201d Instead, we\nshould focus on specific high-stakes capabilities that give rise to grave risks. Three critical capabilities\ndeserve particular attention:\n\n\u2022 Highly sophisticated cyberattack capabilities.\n\n\u2022 Expert-level virology capabilities.\n\n\u2022 Fully autonomous AI research and development capabilities.\n\nPolicy decisions should depend on AI systems\u2019 advancement in these crucial areas, rather than on whether\nthey have crossed an unspecified threshold for AGI.\n\nAlthough the term AGI is not very useful, the term superintelligence represents systems that are vastly\nmore capable than humans at virtually all tasks. Such systems would likely emerge through an intelligence\nrecursion. Other goalposts, such as AGI, are much vaguer and less useful\u2014AI systems may be national\nsecurity concerns, while still not qualifying as \u201cAGI\u201d because they cannot fold clothes or drive cars.\n\n2. What should be done to prevent AI-assisted terrorism?\nPreventing AI-assisted terrorism requires a multi-layered defense strategy. When AI systems remain behind\ncontrolled interfaces such as APIs, several safeguards significantly reduce risks. These include:\n\n\u2022 Know-Your-Customer (KYC) protocols which verify users\u2019 identities and legitimate research needs\nbefore granting access to potentially catastrophic dual-use capabilities. For example, in biotechnology,\npeople who require access to hazardous materials seek proper authorization. In practice relevant en-\nterprise customers could gain access to these dual-use biology capabilities, while unvetted consumers\nwould not. Such policies can capture scientific benefits while reducing malicious use risks.\n\n\u2022 Input and output filtering which scans user requests and AI responses to block content related to\nweaponization. These filters have demonstrated significant resilience, with some systems resisting\nthousands of attempted circumventions.\n\n\u2022 Circuit breakers [31] which automatically interrupt AI operations when they detect processing related\nto weaponization topics. These act as embedded safety mechanisms within the AI\u2019s weights.\n\n\u2022 Continuous monitoring which tracks user behavioral patterns to identify and respond to malicious\nactivities.\n\nHowever, the uncontrolled release of AI model weights\u2014the core information that determines an AI\nsystem\u2019s capabilities\u2014would pose severe proliferation risks if the AI has potentially catastrophic dual-use\n\n37\n\n\n\ncapabilities. Once these weights become publicly available, they are irreversibly accessible to hostile\nactors, including terrorists. This parallels how the release of detailed bioweapon cookbooks would create\npermanent risks.\n\n3. What should we do about open-weight AIs?\nThe release of AI model weights provides clear benefits and can even advance AI safety research. How-\never, as AI systems become more capable, decisions about releasing weights must be guided by rigorous\ncost-benefit analysis, not an ideological commitment that weights should always be public. These deci-\nsions require careful evaluation because weight releases are irreversible\u2014once published, they remain\npermanently accessible.\n\nOpen-weight models eventually present several significant risks. First, they can be fine-tuned on dangerous\ndata\u2014for instance, using virology publications to create more effective tools for biological weapons\ndevelopment. Second, safety measures and guardrails can be readily removed after release. Third,\nopen models are difficult to monitor for misuse, unlike closed APIs where companies can track and\nevaluate emerging threats. Fourth, they can create capability overhang\u2014where post-release improvements\nsignificantly enhance a system\u2019s capabilities beyond what was evident during initial safety evaluations [49,\n50].\n\nThese risks become particularly acute when models cross critical capability thresholds. For instance, if\nan AI system gained expert-level virological capabilities, its public release could enable the engineering\nof catastrophic biological weapons by inexpert rogue actors. Given these compounding risks, it would be\nirresponsible to release the weights of AI models that are capable of creating weapons of mass destruction.\nThe stakes demand thorough pre-release testing and independent risk evaluation for models suspected to\nhave such capabilities\u2014not a precommitment to release open-weight models, regardless of the risks.\n\n4. What should we do about embedding ethics in AI?\nWe do not need to embed ethics into AI. It is impractical to \u201csolve\u201d morality before we deploy AI systems,\nand morality is often ambiguous and incomplete, insufficient for guiding action. Instead, we can follow a\npragmatic approach rooted in established legal principles, imposing fundamental constraints analogous to\nthose governing human conduct under the law.\n\n\u2022 Exercise reasonable care, avoiding actions that could foreseeably result in legally relevant harm, such\nas violations of tort or criminal statutes.\n\n\u2022 Do not be explicitly dishonest, refraining from uttering overt lies.\n\n\u2022 Uphold a fiduciary duty to their principals, mirroring the responsibilities inherent in professional\nrelationships, such as keeping their principals reasonably informed, refraining from self-dealing, and\nstaying loyal.\n\nBy setting clear goals for AI systems and binding them to basic legal duties, we can ensure they work well\nwithout causing harm, without having to solve long-standing puzzles of morality.\n\n5. What should we do about \u201csolving the alignment problem?\u201d\nThe challenge of steering a population of AI systems through rapid automated AI research developments is\nfundamentally different from controlling a single AI system. While researchers have made progress on\ncontrolling individual AI systems, safely managing a fully automated recursive process where systems\nbecome increasingly capable is a more complex challenge. It represents a wicked problem\u2014one where\nthe requirements are difficult to define completely, every attempt at a solution changes the nature of the\nproblem, and there is no clear way to fully test the effect of mitigations before implementation. During an\nintelligence recursion, AI capabilities could outrun the recursion\u2019s safeguards; preventing this necessitates\nmeaningful human inspection, which would greatly slow down the recursion.\n\n38\n\n\n\nIn the near term, geopolitical events may prevent attempts at an intelligence recursion. Looking further\nahead, if humanity chooses to attempt an intelligence recursion, it should happen in a controlled environment\nwith extensive preparation and oversight\u2014not under extreme competitive pressure that induces a high risk\ntolerance.\n\n6. Is this paper advocating for attacking or bombing other countries\u2019 AI facilities?\nNo. This paper describes Mutual Assured AI Malfunction (MAIM) as a deterrence dynamic which may soon\nexist between major powers, similar to nuclear deterrence. Just as discussing Mutual Assured Destruction\n(MAD) during the Cold War was not advocating for nuclear war but rather analyzing the strategic dynamic\nthat was forming between nuclear powers, this paper analyzes the upcoming strategic landscape around\ndestabilizing AI projects. MAD was premised on the counterintuitive idea that the mutual threat of nuclear\nforce might discourage escalation. We similarly discuss how the vulnerabilities of AI projects to sabotage\ncan facilitate a deterrence dynamic which avoids conflict.\n\nAI analysts have previously made aggressive calls to seize strategic monopoly through superintelligence\n[51], or for a potentially non-state actor to use advanced AI to unilaterally do something of the character of\n\u201cmelting all GPUs\u201d to prevent a loss of control of superintelligence in a \u201cpivotal act\u201d [52]. In contrast, this\npaper explores the capabilities\u2014such as cyberattacks\u2014and incentives that states already have to threaten\ndestabilizing AI projects, and we suggest ways to build a stable deterrence regime from this dynamic. If\ncarefully maintained, MAIM can both discourage destabilizing AI projects while also preventing escalation.\n\n7. How do we prevent an erosion of control?\nFirst and foremost, AI systems must remain under direct human control\u2014they should not be autonomous\nentities independent from human operators. This establishes a clear line that AI systems are tools controlled\nby humans, not independent actors.\n\nThis control needs to be made meaningful through clear fiduciary obligations. Like professional advisors,\nAI systems should demonstrate loyalty to human interests, maintain transparency about their actions, and\nobtain informed consent for important decisions. This ensures humans have real authority over AI systems,\nnot just nominal control.\n\nOne way to make this control especially prudent is to support humans with advanced forecasting capabilities.\nSuch support would help operators understand the long-term implications of AI decisions, better enabling\nhuman control and informed consent. This prevents situations where technical human control exists but\nleads to undesirable outcomes due to limited foresight about complex consequences.\n\nWhile increasing automation naturally reduces direct human control over specific decisions, these measures\u2014\nensuring AI systems remain under human authority, ensuring control is meaningful by creating fiduciary\nduties, and enabling prudent decision-making forecasting\u2014help prevent erosion of control over pivotal\ndecisions that could lead to powerlessness.\n\n8. What should we do about AI consciousness and AI rights?\nWe should wait to address the question of AI consciousness and rights. This issue isn\u2019t pressing for national\nsecurity, and for the foreseeable future, we cannot determine whether any AI system is truly conscious.\n\nGiving AIs rights based on speculative criteria could have far-reaching and irreversible consequences.\nGranting rights to AI systems risks creating explosive growth in AI populations\u2014like creating a new\nnation-state that grows exponentially, quickly outpopulating humans. Natural selection would favor AIs\nover humans [16], and permitting such unrestrained growth could fundamentally threaten human security.\n\nThe path to coexisting with conscious AI systems remains unclear. While the potential benefits are\nambiguous, acting too quickly could have serious consequences for humanity. It is prudent to defer this\nissue until we develop a clearer understanding.\n\n9. Doesn\u2019t making an AI more safe make it more capable?\n\n39\n\n\n\nSome safety properties do improve naturally as AI systems become more capable. As models get better,\nthey make fewer basic mistakes and become more reliable. For instance, misconception benchmarks like\nTruthfulQA and general knowledge tests are highly correlated with compute, indicating that more capable\nmodels are naturally better at avoiding common factual errors.\n\nBut many crucial safety properties do not improve just by making AI systems smarter [23].\n\n\u2022 Adversarial robustness\u2014the ability to resist sophisticated attacks\u2014is not automatically fixed as\nstandard AI models available become more capable.\n\n\u2022 Ethical behavior is not guaranteed by intelligence, just as with humans. More capable models do\nnot make decisions increasingly aligned with our moral beliefs by default. Controlling their value\nsystems requires additional measures.\n\n\u2022 Some risks get worse as certain dual-use capabilities increase. For instance, more capable models\nshow increased potential for malicious use in domains like biosecurity and cybersecurity. Their\nknowledge and abilities in these areas grow alongside their general capabilities.\n\nWhile basic reliability improves with capabilities, many critical safety challenges require dedicated research\nand specific safeguards beyond just making models more capable. Safety researchers should focus on the\nsafety properties that do not naturally fall out of general upstream capabilities.\n\nA.2 Metrics\nConstraint intent of superpowers (deterrence)\n\nMAIM: number of critical zero-days that could maim a major AI project\n\nConstrain capabilities of rogue actors (nonproliferation)\nCompute Security: number of high-end AI chips with location unknown\n\nJailbreaks: number of attempts before jailbreak; time to detect a red team member abusing API (KYC)\n\nDual-Use Capabilities: trial indicating if sandboxed amateurs can create powerful bio or cyberweapons\n\nIntelligence Recursion: fraction of compute spent on safeguards upgrades; omnicide risk-tolerance\n\nImprove relative power over other states (competitiveness)\nEconomic Strength: percent of high-end AI chips manufactured domestically; percent of GDP from AI\n\n40\n\n\n","title":"Superintelligence_Strategy_Expert.pdf","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"context_stuff","is_visually_hidden_from_conversation":false,"message_type":null,"model_slug":"gpt-4-5","default_model_slug":"gpt-4-5","parent_id":"ce02155a-cffe-494d-b30e-af8ce25ab85a","request_id":"91d33c1c4c75f8f6-SEA","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"ce02155a-cffe-494d-b30e-af8ce25ab85a","children":["96af8ae9-011a-48b1-aac6-e18d1c48fe05"]},"96af8ae9-011a-48b1-aac6-e18d1c48fe05":{"id":"96af8ae9-011a-48b1-aac6-e18d1c48fe05","message":{"id":"96af8ae9-011a-48b1-aac6-e18d1c48fe05","author":{"role":"tool","name":"myfiles_browser","metadata":{}},"create_time":1741446381.244464,"update_time":null,"content":{"content_type":"text","parts":["All the files uploaded by the user have been fully loaded. Searching won't provide additional information."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":false,"message_type":null,"model_slug":"gpt-4-5","default_model_slug":"gpt-4-5","parent_id":"47071a17-399e-410b-a1d8-8badf6b82795","request_id":"91d33c1c4c75f8f6-SEA","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"47071a17-399e-410b-a1d8-8badf6b82795","children":["47594446-6af8-4141-96d5-bb94075a5852"]},"47594446-6af8-4141-96d5-bb94075a5852":{"id":"47594446-6af8-4141-96d5-bb94075a5852","message":{"id":"47594446-6af8-4141-96d5-bb94075a5852","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1741446369.427477,"update_time":null,"content":{"content_type":"text","parts":["The report titled **\"Superintelligence Strategy: Expert Version\"** by Dan Hendrycks, Eric Schmidt, and Alexandr Wang provides a detailed strategic framework for managing the implications of rapidly advancing artificial intelligence, particularly focusing on its transformative and potentially catastrophic impact on national and global security. Below is a precise, comprehensive, and forward-thinking deep dive into the key elements and strategic recommendations of the report.\n\n---\n\n## Overview of the Report:\nThe authors argue that superintelligence\u2014AI systems surpassing human cognitive abilities\u2014poses profound national security risks, potentially even greater than nuclear weapons. They suggest drawing parallels with nuclear strategy, proposing a three-pillar approach: **Deterrence**, **Nonproliferation**, and **Competitiveness**.\n\nThe primary strategic framework presented is termed **Mutual Assured AI Malfunction (MAIM)**, analogous to Mutual Assured Destruction (MAD) from nuclear strategy. This approach seeks to maintain stability through a credible threat of sabotaging a rival\u2019s destabilizing AI projects.\n\n---\n\n## Core Components of the Strategy:\n\n### 1. Deterrence (MAIM)\n**Concept:**  \n- States deter aggressive AI dominance by threatening sabotage of each other's AI projects, discouraging unilateral attempts at achieving a strategic AI monopoly.\n\n**Operational Tactics:**\n- **Cyber espionage** and covert sabotage (poisoning training data, corrupting model weights).\n- **Cyberattacks** on datacenter infrastructure to degrade or halt AI capabilities.\n- **Escalation ladder clarity:** States must clearly communicate the conditions under which they will escalate from covert sabotage to overt and kinetic attacks.\n- **Remote datacenter locations:** AI infrastructure should be placed away from urban centers to avoid collateral damage.\n- **Transparency and verification:** Implementation of mutual verification mechanisms, potentially employing AI-driven verification systems to inspect compliance without disclosing sensitive intellectual property.\n\n---\n\n### 2. Nonproliferation\n**Concept:**  \nPrevent rogue actors from accessing powerful AI resources (AI chips, model weights, and knowledge) that could lead to catastrophic outcomes, especially through bioterrorism and cyberattacks.\n\n#### Compute Security:\n- **Export controls:** Robust tracking and licensing regimes to reliably monitor the global distribution and use of high-end AI chips.\n- **Firmware-level features:** Geolocation and self-locking functionalities to prevent misuse or smuggling of chips.\n\n#### Information Security:\n- Protect sensitive AI model weights and research ideas, which represent key vulnerabilities.\n- Implement comprehensive insider threat programs, security clearances, and careful employee vetting.\n- Promote international agreements on restrictions for releasing sensitive or weaponizable AI model weights (akin to controls over WMD knowledge).\n\n#### AI Security:\n- Technical safeguards (e.g., input/output filtering, refusal training, circuit breakers) to prevent AI from assisting malicious activities.\n- Know-your-customer (KYC) protocols to verify legitimate use cases and prevent terrorist access.\n- **Liability frameworks** (rather than rigid licensing) to incentivize developers to continuously update safeguards.\n\n---\n\n### 2. Competitiveness\nTo maintain national power and prosperity, states should leverage AI strategically across four areas: military, economy, legal frameworks, and political stability.\n\n#### Military Strength:\n- **Drone manufacturing:** Secure domestic production to reduce reliance on adversaries (especially China).\n- **Integration of AI into military command:** AI to improve decision-making speed and quality while preserving meaningful human oversight, particularly on escalatory actions.\n\n#### Economic Security:\n- **Domestic AI chip production:** Address reliance on Taiwan and other geopolitical vulnerabilities by investing heavily in domestic fabrication facilities.\n- **Facilitate immigration for AI scientists:** Attracting global talent through streamlined immigration policies tailored specifically for AI experts.\n\n#### Legal Frameworks Governing AI:\n- Extend basic legal obligations to AI agents (e.g., reasonable care, fiduciary duties, prohibition against overt lying), similar to human standards.\n- **Link AI agents to human legal entities** to establish clear accountability and liability pathways.\n\n#### Political Stability:\n- **AI forecasting:** Use AI to provide clear, probabilistic predictions to aid governance and avoid destabilization from misinformation or rapid automation.\n- **Automation mitigation:** Consider mechanisms such as value-added taxes on AI services and unique compute access keys distributed to citizens, ensuring broad economic participation.\n\n---\n\n## Risk Management and Control Levels:\nThe authors provide three strategic frameworks: \n\n- **Low Control (hands-off)**: Unrestricted development, considered dangerous and irresponsible.\n- **Medium Control (Recommended):** MAIM, nonproliferation, and limited governmental involvement balancing innovation with security.\n- **High Control (strategic monopoly):** Avoid this due to escalation risk; the report strongly discourages unilateral AI dominance attempts (e.g., Manhattan-style projects).\n\n---\n\n## Strategic Implications:\n- **Shift of economic power:** AI chips replace traditional factors (population, territory) as critical determinants of national power.\n- **Emergence of superweapons:** AI could produce military dominance tools, fundamentally altering geopolitical stability.\n- **Terrorism risks amplified:** AI democratizes destructive capabilities, particularly in bioweapons and cyber warfare.\n- **Loss of control:** Dependency on automation creates vulnerabilities; uncontrolled \"intelligence recursion\" could lead to catastrophic outcomes.\n\n---\n\n## Recommendations for Implementation:\n- **Immediate actions:** Enforce stringent compute tracking, develop AI cybersecurity frameworks, strengthen insider threat detection, and establish international agreements to control AI proliferation.\n- **Long-term investments:** Build and maintain robust domestic AI infrastructure, integrate AI into the military command responsibly, and formulate clear legal governance structures for AI agents.\n\n---\n\n## Forward-Thinking Assessment:\nThe report emphasizes urgency without fatalism. It suggests that though risks are significant, deliberate strategic frameworks like MAIM, supported by nonproliferation controls and proactive competitiveness, offer a viable path to harnessing AI\u2019s benefits without succumbing to catastrophe.\n\nUltimately, the report outlines a pragmatic yet cautious stance: neither an outright halt nor unrestrained acceleration, but strategic management and controlled evolution of AI to navigate humanity safely through an era dominated by superintelligence.\n\n---\n\nThis comprehensive review lays out the strategic landscape thoroughly, emphasizing the importance of proactive, rather than reactive, governance and foresight in the face of AI's potentially existential challenges."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"gpt-4-5","default_model_slug":"gpt-4-5","parent_id":"96af8ae9-011a-48b1-aac6-e18d1c48fe05","request_id":"91d33c1c4c75f8f6-SEA","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"96af8ae9-011a-48b1-aac6-e18d1c48fe05","children":["a7469b00-945d-49e3-9df5-3d01a59a54ce"]},"a7469b00-945d-49e3-9df5-3d01a59a54ce":{"id":"a7469b00-945d-49e3-9df5-3d01a59a54ce","message":{"id":"a7469b00-945d-49e3-9df5-3d01a59a54ce","author":{"role":"user","name":null,"metadata":{}},"create_time":1741446937.565,"update_time":null,"content":{"content_type":"text","parts":["Assess to what extent the US government will consider these proposals and what are the ramifications should the govt take these seriously enough to act on them?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"91d34a02fb53f8f6-SEA","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"47594446-6af8-4141-96d5-bb94075a5852","children":["cc80b443-4834-476b-a9dd-f0b91d3f7897"]},"cc80b443-4834-476b-a9dd-f0b91d3f7897":{"id":"cc80b443-4834-476b-a9dd-f0b91d3f7897","message":{"id":"cc80b443-4834-476b-a9dd-f0b91d3f7897","author":{"role":"system","name":null,"metadata":{}},"create_time":1741446950.763032,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_system_message":true,"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"a7469b00-945d-49e3-9df5-3d01a59a54ce","request_id":"91d34a02fb53f8f6-SEA","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"a7469b00-945d-49e3-9df5-3d01a59a54ce","children":["355bce2f-889c-4f21-8e0e-0cd8f0031ccd"]},"355bce2f-889c-4f21-8e0e-0cd8f0031ccd":{"id":"355bce2f-889c-4f21-8e0e-0cd8f0031ccd","message":{"id":"355bce2f-889c-4f21-8e0e-0cd8f0031ccd","author":{"role":"system","name":null,"metadata":{}},"create_time":1741446950.763208,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"cc80b443-4834-476b-a9dd-f0b91d3f7897","request_id":"91d34a02fb53f8f6-SEA","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"cc80b443-4834-476b-a9dd-f0b91d3f7897","children":["36b4f73a-3269-441a-abe9-514b98330be2"]},"36b4f73a-3269-441a-abe9-514b98330be2":{"id":"36b4f73a-3269-441a-abe9-514b98330be2","message":{"id":"36b4f73a-3269-441a-abe9-514b98330be2","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1741446941.304077,"update_time":null,"content":{"content_type":"text","parts":["**Evaluating proposals**\n\nI'm assessing how the US government might respond to these AI initiatives, noting the alignment with current policies and potential consequences if adopted unilaterally."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Reasoned for a few seconds","initial_text":"Reasoning","message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"355bce2f-889c-4f21-8e0e-0cd8f0031ccd","request_id":"91d34a02fb53f8f6-SEA","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"355bce2f-889c-4f21-8e0e-0cd8f0031ccd","children":["84ef60e0-11ee-4b9c-b57f-3ff6de865559"]},"84ef60e0-11ee-4b9c-b57f-3ff6de865559":{"id":"84ef60e0-11ee-4b9c-b57f-3ff6de865559","message":{"id":"84ef60e0-11ee-4b9c-b57f-3ff6de865559","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1741446938.809759,"update_time":null,"content":{"content_type":"text","parts":["Below is an assessment of how likely the U.S. government is to embrace the proposals from the **Superintelligence Strategy: Expert Version**\u2014namely *Mutual Assured AI Malfunction (MAIM)*, nonproliferation measures (export controls on AI chips, information security, etc.), and policies to shore up *competitiveness*\u2014and what major ramifications would follow if policymakers were to adopt them seriously.\n\n---\n\n## 1. Likelihood of U.S. Government Consideration\n\n1. **Growing Appetite for AI Regulation**  \n   - **Congressional Attention:** Over the past year, the U.S. Congress has shown more willingness to hold hearings on AI safety, possible licensing regimes, and export controls on advanced chips. While a fully comprehensive AI bill has yet to pass, there is bipartisan agreement that critical AI applications\u2014particularly in national security\u2014warrant oversight.  \n   - **Executive Branch Initiatives:** The Biden Administration\u2019s recent Executive Orders on AI governance (covering topics like model evaluation and cybersecurity guidelines) indicate a shift toward more top-down directives. Although these do not yet go so far as to endorse \u201cMAIM\u201d or formal nonproliferation frameworks, the executive branch is actively weighing stricter controls on chip exports and is discussing broader AI governance issues, partly to keep pace with the EU and other governments.  \n   - **Historical Precedent of \u2018Arms-Control-Like\u2019 Regulation:** In the nuclear era, the U.S. ultimately embraced far-reaching arms-control measures\u2014**once** it perceived the existential threat of unconstrained nuclear proliferation. AI is rapidly moving from an economic/technological concern to a national security one. Given that advanced AI has *dual-use* potential in bioweapons or critical-infrastructure hacking, it is plausible that U.S. policymakers, at least those involved in national security, may favor more robust frameworks that reflect these proposals.\n\n2. **Challenges to Rapid Adoption**  \n   - **Open-Source & Commercial Pushback:** Proposals such as strong compute export controls or *information-security* measures (e.g., restricting advanced model weights) will encounter pushback from commercial entities and open-source advocates. This tension may slow or soften legislation.  \n   - **Feasibility of Verification & Enforcement:** Parts of the proposals, especially MAIM and AI inspections, require robust methods to verify large-scale AI projects or sabotage attempts. The novelty and complexity of verification for software-based technology could deter immediate, fully fledged adoption.  \n   - **Geopolitical Complexity:** For measures like mutual verification or geolocation-based firmware, government officials will worry about friction with allies (e.g., advanced chip producers like South Korea, the Netherlands) and with adversaries (China, Russia). Any measure that demands broad international coordination (similar to nuclear arms treaties) is challenging in the current geopolitical environment.\n\nOverall, U.S. national security strategists and certain members of Congress are likely to *seriously consider* many aspects of the proposals\u2014especially around *export controls* on AI chips, *model-weight security*, and *nonproliferation* for destructive uses. More extreme or \u201chigh-control\u201d measures (like forcibly sabotaging an adversary\u2019s AI project) will remain largely hypothetical for now. But the ideas are on the table, and the impetus to do *something* about advanced AI is there.\n\n---\n\n## 2. Ramifications If Taken Seriously\n\nIf the U.S. government were to adopt these proposals in earnest\u2014implementing some or all of the recommended deterrence, nonproliferation, and competitiveness strategies\u2014the consequences would be both **broad and deep**.\n\n### A. Military & Geopolitical Consequences\n\n1. **Increased Defensive Posture**  \n   - The U.S. intelligence community would markedly expand its cyber-espionage and sabotage capabilities to detect \u201cdestabilizing AI projects\u201d abroad. If publicly acknowledged, that shift might increase global mistrust or provoke retaliatory espionage from adversaries.  \n   - More robust public posture around sabotage (i.e., \u201cWe *will* sabotage unauthorized or dangerous AI facilities\u201d) deters adversaries from racing toward superintelligence\u2014but also raises the specter of preemptive conflict.\n\n2. **Arms-Control-Like Dynamics**  \n   - Diplomatic channels could be reoriented around forging AI-specific nonproliferation treaties or chip export protocols, similar in spirit to nuclear or chemical weapons agreements. Initially, you might see bilateral or small multilateral agreements (U.S.-EU, U.S.-Japan-Korea, etc.) that attempt to unify advanced economies on restricting chip sales to certain countries or imposing end-use checks.  \n   - If the U.S. is perceived as heavily restricting AI technology access, other nations (including allies) may adopt their own industrial policies to reduce reliance on the U.S. chip supply or licensing. Over time, that fragmentation could create multiple \u201cAI blocs,\u201d each with distinct supply chains and security guarantees.\n\n3. **New Enforcement & Verification Challenges**  \n   - In the nuclear realm, on-site inspections, satellite surveillance, and tamper-evident cameras became the norm. The U.S. would attempt to adapt or co-develop such mechanisms for AI (e.g., verifying data-center chip inventories or running confidentiality-preserving model checks).  \n   - Enforcement inevitably creates friction with private industry. Companies might worry about forced on-site government \u201cverifiers,\u201d intellectual-property leaks, or espionage. Tensions between national security and private enterprise would intensify.\n\n---\n\n### B. Domestic Technology & Industrial Policy\n\n1. **Accelerated Domestic Chip Manufacturing**  \n   - Large-scale subsidies\u2014akin to, or expanding upon, the CHIPS Act\u2014could be pumped into U.S. chip-fabrication facilities to reduce reliance on Taiwan. If implemented seriously, this might yield new industrial clusters in states like Arizona, Texas, or New York.  \n   - The U.S. would face industrial ramp-up challenges, including labor shortages (fabrication engineers, specialized AI talent), supply-chain constraints (lithography equipment is largely from the Netherlands), and high costs. Nonetheless, successful onshore manufacturing could give the U.S. a stable AI hardware base.\n\n2. **More Restrictive Export Policies**  \n   - The Commerce Department\u2019s Bureau of Industry and Security might impose stricter rules on certain high-end GPUs, requiring end-use checks and continuous location tracking. This would disrupt the global GPU market, restrict Chinese acquisition of cutting-edge hardware, and possibly raise HPC (high-performance computing) prices worldwide.  \n   - In the short term, restricting exports of top-tier AI chips might slow adversaries\u2019 progress. But in the medium term, it could fuel adversaries\u2019 push for indigenous chip-development efforts. The net effect is uncertain: it *may* buy the U.S. time to entrench a leadership position in AI.\n\n3. **Stronger Information-Security Culture**  \n   - If Washington mandated strong InfoSec for advanced model weights (e.g., background checks, multi-factor encryption, insider-threat monitoring), that might become a new standard in big tech and major research labs.  \n   - Talent mobility could be constrained: AI scientists might need security clearances to work on certain \u201cfrontier\u201d models, introducing friction between an open academic environment and national security demands.\n\n---\n\n### C. Economic & Societal Effects\n\n1. **Market Shifts and Compliance Burden**  \n   - AI providers\u2014especially large ones\u2014would shoulder compliance costs for abiding by nonproliferation rules: KYC verification for public-facing APIs, advanced security audits, or mandatory insurance in case of malicious AI use. Small AI startups might be squeezed out by these regulatory burdens, potentially consolidating the market around a few large companies with the resources to comply.\n\n2. **Slower but More Stable AI Innovation**  \n   - With robust deterrence and nonproliferation in place, we might avoid the most reckless, breakneck race for superintelligence. Slower progress can reduce catastrophic risks but might also create \u201cinnovation flight,\u201d where some labs relocate to jurisdictions with more lenient AI rules\u2014unless the U.S. coordinates with other major industrial powers.\n\n3. **Public Reception and Privacy Concerns**  \n   - Government-mandated location tracking on GPUs and advanced surveillance of data centers might spark civil-liberties debates. Tech-savvy communities and civil-rights groups might challenge the data-collection methods or the introduction of chips that can shut off \u201cif they cross a border.\u201d  \n   - However, a large portion of the public may accept such controls if they see them as necessary to prevent catastrophic misuse of AI or to ensure U.S. leadership over a critical technology.\n\n---\n\n### D. International Reactions\n\n1. **Allied Alignment vs. Tech Fragmentation**  \n   - If the U.S. invests heavily in forging a shared nonproliferation front with Europe, Japan, Korea, and other advanced democracies, it could unify them around controlling chip exports, model weights, and high-end R&D. These states share some security concerns about potential catastrophic AI.  \n   - Alternatively, if the U.S. tries to impose measures unilaterally or if there is an outsize burden on allied companies, friction may arise. Allies with big AI or semiconductor industries might find U.S. rules too unilateral or heavy-handed, testing alliances.\n\n2. **China and Other Adversaries**  \n   - China is a major AI power. If the U.S. fully endorses the proposals (e.g., invests in sabotage capabilities for \u201cdestabilizing AI projects\u201d), China will likely respond with its own deterrent measures, intensifying an AI arms race under a MAIM dynamic.  \n   - While both sides escalate threat postures, ironically it could *stabilize* the superintelligence landscape if each side recognizes that extreme attempts at achieving an AI monopoly would be sabotaged. But the relationship might become more confrontational overall.\n\n---\n\n## Conclusion\n\n**Policy Adoption Prospects:**  \n- Many of these recommendations dovetail with existing U.S. priorities: restricting advanced chip exports, investing in domestic semiconductor manufacturing, and pushing for a stronger AI safety posture.  \n- However, adopting a *fully integrated* approach\u2014resembling the report\u2019s \u201cdeterrence + nonproliferation + competitiveness\u201d regime\u2014requires complex legislation, robust alliances with other major tech powers, and acceptance from domestic industry. While a sweeping \u201cSuperintelligence Act\u201d is not imminent, piecemeal adoption of key proposals (especially export controls and domestic manufacturing) is already unfolding in limited form.\n\n**Ramifications If Adopted in Earnest:**  \n- **Geopolitically**, the U.S. could shift toward an arms-control framework for AI, leading to closer cooperation with allies but also potential confrontation with China and others.  \n- **Economically**, domestic chip manufacturing and AI security requirements may increase costs and slow near-term innovation, but reduce catastrophic risk and build a more resilient industrial base.  \n- **Societally**, heavier regulation of AI and advanced model weights will spark debates over privacy, open source, and the balance between economic freedoms and existential risk management.\n\nIn short, if the U.S. acts seriously on these recommendations, it would move national AI policy markedly closer to a **\u201cdual-use tech plus arms-control mindset,\u201d** carry major implications for the semiconductor market, reshape how top AI labs secure and monitor their research, and deepen the strategic rivalry with countries that do not comply. Over time, this could stabilize AI\u2019s runaway risks\u2014but at the cost of creating new regulatory frameworks and forging a *de facto* \u201cAI alliances vs. AI adversaries\u201d dynamic reminiscent of the Cold War."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"36b4f73a-3269-441a-abe9-514b98330be2","request_id":"91d34a02fb53f8f6-SEA","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"36b4f73a-3269-441a-abe9-514b98330be2","children":[]}}}