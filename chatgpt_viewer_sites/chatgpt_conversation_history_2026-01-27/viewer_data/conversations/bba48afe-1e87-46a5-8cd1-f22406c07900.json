{"id":"bba48afe-1e87-46a5-8cd1-f22406c07900","title":"Attention in Various ML Models","create_time":1716773707.053414,"update_time":1716773803.345138,"current_node":"781da699-fafa-4c89-967f-bd1a6a66701a","mapping":{"aaa1967c-6fd0-494b-b0f1-b4086681e9e4":{"id":"aaa1967c-6fd0-494b-b0f1-b4086681e9e4","message":null,"parent":null,"children":["2d5e253f-bf45-4b79-83b6-b62480282b51"]},"2d5e253f-bf45-4b79-83b6-b62480282b51":{"id":"2d5e253f-bf45-4b79-83b6-b62480282b51","message":{"id":"2d5e253f-bf45-4b79-83b6-b62480282b51","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"aaa1967c-6fd0-494b-b0f1-b4086681e9e4","children":["cc86ce6e-148a-4b17-a906-a7672c28c8be"]},"cc86ce6e-148a-4b17-a906-a7672c28c8be":{"id":"cc86ce6e-148a-4b17-a906-a7672c28c8be","message":{"id":"cc86ce6e-148a-4b17-a906-a7672c28c8be","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_user_system_message":true,"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"About Myself: I have a very high level of mathematical knowledge up to the post-graduate level. \n\nThese are my main areas of interest:\n(1) coding: mainly Python\n(2) mathematics up to graduate level and research level\n(3) machine learning and artificial intelligence"}},"recipient":"all","channel":null},"parent":"2d5e253f-bf45-4b79-83b6-b62480282b51","children":["aaa2f1dd-c8e3-4ddb-a26c-e2bcbc0acca3"]},"aaa2f1dd-c8e3-4ddb-a26c-e2bcbc0acca3":{"id":"aaa2f1dd-c8e3-4ddb-a26c-e2bcbc0acca3","message":{"id":"aaa2f1dd-c8e3-4ddb-a26c-e2bcbc0acca3","author":{"role":"user","name":null,"metadata":{}},"create_time":1716773707.059175,"update_time":null,"content":{"content_type":"text","parts":["For what types of machine learning models would it be possible to augment it with an attention mechanism similar to that in the transformer architecture?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"88a24431afc2496f-SIN","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"cc86ce6e-148a-4b17-a906-a7672c28c8be","children":["93a0c2a8-1e7c-4238-9433-8ca7aa01f46e"]},"93a0c2a8-1e7c-4238-9433-8ca7aa01f46e":{"id":"93a0c2a8-1e7c-4238-9433-8ca7aa01f46e","message":{"id":"93a0c2a8-1e7c-4238-9433-8ca7aa01f46e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716773719.784222,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"aaa2f1dd-c8e3-4ddb-a26c-e2bcbc0acca3","request_id":"88a24431afc2496f-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"aaa2f1dd-c8e3-4ddb-a26c-e2bcbc0acca3","children":["19ad67a7-87d2-437f-9f40-34d81b28bf1b"]},"19ad67a7-87d2-437f-9f40-34d81b28bf1b":{"id":"19ad67a7-87d2-437f-9f40-34d81b28bf1b","message":{"id":"19ad67a7-87d2-437f-9f40-34d81b28bf1b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716773719.784421,"update_time":null,"content":{"content_type":"text","parts":["Attention mechanisms, particularly those inspired by the transformer architecture, can be incorporated into various types of machine learning models to enhance their performance. Here are some types of models where attention mechanisms can be effectively integrated:\n\n1. **Recurrent Neural Networks (RNNs) and Long Short-Term Memory Networks (LSTMs)**:\n   - **Seq2Seq Models**: In sequence-to-sequence models, attention mechanisms can help by focusing on relevant parts of the input sequence when generating each part of the output sequence. This is particularly useful in tasks like machine translation, text summarization, and speech recognition.\n   - **Time Series Analysis**: For tasks involving time series data, attention mechanisms can help the model to focus on the most relevant time steps in the input sequence, improving forecasting and anomaly detection performance.\n\n2. **Convolutional Neural Networks (CNNs)**:\n   - **Image Classification**: Attention can be used to focus on the most relevant parts of an image, improving the accuracy of the classification.\n   - **Object Detection**: Attention mechanisms can help in highlighting the regions of interest in an image, enhancing the performance of object detection models.\n   - **Image Captioning**: In models that generate captions for images, attention mechanisms can help focus on different parts of the image while generating each word of the caption.\n\n3. **Graph Neural Networks (GNNs)**:\n   - **Node Classification and Graph Classification**: Attention mechanisms can be used to weigh the importance of different nodes or edges in a graph, improving the performance of tasks like node classification, link prediction, and graph classification.\n\n4. **Transformers and Variants**:\n   - **Natural Language Processing (NLP)**: While transformers are inherently based on self-attention mechanisms, variants and extensions of transformers can incorporate different forms of attention to further enhance performance in tasks like language modeling, question answering, and text generation.\n\n5. **Multimodal Models**:\n   - **Text and Image Fusion**: For models that handle multiple modalities, such as combining text and image data, attention mechanisms can help in aligning and fusing information from different sources effectively.\n   - **Audio-Visual Models**: Attention can be used to align and integrate audio and visual data, improving performance in tasks like lip-reading and video analysis.\n\n6. **Reinforcement Learning**:\n   - **Policy and Value Networks**: Attention mechanisms can be incorporated into the policy and value networks in reinforcement learning to focus on the most relevant parts of the state space, enhancing decision-making processes.\n\nBy integrating attention mechanisms, these models can better handle complex dependencies and improve their ability to focus on the most relevant parts of the input data, leading to enhanced performance across a variety of tasks."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","pad":"AAAAAAAAAAAAAAAAAAAAAAAAAA","parent_id":"93a0c2a8-1e7c-4238-9433-8ca7aa01f46e","request_id":"88a24431afc2496f-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"93a0c2a8-1e7c-4238-9433-8ca7aa01f46e","children":["aaa28c71-a4d1-4c67-a331-e57e926b6110"]},"aaa28c71-a4d1-4c67-a331-e57e926b6110":{"id":"aaa28c71-a4d1-4c67-a331-e57e926b6110","message":{"id":"aaa28c71-a4d1-4c67-a331-e57e926b6110","author":{"role":"user","name":null,"metadata":{}},"create_time":1716773785.16555,"update_time":null,"content":{"content_type":"text","parts":["What research has been done into this?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"88a2461b5f2087a7-SIN","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"19ad67a7-87d2-437f-9f40-34d81b28bf1b","children":["8fe9e854-756b-4b2f-b711-dd22d4c0830e"]},"8fe9e854-756b-4b2f-b711-dd22d4c0830e":{"id":"8fe9e854-756b-4b2f-b711-dd22d4c0830e","message":{"id":"8fe9e854-756b-4b2f-b711-dd22d4c0830e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716773802.4361,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"search(\"research on attention mechanism in RNNs LSTMs CNNs GNNs multimodal models reinforcement learning\")"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"aaa28c71-a4d1-4c67-a331-e57e926b6110","request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute"},"recipient":"browser","channel":null},"parent":"aaa28c71-a4d1-4c67-a331-e57e926b6110","children":["1af773ed-dcaf-4eb4-a5b7-32b2b0698326"]},"1af773ed-dcaf-4eb4-a5b7-32b2b0698326":{"id":"1af773ed-dcaf-4eb4-a5b7-32b2b0698326","message":{"id":"1af773ed-dcaf-4eb4-a5b7-32b2b0698326","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716773802.441326,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"# \u30100\u2020Attention, please! A survey of Neural Attention Models in Deep Learning\u2020ar5iv.labs.arxiv.org\u3011\nEnd-to-end attention models introduce a new class of neural networks in Deep Learning. End-to-end attention research makes sense since no isolated attention center exists in the human brain, and its mechanisms are used in different cognitive processes. ... (e.g. RNNs, CNNs, LSTMs, Transformers). The general idea is that complex data might ...\n# \u30101\u2020Multivariate Time-Series Forecasting: A Review of Deep Learning ... - MDPI\u2020www.mdpi.com\u3011\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations.\n# \u30102\u2020A review on the attention mechanism of deep learning\u2020www.sciencedirect.com\u3011\nThis survey is structured as follows. In Section 2, we introduce a well-known model proposed by [8] and define a general attention model. Section 3 describes the classification of attention models. Section 4 summarizes network architectures in conjunction with the attention mechanism. Section 5 elaborates on the uses of attention in various computer vision (CV) and natural language processing ...\n# \u30103\u2020Attention, please! A survey of neural attention models in deep learning ...\u2020link.springer.com\u3011\nIn humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last 6 years, this property ...\n# \u30104\u2020Deep learning: RNNs and LSTM - ScienceDirect\u2020www.sciencedirect.com\u3011\nAbstract. Recurrent neural networks (RNNs) are a class of neural networks that are naturally suited to processing time-series data and other sequential data. Here we introduce recurrent neural networks as an extension to feedforward networks, in order to allow the processing of variable-length (or even infinite-length) sequences, and some of ...\n# \u30105\u2020Learning Models: CNN, RNN, LSTM, GRU - arXiv.org\u2020arxiv.org\u3011\nmodels. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and ...\n# \u30106\u2020Recurrent Neural Networks (RNNs): Architectures, Training ... - Springer\u2020link.springer.com\u3011\nRecurrent neural networks (RNNs) are neural network architectures with hidden state and which use feedback loops to process a sequence of data that ultimately informs the final output. ... by making the context vector a variable length sequence with added attention mechanism. 2.7 Attention Models (Transformers) Due to the sequential learning ...\n# \u30107\u2020A Novel CNN-based Bi-LSTM parallel model with attention mechanism for ...\u2020www.nature.com\u3011\na sequential fusion of CNNs, LSTMs, and fully connected layers. \u02daakur et al.21 proposed a DL-based unied model composed of CNNs, autoencoders, and LSTMs. e model learnt both spatial features and ...\n# \u30108\u20201 A General Survey on Attention Mechanisms in Deep Learning - arXiv.org\u2020arxiv.org\u3011\nA General Survey on Attention Mechanisms in Deep Learning Gianni Brauwers and Flavius Frasincar Abstract\u2014Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The\n# \u30109\u2020cs224n-2018-lecture12-Transformers and CNNs - Stanford University\u2020web.stanford.edu\u3011\nAbstract. We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vec-tors for sentence-level classification tasks. We show that a simple CNN with lit-tle hyperparameter tuning and static vec-tors achieves excellent results on multi-ple benchmarks.\n# \u301010\u2020Generalized attention-weighted reinforcement learning - ScienceDirect\u2020www.sciencedirect.com\u3011\nAbstract. In neuroscience, attention has been shown to bidirectionally interact with reinforcement learning (RL) to reduce the dimensionality of task representations, restricting computations to relevant features. In machine learning, despite their popularity, attention mechanisms have seldom been administered to decision-making problems.\n# \u301011\u2020A Comprehensive Overview and Comparative Analysis on Deep Learning ...\u2020arxiv.org\u3011\nConsequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep ...\n# \u301012\u2020Abstractive Text Summarization using Attentive Sequence-to-Sequence RNNs\u2020web.stanford.edu\u3011\nSummarization refers to the task of creating a short summary that captures the main ideas of an input text. For our project, we focus on abstractive summarization, which generates the summary through paraphrase. By comparison, extractive summarization works by extracting only words found in the input. Models for abstractive summarization fall ...\n# \u301013\u2020Improving time series forecasting using LSTM and attention models ...\u2020link.springer.com\u3011\nAccurate time series forecasting has been recognized as an essential task in many application domains. Real-world time series data often consist of non-linear patterns with complexities that prevent conventional forecasting techniques from accurate predictions. To forecast a given time series accurately, a hybrid model based on two deep learning methods, i.e., long short-term memory (LSTM) and ...\n# \u301014\u2020Examining Attention Mechanisms in Deep Learning Models for Sentiment ...\u2020www.mdpi.com\u3011\nAttention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study ...\n# \u301015\u2020Performance Comparison of Deep Learning Approaches in Predicting EV ...\u2020www.mdpi.com\u3011\nHowever, this study sought to build upon previous works by incorporating six deep learning models\u2014RNNs, LSTMs, Bi-LSTMs, GRUs, CNNs, and transformers\u2014to compare these models\u2019 performances comprehensively. The goal was to identify the best-performing model for EV charging load forecasting to improve the accuracy of predictions.\n# \u301016\u2020Most Powerful Machine Learning Models Explained (Transformers, CNNs ...\u2020pub.towardsai.net\u3011\nGAN most often refers to the learning method for training these models, as opposed to the models themselves. In fact, the architecture of the 2 models is not overly important to the concept itself, as long as one is generative and the other is a classifier. Let\u2019s begin with describing the standard supervised learning technique for training ...\n# \u301017\u2020A review of deep learning techniques for speech processing\u2020www.sciencedirect.com\u3011\nIt begins by establishing the background, encompassing the definition of speech signals, speech features, and traditional non-neural models. Subsequently, the focus shifts toward an in-depth examination of various deep-learning architectures specifically tailored for speech processing, including RNNs, CNNs, Transformers, GNNs, and diffusion models.\n# \u301018\u2020[2006.16236] Transformers are RNNs: Fast Autoregressive Transformers ...\u2020arxiv.org\u3011\nTransformers are RNNs: Fast Autoregressive Transformers with Linear Attention. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret. Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences.\n# \u301019\u2020Analysis of spectrum sensing using deep learning algorithms: CNNs and RNNs\u2020www.sciencedirect.com\u3011\nIt is seen that DLMs such as CNNs and RNNs obtained a lower SNR of 1.2 dB and 1.6 dB as compared with MF (4 dB) and ED (4.8 dB). Hence, it is concluded that the proposed CNNs and RNNs outperform the conventional method. Download : Download high-res image (110KB) Download : Download full-size image; Fig. 3. SNR Vs Pd.\nVisible: 0% - 100%","summary":null,"assets":[],"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"Attention, please! A survey of Neural Attention Models in Deep Learning","url":"https://ar5iv.labs.arxiv.org/html/2103.16775","text":"\nEnd-to-end attention models introduce a new class of neural networks in Deep Learning. End-to-end attention research makes sense since no isolated attention center exists in the human brain, and its mechanisms are used in different cognitive processes. ... (e.g. RNNs, CNNs, LSTMs, Transformers). The general idea is that complex data might ...\n","pub_date":"2024-05-24T23:29:00.0000000Z","extra":null},{"type":"webpage","title":"Multivariate Time-Series Forecasting: A Review of Deep Learning ... - MDPI","url":"https://www.mdpi.com/2624-6511/6/5/114","text":"\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations.\n","pub_date":"2024-05-25T10:14:00.0000000Z","extra":null},{"type":"webpage","title":"A review on the attention mechanism of deep learning","url":"https://www.sciencedirect.com/science/article/pii/S092523122100477X","text":"\nThis survey is structured as follows. In Section 2, we introduce a well-known model proposed by [8] and define a general attention model. Section 3 describes the classification of attention models. Section 4 summarizes network architectures in conjunction with the attention mechanism. Section 5 elaborates on the uses of attention in various computer vision (CV) and natural language processing ...\n","pub_date":"2024-05-26T00:24:00.0000000Z","extra":null},{"type":"webpage","title":"Attention, please! A survey of neural attention models in deep learning ...","url":"https://link.springer.com/article/10.1007/s10462-022-10148-x","text":"\nIn humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last 6 years, this property ...\n","pub_date":"2024-05-24T19:02:00.0000000Z","extra":null},{"type":"webpage","title":"Deep learning: RNNs and LSTM - ScienceDirect","url":"https://www.sciencedirect.com/science/article/pii/B9780128161760000260","text":"\nAbstract. Recurrent neural networks (RNNs) are a class of neural networks that are naturally suited to processing time-series data and other sequential data. Here we introduce recurrent neural networks as an extension to feedforward networks, in order to allow the processing of variable-length (or even infinite-length) sequences, and some of ...\n","pub_date":"2024-05-25T10:03:00.0000000Z","extra":null},{"type":"webpage","title":"Learning Models: CNN, RNN, LSTM, GRU - arXiv.org","url":"https://arxiv.org/pdf/2305.17473","text":"\nmodels. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and ...\n","pub_date":"2024-05-25T19:09:00.0000000Z","extra":null},{"type":"webpage","title":"Recurrent Neural Networks (RNNs): Architectures, Training ... - Springer","url":"https://link.springer.com/protocol/10.1007/978-1-0716-3195-9_4","text":"\nRecurrent neural networks (RNNs) are neural network architectures with hidden state and which use feedback loops to process a sequence of data that ultimately informs the final output. ... by making the context vector a variable length sequence with added attention mechanism. 2.7 Attention Models (Transformers) Due to the sequential learning ...\n","pub_date":"2024-05-26T01:11:00.0000000Z","extra":null},{"type":"webpage","title":"A Novel CNN-based Bi-LSTM parallel model with attention mechanism for ...","url":"https://www.nature.com/articles/s41598-022-11880-8.pdf","text":"\na sequential fusion of CNNs, LSTMs, and fully connected layers. \u02daakur et al.21 proposed a DL-based unied model composed of CNNs, autoencoders, and LSTMs. e model learnt both spatial features and ...\n","pub_date":"2024-05-25T06:07:00.0000000Z","extra":null},{"type":"webpage","title":"1 A General Survey on Attention Mechanisms in Deep Learning - arXiv.org","url":"https://arxiv.org/pdf/2203.14263","text":"\nA General Survey on Attention Mechanisms in Deep Learning Gianni Brauwers and Flavius Frasincar Abstract\u2014Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The\n","pub_date":"2024-05-24T17:36:00.0000000Z","extra":null},{"type":"webpage","title":"cs224n-2018-lecture12-Transformers and CNNs - Stanford University","url":"https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture12.pdf","text":"\nAbstract. We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vec-tors for sentence-level classification tasks. We show that a simple CNN with lit-tle hyperparameter tuning and static vec-tors achieves excellent results on multi-ple benchmarks.\n","pub_date":"2024-05-21T20:02:00.0000000Z","extra":null},{"type":"webpage","title":"Generalized attention-weighted reinforcement learning - ScienceDirect","url":"https://www.sciencedirect.com/science/article/pii/S0893608021003853","text":"\nAbstract. In neuroscience, attention has been shown to bidirectionally interact with reinforcement learning (RL) to reduce the dimensionality of task representations, restricting computations to relevant features. In machine learning, despite their popularity, attention mechanisms have seldom been administered to decision-making problems.\n","pub_date":"2024-05-22T12:13:00.0000000Z","extra":null},{"type":"webpage","title":"A Comprehensive Overview and Comparative Analysis on Deep Learning ...","url":"https://arxiv.org/abs/2305.17473","text":"\nConsequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep ...\n","pub_date":"2024-05-25T06:33:00.0000000Z","extra":null},{"type":"webpage","title":"Abstractive Text Summarization using Attentive Sequence-to-Sequence RNNs","url":"https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2749095.pdf","text":"\nSummarization refers to the task of creating a short summary that captures the main ideas of an input text. For our project, we focus on abstractive summarization, which generates the summary through paraphrase. By comparison, extractive summarization works by extracting only words found in the input. Models for abstractive summarization fall ...\n","pub_date":"2024-05-24T03:13:00.0000000Z","extra":null},{"type":"webpage","title":"Improving time series forecasting using LSTM and attention models ...","url":"https://link.springer.com/article/10.1007/s12652-020-02761-x","text":"\nAccurate time series forecasting has been recognized as an essential task in many application domains. Real-world time series data often consist of non-linear patterns with complexities that prevent conventional forecasting techniques from accurate predictions. To forecast a given time series accurately, a hybrid model based on two deep learning methods, i.e., long short-term memory (LSTM) and ...\n","pub_date":"2024-05-25T23:12:00.0000000Z","extra":null},{"type":"webpage","title":"Examining Attention Mechanisms in Deep Learning Models for Sentiment ...","url":"https://www.mdpi.com/2076-3417/11/9/3883","text":"\nAttention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study ...\n","pub_date":"2024-05-24T00:14:00.0000000Z","extra":null},{"type":"webpage","title":"Performance Comparison of Deep Learning Approaches in Predicting EV ...","url":"https://www.mdpi.com/2071-1050/15/5/4258","text":"\nHowever, this study sought to build upon previous works by incorporating six deep learning models\u2014RNNs, LSTMs, Bi-LSTMs, GRUs, CNNs, and transformers\u2014to compare these models\u2019 performances comprehensively. The goal was to identify the best-performing model for EV charging load forecasting to improve the accuracy of predictions.\n","pub_date":"2024-05-21T19:48:00.0000000Z","extra":null},{"type":"webpage","title":"Most Powerful Machine Learning Models Explained (Transformers, CNNs ...","url":"https://pub.towardsai.net/most-powerful-machine-learning-models-explained-transformers-cnns-rnns-gans-3afb1180fb70","text":"\nGAN most often refers to the learning method for training these models, as opposed to the models themselves. In fact, the architecture of the 2 models is not overly important to the concept itself, as long as one is generative and the other is a classifier. Let\u2019s begin with describing the standard supervised learning technique for training ...\n","pub_date":"2024-05-23T11:13:00.0000000Z","extra":null},{"type":"webpage","title":"A review of deep learning techniques for speech processing","url":"https://www.sciencedirect.com/science/article/pii/S1566253523001859","text":"\nIt begins by establishing the background, encompassing the definition of speech signals, speech features, and traditional non-neural models. Subsequently, the focus shifts toward an in-depth examination of various deep-learning architectures specifically tailored for speech processing, including RNNs, CNNs, Transformers, GNNs, and diffusion models.\n","pub_date":"2024-05-21T18:18:00.0000000Z","extra":null},{"type":"webpage","title":"[2006.16236] Transformers are RNNs: Fast Autoregressive Transformers ...","url":"https://arxiv.org/abs/2006.16236","text":"\nTransformers are RNNs: Fast Autoregressive Transformers with Linear Attention. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret. Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences.\n","pub_date":"2024-05-26T02:08:00.0000000Z","extra":null},{"type":"webpage","title":"Analysis of spectrum sensing using deep learning algorithms: CNNs and RNNs","url":"https://www.sciencedirect.com/science/article/pii/S2090447923003945","text":"\nIt is seen that DLMs such as CNNs and RNNs obtained a lower SNR of 1.2 dB and 1.6 dB as compared with MF (4 dB) and ED (4.8 dB). Hence, it is concluded that the proposed CNNs and RNNs outperform the conventional method. Download : Download high-res image (110KB) Download : Download full-size image; Fig. 3. SNR Vs Pd.\nVisible: 0% - 100%","pub_date":"2024-05-21T09:29:00.0000000Z","extra":null},{"type":"webpage","title":"Search results for query: 'research on attention mechanism in RNNs LSTMs CNNs GNNs multimodal models reinforcement learning'","url":"","text":"# \u30100\u2020Attention, please! A survey of Neural Attention Models in Deep Learning\u2020ar5iv.labs.arxiv.org\u3011\nEnd-to-end attention models introduce a new class of neural networks in Deep Learning. End-to-end attention research makes sense since no isolated attention center exists in the human brain, and its mechanisms are used in different cognitive processes. ... (e.g. RNNs, CNNs, LSTMs, Transformers). The general idea is that complex data might ...\n# \u30101\u2020Multivariate Time-Series Forecasting: A Review of Deep Learning ... - MDPI\u2020www.mdpi.com\u3011\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations.\n# \u30102\u2020A review on the attention mechanism of deep learning\u2020www.sciencedirect.com\u3011\nThis survey is structured as follows. In Section 2, we introduce a well-known model proposed by [8] and define a general attention model. Section 3 describes the classification of attention models. Section 4 summarizes network architectures in conjunction with the attention mechanism. Section 5 elaborates on the uses of attention in various computer vision (CV) and natural language processing ...\n# \u30103\u2020Attention, please! A survey of neural attention models in deep learning ...\u2020link.springer.com\u3011\nIn humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last 6 years, this property ...\n# \u30104\u2020Deep learning: RNNs and LSTM - ScienceDirect\u2020www.sciencedirect.com\u3011\nAbstract. Recurrent neural networks (RNNs) are a class of neural networks that are naturally suited to processing time-series data and other sequential data. Here we introduce recurrent neural networks as an extension to feedforward networks, in order to allow the processing of variable-length (or even infinite-length) sequences, and some of ...\n# \u30105\u2020Learning Models: CNN, RNN, LSTM, GRU - arXiv.org\u2020arxiv.org\u3011\nmodels. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and ...\n# \u30106\u2020Recurrent Neural Networks (RNNs): Architectures, Training ... - Springer\u2020link.springer.com\u3011\nRecurrent neural networks (RNNs) are neural network architectures with hidden state and which use feedback loops to process a sequence of data that ultimately informs the final output. ... by making the context vector a variable length sequence with added attention mechanism. 2.7 Attention Models (Transformers) Due to the sequential learning ...\n# \u30107\u2020A Novel CNN-based Bi-LSTM parallel model with attention mechanism for ...\u2020www.nature.com\u3011\na sequential fusion of CNNs, LSTMs, and fully connected layers. \u02daakur et al.21 proposed a DL-based unied model composed of CNNs, autoencoders, and LSTMs. e model learnt both spatial features and ...\n# \u30108\u20201 A General Survey on Attention Mechanisms in Deep Learning - arXiv.org\u2020arxiv.org\u3011\nA General Survey on Attention Mechanisms in Deep Learning Gianni Brauwers and Flavius Frasincar Abstract\u2014Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The\n# \u30109\u2020cs224n-2018-lecture12-Transformers and CNNs - Stanford University\u2020web.stanford.edu\u3011\nAbstract. We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vec-tors for sentence-level classification tasks. We show that a simple CNN with lit-tle hyperparameter tuning and static vec-tors achieves excellent results on multi-ple benchmarks.\n# \u301010\u2020Generalized attention-weighted reinforcement learning - ScienceDirect\u2020www.sciencedirect.com\u3011\nAbstract. In neuroscience, attention has been shown to bidirectionally interact with reinforcement learning (RL) to reduce the dimensionality of task representations, restricting computations to relevant features. In machine learning, despite their popularity, attention mechanisms have seldom been administered to decision-making problems.\n# \u301011\u2020A Comprehensive Overview and Comparative Analysis on Deep Learning ...\u2020arxiv.org\u3011\nConsequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep ...\n# \u301012\u2020Abstractive Text Summarization using Attentive Sequence-to-Sequence RNNs\u2020web.stanford.edu\u3011\nSummarization refers to the task of creating a short summary that captures the main ideas of an input text. For our project, we focus on abstractive summarization, which generates the summary through paraphrase. By comparison, extractive summarization works by extracting only words found in the input. Models for abstractive summarization fall ...\n# \u301013\u2020Improving time series forecasting using LSTM and attention models ...\u2020link.springer.com\u3011\nAccurate time series forecasting has been recognized as an essential task in many application domains. Real-world time series data often consist of non-linear patterns with complexities that prevent conventional forecasting techniques from accurate predictions. To forecast a given time series accurately, a hybrid model based on two deep learning methods, i.e., long short-term memory (LSTM) and ...\n# \u301014\u2020Examining Attention Mechanisms in Deep Learning Models for Sentiment ...\u2020www.mdpi.com\u3011\nAttention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study ...\n# \u301015\u2020Performance Comparison of Deep Learning Approaches in Predicting EV ...\u2020www.mdpi.com\u3011\nHowever, this study sought to build upon previous works by incorporating six deep learning models\u2014RNNs, LSTMs, Bi-LSTMs, GRUs, CNNs, and transformers\u2014to compare these models\u2019 performances comprehensively. The goal was to identify the best-performing model for EV charging load forecasting to improve the accuracy of predictions.\n# \u301016\u2020Most Powerful Machine Learning Models Explained (Transformers, CNNs ...\u2020pub.towardsai.net\u3011\nGAN most often refers to the learning method for training these models, as opposed to the models themselves. In fact, the architecture of the 2 models is not overly important to the concept itself, as long as one is generative and the other is a classifier. Let\u2019s begin with describing the standard supervised learning technique for training ...\n# \u301017\u2020A review of deep learning techniques for speech processing\u2020www.sciencedirect.com\u3011\nIt begins by establishing the background, encompassing the definition of speech signals, speech features, and traditional non-neural models. Subsequently, the focus shifts toward an in-depth examination of various deep-learning architectures specifically tailored for speech processing, including RNNs, CNNs, Transformers, GNNs, and diffusion models.\n# \u301018\u2020[2006.16236] Transformers are RNNs: Fast Autoregressive Transformers ...\u2020arxiv.org\u3011\nTransformers are RNNs: Fast Autoregressive Transformers with Linear Attention. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret. Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences.\n# \u301019\u2020Analysis of spectrum sensing using deep learning algorithms: CNNs and RNNs\u2020www.sciencedirect.com\u3011\nIt is seen that DLMs such as CNNs and RNNs obtained a lower SNR of 1.2 dB and 1.6 dB as compared with MF (4 dB) and ED (4.8 dB). Hence, it is concluded that the proposed CNNs and RNNs outperform the conventional method. Download : Download high-res image (110KB) Download : Download full-size image; Fig. 3. SNR Vs Pd.\nVisible: 0% - 100%","pub_date":null,"extra":null}],"original_query":null},"command":"search","args":["research on attention mechanism in RNNs LSTMs CNNs GNNs multimodal models reinforcement learning"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"8fe9e854-756b-4b2f-b711-dd22d4c0830e","request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"8fe9e854-756b-4b2f-b711-dd22d4c0830e","children":["6f97bc91-5067-45fb-bb28-6e2f4a3e4b31"]},"6f97bc91-5067-45fb-bb28-6e2f4a3e4b31":{"id":"6f97bc91-5067-45fb-bb28-6e2f4a3e4b31","message":{"id":"6f97bc91-5067-45fb-bb28-6e2f4a3e4b31","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716773802.445595,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"mclick([0, 2, 3, 6, 10])"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"1af773ed-dcaf-4eb4-a5b7-32b2b0698326","request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute"},"recipient":"browser","channel":null},"parent":"1af773ed-dcaf-4eb4-a5b7-32b2b0698326","children":["800dd3d3-8b29-47b0-a66d-d5402c5756bf"]},"800dd3d3-8b29-47b0-a66d-d5402c5756bf":{"id":"800dd3d3-8b29-47b0-a66d-d5402c5756bf","message":{"id":"800dd3d3-8b29-47b0-a66d-d5402c5756bf","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716773802.451673,"update_time":null,"content":{"content_type":"system_error","name":"tool_error","text":"Error when executing command `mclick([0, 2, 3, 6, 10])`\n\n5 failed clicks:\nrecorded='click(0)' temporary=None permanent='Failed to fetch URL: https://ar5iv.labs.ar5iv.org/html/2103.16775' new_state=None final=None success=False feedback=None metadata={}\nrecorded='click(2)' temporary=None permanent='Failed to fetch page w/ URL: https://www.sciencedirect.com/science/article/pii/S092523122100477X' new_state=None final=None success=False feedback='Error fetching https://www.sciencedirect.com/science/article/pii/S092523122100477X' metadata={}\nrecorded='click(3)' temporary=None permanent='Failed to fetch page w/ URL: https://link.springer.com/article/10.1007/s10462-022-10148-x' new_state=None final=None success=False feedback='Fetch denied by robots.txt' metadata={}\nrecorded='click(6)' temporary=None permanent='Failed to fetch page w/ URL: https://link.springer.com/protocol/10.1007/978-1-0716-3195-9_4' new_state=None final=None success=False feedback='Fetch denied by robots.txt' metadata={}\nrecorded='click(10)' temporary=None permanent='Failed to fetch page w/ URL: https://www.sciencedirect.com/science/article/pii/S0893608021003853' new_state=None final=None success=False feedback='Error fetching https://www.sciencedirect.com/science/article/pii/S0893608021003853' metadata={}"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"mclick","status":"failed","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"6f97bc91-5067-45fb-bb28-6e2f4a3e4b31","request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"6f97bc91-5067-45fb-bb28-6e2f4a3e4b31","children":["7acf8d77-38da-4f51-ad44-e7cb720ed03b"]},"7acf8d77-38da-4f51-ad44-e7cb720ed03b":{"id":"7acf8d77-38da-4f51-ad44-e7cb720ed03b","message":{"id":"7acf8d77-38da-4f51-ad44-e7cb720ed03b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716773802.451864,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"mclick([1, 4, 8, 12, 14])"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"gizmo_id":null,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"800dd3d3-8b29-47b0-a66d-d5402c5756bf","finish_details":{"type":"stop","stop_tokens":[200007]},"is_complete":true,"request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute"},"recipient":"browser","channel":null},"parent":"800dd3d3-8b29-47b0-a66d-d5402c5756bf","children":["4609fd68-9c84-467b-9420-41d82d4fa1de"]},"4609fd68-9c84-467b-9420-41d82d4fa1de":{"id":"4609fd68-9c84-467b-9420-41d82d4fa1de","message":{"id":"4609fd68-9c84-467b-9420-41d82d4fa1de","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716773802.457888,"update_time":null,"content":{"content_type":"tether_quote","url":"https://www.mdpi.com/2624-6511/6/5/114","domain":"www.mdpi.com","text":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","text":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[1, 4, 8, 12, 14]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"7acf8d77-38da-4f51-ad44-e7cb720ed03b","request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"7acf8d77-38da-4f51-ad44-e7cb720ed03b","children":["30e28ff0-04b0-44a2-aff0-5392cd032139"]},"30e28ff0-04b0-44a2-aff0-5392cd032139":{"id":"30e28ff0-04b0-44a2-aff0-5392cd032139","message":{"id":"30e28ff0-04b0-44a2-aff0-5392cd032139","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716773802.458164,"update_time":null,"content":{"content_type":"tether_quote","url":"https://ar5iv.org/pdf/2203.14263","domain":"ar5iv.org","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[1, 4, 8, 12, 14]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"4609fd68-9c84-467b-9420-41d82d4fa1de","request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"4609fd68-9c84-467b-9420-41d82d4fa1de","children":["6ae80474-64f2-47dc-b02d-3024cdf5641c"]},"6ae80474-64f2-47dc-b02d-3024cdf5641c":{"id":"6ae80474-64f2-47dc-b02d-3024cdf5641c","message":{"id":"6ae80474-64f2-47dc-b02d-3024cdf5641c","author":{"role":"tool","name":"browser","metadata":{}},"create_time":1716773802.458421,"update_time":null,"content":{"content_type":"tether_quote","url":"https://www.mdpi.com/2076-3417/11/9/3883","domain":"www.mdpi.com","text":"\n#  Examining Attention Mechanisms in Deep Learning Models for Sentiment Analysis \n\nby \n\nSpyridon Kardakis\n\n[Image 0]Spyridon Kardakis\n\n\u301069\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301070\u2020 Scilit \u2020scilit.net\u3011\u301071\u2020 Preprints.org \u2020www.preprints.org\u3011\u301055\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1[Image 1: ORCID], \n\nIsidoros Perikos\n\n[Image 2]Isidoros Perikos\n\n\u301072\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301073\u2020 Scilit \u2020scilit.net\u3011\u301074\u2020 Preprints.org \u2020www.preprints.org\u3011\u301056\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1,2,*[Image 3: ORCID], \n\nFoteini Grivokostopoulou\n\n[Image 4]Foteini Grivokostopoulou\n\n\u301075\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301076\u2020 Scilit \u2020scilit.net\u3011\u301077\u2020 Preprints.org \u2020www.preprints.org\u3011\u301057\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1,2[Image 5: ORCID] and \n\nIoannis Hatzilygeroudis\n\n[Image 6]Ioannis Hatzilygeroudis\n\n\u301078\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301079\u2020 Scilit \u2020scilit.net\u3011\u301080\u2020 Preprints.org \u2020www.preprints.org\u3011\u301058\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1\n\n^{1}\n\nComputer Engineering and Informatics Department, University of Patras, 26504 Patras, Greece\n\n^{2}\n\nComputer Technology Institute and Press \u201cDiophantus\u201d, 26504 Patras, Greece\n\n*\n\nAuthor to whom correspondence should be addressed. \n\nAppl. Sci. 2021, 11(9), 3883; \u301081\u2020https://doi.org/10.3390/app11093883\u2020doi.org\u3011 \n\nSubmission received: 2 April 2021 / Revised: 21 April 2021 / Accepted: 23 April 2021 / Published: 25 April 2021\n\n(This article belongs to the Section \u301082\u2020Computing and Artificial Intelligence\u3011)  \n\nDownload keyboard_arrow_down\n\n \u301067\u2020Download PDF\u3011   \nDownload PDF with Cover   \nDownload XML   \n\u301083\u2020Download Epub\u3011   \n\nBrowse Figures \n\n \u301084\u2020 \u2020pub.mdpi-res.com\u3011 \u301085\u2020 \u2020pub.mdpi-res.com\u3011 \u301086\u2020 \u2020pub.mdpi-res.com\u3011 \u301087\u2020 \u2020pub.mdpi-res.com\u3011 \u301088\u2020 \u2020pub.mdpi-res.com\u3011 \u301089\u2020 \u2020pub.mdpi-res.com\u3011 \n\n\u301090\u2020Versions Notes\u3011 \n\n## Abstract\n\n:\n\nAttention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study attention-based models built on recurrent neural networks (RNNs) and examine their performance in various contexts of sentiment analysis. Self-attention, global-attention and hierarchical-attention methods are examined under various deep neural models, training methods and hyperparameters. Even though attention mechanisms are a powerful recent concept in the field of deep learning, their exact effectiveness in sentiment analysis is yet to be thoroughly assessed. A comparative analysis is performed in a text sentiment classification task where baseline models are compared with and without the use of attention for every experiment. The experimental study additionally examines the proposed models\u2019 ability in recognizing opinions and emotions in movie reviews. The results indicate that attention-based models lead to great improvements in the performance of deep neural models showcasing up to a 3.5% improvement in their accuracy.\n\nKeywords: \n\n\u301091\u2020attention mechanism\u3011; \u301092\u2020deep neural networks\u3011; \u301093\u2020global-attention\u3011; \u301094\u2020hierarchical-attention\u3011; \u301095\u2020self-attention\u3011; \u301096\u2020sentiment analysis\u3011\n\n##  1. Introduction \n\nIn the past decade, the dramatic decrease in computation cost and the drastic increase in data availability led to the emergence of a new sub-field of machine learning, called deep learning, which outperforms its predecessor by achieving very high performance on large data. Deep learning, utilizes deep artificial neural networks, such as CNNs, RNNs [\u301097\u20201\u3011], LSTMs [\u301098\u20202\u3011] and GRUs [\u301099\u20203\u3011], which achieve remarkable performance in various domains, such as speech recognition [\u3010100\u20204\u3011,\u3010101\u20205\u3011,\u3010102\u20206\u3011], signal and EEG analysis [\u3010103\u20207\u3011], computer vision [\u3010104\u20208\u3011,\u3010105\u20209\u3011,\u3010106\u202010\u3011], emotion recognition [\u3010107\u202011\u3011,\u3010108\u202012\u3011,\u3010109\u202013\u3011], disease and cancer recognition [\u3010110\u202014\u3011] as well as in text classification and sentiment analysis [\u3010111\u202015\u3011,\u3010112\u202016\u3011,\u3010113\u202017\u3011]. Indeed, in-text classification and sentiment analysis, deep neural networks have demonstrated quite remarkable performance [\u3010114\u202018\u3011]. Despite their interpretability disadvantages and their computational cost, deep neural networks can model complex nonlinear relationships. The extra layers enable the composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network [\u3010115\u202019\u3011].\n\nAttention is arguably one of the most powerful recent concepts in deep learning, bringing out many breakthroughs in the field and improving the performance of neural networks [\u3010116\u202020\u3011]. An attention mechanism effectively gives a model the ability to \u201cattend to\u201d a certain part of the input sequence, which would arguably be a part with higher importance [\u3010117\u202021\u3011]. For example, it allows an LSTM to focus on specific parts of a docum","title":"Applied Sciences | Free Full-Text | Examining Attention Mechanisms in Deep Learning Models for Sentiment Analysis","tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"_cite_metadata":{"citation_format":{"name":"tether_og","regex":"\u3010(\\d+)(?::(\\d+))?\u2020([^\u2020\u3010\u3011]*)[^\u3010\u3011]*?\u3011"},"metadata_list":[{"type":"webpage","title":"Applied Sciences | Free Full-Text | Examining Attention Mechanisms in Deep Learning Models for Sentiment Analysis","url":"https://www.mdpi.com/2076-3417/11/9/3883","text":"\n#  Examining Attention Mechanisms in Deep Learning Models for Sentiment Analysis \n\nby \n\nSpyridon Kardakis\n\n[Image 0]Spyridon Kardakis\n\n\u301069\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301070\u2020 Scilit \u2020scilit.net\u3011\u301071\u2020 Preprints.org \u2020www.preprints.org\u3011\u301055\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1[Image 1: ORCID], \n\nIsidoros Perikos\n\n[Image 2]Isidoros Perikos\n\n\u301072\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301073\u2020 Scilit \u2020scilit.net\u3011\u301074\u2020 Preprints.org \u2020www.preprints.org\u3011\u301056\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1,2,*[Image 3: ORCID], \n\nFoteini Grivokostopoulou\n\n[Image 4]Foteini Grivokostopoulou\n\n\u301075\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301076\u2020 Scilit \u2020scilit.net\u3011\u301077\u2020 Preprints.org \u2020www.preprints.org\u3011\u301057\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1,2[Image 5: ORCID] and \n\nIoannis Hatzilygeroudis\n\n[Image 6]Ioannis Hatzilygeroudis\n\n\u301078\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301079\u2020 Scilit \u2020scilit.net\u3011\u301080\u2020 Preprints.org \u2020www.preprints.org\u3011\u301058\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1\n\n^{1}\n\nComputer Engineering and Informatics Department, University of Patras, 26504 Patras, Greece\n\n^{2}\n\nComputer Technology Institute and Press \u201cDiophantus\u201d, 26504 Patras, Greece\n\n*\n\nAuthor to whom correspondence should be addressed. \n\nAppl. Sci. 2021, 11(9), 3883; \u301081\u2020https://doi.org/10.3390/app11093883\u2020doi.org\u3011 \n\nSubmission received: 2 April 2021 / Revised: 21 April 2021 / Accepted: 23 April 2021 / Published: 25 April 2021\n\n(This article belongs to the Section \u301082\u2020Computing and Artificial Intelligence\u3011)  \n\nDownload keyboard_arrow_down\n\n \u301067\u2020Download PDF\u3011   \nDownload PDF with Cover   \nDownload XML   \n\u301083\u2020Download Epub\u3011   \n\nBrowse Figures \n\n \u301084\u2020 \u2020pub.mdpi-res.com\u3011 \u301085\u2020 \u2020pub.mdpi-res.com\u3011 \u301086\u2020 \u2020pub.mdpi-res.com\u3011 \u301087\u2020 \u2020pub.mdpi-res.com\u3011 \u301088\u2020 \u2020pub.mdpi-res.com\u3011 \u301089\u2020 \u2020pub.mdpi-res.com\u3011 \n\n\u301090\u2020Versions Notes\u3011 \n\n## Abstract\n\n:\n\nAttention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study attention-based models built on recurrent neural networks (RNNs) and examine their performance in various contexts of sentiment analysis. Self-attention, global-attention and hierarchical-attention methods are examined under various deep neural models, training methods and hyperparameters. Even though attention mechanisms are a powerful recent concept in the field of deep learning, their exact effectiveness in sentiment analysis is yet to be thoroughly assessed. A comparative analysis is performed in a text sentiment classification task where baseline models are compared with and without the use of attention for every experiment. The experimental study additionally examines the proposed models\u2019 ability in recognizing opinions and emotions in movie reviews. The results indicate that attention-based models lead to great improvements in the performance of deep neural models showcasing up to a 3.5% improvement in their accuracy.\n\nKeywords: \n\n\u301091\u2020attention mechanism\u3011; \u301092\u2020deep neural networks\u3011; \u301093\u2020global-attention\u3011; \u301094\u2020hierarchical-attention\u3011; \u301095\u2020self-attention\u3011; \u301096\u2020sentiment analysis\u3011\n\n##  1. Introduction \n\nIn the past decade, the dramatic decrease in computation cost and the drastic increase in data availability led to the emergence of a new sub-field of machine learning, called deep learning, which outperforms its predecessor by achieving very high performance on large data. Deep learning, utilizes deep artificial neural networks, such as CNNs, RNNs [\u301097\u20201\u3011], LSTMs [\u301098\u20202\u3011] and GRUs [\u301099\u20203\u3011], which achieve remarkable performance in various domains, such as speech recognition [\u3010100\u20204\u3011,\u3010101\u20205\u3011,\u3010102\u20206\u3011], signal and EEG analysis [\u3010103\u20207\u3011], computer vision [\u3010104\u20208\u3011,\u3010105\u20209\u3011,\u3010106\u202010\u3011], emotion recognition [\u3010107\u202011\u3011,\u3010108\u202012\u3011,\u3010109\u202013\u3011], disease and cancer recognition [\u3010110\u202014\u3011] as well as in text classification and sentiment analysis [\u3010111\u202015\u3011,\u3010112\u202016\u3011,\u3010113\u202017\u3011]. Indeed, in-text classification and sentiment analysis, deep neural networks have demonstrated quite remarkable performance [\u3010114\u202018\u3011]. Despite their interpretability disadvantages and their computational cost, deep neural networks can model complex nonlinear relationships. The extra layers enable the composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network [\u3010115\u202019\u3011].\n\nAttention is arguably one of the most powerful recent concepts in deep learning, bringing out many breakthroughs in the field and improving the performance of neural networks [\u3010116\u202020\u3011]. An attention mechanism effectively gives a model the ability to \u201cattend to\u201d a certain part of the input sequence, which would arguably be a part with higher importance [\u3010117\u202021\u3011]. For example, it allows an LSTM to focus on specific parts of a docum","pub_date":null,"extra":null}],"original_query":null},"command":"mclick","args":["[1, 4, 8, 12, 14]"],"status":"finished","is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","parent_id":"30e28ff0-04b0-44a2-aff0-5392cd032139","request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"30e28ff0-04b0-44a2-aff0-5392cd032139","children":["781da699-fafa-4c89-967f-bd1a6a66701a"]},"781da699-fafa-4c89-967f-bd1a6a66701a":{"id":"781da699-fafa-4c89-967f-bd1a6a66701a","message":{"id":"781da699-fafa-4c89-967f-bd1a6a66701a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1716773802.458673,"update_time":null,"content":{"content_type":"text","parts":["Recent research has explored the integration of attention mechanisms into various types of machine learning models, enhancing their performance across multiple domains. Here are some key areas and findings:\n\n1. **Recurrent Neural Networks (RNNs) and Long Short-Term Memory Networks (LSTMs)**:\n   - **Seq2Seq Models**: Attention mechanisms have been successfully integrated into sequence-to-sequence models for tasks like machine translation, where they help the model focus on relevant parts of the input sequence during the generation of the output sequence\u301012\u2020source\u3011.\n   - **Time Series Forecasting**: Combining attention mechanisms with LSTMs has shown significant improvements in forecasting accuracy by enabling the model to focus on important time steps within the sequence\u301011\u2020source\u3011.\n\n2. **Convolutional Neural Networks (CNNs)**:\n   - **Image Classification and Object Detection**: Attention mechanisms help CNNs focus on the most relevant parts of an image, which is particularly useful in image classification and object detection tasks\u301012\u2020source\u3011.\n   - **Image Captioning**: Attention mechanisms in CNN-based image captioning models allow the model to focus on different parts of an image while generating each word of the caption\u301012\u2020source\u3011.\n\n3. **Graph Neural Networks (GNNs)**:\n   - **Node and Graph Classification**: Attention mechanisms have been applied to GNNs to weigh the importance of different nodes or edges, improving performance in tasks such as node classification and link prediction\u301012\u2020source\u3011.\n\n4. **Multimodal Models**:\n   - **Text and Image Fusion**: Attention mechanisms facilitate the integration of information from different modalities, such as text and images, by aligning and fusing features from each modality effectively\u301011\u2020source\u3011\u301012\u2020source\u3011.\n   - **Audio-Visual Models**: In models handling both audio and visual data, attention mechanisms help align audio features with visual cues, improving performance in tasks like lip-reading and video analysis\u301011\u2020source\u3011.\n\n5. **Reinforcement Learning (RL)**:\n   - **Policy and Value Networks**: Attention mechanisms in RL can help focus on the most relevant parts of the state space, improving decision-making processes. This integration is still an emerging area, with ongoing research exploring various ways to apply attention to RL models\u301012\u2020source\u3011.\n\nThese studies highlight the versatility and effectiveness of attention mechanisms in enhancing different types of machine learning models. By focusing on the most relevant parts of the input data, attention mechanisms help models achieve better performance across a variety of tasks and domains\u301011\u2020source\u3011\u301012\u2020source\u3011\u301013\u2020source\u3011."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"citations":[{"start_ix":558,"end_ix":569,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":780,"end_ix":791,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","text":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":1047,"end_ix":1058,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":1242,"end_ix":1253,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":1511,"end_ix":1522,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":1760,"end_ix":1771,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","text":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":1771,"end_ix":1782,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":1992,"end_ix":2003,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","text":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":2324,"end_ix":2335,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":2632,"end_ix":2643,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","text":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","pub_date":null,"extra":{"cited_message_idx":11,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":2643,"end_ix":2654,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","text":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","pub_date":null,"extra":{"cited_message_idx":12,"search_result_idx":null,"evidence_text":"source"}}},{"start_ix":2654,"end_ix":2665,"citation_format_type":"tether_og","metadata":{"type":"webpage","title":"Applied Sciences | Free Full-Text | Examining Attention Mechanisms in Deep Learning Models for Sentiment Analysis","url":"https://www.mdpi.com/2076-3417/11/9/3883","text":"\n#  Examining Attention Mechanisms in Deep Learning Models for Sentiment Analysis \n\nby \n\nSpyridon Kardakis\n\n[Image 0]Spyridon Kardakis\n\n\u301069\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301070\u2020 Scilit \u2020scilit.net\u3011\u301071\u2020 Preprints.org \u2020www.preprints.org\u3011\u301055\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1[Image 1: ORCID], \n\nIsidoros Perikos\n\n[Image 2]Isidoros Perikos\n\n\u301072\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301073\u2020 Scilit \u2020scilit.net\u3011\u301074\u2020 Preprints.org \u2020www.preprints.org\u3011\u301056\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1,2,*[Image 3: ORCID], \n\nFoteini Grivokostopoulou\n\n[Image 4]Foteini Grivokostopoulou\n\n\u301075\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301076\u2020 Scilit \u2020scilit.net\u3011\u301077\u2020 Preprints.org \u2020www.preprints.org\u3011\u301057\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1,2[Image 5: ORCID] and \n\nIoannis Hatzilygeroudis\n\n[Image 6]Ioannis Hatzilygeroudis\n\n\u301078\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301079\u2020 Scilit \u2020scilit.net\u3011\u301080\u2020 Preprints.org \u2020www.preprints.org\u3011\u301058\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1\n\n^{1}\n\nComputer Engineering and Informatics Department, University of Patras, 26504 Patras, Greece\n\n^{2}\n\nComputer Technology Institute and Press \u201cDiophantus\u201d, 26504 Patras, Greece\n\n*\n\nAuthor to whom correspondence should be addressed. \n\nAppl. Sci. 2021, 11(9), 3883; \u301081\u2020https://doi.org/10.3390/app11093883\u2020doi.org\u3011 \n\nSubmission received: 2 April 2021 / Revised: 21 April 2021 / Accepted: 23 April 2021 / Published: 25 April 2021\n\n(This article belongs to the Section \u301082\u2020Computing and Artificial Intelligence\u3011)  \n\nDownload keyboard_arrow_down\n\n \u301067\u2020Download PDF\u3011   \nDownload PDF with Cover   \nDownload XML   \n\u301083\u2020Download Epub\u3011   \n\nBrowse Figures \n\n \u301084\u2020 \u2020pub.mdpi-res.com\u3011 \u301085\u2020 \u2020pub.mdpi-res.com\u3011 \u301086\u2020 \u2020pub.mdpi-res.com\u3011 \u301087\u2020 \u2020pub.mdpi-res.com\u3011 \u301088\u2020 \u2020pub.mdpi-res.com\u3011 \u301089\u2020 \u2020pub.mdpi-res.com\u3011 \n\n\u301090\u2020Versions Notes\u3011 \n\n## Abstract\n\n:\n\nAttention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study attention-based models built on recurrent neural networks (RNNs) and examine their performance in various contexts of sentiment analysis. Self-attention, global-attention and hierarchical-attention methods are examined under various deep neural models, training methods and hyperparameters. Even though attention mechanisms are a powerful recent concept in the field of deep learning, their exact effectiveness in sentiment analysis is yet to be thoroughly assessed. A comparative analysis is performed in a text sentiment classification task where baseline models are compared with and without the use of attention for every experiment. The experimental study additionally examines the proposed models\u2019 ability in recognizing opinions and emotions in movie reviews. The results indicate that attention-based models lead to great improvements in the performance of deep neural models showcasing up to a 3.5% improvement in their accuracy.\n\nKeywords: \n\n\u301091\u2020attention mechanism\u3011; \u301092\u2020deep neural networks\u3011; \u301093\u2020global-attention\u3011; \u301094\u2020hierarchical-attention\u3011; \u301095\u2020self-attention\u3011; \u301096\u2020sentiment analysis\u3011\n\n##  1. Introduction \n\nIn the past decade, the dramatic decrease in computation cost and the drastic increase in data availability led to the emergence of a new sub-field of machine learning, called deep learning, which outperforms its predecessor by achieving very high performance on large data. Deep learning, utilizes deep artificial neural networks, such as CNNs, RNNs [\u301097\u20201\u3011], LSTMs [\u301098\u20202\u3011] and GRUs [\u301099\u20203\u3011], which achieve remarkable performance in various domains, such as speech recognition [\u3010100\u20204\u3011,\u3010101\u20205\u3011,\u3010102\u20206\u3011], signal and EEG analysis [\u3010103\u20207\u3011], computer vision [\u3010104\u20208\u3011,\u3010105\u20209\u3011,\u3010106\u202010\u3011], emotion recognition [\u3010107\u202011\u3011,\u3010108\u202012\u3011,\u3010109\u202013\u3011], disease and cancer recognition [\u3010110\u202014\u3011] as well as in text classification and sentiment analysis [\u3010111\u202015\u3011,\u3010112\u202016\u3011,\u3010113\u202017\u3011]. Indeed, in-text classification and sentiment analysis, deep neural networks have demonstrated quite remarkable performance [\u3010114\u202018\u3011]. Despite their interpretability disadvantages and their computational cost, deep neural networks can model complex nonlinear relationships. The extra layers enable the composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network [\u3010115\u202019\u3011].\n\nAttention is arguably one of the most powerful recent concepts in deep learning, bringing out many breakthroughs in the field and improving the performance of neural networks [\u3010116\u202020\u3011]. An attention mechanism effectively gives a model the ability to \u201cattend to\u201d a certain part of the input sequence, which would arguably be a part with higher importance [\u3010117\u202021\u3011]. For example, it allows an LSTM to focus on specific parts of a docum","pub_date":null,"extra":{"cited_message_idx":13,"search_result_idx":null,"evidence_text":"source"}}}],"gizmo_id":null,"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"message_type":null,"model_slug":"gpt-4o","default_model_slug":"gpt-4o","pad":"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA","parent_id":"6ae80474-64f2-47dc-b02d-3024cdf5641c","request_id":"88a2461b5f2087a7-SIN","timestamp_":"absolute","content_references":[{"matched_text":"\u301012\u2020source\u3011","start_idx":558,"end_idx":569,"alt":null,"type":"webpage_extended","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","pub_date":null,"snippet":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","attribution":"ar5iv.org","icon_type":null},{"matched_text":"\u301011\u2020source\u3011","start_idx":780,"end_idx":791,"alt":null,"type":"webpage_extended","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","pub_date":null,"snippet":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","attribution":"mdpi.com","icon_type":null},{"matched_text":"\u301012\u2020source\u3011","start_idx":1047,"end_idx":1058,"alt":null,"type":"webpage_extended","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","pub_date":null,"snippet":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","attribution":"ar5iv.org","icon_type":null},{"matched_text":"\u301012\u2020source\u3011","start_idx":1242,"end_idx":1253,"alt":null,"type":"webpage_extended","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","pub_date":null,"snippet":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","attribution":"ar5iv.org","icon_type":null},{"matched_text":"\u301012\u2020source\u3011","start_idx":1511,"end_idx":1522,"alt":null,"type":"webpage_extended","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","pub_date":null,"snippet":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","attribution":"ar5iv.org","icon_type":null},{"matched_text":"\u301011\u2020source\u3011","start_idx":1760,"end_idx":1771,"alt":null,"type":"webpage_extended","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","pub_date":null,"snippet":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","attribution":"mdpi.com","icon_type":null},{"matched_text":"\u301012\u2020source\u3011","start_idx":1771,"end_idx":1782,"alt":null,"type":"webpage_extended","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","pub_date":null,"snippet":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","attribution":"ar5iv.org","icon_type":null},{"matched_text":"\u301011\u2020source\u3011","start_idx":1992,"end_idx":2003,"alt":null,"type":"webpage_extended","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","pub_date":null,"snippet":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","attribution":"mdpi.com","icon_type":null},{"matched_text":"\u301012\u2020source\u3011","start_idx":2324,"end_idx":2335,"alt":null,"type":"webpage_extended","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","pub_date":null,"snippet":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","attribution":"ar5iv.org","icon_type":null},{"matched_text":"\u301011\u2020source\u3011","start_idx":2632,"end_idx":2643,"alt":null,"type":"webpage_extended","title":"Smart Cities | Free Full-Text | Multivariate Time-Series Forecasting: A Review of Deep Learning Methods in Internet of Things Applications to Smart Cities","url":"https://www.mdpi.com/2624-6511/6/5/114","pub_date":null,"snippet":"\nBeyond their technical implications, the implementation of such technologies brings profound socioeconomic and environmental outcomes for cities and their residents [\u3010113\u202015\u3011]. Indicatively, it can foster economic growth by attracting talented individuals and entrepreneurs, potentially turning cities into innovation hubs, which, in turn, can lead to job creation and increased economic competitiveness [\u3010114\u202016\u3011]. As smart cities become more prosperous through economic growth, healthcare and education become more accessible and more inclusive, which results in more engaged and empowered citizens, contributing to social cohesion and overall well-being [\u3010115\u202017\u3011]. Moreover, AI-driven efficiency improvements in resource management can make cities more environmentally sustainable, addressing global challenges such as climate change [\u3010116\u202018\u3011].\n\nThere have been several surveys around deep learning for time-series forecasting, both in theoretical [\u3010103\u20205\u3011] and experimental [\u3010117\u202019\u3011] contexts. Looking at smart cities, deep learning has been used in various domains, but since this is still an emerging application area, only a few surveys have studied the current state-of-the-art models. Many of these, such as [\u3010118\u202020\u3011,\u3010119\u202021\u3011], describe deep learning as part of a broader view of machine learning approaches and examine a limited number of models. Other studies focusing on deep learning methods consider a wide set of data types, such as text and/or images [\u3010120\u202022\u3011,\u3010121\u202023\u3011,\u3010122\u202024\u3011,\u3010123\u202025\u3011], or address tasks beyond forecasting (e.g., classification), thus not providing a comprehensive overview on time-series forecasting in IoT-based smart city applications. More importantly, these studies do not include research works pertaining to contemporary deep learning architectures (see \u3010124\u2020Table 1\u3011). This study endeavors to address this gap by providing a more recent and concise overview of how deep learning methods enhance smart cities through the modeling and forecasting of multivariate time-series IoT data.\n\nSmart city applications can be grouped into six major domains [\u3010115\u202017\u3011]: smart environment, smart mobility, smart buildings, smart governance, smart living, and smart economy and people. In this paper, we focus on two domains\u2014smart environment and smart mobility\u2014that have witnessed the extensive adoption of deep learning methods (see \u3010124\u2020Table 1\u3011), analyzing six prominent applications: air-quality, water-quality, and energy-demand management from the smart environment domain, and car park occupancy, traffic-flow monitoring, and passenger flow from the smart mobility domain. We intentionally narrow our scope to provide an in-depth examination of these applications, emphasizing the trends and dynamics of deep learning models for time-series forecasting within the context of smart cities.\n\nThe remainder of this paper is structured as follows: Section 2 introduces existing advances in deep learning algorithms for multivariate time-series forecasting, encompassing recurrent neural networks (RNNs), convolutional neural networks (CNNs), the attention mechanism, graph neural networks (GNNs), and their combinations. Section 3 delves into the applications of these models in the smart environment and smart mobility domains, exploring their characteristics, popularity, and performance. Section 4 highlights the challenges and limitations present in the existing literature, each paving the way for a discussion on future directions. Finally, Section 5 concludes with a summary of our study and offers insights into emerging trends in state-of-the-art deep learning models, considering their applicability to the reviewed domains.\n\n##  2. Deep Learning Architectures for Multivariate Time-Series Forecasting \n\nDeep learning architectures model complex relationships through a series of nonlinear layers\u2014the set of nodes of each intermediate layer capturing the corresponding feature representation of the input [\u3010125\u202026\u3011]. In a time-series context, these feature representations correspond to relevant temporal information up to that point in time, encoded into some latent variable at each step. In the final layer, the very last encoding is used to make a forecast. In this section, the most common types of deep learning building blocks for multivariate time-series forecasting are outlined.\n\n####  2.1. Recurrent Neural Networks \n\nRecurrent neural networks have a long and well-established history when it comes to time-series forecasting [\u3010126\u202027\u3011] that continues to date. The core building block of RNNs is the RNN cells that essentially act as an internal memory state. Their purpose is to maintain a condensed summary of past information deemed useful by the network for forecasting future values. At each time step, the network is presented with fresh observations, and cells are updated accordingly with new information. The standard structure of an RNN and its unfolded-in-time version are shown in \u3010127\u2020Figure 1\u3011. In the case of multivariate time series, the inputs x and outputs t are multidimensional in each of the time steps.\n\nOlder versions of RNNs were notorious for failing to adequately capture long-term dependencies, a problem commonly known as \u2019exploding and vanishing gradients\u2019 [\u3010128\u202029\u3011]. More specifically, the lack of restriction on their look-back window range meant that the RNNs cells were unable to contain all the relevant information from the beginning of the sequence [\u3010129\u202030\u3011]. The advent of long short-term memory networks (LSTMs) [\u3010130\u202031\u3011] and other closely related variants, such as the gated recurrent units (GRUs [\u3010131\u202032\u3011]), largely alleviated this problem, by allowing the gradients to flow more stably within the network. In \u3010132\u2020Figure 2\u3011, the different cells used by the LSTMs and GRUs are displayed.\n\nAnother shortcoming of conventional RNNs was the inability to make use of future time steps. To overcome this limitation, a new type of architecture, bidirectional RNNs (BiRNNs), was proposed by Schuster and Paliwal [\u3010133\u202034\u3011]. The novelty of BiRNNs","attribution":"mdpi.com","icon_type":null},{"matched_text":"\u301012\u2020source\u3011","start_idx":2643,"end_idx":2654,"alt":null,"type":"webpage_extended","title":"[2203.14263] A General Survey on Attention Mechanisms in Deep Learning","url":"https://ar5iv.org/pdf/2203.14263","pub_date":null,"snippet":"\n  (4)  \n\nAs illustrated in Fig. 1, the context vector is then used in the output model to create the output . This output model translates the context vector into an output prediction. For example, it could be a simple softmax layer that takes as input the context vector , as shown in (\u301035\u20205\u3011).\n\n  (5)  \n\nwhere is the number of output choices or classes, and and are trainable weights.\n\n###  2.3 Attention Applications\n\nAttention is a rather general mechanism that can be used in a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the problem of image classification using a basic CNN model. While an RNN produces a sequence of hidden state vectors, a CNN creates feature maps, where each region in the image is represented by a feature vector. The RNN hidden states are organized sequentially, while the CNN feature maps are organized spatially. Yet, attention can still be applied in both situations, since the attention mechanism does not inherently depend on the organization of the feature vectors. This characteristic makes attention easy to implement in a wide variety of models in different domains.\n\nAnother domain where attention can be applied is audio processing [\u301036\u202024\u3011, \u301037\u202025\u3011]. Acoustic sequences can be represented by a sequence of feature vectors that relate to certain time periods of the audio sample. These vectors could simply be the raw input audio, or they can be extracted via, for example, an RNN or CNN. Video processing is another domain where attention can be applied intuitively [\u301038\u202026\u3011, \u301039\u202027\u3011]. Video data consists of sequences of images, so attention can be applied to the individual images, as well as the entire sequence. Recommender systems often incorporate a user\u2019s interaction history to produce recommendations. Feature vectors can be extracted based on, for example, the id\u2019s or other characteristics of the products the user interacted with, and attention can be applied to them [\u301040\u202028\u3011]. Attention can generally also be applied to many problems that use a time series as input, be it medical [\u301041\u202029\u3011], financial [\u301042\u202030\u3011], or anything else, as long as feature vectors can be extracted.\n\nThe fact that attention does not rely on the organization of the feature vectors allows it to be applied to various problems that each use data with different structures, as illustrated by the previous domain examples. Yet, this can be taken even further by applying attention to data where there is irregular structure. For example, protein structures, city traffic flows, and communication networks cannot always be represented using neatly structured organizations, such as sequences, like time series, or grids, like images. In such cases, the different aspects of the data are often represented as nodes in a graph. These nodes can be represented by feature vectors, meaning that attention can be applied in domains that use graph-structured data as well [\u301043\u202031\u3011, \u301018\u202019\u3011].\n\nIn general, attention can be applied to any problem for which a set of feature vectors can be defined or extracted. As such, the general attention model presented in Fig. 2 is applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications and extensions of the general attention module. As such, in Section \u301023\u20203\u3011, a comprehensive overview is provided of a collection of different attention mechanisms.\n\n[Image 2: Refer to caption] Figure 3: A taxonomy of attention mechanisms.  \n\n##  3 Attention Taxonomy\n\nThere are many different types of attention mechanisms and extensions, and a model can use different combinations of these attention techniques. As such, we propose a taxonomy that can be used to classify different types of attention mechanisms. Fig. 3 provides a visual overview of the different categories and subcategories that the attention mechanisms can be organized in. The three major categories are based on whether an attention technique is designed to handle specific types of feature vectors (feature-related), specific types of model queries (query-related), or whether it is simply a general mechanism that is related to neither the feature model, nor the query model (general). Further explanations of these categories and their subcategories are provided in the following subsections. Each mechanism discussed in this section is either a modification to the existing inner mechanisms of the general attention module presented in Section \u301022\u20202\u3011, or an extension of it.\n\nThe presented taxonomy can also be used to analyze the architecture of attention models. Namely, the major categories and their subcategories can be interpreted as orthogonal dimensions of an attention model. An attention model can consist of a combination of techniques taken from any or all categories. Some characteristics, such as the scoring and alignment functions, are generally required for any attention model. Other mechanisms, such as multi-head attention or co-attention are not necessary in every situation. Lastly, in Table \u301044\u2020I\u3011, an overview of used notation with corresponding descriptions is provided.\n\n TABLE I: Notation.\n\n   Symbol   Description  \n    Matrix of size containing the feature vectors as columns. These feature vectors are extracted by the feature model.  \n    Matrix of size containing the key vectors as columns. These vectors are used to calculate the attention scores.  \n    Matrix of size containing the value vectors as columns. These vectors are used to calculate the context vector.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Weights matrix of size used to create the matrix from the matrix.  \n    Query vector of size . This vector essentially represents a question, and is used to calculate the attention scores.  \n    Context vector of size . This vector is the output of the attention model.  \n    Score vector of size containing the attention scores . These are used to calculate the attention weights.  \n    Attention weights vector of size containing the attention weights . These are the weights used in the calculation of the context vector.  \n\n###","attribution":"ar5iv.org","icon_type":null},{"matched_text":"\u301013\u2020source\u3011","start_idx":2654,"end_idx":2665,"alt":null,"type":"webpage_extended","title":"Applied Sciences | Free Full-Text | Examining Attention Mechanisms in Deep Learning Models for Sentiment Analysis","url":"https://www.mdpi.com/2076-3417/11/9/3883","pub_date":null,"snippet":"\n#  Examining Attention Mechanisms in Deep Learning Models for Sentiment Analysis \n\nby \n\nSpyridon Kardakis\n\n[Image 0]Spyridon Kardakis\n\n\u301069\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301070\u2020 Scilit \u2020scilit.net\u3011\u301071\u2020 Preprints.org \u2020www.preprints.org\u3011\u301055\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1[Image 1: ORCID], \n\nIsidoros Perikos\n\n[Image 2]Isidoros Perikos\n\n\u301072\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301073\u2020 Scilit \u2020scilit.net\u3011\u301074\u2020 Preprints.org \u2020www.preprints.org\u3011\u301056\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1,2,*[Image 3: ORCID], \n\nFoteini Grivokostopoulou\n\n[Image 4]Foteini Grivokostopoulou\n\n\u301075\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301076\u2020 Scilit \u2020scilit.net\u3011\u301077\u2020 Preprints.org \u2020www.preprints.org\u3011\u301057\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1,2[Image 5: ORCID] and \n\nIoannis Hatzilygeroudis\n\n[Image 6]Ioannis Hatzilygeroudis\n\n\u301078\u2020 SciProfiles \u2020sciprofiles.com\u3011\u301079\u2020 Scilit \u2020scilit.net\u3011\u301080\u2020 Preprints.org \u2020www.preprints.org\u3011\u301058\u2020 Google Scholar \u2020scholar.google.com\u3011\n\n1\n\n^{1}\n\nComputer Engineering and Informatics Department, University of Patras, 26504 Patras, Greece\n\n^{2}\n\nComputer Technology Institute and Press \u201cDiophantus\u201d, 26504 Patras, Greece\n\n*\n\nAuthor to whom correspondence should be addressed. \n\nAppl. Sci. 2021, 11(9), 3883; \u301081\u2020https://doi.org/10.3390/app11093883\u2020doi.org\u3011 \n\nSubmission received: 2 April 2021 / Revised: 21 April 2021 / Accepted: 23 April 2021 / Published: 25 April 2021\n\n(This article belongs to the Section \u301082\u2020Computing and Artificial Intelligence\u3011)  \n\nDownload keyboard_arrow_down\n\n \u301067\u2020Download PDF\u3011   \nDownload PDF with Cover   \nDownload XML   \n\u301083\u2020Download Epub\u3011   \n\nBrowse Figures \n\n \u301084\u2020 \u2020pub.mdpi-res.com\u3011 \u301085\u2020 \u2020pub.mdpi-res.com\u3011 \u301086\u2020 \u2020pub.mdpi-res.com\u3011 \u301087\u2020 \u2020pub.mdpi-res.com\u3011 \u301088\u2020 \u2020pub.mdpi-res.com\u3011 \u301089\u2020 \u2020pub.mdpi-res.com\u3011 \n\n\u301090\u2020Versions Notes\u3011 \n\n## Abstract\n\n:\n\nAttention-based methods for deep neural networks constitute a technique that has attracted increased interest in recent years. Attention mechanisms can focus on important parts of a sequence and, as a result, enhance the performance of neural networks in a variety of tasks, including sentiment analysis, emotion recognition, machine translation and speech recognition. In this work, we study attention-based models built on recurrent neural networks (RNNs) and examine their performance in various contexts of sentiment analysis. Self-attention, global-attention and hierarchical-attention methods are examined under various deep neural models, training methods and hyperparameters. Even though attention mechanisms are a powerful recent concept in the field of deep learning, their exact effectiveness in sentiment analysis is yet to be thoroughly assessed. A comparative analysis is performed in a text sentiment classification task where baseline models are compared with and without the use of attention for every experiment. The experimental study additionally examines the proposed models\u2019 ability in recognizing opinions and emotions in movie reviews. The results indicate that attention-based models lead to great improvements in the performance of deep neural models showcasing up to a 3.5% improvement in their accuracy.\n\nKeywords: \n\n\u301091\u2020attention mechanism\u3011; \u301092\u2020deep neural networks\u3011; \u301093\u2020global-attention\u3011; \u301094\u2020hierarchical-attention\u3011; \u301095\u2020self-attention\u3011; \u301096\u2020sentiment analysis\u3011\n\n##  1. Introduction \n\nIn the past decade, the dramatic decrease in computation cost and the drastic increase in data availability led to the emergence of a new sub-field of machine learning, called deep learning, which outperforms its predecessor by achieving very high performance on large data. Deep learning, utilizes deep artificial neural networks, such as CNNs, RNNs [\u301097\u20201\u3011], LSTMs [\u301098\u20202\u3011] and GRUs [\u301099\u20203\u3011], which achieve remarkable performance in various domains, such as speech recognition [\u3010100\u20204\u3011,\u3010101\u20205\u3011,\u3010102\u20206\u3011], signal and EEG analysis [\u3010103\u20207\u3011], computer vision [\u3010104\u20208\u3011,\u3010105\u20209\u3011,\u3010106\u202010\u3011], emotion recognition [\u3010107\u202011\u3011,\u3010108\u202012\u3011,\u3010109\u202013\u3011], disease and cancer recognition [\u3010110\u202014\u3011] as well as in text classification and sentiment analysis [\u3010111\u202015\u3011,\u3010112\u202016\u3011,\u3010113\u202017\u3011]. Indeed, in-text classification and sentiment analysis, deep neural networks have demonstrated quite remarkable performance [\u3010114\u202018\u3011]. Despite their interpretability disadvantages and their computational cost, deep neural networks can model complex nonlinear relationships. The extra layers enable the composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network [\u3010115\u202019\u3011].\n\nAttention is arguably one of the most powerful recent concepts in deep learning, bringing out many breakthroughs in the field and improving the performance of neural networks [\u3010116\u202020\u3011]. An attention mechanism effectively gives a model the ability to \u201cattend to\u201d a certain part of the input sequence, which would arguably be a part with higher importance [\u3010117\u202021\u3011]. For example, it allows an LSTM to focus on specific parts of a docum","attribution":"mdpi.com","icon_type":null}]},"recipient":"all","channel":null},"parent":"6ae80474-64f2-47dc-b02d-3024cdf5641c","children":[]}}}