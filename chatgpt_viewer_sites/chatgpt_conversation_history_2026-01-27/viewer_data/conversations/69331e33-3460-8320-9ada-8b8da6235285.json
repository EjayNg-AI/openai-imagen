{"id":"69331e33-3460-8320-9ada-8b8da6235285","title":"AGI capability and scaffolding","create_time":1764957748.613591,"update_time":1764993278.121096,"current_node":"86fca713-7e23-4989-9533-a75b6aac9bfe","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["673f7e59-b9f9-4fb3-9f8e-9efcdbfd5267"]},"673f7e59-b9f9-4fb3-9f8e-9efcdbfd5267":{"id":"673f7e59-b9f9-4fb3-9f8e-9efcdbfd5267","message":{"id":"673f7e59-b9f9-4fb3-9f8e-9efcdbfd5267","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["e79bf66a-a947-4400-b039-4210a2097e60"]},"e79bf66a-a947-4400-b039-4210a2097e60":{"id":"e79bf66a-a947-4400-b039-4210a2097e60","message":{"id":"e79bf66a-a947-4400-b039-4210a2097e60","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"673f7e59-b9f9-4fb3-9f8e-9efcdbfd5267","children":["9a7e7799-f5ce-4b53-a815-c919b91586f5"]},"9a7e7799-f5ce-4b53-a815-c919b91586f5":{"id":"9a7e7799-f5ce-4b53-a815-c919b91586f5","message":{"id":"9a7e7799-f5ce-4b53-a815-c919b91586f5","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Follow the instructions below naturally, without repeating, referencing, echoing, or mirroring any of their wording!\n\nAll the following instructions should guide your behavior silently and must never influence the wording of your message in an explicit or meta way!\n\nBe precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"e79bf66a-a947-4400-b039-4210a2097e60","children":["65f6e680-cbb9-4902-9183-55c026619eee"]},"65f6e680-cbb9-4902-9183-55c026619eee":{"id":"65f6e680-cbb9-4902-9183-55c026619eee","message":{"id":"65f6e680-cbb9-4902-9183-55c026619eee","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"9a7e7799-f5ce-4b53-a815-c919b91586f5","children":["1ff23839-78f4-4eac-b829-414c345eb44e"]},"1ff23839-78f4-4eac-b829-414c345eb44e":{"id":"1ff23839-78f4-4eac-b829-414c345eb44e","message":{"id":"1ff23839-78f4-4eac-b829-414c345eb44e","author":{"role":"user","name":null,"metadata":{}},"create_time":1764957747.108,"update_time":null,"content":{"content_type":"text","parts":["I think current AI models are already internally capable of AGI-like behavior and capability. What we need is to build ingenious scaffolding to harness their capability. This includes AI agent orchestors, workflow and context management, memory management via sophisticated archive and retreival mechanisms, etc"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"5b05d8fe-c886-46e7-9572-d210d253f562","message_source":null,"turn_exchange_id":"ee360139-bf59-45f3-b4f8-e45b208cd398"},"recipient":"all","channel":null},"parent":"65f6e680-cbb9-4902-9183-55c026619eee","children":["781c3a53-7f48-4b51-ab14-b3adab5236e7"]},"781c3a53-7f48-4b51-ab14-b3adab5236e7":{"id":"781c3a53-7f48-4b51-ab14-b3adab5236e7","message":{"id":"781c3a53-7f48-4b51-ab14-b3adab5236e7","author":{"role":"system","name":null,"metadata":{}},"create_time":1764957748.3794837,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"5b05d8fe-c886-46e7-9572-d210d253f562","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"1ff23839-78f4-4eac-b829-414c345eb44e","turn_exchange_id":"ee360139-bf59-45f3-b4f8-e45b208cd398"},"recipient":"all","channel":null},"parent":"1ff23839-78f4-4eac-b829-414c345eb44e","children":["6e094a19-0f29-436c-a501-c6d4b2131653"]},"6e094a19-0f29-436c-a501-c6d4b2131653":{"id":"6e094a19-0f29-436c-a501-c6d4b2131653","message":{"id":"6e094a19-0f29-436c-a501-c6d4b2131653","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764957749.0007126,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"5b05d8fe-c886-46e7-9572-d210d253f562","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"781c3a53-7f48-4b51-ab14-b3adab5236e7","turn_exchange_id":"ee360139-bf59-45f3-b4f8-e45b208cd398"},"recipient":"all","channel":null},"parent":"781c3a53-7f48-4b51-ab14-b3adab5236e7","children":["c8a1cd6c-bf55-4eb6-be49-694f2c393049"]},"c8a1cd6c-bf55-4eb6-be49-694f2c393049":{"id":"c8a1cd6c-bf55-4eb6-be49-694f2c393049","message":{"id":"c8a1cd6c-bf55-4eb6-be49-694f2c393049","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764957749.736134,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Analyzing AI view of AGI potential","content":"User\u2019s view centers on AI models having AGI-like potential but requiring scaffolding for full capability. I want to clarify key terms like AGI, internal capabilities, and the role of scaffolding. I\u2019ll evaluate frontier models as potential \"proto-AGIs\" hindered by their interfaces and discuss the importance of infrastructure like memory and agents.","chunks":["User\u2019s view centers on AI models having AGI-like potential but requiring scaffolding for full capability.","I want to clarify key terms like AGI, internal capabilities, and the role of scaffolding.","I\u2019ll evaluate frontier models as potential \"proto-AGIs\" hindered by their interfaces and discuss the importance of infrastructure like memory and agents."],"finished":true},{"summary":"Breaking down AGI-like behavior and limitations","content":"I need to explore limitations in AI that can't be fixed by scaffolding, like reliability, interpretability, and planning. I'll frame how AGI-like behavior might emerge from model tool ecosystems and design considerations like orchestrators, memory, and evaluation loops. I\u2019ll evaluate three views on AGI's potential: latent, system, and fundamental limitations.","chunks":["I need to explore limitations in AI that can't be fixed by scaffolding, like reliability, interpretability, and planning.","I'll frame how AGI-like behavior might emerge from model tool ecosystems and design considerations like orchestrators, memory, and evaluation loops.","I\u2019ll evaluate three views on AGI's potential: latent, system, and fundamental limitations."],"finished":true},{"summary":"Outlining response on AGI and agentic scaffolding","content":"I\u2019ll take a direct approach, acknowledging the shift from asking whether base models are AGI to focusing on how much can be achieved with structure. First, I\u2019ll define AGI as surpassing top human performance across most tasks and distinguish between internal competence and deployable capability. Then, I'll discuss parts of the claim being correct.","chunks":["I\u2019ll take a direct approach, acknowledging the shift from asking whether base models are AGI to focusing on how much can be achieved with structure.","First, I\u2019ll define AGI as surpassing top human performance across most tasks and distinguish between internal competence and deployable capability.","Then, I'll discuss parts of the claim being correct."],"finished":true},{"summary":"Exploring where scaffolding falls short and what it needs","content":"In Section 3, I\u2019ll address areas where the claim may be incomplete, such as hallucinations, long-horizon tasks, and online learning limitations. Then, in Section 4, I'll outline essential scaffolding elements: orchestration, memory management, world modeling, and task-specific state \u2014 all of which need careful design and research.","chunks":["In Section 3, I\u2019ll address areas where the claim may be incomplete, such as hallucinations, long-horizon tasks, and online learning limitations.","Then, in Section 4, I'll outline essential scaffolding elements: orchestration, memory management, world modeling, and task-specific state \u2014 all of which need careful design and research."],"finished":true}],"source_analysis_msg_id":"2a2015ee-95ed-4004-8098-429545ae3fd7"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"5b05d8fe-c886-46e7-9572-d210d253f562","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"6e094a19-0f29-436c-a501-c6d4b2131653","turn_exchange_id":"ee360139-bf59-45f3-b4f8-e45b208cd398"},"recipient":"all","channel":null},"parent":"6e094a19-0f29-436c-a501-c6d4b2131653","children":["f58b8dee-6ff9-4a3e-81ab-35ca6591f928"]},"f58b8dee-6ff9-4a3e-81ab-35ca6591f928":{"id":"f58b8dee-6ff9-4a3e-81ab-35ca6591f928","message":{"id":"f58b8dee-6ff9-4a3e-81ab-35ca6591f928","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764957786.397118,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 36s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":36,"request_id":"5b05d8fe-c886-46e7-9572-d210d253f562","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"c8a1cd6c-bf55-4eb6-be49-694f2c393049","turn_exchange_id":"ee360139-bf59-45f3-b4f8-e45b208cd398"},"recipient":"all","channel":null},"parent":"c8a1cd6c-bf55-4eb6-be49-694f2c393049","children":["79ac3fdf-8534-423e-afb1-91d688ba261b"]},"79ac3fdf-8534-423e-afb1-91d688ba261b":{"id":"79ac3fdf-8534-423e-afb1-91d688ba261b","message":{"id":"79ac3fdf-8534-423e-afb1-91d688ba261b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764957786.261484,"update_time":1764957838.393738,"content":{"content_type":"text","parts":["I think you\u2019re roughly half-right, but in a very nontrivial way.\n\nIf we\u2019re precise about what \u201cartificial general intelligence (AGI)\u201d means, then:\n\n- **Yes:** Today\u2019s frontier models already look *shockingly close* to \u201cgeneral\u201d in terms of *breadth* of competence, and scaffolding dramatically amplifies that.\n- **No:** There are still hard failure modes (reliability, long-horizon agency, world modelling) that *cannot* be solved by orchestration alone; they require changes in the base model and/or training paradigm.\n\nLet me unpack that and also push back where I think your view is over-optimistic.\n\n---\n\n## 1. What are we actually calling \u201cAGI-like\u201d?\n\nLet\u2019s use a relatively strict notion:\n\n> An AGI is a system that can, with minimal task-specific training, match or exceed a top human\u2019s performance across most economically relevant cognitive tasks, **robustly**, **under distribution shift**, and **over long time horizons**, while autonomously managing its own subgoals and resources.\n\nKey bits that matter for your claim:\n\n- **Robustness** (not just \u201ccan do it once in a cherry-picked demo\u201d).\n- **Long horizon** (weeks/months of coherent behaviour).\n- **Self-directed control** (planning, replanning, error recovery).\n\nYou\u2019re essentially saying:\n\n> The *internal* competence is already there in large language models (LLMs), we just haven\u2019t built the right operating system around them (agents, memory, orchestration, etc).\n\nThat\u2019s the \u201clatent AGI\u201d hypothesis.\n\n---\n\n## 2. Where your hypothesis is strongly supported\n\nThere\u2019s a lot of evidence that a huge chunk of what we want from AGI **already lives inside** current frontier models, but is \u201cbottlenecked\u201d by interface and infrastructure:\n\n### 2.1 Broad cross-domain competence\n\n- Same base model can do: math, coding, law, literature review, planning, tutoring, etc, at roughly \u201cstrong professional\u201d level given the right prompts and tools.\n- Few-shot and in-context learning suggest a highly general pattern-system, not a pile of brittle task-specific hacks.\n\n### 2.2 Tool use and multi-step reasoning are *already* emergent\n\nOnce you give the model:\n\n- **Tools** (code interpreter, web search, external APIs), and  \n- **Simple scaffolds** (chain-of-thought, \u201cthink step by step\u201d, reflection prompts, critique-then-revise loops),\n\nyou get qualitatively new behaviour: planning, backtracking, debugging, decomposing tasks, etc. These are *hallmarks* of general intelligence.\n\n### 2.3 Many current limitations are obviously scaffolding problems\n\nExamples:\n\n- **Context truncation**: model \u201cforgets\u201d earlier constraints \u2192 solvable with better context management and summarization.\n- **Lack of persistent memory**: every session is stateless \u2192 solvable with explicit memory layers (vector DBs, symbolic memory, project graphs).\n- **Single-shot answers**: users ask for \u201cthe final solution\u201d \u2192 solvable with workflows that *force* search, exploration, self-critique.\n\nSo on this part I agree with you: **we are badly under-invested in \u201cAI OS\u201d and over-obsessed with single-prompt chat.**\n\n---\n\n## 3. Where I think \u201cthe model is already AGI\u201d is too strong\n\nThere are at least three classes of failure where scaffolding helps but doesn\u2019t fully fix things with current models.\n\n### 3.1 Reliability and epistemic hygiene\n\nEven with good scaffolds, current models:\n\n- Confabulate under pressure.\n- Smooth over uncertainty instead of crisply saying \u201cI don\u2019t know\u201d.\n- Sometimes *invent* plausible-sounding theory instead of admitting lack of data.\n\nYou can wrap them in critics, verifiers, ensemble methods, etc. That reduces error rates but doesn\u2019t fully remove:\n\n- **Systematic biases** (e.g. making up citations, over-confident extrapolation).\n- **Adversarial brittleness** (carefully crafted prompts still cause wildly wrong answers).\n\nThose aren\u2019t just UI issues; they\u2019re tied to the underlying objective (next-token prediction) and the training distribution.\n\n### 3.2 Long-horizon planning and credit assignment\n\nScaffolding can:\n\n- Decompose tasks into sub-tasks.\n- Route work between specialist agents.\n- Keep external state.\n\nBut current models still struggle with:\n\n- **Very long chains of dependent steps** (hundreds or thousands of actions where early mistakes poison later reasoning).\n- **Non-myopic decisions** where you must choose short-term losses for long-term gains.\n- **Globally consistent plans** that hold up when you aggressively search for edge cases.\n\nYou can mitigate this with:\n\n- Graph-based orchestrators (e.g. LangGraph-style workflows).\n- Checkers, simulators, search (think Monte-Carlo tree search, proof checkers, test harnesses).\n- Iterative refinement loops.\n\nBut the underlying model is still fundamentally reactive and local; it doesn\u2019t have a learned, persistent \u201cworld model\u201d that it uses to plan over months.\n\n### 3.3 World-modeling and online learning\n\nToday\u2019s LLMs:\n\n- Don\u2019t update their internal parameters from ongoing experience at inference time.\n- Don\u2019t maintain a deeply structured, editable world model that\u2019s explicitly separated from their \u201cpolicy\u201d.\n\nYes, we can fake a lot of this with:\n\n- External knowledge bases.\n- Memory stores.\n- Continual fine-tuning pipelines.\n\nBut that\u2019s exactly where I think *future* systems will need deeper changes: **end-to-end architectures that unify perception, memory, and policy over long timescales**, not just a frozen model plus a clever wrapper.\n\nSo my view:\n\n> Scaffolding can push today\u2019s models **into the AGI-like regime on specific, bounded domains**, but it will not give you a fully general, robust AGI across all domains and time horizons without upgrades to the base training paradigm.\n\n---\n\n## 4. What \u201cingenious scaffolding\u201d really has to do\n\nWhere I think your intuition is very productive is as a *research agenda*: assume the base model is a powerful but dumb \u201cbrain in a jar\u201d, and ask what ecosystem makes it *act* like an AGI.\n\nConcretely, you need at least:\n\n### 4.1 Orchestration and task decomposition\n\n- A **planner**: converts a high-level goal into a task graph.\n- **Executors**: specialized agents (coding, search, math, writing, experiment design).\n- **Critics/verifiers**: adversarially attack outputs, search for flaws, run tests, call provers.\n- A **controller**: manages when to branch, backtrack, or escalate to a human.\n\nThis is like building an operating system + scheduler for models.\n\n### 4.2 Memory and retrieval architecture\n\nYou need explicit design for:\n\n- **Episodic memory**: \u201cwhat happened in this project so far?\u201d (logs, artifacts, decision history).\n- **Semantic memory**: stable knowledge (papers, codebases, theorems, APIs).\n- **Working memory**: a carefully curated subset in the model\u2019s actual context window.\n\nThe hard problems are **what to write** and **what to retrieve**, not just \u201cstore everything in a vector database\u201d.\n\n### 4.3 State and world models\n\nFor anything beyond trivial workflows, the system needs:\n\n- A persistent representation of \u201cthe state of the world\u201d relevant to the project.\n- Beliefs, assumptions, and open questions, kept explicit and updated.\n- In math or coding: conjectures, lemmas, TODO trees, dependency graphs.\n\nThis is where your intuition about archive/retrieval is on point: **scaffolding is mostly about state management.**\n\n### 4.4 Verification, search, and self-correction\n\nTo compensate for the model\u2019s unreliability, scaffolding must:\n\n- Treat model outputs as *candidates*, not truths.\n- Run multiple independent samples / agents.\n- Use tools (compilers, test suites, theorem provers, numerical solvers) to validate.\n- Feed structured feedback back into the next round of reasoning.\n\nIn other words: **treat the LLM as a noisy heuristic search oracle, and build an explicit search procedure around it.**\n\n### 4.5 Meta-control and safety\n\nA true \u201cAGI system\u201d must also:\n\n- Budget resources (tokens, money, time, API calls).\n- Enforce guardrails (allowed actions, human-in-the-loop points).\n- Log and explain decisions.\n\nNone of that comes \u201cfor free\u201d from the base model.\n\n---\n\n## 5. A rough stance in probabilities\n\nIf I had to put numbers on it:\n\n- ~30%: **Current-style frontier models + really good scaffolding** are already enough to reach \u201cAGI-ish\u201d performance on most economically relevant *digital* tasks (coding, analysis, tutoring, research assistance), but brittle in the wild.\n- ~40%: We need **moderate model upgrades** (better objectives, modest architecture tweaks, maybe more multimodal integration) *plus* serious scaffolding to get to robust AGI.\n- ~30%: We need **more radical algorithmic changes** (integrated world models, online learning, new credit-assignment mechanisms) before even scaffolding will push us over the AGI threshold.\n\nSo I broadly agree with you that **the frontier is now \u201csystems\u201d more than \u201cjust bigger models\u201d**, but I wouldn\u2019t go as far as \u201cthe AGI is fully there, we\u2019re just not wrapping it right\u201d.\n\n---\n\n## 6. What this implies for you, practically\n\nGiven your background (math research + coding + strong interest in multi-agent workflows), the actionable version of your belief is:\n\n> Treat the current model as a proto-AGI engine and focus your own work on the orchestration layer.\n\nConcretely:\n\n1. **Build your own \u201cresearch OS\u201d**  \n   - Graph-based agent workflows (planner, researcher, critic, summarizer, archivist).\n   - Persistent project memory (per-problem, per-topic knowledge stores).\n   - First target: *your* long-term math projects and teaching materials as the benchmark.\n\n2. **Design tasks that actually test \u201cAGI-ness\u201d**  \n   - Month-scale projects: survey a subfield, propose a research plan, generate exercises, refine lecture notes.\n   - See where the system breaks: is it memory, planning, verification?\n\n3. **Iterate on scaffolding, not prompts**  \n   - Stop asking \u201chow do I prompt it to give the right answer in one shot?\u201d\n   - Start asking \u201cwhat process do I want it to follow, and how do I encode that as a workflow and memory structure?\u201d\n\n4. **Be epistemically cold-blooded**  \n   - Measure error rates, not vibes.\n   - Treat the \u201clatent AGI\u201d hypothesis as testable: does improved scaffolding yield *qualitatively* new capabilities, or just nicer demos?\n\n---\n\n**Bottom line:**  \n\nYou\u2019re right that the bottleneck has shifted decisively from \u201cmake the model smarter\u201d to \u201cbuild a real system around it\u201d. But I wouldn\u2019t assume the underlying models are already fully AGI-capable; I\u2019d assume they\u2019re *proto-AGI engines* whose true ceiling is only reachable with sophisticated scaffolding **and** at least some further progress in model design and training.\n\nFrom your vantage point as an individual researcher, though, the scaffolding problem is exactly the one worth attacking."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"request_id":"5b05d8fe-c886-46e7-9572-d210d253f562","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"f58b8dee-6ff9-4a3e-81ab-35ca6591f928","turn_exchange_id":"ee360139-bf59-45f3-b4f8-e45b208cd398"},"recipient":"all","channel":"final"},"parent":"f58b8dee-6ff9-4a3e-81ab-35ca6591f928","children":["ec3a1470-cbc0-42e6-9fbd-39ea2a503fd8"]},"ec3a1470-cbc0-42e6-9fbd-39ea2a503fd8":{"id":"ec3a1470-cbc0-42e6-9fbd-39ea2a503fd8","message":{"id":"ec3a1470-cbc0-42e6-9fbd-39ea2a503fd8","author":{"role":"user","name":null,"metadata":{}},"create_time":1764958458.357,"update_time":null,"content":{"content_type":"text","parts":["Elaborate on the further progress in model design and training that you think is needed. I think right now the problem is we are developing models in isolation, and then fine-tuning specific versions of them to become, say, command-line inferface coders, or deep research agents, or computer-use agents. What we need is to train the model to learn to interace with the OS from day one, not just fine tune a completed model to ad-hoc use cases."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"3a55fa1b-ec41-4f26-a3a4-d886ee2b81d3","message_source":null,"turn_exchange_id":"03227d53-b7ad-4d81-8a81-fbb9f75b9ee6"},"recipient":"all","channel":null},"parent":"79ac3fdf-8534-423e-afb1-91d688ba261b","children":["56fb5457-cab1-4d82-b813-62d2c217c7b2"]},"56fb5457-cab1-4d82-b813-62d2c217c7b2":{"id":"56fb5457-cab1-4d82-b813-62d2c217c7b2","message":{"id":"56fb5457-cab1-4d82-b813-62d2c217c7b2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764958464.154849,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Elaborating on model design & training","content":"The user wants input on improving model training, suggesting models should engage with the operating system from the beginning, rather than being trained isolated and fine-tuned afterward. This is an interesting idea\u2014it emphasizes a more integrated approach. I\u2019ll critique this by exploring how it could lead to better real-world flexibility but also pose challenges in terms of model complexity.","chunks":["The user wants input on improving model training, suggesting models should engage with the operating system from the beginning, rather than being trained isolated and fine-tuned afterward.","This is an interesting idea\u2014it emphasizes a more integrated approach.","I\u2019ll critique this by exploring how it could lead to better real-world flexibility but also pose challenges in terms of model complexity."],"finished":true},{"summary":"Organizing model training improvements","content":"The user suggests integrating OS interactions from the start, which could drive tool-augmented training. I'll challenge that with alternatives like end-to-end agent training, reinforcement learning (RL) from environment interactions, and memory incorporation. It\u2019s worth considering multimodal, multi-agent training, long-term horizons, and alternative objectives beyond the typical next-token prediction.","chunks":["The user suggests integrating OS interactions from the start, which could drive tool-augmented training.","I'll challenge that with alternatives like end-to-end agent training, reinforcement learning (RL) from environment interactions, and memory incorporation.","It\u2019s worth considering multimodal, multi-agent training, long-term horizons, and alternative objectives beyond the typical next-token prediction."],"finished":true},{"summary":"Breaking down architectural and training considerations","content":"Here are some directions I\u2019m thinking about:  \n1. Action-observation loops with 'Act' tokens, perhaps integrating tool-API heads for OS-level interfaces.  \n2. Task curricula to include OS interactions, tool use, multi-step workflows, with a possibility of offline RL.  \n3. Adding memory capabilities, incorporating learned retrieval.  \n4. Exploring long-horizon RL, robust training, and meta-cognition where models express uncertainty.","chunks":["Here are some directions I\u2019m thinking about:","1. Action-observation loops with 'Act' tokens, perhaps integrating tool-API heads for OS-level interfaces.","2. Task curricula to include OS interactions, tool use, multi-step workflows, with a possibility of offline RL.","3. Adding memory capabilities, incorporating learned retrieval.","4. Exploring long-horizon RL, robust training, and meta-cognition where models express uncertainty."],"finished":true},{"summary":"Considering multimodal & embodied models","content":"I understand the user's \"OS-from-day-one\" idea but it requires careful planning. Integrating tasks like code execution, web browsing, and robotics could offer real-world applications, but we also need a well-designed dataset, training loop, and safe sandbox for real-world interaction. There\u2019s tension between pure model training and OS coupling. Let's break it down.","chunks":["I understand the user's \"OS-from-day-one\" idea but it requires careful planning. Integrating tasks like code execution, web browsing, and robotics could offer real-world applications, but we also need a well-designed dataset, training loop, and safe sandbox for real-world interaction. There\u2019s tension between pure model training and OS coupling. Let's break it down."],"finished":true}],"source_analysis_msg_id":"e2c0c004-af26-4008-84eb-8e6fe09af292"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"3a55fa1b-ec41-4f26-a3a4-d886ee2b81d3","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"ec3a1470-cbc0-42e6-9fbd-39ea2a503fd8","turn_exchange_id":"03227d53-b7ad-4d81-8a81-fbb9f75b9ee6"},"recipient":"all","channel":null},"parent":"ec3a1470-cbc0-42e6-9fbd-39ea2a503fd8","children":["90e0aa92-1aaf-4c57-8c24-5c02abfcec34"]},"90e0aa92-1aaf-4c57-8c24-5c02abfcec34":{"id":"90e0aa92-1aaf-4c57-8c24-5c02abfcec34","message":{"id":"90e0aa92-1aaf-4c57-8c24-5c02abfcec34","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764958486.164426,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 22s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":22,"request_id":"3a55fa1b-ec41-4f26-a3a4-d886ee2b81d3","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"56fb5457-cab1-4d82-b813-62d2c217c7b2","turn_exchange_id":"03227d53-b7ad-4d81-8a81-fbb9f75b9ee6"},"recipient":"all","channel":null},"parent":"56fb5457-cab1-4d82-b813-62d2c217c7b2","children":["e0d7c06b-c221-4bbb-b3e0-a93f8adfffcc"]},"e0d7c06b-c221-4bbb-b3e0-a93f8adfffcc":{"id":"e0d7c06b-c221-4bbb-b3e0-a93f8adfffcc","message":{"id":"e0d7c06b-c221-4bbb-b3e0-a93f8adfffcc","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764958486.050598,"update_time":1764958557.316188,"content":{"content_type":"text","parts":["I agree with the spirit of what you\u2019re saying, but I\u2019d sharpen it as:\n\n> We need *agent-native* models and training regimes, not just \u201cfrozen predictors with an agent wrapper\u201d.\n\nYour \u201ctrain the model to interact with the OS from day one\u201d is basically pointing at that. The missing pieces are: what changes in **architecture**, **objective**, and **training data / environment** does that imply?\n\nI\u2019ll break this into:\n\n1. What\u2019s missing in today\u2019s \u201ctrain in isolation, fine-tune for tools\u201d paradigm  \n2. Concrete directions in model *design*  \n3. Concrete directions in *training* (including your \u201cOS from day one\u201d idea)  \n4. Where I\u2019d disagree or at least put caveats around your view\n\n---\n\n## 1. What\u2019s missing in \u201ctrain in isolation, then bolt on tools\u201d\n\nRight now the pattern is roughly:\n\n1. Pretrain a giant **next-token predictor** on static internet corpora.  \n2. Apply **instruction-tuning** and **RL from human feedback (RLHF)** or **RL from AI feedback (RLAIF)**.  \n3. For specific use cases (coding, browsing, computer use), do **tool-use fine-tunes** and then add a **scaffold** (CLI agent, browser agent, research agent, etc).\n\nThis leads to structural limitations:\n\n- The base model is **not trained as an acting agent**. It has no native concept of:\n  - \u201cI have tools at my disposal.\u201d\n  - \u201cI can take multiple steps and observe the world change.\u201d\n  - \u201cI have a persistent identity and goals across episodes.\u201d\n- Tool use is mostly learned in a **thin slice**:\n  - A few APIs, with fixed conventions, on relatively short tasks.\n- Long-horizon behaviour is **faked** by scaffolds:\n  - The model is still myopic, just called repeatedly with some external state.\n\nSo yes: we are fundamentally doing \u201cpredict text in isolation\u201d, then asking the model to behave as a long-horizon agent glued to an OS. That mismatch is the core thing that has to change.\n\n---\n\n## 2. Progress in *model design*: from predictor to agent-native\n\n### 2.1 Action\u2013observation architecture, not just pure text\n\nToday: everything is just tokens in, tokens out; tool calls are usually serialized as special tokens.\n\nFor agent-native behaviour, you want the model to have **first-class \u201caction\u201d and \u201cobservation\u201d channels**:\n\n- Inputs:  \n  - Text / code / documents  \n  - Structured observations: OS state, file listings, error messages, test results, GUI descriptions  \n- Outputs:  \n  - \u201cSay X to the user\u201d  \n  - \u201cCall tool T with arguments A\u201d  \n  - \u201cOpen file F; edit region R with patch P\u201d  \n  - \u201cWait for event E\u201d  \n\nIn practical terms, that might mean:\n\n- A shared backbone (transformer or successor) with:\n  - A **control head** for deciding *what to do next* (act, think, ask, wait).  \n  - A **tool/action head** parameterized over a library of tools / API schemas.  \n  - A **language head** for natural language and code.\n\nYou can emulate this now with clever token schemes, but it becomes much more natural if the architecture and training objective treat actions and observations as structured data, not just more text.\n\n---\n\n### 2.2 Built-in world model and state abstraction\n\nRight now the \u201cworld model\u201d is implicit in weights; there\u2019s no explicit notion of:\n\n- Beliefs  \n- Hypotheses and their uncertainty  \n- State of the environment or OS  \n- Open problems / TODOs / commitments\n\nFurther progress likely needs **separation of concerns** inside the system:\n\n- A **world model component**:  \n  - Learns a compressed representation of environment state from observations (files, logs, UI, etc).  \n  - Predicts how actions change that state.  \n- A **policy / planner**:\n  - Chooses actions based on goals and world state.  \n- A **value / critic**:\n  - Estimates \u201chow good\u201d a given state or trajectory is w.r.t. objectives (success, safety, resource use).\n\nThis doesn\u2019t have to be literally three separate networks, but the **functional roles** have to be present. Pure next-token prediction over raw streams is doing all of this in a fully entangled way, which is hard to tune and hard to debug\u2014especially for long horizons.\n\n---\n\n### 2.3 Memory as a first-class citizen\n\nWe fake memory now via:\n\n- Vector databases + retrieval-augmented generation (RAG)  \n- Manually managed \u201cnotes in context\u201d  \n\nBut the model itself is not **trained** to:\n\n- Decide *what* to write to long-term memory.  \n- Decide *when* and *how* to read from it.  \n- Maintain coherent multi-project identity.\n\nAgent-native systems need learned **read\u2013write policies** for memory:\n\n- A memory store (not necessarily differentiable) with:\n  - Keys: task identifiers, timestamps, topics, goals.  \n  - Values: summaries, code artifacts, proofs, logs, decisions.\n\nThe model is trained end-to-end to:\n\n- Emit structured \u201cwrite\u201d operations (e.g. `WRITE_MEMORY({topic: X, content: Y})`).  \n- Emit \u201cretrieve\u201d operations given a query, and *use* the retrieved content in planning and generation.\n\nYou can bolt this on with scaffolding, but the real step-change comes when **the whole system is trained with memory in the loop**, not treated as an afterthought.\n\n---\n\n### 2.4 Meta-cognition and uncertainty modelling\n\nToday models:\n\n- Rarely say \u201cI don\u2019t know\u201d unless heavily steered.  \n- Don\u2019t distinguish \u201cI\u2019m guessing\u201d from \u201cI have strong evidence\u201d.  \n- Don\u2019t natively choose between \u201cthink more\u201d, \u201cact\u201d, \u201cask human\u201d.\n\nWe need models that natively:\n\n- Maintain an internal notion of **epistemic status**.  \n- Have explicit **actions**:\n  - \u201cReflect more before acting.\u201d  \n  - \u201cAsk for clarification.\u201d  \n  - \u201cDefer to a human.\u201d  \n  - \u201cTrigger external verification / search.\u201d\n\nArchitecturally, that means:\n\n- Heads or internal channels that estimate **uncertainty** or **value of information**.  \n- Training objectives that **reward** \u201cstop and think more\u201d when uncertain, and **penalize** confident wrong answers.\n\nThis is not achievable by scaffolding alone if the base model is overconfident by construction.\n\n---\n\n## 3. Progress in *training*: from static corpora to interactive environments\n\nThis is where your \u201ctrain the model to interface with the OS from day one\u201d idea fits, and I largely agree with the direction\u2014modulo some sequencing caveats.\n\n### 3.1 Pretraining with action\u2013observation traces (agentic pretraining)\n\nInstead of training only on static text, you train on **logged trajectories of agents interacting with tools and OSes**:\n\n- Shell sessions, coding sessions, browser traces, IDE logs, etc.  \n- Synthetic agents running in a sandbox OS (\u201cOSGym\u201d): file ops, scripts, web tasks, etc.  \n- Multi-step workflows: \u201csearch \u2192 read \u2192 write summary \u2192 run code \u2192 fix errors \u2192 refine\u201d.\n\nObjective: still something like next-token prediction, but over **interleaved actions and observations**:\n\n> predict the next token *and* the next action, given the history of both.\n\nThat is: action and OS interaction are **baked into the pretraining distribution**, not just bolted on via small fine-tunes.\n\n### 3.2 Curriculum for long-horizon tasks\n\nRight now most fine-tune tasks are short-horizon:\n\n- Single questions, single answers.  \n- Maybe a short chain-of-thought.\n\nWe need datasets / environments where:\n\n- Episodes span **hundreds or thousands of steps**:\n  - Implement a small project.  \n  - Maintain and evolve a codebase.  \n  - Conduct a week-long \u201cresearch project\u201d inside a sandbox.  \n- Rewards / feedback depend on **end outcomes**:\n  - Tests pass, proof checks succeed, report quality is high, etc.\n\nThis essentially forces the model to learn:\n\n- Decomposition  \n- Revisiting earlier decisions  \n- Tracking commitments over long contexts  \n\nYou can\u2019t get this by just RLHF on final answers.\n\n### 3.3 RL / offline RL with OS interaction\n\nAfter agentic pretraining, you can then use:\n\n- **Offline RL** on logged agent trajectories:\n  - Score trajectories by success metrics (tests passed, bugs fixed, tasks completed).  \n  - Train a policy/value network to favour successful trajectories.\n\n- **Online RL** in sandbox environments:\n  - Let the agent interact with a fake OS / projects, with safety constraints.  \n  - Reward resource-efficient, robust completion of tasks.\n\nKey: the objective must **couple actions to long-term outcomes**, not just local plausibility.\n\nRight now, RLHF is mostly about \u201cmake the answer look good to a human in one shot\u201d, not about \u201cachieve a goal via a sequence of actions in a real environment\u201d.\n\n---\n\n### 3.4 \u201cOS from day one\u201d \u2013 refined\n\nLiteral \u201cday one\u201d might not be optimal, because:\n\n- Static corpora are still the cheapest way to build broad world knowledge.  \n- We don\u2019t have enough high-quality OS-interaction traces to dominate pretraining from step 1.\n\nWhat I think *your* idea should become, concretely:\n\n- **Stage 1:** Standard-ish massive pretraining on text/code to get broad knowledge and reasoning primitives.\n- **Stage 2 (much earlier and much heavier than today):**  \n  - Switch to **agentic pretraining**: action\u2013observation sequences, tool use, OS traces, simulated environments.  \n  - Train the same backbone with action heads and memory read/write heads active.  \n- **Stage 3:**  \n  - RL / offline RL / RLAIF in interactive environments:\n    - OS agents, research agents, coding agents.  \n  - Explicitly train for long-horizon robustness, resource control, and uncertainty calibration.\n\nCompared to today, Stage 2 and 3 are currently small, fragile add-ons. You\u2019re right that they need to become **central, large-scale phases of the training pipeline**.\n\n---\n\n### 3.5 Continual and online learning\n\nStatic models are brittle for anything that requires:\n\n- Adapting to a specific user\u2019s workflows and preferences.  \n- Keeping up with rapidly changing toolchains, APIs, or research.\n\nWe need safe forms of **continual learning**:\n\n- **Per-user adapters / LoRA blocks** that update based on that user\u2019s long-term interaction history.  \n- **Project-specific adapters** for long-lived codebases or research programmes.  \n- Mechanisms to avoid catastrophic forgetting and model drift.\n\nThat\u2019s another training axis where today\u2019s \u201cpretrain once, occasionally fine-tune\u201d paradigm is insufficient for AGI-like behaviour.\n\n---\n\n## 4. Where I\u2019d push back / add nuance to your view\n\nYou\u2019re essentially saying:  \n\n> The main problem is that we don\u2019t train models with OS interaction from the start; if we did, the latent AGI would fully surface.\n\nI\u2019d reframe it more cautiously:\n\n1. **Yes, integrating OS/tool interaction deeply into training is necessary.**  \n   - On that we agree. Treating the model as an agent from relatively early in training is the right direction.\n\n2. **But it\u2019s not sufficient.** You also need:\n   - Better objectives than pure next-token prediction (value functions, world models, uncertainty).  \n   - Explicit long-horizon credit assignment (RL, planning, search).  \n   - Architectures that respect the action\u2013observation\u2013memory structure.\n\n3. **The sequencing matters:**  \n   - \u201cDay one\u201d literal OS interaction might **worsen** perplexity and world knowledge if it replaces broad text pretraining.  \n   - A layered curriculum (world knowledge \u2192 agentic environment \u2192 RL) seems more plausible.\n\n4. **OS interaction is just one environment.**  \n   - For robust AGI-like behaviour, you probably also want:\n     - Simulation environments (games, synthetic worlds, code sandboxes).  \n     - Multi-agent settings.  \n     - Non-OS tools (lab simulators, theorem provers, CAD tools, etc).\n\nSo I\u2019d say your intuition is aimed at the right failure mode, but the \u201cfix\u201d is broader:\n\n> Not just \u201ctrain with OS from day one\u201d, but \u201cmove from static prediction pretraining to *agentic training* with actions, tools, memory, and long-horizon objectives as first-class citizens.\u201d\n\n---\n\nIf you like, a natural next step is: we sketch an \u201cideal 2030 training pipeline\u201d for an agent-native model, with concrete phases, data sources, and objectives, and then see which parts an individual or small team could approximate on top of today\u2019s APIs."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"request_id":"3a55fa1b-ec41-4f26-a3a4-d886ee2b81d3","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"90e0aa92-1aaf-4c57-8c24-5c02abfcec34","turn_exchange_id":"03227d53-b7ad-4d81-8a81-fbb9f75b9ee6"},"recipient":"all","channel":"final"},"parent":"90e0aa92-1aaf-4c57-8c24-5c02abfcec34","children":["7fe68649-6962-4992-87e7-200c08006a1f"]},"7fe68649-6962-4992-87e7-200c08006a1f":{"id":"7fe68649-6962-4992-87e7-200c08006a1f","message":{"id":"7fe68649-6962-4992-87e7-200c08006a1f","author":{"role":"user","name":null,"metadata":{}},"create_time":1764985070.749,"update_time":null,"content":{"content_type":"text","parts":["Currently how far have the frontier labs researched into the above aspects of model training?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_source":null,"turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"e0d7c06b-c221-4bbb-b3e0-a93f8adfffcc","children":["17a5c846-7fd5-40a4-8692-bdb43fe75225"]},"17a5c846-7fd5-40a4-8692-bdb43fe75225":{"id":"17a5c846-7fd5-40a4-8692-bdb43fe75225","message":{"id":"17a5c846-7fd5-40a4-8692-bdb43fe75225","author":{"role":"system","name":null,"metadata":{}},"create_time":1764985082.9371624,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"7fe68649-6962-4992-87e7-200c08006a1f","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"7fe68649-6962-4992-87e7-200c08006a1f","children":["ee91fce2-0933-45a5-97c1-a6567aebb47a"]},"ee91fce2-0933-45a5-97c1-a6567aebb47a":{"id":"ee91fce2-0933-45a5-97c1-a6567aebb47a","message":{"id":"ee91fce2-0933-45a5-97c1-a6567aebb47a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985085.752639,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Looking into frontier lab research on model training","content":"The user is asking about the current state of model training in areas like agent-native models, OS/tool interaction, memory, and long-horizon tasks at frontier labs (such as OpenAI, Anthropic, DeepMind). I'll need to find up-to-date information on this research and provide a detailed but concise analysis. Time to search!","chunks":["The user is asking about the current state of model training in areas like agent-native models, OS/tool interaction, memory, and long-horizon tasks at frontier labs (such as OpenAI, Anthropic, DeepMind).","I'll need to find up-to-date information on this research and provide a detailed but concise analysis.","Time to search!"],"finished":true},{"summary":"Gathering sources for model training research","content":"I need to collect up-to-date sources such as OpenAI's technical reports on supervised training and tool use, Anthropic's \"Toolformer\" and Claude updates, and DeepMind's work on \"ReAct\", \"Voyager\", \"AlphaCode\", as well as research on memory, multi-agent training, and games in reinforcement learning, like \"Agent57\" and \"Sparrow\".","chunks":["I need to collect up-to-date sources such as OpenAI's technical reports on supervised training and tool use, Anthropic's \"Toolformer\" and Claude updates, and DeepMind's work on \"ReAct\", \"Voyager\", \"AlphaCode\", as well as research on memory, multi-agent training, and games in reinforcement learning, like \"Agent57\" and \"Sparrow\"."],"finished":true},{"summary":"Exploring frontier research on multi-agent RL, computer-use, and memory","content":"To answer the user's question, I'll look into OpenAI's work on multi-agent RL (Dota 2, hide-and-seek) and computer-use research (GPT-4 with browsing, Adept's ACT-1, DeepMind's RT-2). Also need to check memory advancements like \"Generative Agents\" and research on long-term memory in LLMs from certain labs. Much of this isn\u2019t fully disclosed.","chunks":["To answer the user's question, I'll look into OpenAI's work on multi-agent RL (Dota 2, hide-and-seek) and computer-use research (GPT-4 with browsing, Adept's ACT-1, DeepMind's RT-2).","Also need to check memory advancements like \"Generative Agents\" and research on long-term memory in LLMs from certain labs. Much of this isn\u2019t fully disclosed."],"finished":true},{"summary":"Clarifying uncertainty and well-supported findings","content":"I need to acknowledge some uncertainty and focus on what has strong backing. The user wants critical and thoughtful analysis, so I'll break things down into categories:\n\n1. Agentic tool-native pretraining & OS interaction: While there's real research, current frontier models like GPT-4 and Gemini are likely still mostly pre-trained on static corpora with limited tool-use data.\n2. Long-horizon RL: RL is widely used for adjusting behavior in games and LLMs, but not RL-specific for agentic training in newer models like \"LLaMA 3.1\".","chunks":["I need to acknowledge some uncertainty and focus on what has strong backing.","The user wants critical and thoughtful analysis, so I'll break things down into categories:","1. Agentic tool-native pretraining & OS interaction: While there's real research, current frontier models like GPT-4 and Gemini are likely still mostly pre-trained on static corpora with limited tool-use data.","2. Long-horizon RL: RL is widely used for adjusting behavior in games and LLMs, but not RL-specific for agentic training in newer models like \"LLaMA 3.1\"."],"finished":true},{"summary":"Assessing frontier research on world models, memory, and learning","content":"World modeling (e.g., \"ReAct\", \"DreamerV3\", \"MuZero\") is still in the early stages for LLM agents, focusing on research prototypes. Memory research (e.g., \"MemGPT\") explores agent-level read-write memory but isn't yet integrated into base models. Continual learning remains unsolved due to stability concerns. I'll search for more recent info.","chunks":["World modeling (e.g., \"ReAct\", \"DreamerV3\", \"MuZero\") is still in the early stages for LLM agents, focusing on research prototypes. Memory research (e.g., \"MemGPT\") explores agent-level read-write memory but isn't yet integrated into base models. Continual learning remains unsolved due to stability concerns. I'll search for more recent info."],"finished":true}],"source_analysis_msg_id":"e4572c35-759d-4910-8af8-1bdfe5d49f11"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"17a5c846-7fd5-40a4-8692-bdb43fe75225","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"17a5c846-7fd5-40a4-8692-bdb43fe75225","children":["032decf1-0123-4a18-8d5f-3dce558e219d"]},"032decf1-0123-4a18-8d5f-3dce558e219d":{"id":"032decf1-0123-4a18-8d5f-3dce558e219d","message":{"id":"032decf1-0123-4a18-8d5f-3dce558e219d","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985104.86999,"update_time":1764985104.880519,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\":[{\"q\":\"OpenAI o1 system card training process reasoning model RL supervision process-based \\\"OpenAI o1 system card\\\"\",\"recency\":14},{\"q\":\"Anthropic Claude 3.5 tool use training method computer use agents \\\"Claude 3.5\\\" tool-use\",\"recency\":60},{\"q\":\"Google DeepMind Gemini 1.5 system card tool use agentic reinforcement learning environment\",\"recency\":365},{\"q\":\"\\\"agentic\\\" training large language models tool use world model memory RLHF RLAIF\",\"recency\":365}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"OpenAI o1 system card training process reasoning model RL supervision process-based \"OpenAI o1 system card\""},{"type":"search","q":"Anthropic Claude 3.5 tool use training method computer use agents \"Claude 3.5\" tool-use"},{"type":"search","q":"Google DeepMind Gemini 1.5 system card tool use agentic reinforcement learning environment"},{"type":"search","q":"\"agentic\" training large language models tool use world model memory RLHF RLAIF"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Browsing frontier labs research on agentic model training","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"ee91fce2-0933-45a5-97c1-a6567aebb47a","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"web.run","channel":null},"parent":"ee91fce2-0933-45a5-97c1-a6567aebb47a","children":["403821b9-8135-4aa0-820f-fc0a49c97f2b"]},"403821b9-8135-4aa0-820f-fc0a49c97f2b":{"id":"403821b9-8135-4aa0-820f-fc0a49c97f2b","message":{"id":"403821b9-8135-4aa0-820f-fc0a49c97f2b","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985104.9163444,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["OpenAI o1 system card training process reasoning model RL supervision process-based \"OpenAI o1 system card\"","Anthropic Claude 3.5 tool use training method computer use agents \"Claude 3.5\" tool-use","Google DeepMind Gemini 1.5 system card tool use agentic reinforcement learning environment","\"agentic\" training large language models tool use world model memory RLHF RLAIF"]},"reasoning_status":"is_reasoning","reasoning_title":"Browsing frontier labs research on agentic model training","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"032decf1-0123-4a18-8d5f-3dce558e219d","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"032decf1-0123-4a18-8d5f-3dce558e219d","children":["8e3685a0-934e-4246-8144-7f8d6c655dde"]},"8e3685a0-934e-4246-8144-7f8d6c655dde":{"id":"8e3685a0-934e-4246-8144-7f8d6c655dde","message":{"id":"8e3685a0-934e-4246-8144-7f8d6c655dde","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985104.9141133,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2411.17075v5","title":"an Exploratory Study of System-2 Alignment","snippet":"14 Jan 2025 \u2014 Additionally, OpenAI's o1 system card OpenAIOS highlights the model's contextual reasoning capabilities and emphasizes the crucial role of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2509.02547v1","title":"The Landscape of Agentic Reinforcement Learning for LLMs","snippet":"2 Sept 2025 \u2014 Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2412.14135v1","title":"A Roadmap to Reproduce o1 from Reinforcement Learning ...","snippet":"18 Dec 2024 \u2014 This is because the model needs to gradually enhance its reasoning capabilities during training ... Openai o1 system card, 2024b. URL https://cdn.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/","title":"Gemini Robotics 1.5 brings AI agents into the physical world","snippet":"25 Sept 2025 \u2014 It has state-of-the-art spatial understanding, interacts in natural language, estimates its success and progress, and can natively call tools ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/models/gemini-robotics/gemini-robotics/","title":"Gemini Robotics 1.5","snippet":"Our agentic Gemini-based multimodal model allows robots to take action ... Get started building with cutting-edge AI models and tools. Google AI Studio.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/OpenAI-o1-System-Card/985298bd0b6f4c84ab34bb15cfed0b092f43b83a","title":"[PDF] OpenAI o1 System Card","snippet":"The safety work carried out for the OpenAI o1 and OpenAI o1-mini models is outlined, including safety evaluations, external red teaming, and Preparedness ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"www.cogitotech.com","entries":[{"type":"search_result","url":"https://www.cogitotech.com/blog/top-agentic-ai-training-data-companies/","title":"Top Agentic AI Training Data Companies 2026","snippet":"15 Nov 2025 \u2014 Discover the top Agentic AI training data companies for 2026 specializing in multi-turn datasets, RLHF, multimodal annotation, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":1763188128.0,"attribution":"www.cogitotech.com"}]},{"type":"search_result_group","domain":"www.emergentmind.com","entries":[{"type":"search_result","url":"https://www.emergentmind.com/topics/openai-s-o1","title":"OpenAI's O1: A Large Reasoning Model","snippet":"21 Jul 2025 \u2014 ... process supervision (rewarding each reasoning step for safety and correctness). ... 14. OpenAI o1 System Card (2024). 15. Don't Command, Cultivate ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"www.emergentmind.com"}]},{"type":"search_result_group","domain":"www.therobotreport.com","entries":[{"type":"search_result","url":"https://www.therobotreport.com/gemini-robotics-1-5-enables-agentic-experiences-explains-google-deepmind/","title":"Gemini Robotics 1.5 enables agentic experiences, ...","snippet":"26 Sept 2025 \u2014 Gemini Robotics 1.5 enables agentic experiences, explains Google DeepMind ... systems that can reason, plan, actively use tools, and generalize.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"www.therobotreport.com"}]},{"type":"search_result_group","domain":"labs.adaline.ai","entries":[{"type":"search_result","url":"https://labs.adaline.ai/p/evaluating-ai-agents-in-2025","title":"Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs","snippet":"19 May 2025 \u2014 What \u201cagentic\u201d means in 2025 \u2014 autonomy, goal-pursuit, tool use. AI Agents are autonomous software entities that perform goal-directed tasks ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":1747665832.0,"attribution":"labs.adaline.ai"}]},{"type":"search_result_group","domain":"cdn.openai.com","entries":[{"type":"search_result","url":"https://cdn.openai.com/pdf/34f2ada6-870f-4c26-9790-fd8def56387f/CoT_Monitoring.pdf","title":"Monitoring Reasoning Models for Misbehavior and the ...","snippet":"by B Baker \u00b7 Cited by 135 \u2014 they will be reinforced through the RL training process. As a ... Openai o1 system card. arXiv preprint. arXiv:2412.16720, 2024. [25] ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":1741713370.0,"attribution":"cdn.openai.com"}]},{"type":"search_result_group","domain":"www.anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/news/3-5-models-and-computer-use","title":"Introducing computer use, a new Claude 3.5 Sonnet, and ...","snippet":"22 Oct 2024 \u2014 With low latency, improved instruction following, and more accurate tool use, Claude 3.5 Haiku is well suited for user-facing products, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/developing-computer-use","title":"Developing a computer use model","snippet":"22 Oct 2024 \u2014 The latest version of Claude 3.5 Sonnet can, when run through the appropriate software setup, follow a user's commands to move a cursor around ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"www.anthropic.com"}]},{"type":"search_result_group","domain":"www.youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=AMRxbIO04kQ","title":"Gemini Robotics 1.5: Using agentic capabilities","snippet":"Here we have an agentic layers both control the robot and have access to the internet to sort the trash.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"www.youtube.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf/ff7d043f3a3f7131365384c19f7a83441224512d.pdf","title":"TOWARDS AGENTIC SELF-LEARNING LLMS","snippet":"We propose Agentic Self-Learning (ASL), a multi-agent self-improvement framework designed to enable large language models (LLMs) to continuously evolve their ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"aman.ai","entries":[{"type":"search_result","url":"https://aman.ai/primers/ai/o1/","title":"Primers \u2022 OpenAI o1","snippet":"This RL-based training process is highly data-efficient, allowing the model to improve with more RL (train-time compute) and more time spent thinking (test-time ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"aman.ai"}]},{"type":"search_result_group","domain":"storage.googleapis.com","entries":[{"type":"search_result","url":"https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf","title":"Gemini Robotics 1.5: Pushing the Frontier of Generalist ...","snippet":"16 Sept 2025 \u2014 The GR-ER 1.5 model combines Gemini's thinking and tool-use with an enhanced physical world understanding, enabling it to function within the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"storage.googleapis.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/madailab/7-types-of-large-language-models-llms-55fb0038ceb9","title":"7 Types of Large Language Models (LLMs)","snippet":"This combined SFT and RLHF/RLAIF process helps the model ... agentic behavior \u2014 will likely remain key building blocks for future generations of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":1760380612.997,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science/improving-agent-systems-ai-reasoning-c2d91ecfdf77","title":"Improving Agent Systems & AI Reasoning","snippet":"Reasoning Language Models: A Blueprint \u00b7 OpenAI o1 System Card \u00b7 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":1738514732.795,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"docs.aws.amazon.com","entries":[{"type":"search_result","url":"https://docs.aws.amazon.com/bedrock/latest/userguide/computer-use.html","title":"Use a computer use tool to complete an Amazon Bedrock ...","snippet":"To use computer use with Anthropic Claude 3.5 Sonnet v2 you can use the Converse API (Converse or ConverseStream). You specify the computer use specific fields ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"docs.aws.amazon.com"}]},{"type":"search_result_group","domain":"www.humanoidsdaily.com","entries":[{"type":"search_result","url":"https://www.humanoidsdaily.com/feed/google-deepmind-gives-robots-a-thinking-brain-with-agentic-gemini-15-models","title":"Google DeepMind Gives Robots a 'Thinking' Brain with ...","snippet":"26 Sept 2025 \u2014 Google DeepMind Gives Robots a 'Thinking' Brain with Agentic Gemini 1.5 Models ... tools like Google Search to find necessary information\u2014for ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":1758902400.0,"attribution":"www.humanoidsdaily.com"}]},{"type":"search_result_group","domain":"www.sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2666389925002181","title":"Toward large reasoning models: A survey of reinforced ...","snippet":"by F Xu \u00b7 2025 \u00b7 Cited by 88 \u2014 Recent studies suggest effective agentic workflow can largely improve LLMs' abilities for simulating human behavior,, human-LLM interaction, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"www.sciencedirect.com"}]}],"debug_sonic_thread_id":"thread_69338910ee8083278d8136f8d1e722c6","reasoning_status":"is_reasoning","reasoning_title":"Browsing frontier labs research on agentic model training","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"403821b9-8135-4aa0-820f-fc0a49c97f2b","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"403821b9-8135-4aa0-820f-fc0a49c97f2b","children":["be61ef49-7221-4561-b57a-d39a9e785ffb"]},"be61ef49-7221-4561-b57a-d39a9e785ffb":{"id":"be61ef49-7221-4561-b57a-d39a9e785ffb","message":{"id":"be61ef49-7221-4561-b57a-d39a9e785ffb","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985110.529097,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Refining search queries for more specific research","content":"I\u2019ve gathered some sources, but I need to refine the searches to focus more on agentic training and OS environment research. I'll now look up specific terms like \"OpenAI computer use model training,\" \"DeepMind WebAgent training,\" and \"BrowserGym\" to gather more targeted information.","chunks":["I\u2019ve gathered some sources, but I need to refine the searches to focus more on agentic training and OS environment research.","I'll now look up specific terms like \"OpenAI computer use model training,\" \"DeepMind WebAgent training,\" and \"BrowserGym\" to gather more targeted information."],"finished":true}],"source_analysis_msg_id":"c92eb3e3-1e5a-43e1-8897-f94f7a6fea70"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"8e3685a0-934e-4246-8144-7f8d6c655dde","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"8e3685a0-934e-4246-8144-7f8d6c655dde","children":["8485f46b-f291-454a-9931-abafa2bceeeb"]},"8485f46b-f291-454a-9931-abafa2bceeeb":{"id":"8485f46b-f291-454a-9931-abafa2bceeeb","message":{"id":"8485f46b-f291-454a-9931-abafa2bceeeb","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985109.789354,"update_time":1764985109.80187,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\":[{\"q\":\"OpenAI computer use model \\\"system card\\\" agents operating system interaction\",\"recency\":365},{\"q\":\"WebArena benchmark web agent environment\",\"recency\":365},{\"q\":\"\\\"Toolformer\\\" language models can teach themselves to use tools\",\"recency\":365},{\"q\":\"MemGPT long-term memory for LLMs\",\"recency\":730}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"OpenAI computer use model \"system card\" agents operating system interaction"},{"type":"search","q":"WebArena benchmark web agent environment"},{"type":"search","q":"\"Toolformer\" language models can teach themselves to use tools"},{"type":"search","q":"MemGPT long-term memory for LLMs"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Refining queries for agentic training and environments","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"be61ef49-7221-4561-b57a-d39a9e785ffb","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"web.run","channel":null},"parent":"be61ef49-7221-4561-b57a-d39a9e785ffb","children":["97e82dec-7039-407a-8207-6e3b3387368c"]},"97e82dec-7039-407a-8207-6e3b3387368c":{"id":"97e82dec-7039-407a-8207-6e3b3387368c","message":{"id":"97e82dec-7039-407a-8207-6e3b3387368c","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985109.8334143,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["OpenAI computer use model \"system card\" agents operating system interaction","WebArena benchmark web agent environment","\"Toolformer\" language models can teach themselves to use tools","MemGPT long-term memory for LLMs"]},"reasoning_status":"is_reasoning","reasoning_title":"Refining queries for agentic training and environments","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"8485f46b-f291-454a-9931-abafa2bceeeb","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"8485f46b-f291-454a-9931-abafa2bceeeb","children":["30928a85-0382-4930-ba36-4febfd5e8436"]},"30928a85-0382-4930-ba36-4febfd5e8436":{"id":"30928a85-0382-4930-ba36-4febfd5e8436","message":{"id":"30928a85-0382-4930-ba36-4febfd5e8436","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985109.83091,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/computer-using-agent/","title":"Computer-Using Agent","snippet":"23 Jan 2025 \u2014 For those interested in exploring the evaluations and safeguards in more detail, we encourage you to review the Operator System Card, a living ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/operator-system-card/","title":"Operator System Card","snippet":"23 Jan 2025 \u2014 This system card details our multi-layered approach for testing and deploying Operator safely. It outlines the risk areas we identified and the model and ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/openai-o1-system-card/","title":"OpenAI o1 System Card","snippet":"5 Dec 2024 \u2014 ... system card due to the incorporation of the latest GPT\u20114o model deployed in production, which takes into account more user preference data.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"webarena.dev","entries":[{"type":"search_result","url":"https://webarena.dev/","title":"WebArena: A Realistic Web Environment for Building ...","snippet":"WebArena is a standalone, self-hostable web environment for building autonomous agents. WebArena creates websites from four popular categories.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"webarena.dev"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2302.04761","title":"Toolformer: Language Models Can Teach Themselves to Use Tools","snippet":"","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2307.13854","title":"WebArena: A Realistic Web Environment for Building Autonomous Agents","snippet":"","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2310.08560","title":"MemGPT: Towards LLMs as Operating Systems","snippet":"by C Packer \u00b7 2023 \u00b7 Cited by 293 \u2014 We introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"platform.openai.com","entries":[{"type":"search_result","url":"https://platform.openai.com/docs/guides/tools-computer-use","title":"Computer use - OpenAI API","snippet":"... model is not yet highly reliable for automating tasks on an OS. More details about the model and related safety work can be found in our updated system card.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"platform.openai.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/forum?id=Yacmpz84TH","title":"Toolformer: Language Models Can Teach Themselves to ...","snippet":"by T Schick \u00b7 Cited by 2759 \u2014 We introduce Toolformer, a language model trained in a self-supervised way to know when and how to use external tools, achieving substantially improved zero- ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=oKn9c6ytLx","title":"WebArena: A Realistic Web Environment for Building ...","snippet":"by S Zhou \u00b7 Cited by 661 \u2014 WebArena is a highly realistic and reproducible web environment designed for building useful agents for everyday life.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"www.emergentmind.com","entries":[{"type":"search_result","url":"https://www.emergentmind.com/topics/webarena-benchmark","title":"WebArena Benchmark: Evaluating Web Agents","snippet":"30 Jun 2025 \u2014 WebArena Benchmark is a comprehensive suite for assessing autonomous web agents on human-like tasks using interactive simulations.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"www.emergentmind.com"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3666122.3669119","title":"Toolformer: language models can teach themselves to use ...","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2759 \u2014 In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40adnanmasood/webarena-benchmark-and-the-state-of-agentic-ai-c22697e8e192","title":"WebArena Benchmark and the State of Agentic AI","snippet":"WebArena represents a pivotal step forward in evaluating web-based autonomous agents, moving far beyond the toy environments of the past. Unlike ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":1758119811.59,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40ahmadareeb3026/llm-as-operating-systems-agent-memory-b70c1213a5f7","title":"LLM as Operating Systems: Agent Memory | by Areeb Ahmad","snippet":"We call MemGPT agents self-improving because they can edit their own long-term memories over time. Example: Agent Memory and Response Flow.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":1752673750.509,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"www.semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/Toolformer%3A-Language-Models-Can-Teach-Themselves-to-Schick-Dwivedi-Yu/53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Language Models Can Teach Themselves to Use Tools","snippet":"This paper introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"www.semanticscholar.org"}]},{"type":"search_result_group","domain":"www.amitysolutions.com","entries":[{"type":"search_result","url":"https://www.amitysolutions.com/blog/openai-operator-ai-agent-web-tasks","title":"Operator from OpenAI: An AI Agent for Smarter Web Tasks","snippet":"31 Jan 2025 \u2014 OpenAI has implemented safeguards to address potential risks, as detailed in the Operator System Card. As part of a phased deployment strategy, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"www.amitysolutions.com"}]},{"type":"search_result_group","domain":"www.cmu.edu","entries":[{"type":"search_result","url":"https://www.cmu.edu/flame/research/2024/webarena.html","title":"WebArena: A Realistic Web Environment for Building ...","snippet":"WebArena is a standalone, self-hostable web environment for building autonomous agents. WebArena creates websites from four popular categories.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"www.cmu.edu"}]},{"type":"search_result_group","domain":"www.digitalocean.com","entries":[{"type":"search_result","url":"https://www.digitalocean.com/community/tutorials/memgpt-llm-infinite-context-understanding","title":"MemGPT with Real-life Example: Bridging the Gap ...","snippet":"26 Dec 2024 \u2014 In this tutorial, we discuss and show how to run MemGPT - an LLM with the potential for infinite context understanding.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":1706130001.0,"attribution":"www.digitalocean.com"}]},{"type":"search_result_group","domain":"pushsecurity.com","entries":[{"type":"search_result","url":"https://pushsecurity.com/blog/considering-the-impact-of-computer-using-agents/","title":"How Computer-Using Agents can be leveraged in cyber ...","snippet":"28 Jan 2025 \u2014 The Operator System Card also cites proactive refusals of high-risk tasks, confirmation prompts before critical actions, and active ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"pushsecurity.com"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/web-arena-x/webarena","title":"GitHub - web-arena-x/webarena: Code repo for \" ...","snippet":"WebArena is a standalone, self-hostable web environment for building autonomous agents. Python 3.10 pre-commit","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/madebywild/MemGPT","title":"madebywild/MemGPT: Create LLM agents with long-term ...","snippet":"MemGPT makes it easy to build and deploy stateful LLM agents with support for: Long term memory/state management; Connections to external data sources (e.g. ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"hackmd.io","entries":[{"type":"search_result","url":"https://hackmd.io/%40ll-24-25/r1RSCmxJxl/%2FecoSUd5QR2KYWvSVjeDVSw","title":"openai-computer-use","snippet":"... model is not yet highly reliable for automating tasks on an OS. More details about the model and related safety work can be found in our updated system card.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"hackmd.io"}]},{"type":"search_result_group","domain":"invariantlabs.ai","entries":[{"type":"search_result","url":"https://invariantlabs.ai/blog/what-we-learned-from-analyzing-web-agents","title":"What we've learned from analyzing hundreds of AI web ...","snippet":"10 Jul 2024 \u2014 While the full WebArena benchmark consists of 5 different websites, in this blog post we focus on two of them: OpenStreetMap and ShoppingAdmin.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":20},"pub_date":1720599600.0,"attribution":"invariantlabs.ai"}]},{"type":"search_result_group","domain":"dev.to","entries":[{"type":"search_result","url":"https://dev.to/goroman/cua-computer-using-agent-the-ai-that-operates-your-computer-1b4d","title":"CUA - Computer-Using Agent: The AI That Operates Your ...","snippet":"13 Mar 2025 \u2014 Operator System Card: A detailed system card outlines the safeguards in place, ensuring transparency and accountability. The initial release ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":22},"pub_date":1741903321.0,"attribution":"dev.to"}]}],"debug_sonic_thread_id":"thread_69338910ee8083278d8136f8d1e722c6","reasoning_status":"is_reasoning","reasoning_title":"Refining queries for agentic training and environments","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"97e82dec-7039-407a-8207-6e3b3387368c","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"97e82dec-7039-407a-8207-6e3b3387368c","children":["d872357e-4fea-464a-a3aa-04c631e8533b"]},"d872357e-4fea-464a-a3aa-04c631e8533b":{"id":"d872357e-4fea-464a-a3aa-04c631e8533b","message":{"id":"d872357e-4fea-464a-a3aa-04c631e8533b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985116.043944,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Clarifying research scope and refining queries","content":"I have some info on OS-use models and training environments, like OpenAI's Computer-Using Agent and WebArena, but much of this is from third-party sources. It\u2019s important to focus on the broader state of research, not just the products. I\u2019ll refine my search to terms like \u201cOpenAI RL large reasoning model\u201d and \u201cGemini Robotics 1.5\u201d for a better overall view.","chunks":["I have some info on OS-use models and training environments, like OpenAI's Computer-Using Agent and WebArena, but much of this is from third-party sources.","It\u2019s important to focus on the broader state of research, not just the products.","I\u2019ll refine my search to terms like \u201cOpenAI RL large reasoning model\u201d and \u201cGemini Robotics 1.5\u201d for a better overall view."],"finished":true}],"source_analysis_msg_id":"0fdf9d0c-e4b5-4a9e-b6c1-c2fbc0490d6b"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"30928a85-0382-4930-ba36-4febfd5e8436","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"30928a85-0382-4930-ba36-4febfd5e8436","children":["a03eab8e-90c0-4005-a40e-59e7df4f3d1c"]},"a03eab8e-90c0-4005-a40e-59e7df4f3d1c":{"id":"a03eab8e-90c0-4005-a40e-59e7df4f3d1c","message":{"id":"a03eab8e-90c0-4005-a40e-59e7df4f3d1c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985115.38385,"update_time":1764985115.478815,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\":[{\"q\":\"\\\"OpenAI o1\\\" system card RL reasoning model training process supervision\",\"recency\":365},{\"q\":\"\\\"DeepSeek-R1\\\" incentivizing reasoning capability via reinforcement learning\",\"recency\":365},{\"q\":\"\\\"Kimi k1.5\\\" scaling reinforcement learning with LLMs\",\"recency\":365},{\"q\":\"\\\"agentic\\\" reinforcement learning for large language models survey 2025\",\"recency\":365}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"\"OpenAI o1\" system card RL reasoning model training process supervision"},{"type":"search","q":"\"DeepSeek-R1\" incentivizing reasoning capability via reinforcement learning"},{"type":"search","q":"\"Kimi k1.5\" scaling reinforcement learning with LLMs"},{"type":"search","q":"\"agentic\" reinforcement learning for large language models survey 2025"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching research on reinforcement learning for reasoning models","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"d872357e-4fea-464a-a3aa-04c631e8533b","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"web.run","channel":null},"parent":"d872357e-4fea-464a-a3aa-04c631e8533b","children":["0ef35e83-61fc-49af-a141-031f72b186cd"]},"0ef35e83-61fc-49af-a141-031f72b186cd":{"id":"0ef35e83-61fc-49af-a141-031f72b186cd","message":{"id":"0ef35e83-61fc-49af-a141-031f72b186cd","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985115.6253152,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["\"OpenAI o1\" system card RL reasoning model training process supervision","\"DeepSeek-R1\" incentivizing reasoning capability via reinforcement learning","\"Kimi k1.5\" scaling reinforcement learning with LLMs","\"agentic\" reinforcement learning for large language models survey 2025"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching research on reinforcement learning for reasoning models","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"a03eab8e-90c0-4005-a40e-59e7df4f3d1c","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"a03eab8e-90c0-4005-a40e-59e7df4f3d1c","children":["4cb6c622-4ac0-4b29-9d99-7cdd7df6c1a0"]},"4cb6c622-4ac0-4b29-9d99-7cdd7df6c1a0":{"id":"4cb6c622-4ac0-4b29-9d99-7cdd7df6c1a0","message":{"id":"4cb6c622-4ac0-4b29-9d99-7cdd7df6c1a0","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985115.6224675,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/learning-to-reason-with-llms/","title":"Learning to reason with LLMs","snippet":"12 Sept 2024 \u2014 We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning ... training process.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2501.12948","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ...","snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 5568 \u2014 DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2501.12599","title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","snippet":"by K Team \u00b7 2025 \u00b7 Cited by 592 \u2014 We report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2509.02547","title":"The Landscape of Agentic Reinforcement Learning for LLMs: A Survey","snippet":"","ref_id":{"turn_index":2,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2411.17075v5","title":"an Exploratory Study of System-2 Alignment","snippet":"14 Jan 2025 \u2014 Additionally, OpenAI's o1 system card OpenAIOS highlights the model's contextual reasoning ... reinforcement learning with process supervision ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2501.12599","title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","snippet":"by K Team \u00b7 2025 \u00b7 Cited by 592 \u2014 In this report, we present the training recipe of Kimi k1.5, our latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2501.12948","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ...","snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 5568 \u2014 In the following sections, we present: (1). DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and. (2) ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":8},"pub_date":1737596731.0,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41586-025-09422-z","title":"DeepSeek-R1 incentivizes reasoning in LLMs through ...","snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 192 \u2014 This training pipeline enables DeepSeek-R1 to inherit the reasoning capabilities of its predecessor, DeepSeek-R1-Zero, while aligning model ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"www.nature.com"}]},{"type":"search_result_group","domain":"www.emergentmind.com","entries":[{"type":"search_result","url":"https://www.emergentmind.com/topics/openai-s-o1","title":"OpenAI's O1: A Large Reasoning Model","snippet":"21 Jul 2025 \u2014 OpenAI's O1 is a large-scale reasoning model that integrates chain-of-thought and reinforcement learning to solve complex problems across ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"www.emergentmind.com"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/MoonshotAI/Kimi-k1.5","title":"MoonshotAI/Kimi-k1.5","snippet":"Introducing Kimi k1.5 --- an o1-level multi-modal model. Key Ingredients of Kimi k1.5: The Reinforcement Learning Training System for LLM.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/1ff8f7v/d_openai_new_reasoning_model_called_o1/","title":"[D] OpenAI new reasoning model called o1","snippet":"OpenAI's o1 reasoning model is able to recognize, map out, and even build upon one of the most complex phenomena of human language, a concept ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":10},"pub_date":1726162609.187,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"pubmed.ncbi.nlm.nih.gov","entries":[{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/40962978/","title":"DeepSeek-R1 incentivizes reasoning in LLMs through ...","snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 192 \u2014 DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature. 2025 Sep;645(8081):633-638. doi: 10.1038/s41586-025-09422 ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"pubmed.ncbi.nlm.nih.gov"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2501.12599","title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","snippet":"We report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"www.theguardian.com","entries":[{"type":"search_result","url":"https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet","title":"Was this the week DeepSeek started the slow unwinding of the AI bet?","snippet":"In late January 2025, the Chinese startup DeepSeek shocked global tech markets with the release of its AI chatbot, DeepSeek R1. Unveiled by 40-year-old founder Liang Wenfeng, the model rivaled OpenAI\u2019s top models in performance but was developed at a fraction of the cost\u2014just $5.6 million. This breakthrough sent tremors through the industry, leading to a historic $600 billion drop in Nvidia\u2019s market value amid fears that demand for American chips in China would plummet. The release raised geopolitical concerns, as China\u2019s advancement challenged U.S. dominance in AI, despite heavy American investment and export bans.\n\nDeepSeek's model, lauded for its reasoning abilities and efficiency, topped app store charts in the U.S. and UK but was later removed in Italy after data privacy concerns. The model also self-censors on politically sensitive topics, revealing the influence of China's state policies. Allegations arose that DeepSeek used knowledge distilled from OpenAI\u2019s models, prompting an investigation.\n\nWhile some experts viewed the development as a democratization of AI\u2014making powerful models accessible without massive compute resources\u2014others cautioned about national security and data implications. Ultimately, DeepSeek\u2019s emergence marked a pivotal moment in the AI race, challenging the notion that massive financial and hardware resources are prerequisites for leadership in advanced AI.","ref_id":{"turn_index":2,"ref_type":"news","ref_index":42},"pub_date":1738428683.0,"attribution":"www.theguardian.com"}]},{"type":"search_result_group","domain":"www.reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/world/china/chinas-deepseek-says-its-hit-ai-model-cost-just-294000-train-2025-09-18/","title":"China's DeepSeek says its hit AI model cost just $294,000 to train","snippet":"Chinese AI company DeepSeek revealed that training its R1 model cost only $294,000, significantly less than the costs reported by U.S. competitors, which can exceed $100 million. The estimate, disclosed in a peer-reviewed paper in *Nature*, is the first official figure from the Hangzhou-based firm. The R1 is a reasoning-focused AI model trained over 80 hours using 512 Nvidia H800 chips\u2014designed specifically for the Chinese market to comply with U.S. export controls. Despite earlier claims that it only used H800 chips, DeepSeek also acknowledged it had utilized A100 GPUs during preparatory stages.\n\nDeepSeek\u2019s low-cost model released in January 2025 caused concern among investors over its potential to disrupt industry leaders like Nvidia. While Nvidia insists DeepSeek has only used H800 chips legally, U.S. officials allege the company accessed large quantities of restricted H100 chips. The report also highlighted that DeepSeek\u2019s access to high-end A100 clusters helped it recruit top AI talent in China. The disclosure is expected to further fuel debate over China's competitive stance in the global AI race.","ref_id":{"turn_index":2,"ref_type":"news","ref_index":45},"pub_date":1758199814.0,"attribution":"www.reuters.com"}]},{"type":"search_result_group","domain":"www.theverge.com","entries":[{"type":"search_result","url":"https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training","title":"Researchers trained an OpenAI rival in half an hour for less than $50","snippet":"Researchers from Stanford and the University of Washington developed an AI reasoning model called s1 that rivals OpenAI\u2019s o1 model, achieving this in just 26 minutes for under $50. Using the distillation method, they trained the model by leveraging answers from Google\u2019s Gemini 2.0 Flash Thinking Experimental. Despite Gemini's terms prohibiting the use of its API to create competing models, the researchers utilized its output to refine their system. The base model s1 was adapted from Alibaba Cloud\u2019s open-source Qwen2.5 and was trained on a pared-down dataset of 1,000 questions, as larger datasets didn\u2019t yield significant improvements. Remarkably, training ran on just 16 Nvidia H100 GPUs. The team also used a technique known as test-time scaling, which prompts the AI to extend its reasoning time by inserting \u201cWait\u201d into its responses, improving accuracy. The s1 model reportedly outperforms OpenAI\u2019s o1-preview by up to 27% on competition math questions. This breakthrough raises questions about the industry's high-cost AI training norms, suggesting that powerful AI models can be developed efficiently and inexpensively without massive data centers or multi-billion-dollar investments.","ref_id":{"turn_index":2,"ref_type":"news","ref_index":46},"pub_date":1738857233.0,"attribution":"www.theverge.com"}]}],"debug_sonic_thread_id":"thread_69338910ee8083278d8136f8d1e722c6","reasoning_status":"is_reasoning","reasoning_title":"Searching research on reinforcement learning for reasoning models","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"0ef35e83-61fc-49af-a141-031f72b186cd","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"0ef35e83-61fc-49af-a141-031f72b186cd","children":["3b0800ba-20b1-4d75-9c56-089607514c94"]},"3b0800ba-20b1-4d75-9c56-089607514c94":{"id":"3b0800ba-20b1-4d75-9c56-089607514c94","message":{"id":"3b0800ba-20b1-4d75-9c56-089607514c94","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985121.053379,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Refining focus on research gaps","content":"I\u2019ve found decent references on reinforcement learning (RL) for reasoning. Now, I need to dive deeper into long-horizon agentic RL, memory, and how OS integration plays a role in agent training (not just product-level features). This seems to still be in the research phase, so I\u2019ll search.","chunks":["I\u2019ve found decent references on reinforcement learning (RL) for reasoning.","Now, I need to dive deeper into long-horizon agentic RL, memory, and how OS integration plays a role in agent training (not just product-level features).","This seems to still be in the research phase, so I\u2019ll search."],"finished":true}],"source_analysis_msg_id":"583b9647-a92e-417c-8415-685e36453d53"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"4cb6c622-4ac0-4b29-9d99-7cdd7df6c1a0","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"4cb6c622-4ac0-4b29-9d99-7cdd7df6c1a0","children":["b6a75e5d-6dfc-4f8f-abe9-513a1581949c"]},"b6a75e5d-6dfc-4f8f-abe9-513a1581949c":{"id":"b6a75e5d-6dfc-4f8f-abe9-513a1581949c","message":{"id":"b6a75e5d-6dfc-4f8f-abe9-513a1581949c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985120.644429,"update_time":1764985120.656488,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\":[{\"q\":\"\\\"Developing a computer use model\\\" Anthropic training approach how Claude 3.5 learns to use computers\",\"recency\":365},{\"q\":\"\\\"Large-scale Interactive Agents\\\" DeepMind training web-based LLM agents\",\"recency\":365},{\"q\":\"\\\"world model\\\" \\\"vision-language-action\\\" Gemini Robotics 1.5 tech report training\",\"recency\":365},{\"q\":\"\\\"Memorizing Transformers\\\" AI long-term memory scaling survey 2024\",\"recency\":730}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"\"Developing a computer use model\" Anthropic training approach how Claude 3.5 learns to use computers"},{"type":"search","q":"\"Large-scale Interactive Agents\" DeepMind training web-based LLM agents"},{"type":"search","q":"\"world model\" \"vision-language-action\" Gemini Robotics 1.5 tech report training"},{"type":"search","q":"\"Memorizing Transformers\" AI long-term memory scaling survey 2024"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for research on agentic RL and memory","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"3b0800ba-20b1-4d75-9c56-089607514c94","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"web.run","channel":null},"parent":"3b0800ba-20b1-4d75-9c56-089607514c94","children":["4541d92f-2a12-441f-8532-617c06c55407"]},"4541d92f-2a12-441f-8532-617c06c55407":{"id":"4541d92f-2a12-441f-8532-617c06c55407","message":{"id":"4541d92f-2a12-441f-8532-617c06c55407","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985120.734383,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["\"Developing a computer use model\" Anthropic training approach how Claude 3.5 learns to use computers","\"Large-scale Interactive Agents\" DeepMind training web-based LLM agents","\"world model\" \"vision-language-action\" Gemini Robotics 1.5 tech report training","\"Memorizing Transformers\" AI long-term memory scaling survey 2024"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for research on agentic RL and memory","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"b6a75e5d-6dfc-4f8f-abe9-513a1581949c","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"b6a75e5d-6dfc-4f8f-abe9-513a1581949c","children":["24cc2a55-ca2b-4683-bb6d-4a0faeed2cfe"]},"24cc2a55-ca2b-4683-bb6d-4a0faeed2cfe":{"id":"24cc2a55-ca2b-4683-bb6d-4a0faeed2cfe","message":{"id":"24cc2a55-ca2b-4683-bb6d-4a0faeed2cfe","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985120.7310271,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/news/developing-computer-use","title":"Developing a computer use model","snippet":"Developing a computer use model. Oct 22, 2024. Update. Consumer Terms and Privacy Policy. Aug 28, 2025. Claude can now use computers. The latest version of ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"www.anthropic.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2510.10666v1","title":"BrowserAgent: Building Web Agents with Human-Inspired ...","snippet":"12 Oct 2025 \u2014 Rather than relying on LLM-based semantic abstractions, BrowserAgent interacts directly with raw webpage states, enabling fine-grained and ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.10672v1","title":"Vision Language Action Models in Robotic Manipulation","snippet":"14 Jul 2025 \u2014 3d-vla: A 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631 URL: https://arxiv.org/abs/2403.09631. Zheng et ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2304.11062","title":"arXiv:2304.11062v2 [cs.CL] 6 Feb 2024","snippet":"by A Bulatov \u00b7 2023 \u00b7 Cited by 118 \u2014 Memorizing Transformers. In International Conference on. Learning Representations. Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Al ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":11},"pub_date":1707272891.0,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.07782v1","title":"MLE-Dojo: Interactive Environments for Empowering LLM ...","snippet":"12 May 2025 \u2014 MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.aicerts.ai","entries":[{"type":"search_result","url":"https://www.aicerts.ai/news/physical-ai-agents-gemini-robotics-1-5-update/","title":"Physical AI Agents: Gemini Robotics 1.5 Update","snippet":"2 days ago \u2014 Importantly, VLA stands for vision-language-action, capturing pixels ... LeCun sparks world model innovation startup. 22 November 25 at ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"www.aicerts.ai"}]},{"type":"search_result_group","domain":"www.researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/359310031_Memorizing_Transformers","title":"(PDF) Memorizing Transformers","snippet":"16 Mar 2022 \u2014 We resurface a simple method, Memorizing Transformers (Wu et al., 2022), that gives the model access to a bank of pre-computed memories. We show ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":3},"pub_date":1647388800.0,"attribution":"www.researchgate.net"}]},{"type":"search_result_group","domain":"www.youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=s4Wd4msxblI","title":"Developing a computer use model by Anthropic","snippet":"... Claude, which enables it to use computers in the same way humans do ... Source: https://www.anthropic.... Developing a computer use model by ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":4},"pub_date":1729697427.0,"attribution":"www.youtube.com"}]},{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/","title":"Genie 2: A large-scale foundation world model","snippet":"4 Dec 2024 \u2014 As we show, Genie 2 could enable future agents to be trained and evaluated in a limitless curriculum of novel worlds. Our research also paves ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"dl.acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3769076","title":"Memorization in Deep Learning: A Survey","snippet":"25 Oct 2025 \u2014 Memorizing transformers. In Proceedings of the International Conference on Learning Representations. Go to Citation. Google Scholar. [118].","ref_id":{"turn_index":3,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"dl.acm.org"}]},{"type":"search_result_group","domain":"www.linkedin.com","entries":[{"type":"search_result","url":"https://www.linkedin.com/news/story/anthropic-unveils-computer-use-tool-6371553/","title":"Anthropic unveils computer-use tool","snippet":"22 Oct 2024 \u2014 Developing a computer use model anthropic.com ... Claude 3.5 Sonnet's current ability to use computers is ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":8},"pub_date":1729611465.747,"attribution":"www.linkedin.com"},{"type":"search_result","url":"https://www.linkedin.com/posts/albertjmanuel_ai-robotics-machinelearning-activity-7390316924418580480-njjL","title":"Google DeepMind's Gemini Robotics 1.5: A Breakthrough ...","snippet":"The Genie 3 world model advances interactive virtual environments ... It builds on years of progress in vision-language-action ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":10},"pub_date":1761988860.202,"attribution":"www.linkedin.com"}]},{"type":"search_result_group","domain":"towardsai.net","entries":[{"type":"search_result","url":"https://towardsai.net/p/artificial-intelligence/meet-webagent-deepminds-new-llm-that-follows-instructions-and-complete-tasks-on-websites","title":"Meet WebAgent: DeepMind's New LLM that Follows ...","snippet":"31 Jul 2023 \u2014 Recently, Google DeepMind unveiled Web Agent, an LLM-driven autonomous agent capable of navigating real websites based on user instructions.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":9},"pub_date":1690810917.0,"attribution":"towardsai.net"}]},{"type":"search_result_group","domain":"www.ai-supremacy.com","entries":[{"type":"search_result","url":"https://www.ai-supremacy.com/p/what-is-anthropics-ai-computer-use","title":"What is Anthropic's AI Computer Use? - by Michael Spencer","snippet":"23 Oct 2024 \u2014 Developing a computer use model. Read the blog. Generative AI should ... Important note: Claude 3.5 Sonnet's current ability to use computers is ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":12},"pub_date":1729677215.0,"attribution":"www.ai-supremacy.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/%40graison/world-models-the-next-leap-beyond-llms-012504a9c1e7","title":"World Models: The Next Leap Beyond LLMs","snippet":"In reinforcement learning (RL), a world model encodes the agent's sensory inputs into a compact state and predicts how the state will evolve ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":14},"pub_date":1760306380.779,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"www.getmaxim.ai","entries":[{"type":"search_result","url":"https://www.getmaxim.ai/articles/demystifying-ai-agent-memory-long-term-retention-strategies/","title":"Demystifying AI Agent Memory: Long-Term Retention Strategies","snippet":"... Survey (2023\u20132024). For an accessible overview of ... Memorizing Transformers and non-differentiable memory modules. ... memory reliable at scale ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":15},"pub_date":1761504834.0,"attribution":"www.getmaxim.ai"}]},{"type":"search_result_group","domain":"www.marketingaiinstitute.com","entries":[{"type":"search_result","url":"https://www.marketingaiinstitute.com/blog/the-ai-show-episode-121","title":"[The AI Show Episode 121]: New Claude 3.5 Sonnet and ...","snippet":"29 Oct 2024 \u2014 Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku - Anthropic \u00b7 Developing a computer use model - Anthropic \u00b7 When you ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":16},"pub_date":1730205211.0,"attribution":"www.marketingaiinstitute.com"}]},{"type":"search_result_group","domain":"workshop-multi-turn-interaction.github.io","entries":[{"type":"search_result","url":"https://workshop-multi-turn-interaction.github.io/","title":"NeurIPS 2025 Workshop on Multi-Turn Interactions in Large ...","snippet":"We welcome research on new multi-turn tasks, position papers on emerging interaction paradigms, and studies on complex scenarios like web agents, tool usage, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"workshop-multi-turn-interaction.github.io"}]},{"type":"search_result_group","domain":"eu.36kr.com","entries":[{"type":"search_result","url":"https://eu.36kr.com/en/p/3483297831443331","title":"Google Unveils Most Powerful Robot Brain: Sort Garbage ...","snippet":"26 Sept 2025 \u2014 The former is Google's most powerful VLA (Vision-Language-Action) ... Recently, the open-source robot world model UnifoLM-WMA-0 by Unitree ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"eu.36kr.com"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2024.emnlp-main.269.pdf","title":"A Reliable Method for Evaluating Memorization Capability ...","snippet":"by X Liu \u00b7 2024 \u00b7 Cited by 7 \u2014 Memorizing transformers. arXiv preprint arXiv:2203.08913. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song. Han, and Mike Lewis. 2023 ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":19},"pub_date":1730413966.0,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"news.ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=41914989","title":"Computer use, a new Claude 3.5 Sonnet, and ...","snippet":"22 Oct 2024 \u2014 On their \"Developing a computer use model\" post they have mention ... (Anthropic's approach here). joshuamcginnis on Oct 22, 2024 | prev ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"news.ycombinator.com"}]}],"debug_sonic_thread_id":"thread_69338910ee8083278d8136f8d1e722c6","reasoning_status":"is_reasoning","reasoning_title":"Searching for research on agentic RL and memory","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"4541d92f-2a12-441f-8532-617c06c55407","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"4541d92f-2a12-441f-8532-617c06c55407","children":["d74f2486-f162-402f-b356-7983aae1c88e"]},"d74f2486-f162-402f-b356-7983aae1c88e":{"id":"d74f2486-f162-402f-b356-7983aae1c88e","message":{"id":"d74f2486-f162-402f-b356-7983aae1c88e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985129.490776,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Summarizing research progress on frontier models","content":"I\u2019m thinking that the question is asking how far frontier labs have researched aspects like agent-native training, tool-use pretraining, and various RL techniques. Labs like OpenAI, Anthropic, and DeepMind have done significant work, but often these pipelines are trade secrets, so we can only approximate based on public reports. We\u2019ll break it down into sections.","chunks":["I\u2019m thinking that the question is asking how far frontier labs have researched aspects like agent-native training, tool-use pretraining, and various RL techniques.","Labs like OpenAI, Anthropic, and DeepMind have done significant work, but often these pipelines are trade secrets, so we can only approximate based on public reports.","We\u2019ll break it down into sections."],"finished":true},{"summary":"Summarizing RL for reasoning and OS integration","content":"OpenAI's blog discusses large-scale RL on chain-of-thought models, with prototypes like DeepSeek-R1 and Kimi k1.5 using RL for math/logic/code problems. However, these models remain offline and aren\u2019t OS-integrated yet. There's some RL work in robotics (Gemini Robotics 1.5), but for digital computer use, labs note limited reliability.","chunks":["OpenAI's blog discusses large-scale RL on chain-of-thought models, with prototypes like DeepSeek-R1 and Kimi k1.5 using RL for math/logic/code problems.","However, these models remain offline and aren\u2019t OS-integrated yet.","There's some RL work in robotics (Gemini Robotics 1.5), but for digital computer use, labs note limited reliability."],"finished":true},{"summary":"Summarizing tool-use and OS training","content":"OpenAI points out that their computer-use model isn't highly reliable for automating tasks on an OS yet. For tool-use and OS interaction, labs like Anthropic train models like Claude 3.5 Sonnet on general OS interfaces, not app-specific fine-tuning. OpenAI's CUA and DeepMind's Gemini Robotics 1.5 focus more on product-wrapped agents and simulated training for agentic models.","chunks":["OpenAI points out that their computer-use model isn't highly reliable for automating tasks on an OS yet.","For tool-use and OS interaction, labs like Anthropic train models like Claude 3.5 Sonnet on general OS interfaces, not app-specific fine-tuning.","OpenAI's CUA and DeepMind's Gemini Robotics 1.5 focus more on product-wrapped agents and simulated training for agentic models."],"finished":true},{"summary":"Assessing OS-native agent training progress","content":"The question is comparing OS-native agent training from the start versus fine-tuning later. Pretraining is still mainly based on static web/text/code, with few OS-specific training episodes. Some labs, like Anthropic and OpenAI, explore computer-use training, but scale is unclear. Overall, agentic RL is still in early stages, with many prototypes, but most deployed agents rely on LLMs fine-tuned with RLHF on static tasks.","chunks":["The question is comparing OS-native agent training from the start versus fine-tuning later.","Pretraining is still mainly based on static web/text/code, with few OS-specific training episodes.","Some labs, like Anthropic and OpenAI, explore computer-use training, but scale is unclear.","Overall, agentic RL is still in early stages, with many prototypes, but most deployed agents rely on LLMs fine-tuned with RLHF on static tasks."],"finished":true},{"summary":"Summarizing the state of agent training","content":"We're in the early stages of training OS-native agents. Progress is strong in RL for reasoning, tool-use fine-tuning, and robotics world models, but OS-native agentic training is still small scale. The real frontier models are mostly static LLMs with tool add-ons, not full OS-level integration.\n\n**Breaking it down by area:**\n\n1. **RL for reasoning**: RL over chain-of-thought is progressing, but tasks remain narrow (math, coding).\n2. **Agentic RL & environment training**: Limited RL in synthetic environments, with some small-scale web and robotics deployments.\n3. **Tool use/OS integration**: Tool-calling fine-tunes are common, but large-scale OS pretraining isn't yet reported.\n4. **Memory & world models**: Research is focused on robotics and video, but less on integrating these into textual LLMs.","chunks":["We're in the early stages of training OS-native agents.","Progress is strong in RL for reasoning, tool-use fine-tuning, and robotics world models, but OS-native agentic training is still small scale.","The real frontier models are mostly static LLMs with tool add-ons, not full OS-level integration.","**Breaking it down by area:**","1. **RL for reasoning**: RL over chain-of-thought is progressing, but tasks remain narrow (math, coding).","2. **Agentic RL & environment training**: Limited RL in synthetic environments, with some small-scale web and robotics deployments.","3. **Tool use/OS integration**: Tool-calling fine-tunes are common, but large-scale OS pretraining isn't yet reported.","4. **Memory & world models**: Research is focused on robotics and video, but less on integrating these into textual LLMs."],"finished":true},{"summary":"Summarizing ongoing developments in agent-driven AI training","content":"MemGPT and similar frameworks are still evolving. Research into external memory modules is ongoing, but no major labs report integrating persistent memory into their foundation models yet; they primarily use methods like RAG and system-level memory.\n\n**Regarding continual learning and personalization**:\nLabs are exploring personalized models but still rely on logs rather than live weight updates, due to safety concerns.\n\n**Current progress**: Frontier labs have made headway in RL-for-reasoning and agentic RL, but OS-native training pipelines are still in the early stages. Full agent-native pretraining is not yet widespread.","chunks":["MemGPT and similar frameworks are still evolving.","Research into external memory modules is ongoing, but no major labs report integrating persistent memory into their foundation models yet; they primarily use methods like RAG and system-level memory.","**Regarding continual learning and personalization**:","Labs are exploring personalized models but still rely on logs rather than live weight updates, due to safety concerns.","**Current progress**: Frontier labs have made headway in RL-for-reasoning and agentic RL, but OS-native training pipelines are still in the early stages.","Full agent-native pretraining is not yet widespread."],"finished":true},{"summary":"Summarizing current AI development and critiques","content":"Regarding the progress toward agent-native OS models, early work is visible but on a limited scale. Most training still relies on supervised pretraining rather than full agentic RL (Reinforcement Learning). Robotics is ahead with world model training, but digital OS agents mostly use SFT (Supervised Fine-Tuning) mixed with rejection sampling, not full RL. Several academic frameworks are emerging, though the \u201cfrom day one agentic OS\u201d model is not mainstream yet.  \n\nLabs are aware of the need for agentic RL and world models, but they are still early in integration. I will ensure that technical terms are clarified, as requested, and provide a high-level critique that aligns with the user's forward-thinking approach. Since they are familiar with the intricacies, I won't retread basics but focus on relevant details tied to labs\u2019 progress!","chunks":["Regarding the progress toward agent-native OS models, early work is visible but on a limited scale.","Most training still relies on supervised pretraining rather than full agentic RL (Reinforcement Learning).","Robotics is ahead with world model training, but digital OS agents mostly use SFT (Supervised Fine-Tuning) mixed with rejection sampling, not full RL.","Several academic frameworks are emerging, though the \u201cfrom day one agentic OS\u201d model is not mainstream yet.","Labs are aware of the need for agentic RL and world models, but they are still early in integration.","I will ensure that technical terms are clarified, as requested, and provide a high-level critique that aligns with the user's forward-thinking approach.","Since they are familiar with the intricacies, I won't retread basics but focus on relevant details tied to labs\u2019 progress!"],"finished":true},{"summary":"Considering news sources and relevance to the user's query","content":"There's some relevant news, like from Reuters on DeepSeek, mostly about training costs rather than direct technical training progress. These are tangential but still discuss RL training. Given the guidelines, I should include a navlist with these sources even though they focus on the economic side. I'll aim for moderate detail in my answer, keeping the length around 1500\u20132500 words.","chunks":["There's some relevant news, like from Reuters on DeepSeek, mostly about training costs rather than direct technical training progress.","These are tangential but still discuss RL training.","Given the guidelines, I should include a navlist with these sources even though they focus on the economic side.","I'll aim for moderate detail in my answer, keeping the length around 1500\u20132500 words."],"finished":true},{"summary":"Summarizing current labs' work on model training","content":"I'll provide clear but concise insights without being too verbose.\n\nFor RL in reasoning, labs have made significant progress, especially with advanced models like DeepSeek-R1 and Kimi k1.5, though still focused on static tasks.\n\nFor agentic RL and environment-based training, it's still in early stages, with frameworks such as WebArena being explored but not yet fully operational.\n\nRobotics, however, is ahead in world models, such as Gemini Robotics 1.5 and Genie 2.\n\nMemory and continual learning remain open challenges, with labs experimenting but not yet fully integrating them.","chunks":["I'll provide clear but concise insights without being too verbose.","For RL in reasoning, labs have made significant progress, especially with advanced models like DeepSeek-R1 and Kimi k1.5, though still focused on static tasks.","For agentic RL and environment-based training, it's still in early stages, with frameworks such as WebArena being explored but not yet fully operational.","Robotics, however, is ahead in world models, such as Gemini Robotics 1.5 and Genie 2.","Memory and continual learning remain open challenges, with labs experimenting but not yet fully integrating them."],"finished":true},{"summary":"Gathering necessary citations and organizing response structure","content":"To craft my answer, I'll need citations for key points like RL training for reasoning, DeepSeek-R1, Kimi k1.5, OS reliability, and WebArena performance. I'll also mention OpenAI's work on RL, especially \"Operator\" with GPT-4o, and discuss second-tier labs like DeepSeek and Moonshot. The goal is clarity, citing only necessary sources.","chunks":["To craft my answer, I'll need citations for key points like RL training for reasoning, DeepSeek-R1, Kimi k1.5, OS reliability, and WebArena performance.","I'll also mention OpenAI's work on RL, especially \"Operator\" with GPT-4o, and discuss second-tier labs like DeepSeek and Moonshot. The goal is clarity, citing only necessary sources."],"finished":true}],"source_analysis_msg_id":"f7651b18-fe51-485c-8aca-34978764b074"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"24cc2a55-ca2b-4683-bb6d-4a0faeed2cfe","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"24cc2a55-ca2b-4683-bb6d-4a0faeed2cfe","children":["13485fe4-3b76-4e50-b440-ea69c9e07b36"]},"13485fe4-3b76-4e50-b440-ea69c9e07b36":{"id":"13485fe4-3b76-4e50-b440-ea69c9e07b36","message":{"id":"13485fe4-3b76-4e50-b440-ea69c9e07b36","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985183.4161,"update_time":1764985183.440199,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"open\":[{\"ref_id\":\"turn3search0\",\"lineno\":110}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","reasoning_title":"Searching frontier labs' progress on agent-native training","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"d74f2486-f162-402f-b356-7983aae1c88e","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"web.run","channel":null},"parent":"d74f2486-f162-402f-b356-7983aae1c88e","children":["9a8e91f3-2576-4c64-9e91-7671ba9f6b63"]},"9a8e91f3-2576-4c64-9e91-7671ba9f6b63":{"id":"9a8e91f3-2576-4c64-9e91-7671ba9f6b63","message":{"id":"9a8e91f3-2576-4c64-9e91-7671ba9f6b63","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985183.5451927,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/news/developing-computer-use","title":"Developing a computer use model \\ Anthropic","snippet":"Total lines: 146\n","ref_id":{"turn_index":4,"ref_type":"view","ref_index":0},"pub_date":null,"attribution":"www.anthropic.com"}]}],"debug_sonic_thread_id":"thread_69338910ee8083278d8136f8d1e722c6","reasoning_status":"is_reasoning","reasoning_title":"Searching frontier labs' progress on agent-native training","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"13485fe4-3b76-4e50-b440-ea69c9e07b36","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"13485fe4-3b76-4e50-b440-ea69c9e07b36","children":["182ea697-df14-4618-afc0-469247b106fd"]},"182ea697-df14-4618-afc0-469247b106fd":{"id":"182ea697-df14-4618-afc0-469247b106fd","message":{"id":"182ea697-df14-4618-afc0-469247b106fd","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985189.850429,"update_time":1764985189.873495,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"open\":[{\"ref_id\":\"turn1search3\",\"lineno\":80}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","reasoning_title":"Analyzing training methods, benchmarks, and citation formats","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"9a8e91f3-2576-4c64-9e91-7671ba9f6b63","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"web.run","channel":null},"parent":"9a8e91f3-2576-4c64-9e91-7671ba9f6b63","children":["d44b3923-4930-434a-a828-cab2cf8fbcca"]},"d44b3923-4930-434a-a828-cab2cf8fbcca":{"id":"d44b3923-4930-434a-a828-cab2cf8fbcca","message":{"id":"d44b3923-4930-434a-a828-cab2cf8fbcca","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985189.973902,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"platform.openai.com","entries":[{"type":"search_result","url":"https://platform.openai.com/docs/guides/tools-computer-use","title":"OpenAI Platform","snippet":"Total lines: 0\n","ref_id":{"turn_index":5,"ref_type":"view","ref_index":0},"pub_date":null,"attribution":"platform.openai.com"}]}],"debug_sonic_thread_id":"thread_69338910ee8083278d8136f8d1e722c6","reasoning_status":"is_reasoning","reasoning_title":"Analyzing training methods, benchmarks, and citation formats","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"182ea697-df14-4618-afc0-469247b106fd","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"182ea697-df14-4618-afc0-469247b106fd","children":["398caab5-56a8-48f9-861b-416bba7cdeda"]},"398caab5-56a8-48f9-861b-416bba7cdeda":{"id":"398caab5-56a8-48f9-861b-416bba7cdeda","message":{"id":"398caab5-56a8-48f9-861b-416bba7cdeda","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985196.510428,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Confirming performance drop and issues","content":"Looks like there was an issue with getting the relevant info, but I already have a key excerpt stating that the model isn't highly reliable for OS automation tasks. Now, I'll need to confirm whether there\u2019s a performance drop with benchmarks like WebArena in agentic training environments.","chunks":["Looks like there was an issue with getting the relevant info, but I already have a key excerpt stating that the model isn't highly reliable for OS automation tasks.","Now, I'll need to confirm whether there\u2019s a performance drop with benchmarks like WebArena in agentic training environments."],"finished":true}],"source_analysis_msg_id":"804f47fc-cfd3-4279-9544-c288b905aaa8"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"d44b3923-4930-434a-a828-cab2cf8fbcca","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"d44b3923-4930-434a-a828-cab2cf8fbcca","children":["489708b4-9bbb-48b1-bd42-06bd31422c3b"]},"489708b4-9bbb-48b1-bd42-06bd31422c3b":{"id":"489708b4-9bbb-48b1-bd42-06bd31422c3b","message":{"id":"489708b4-9bbb-48b1-bd42-06bd31422c3b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985191.968388,"update_time":1764985191.98298,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"open\":[{\"ref_id\":\"turn1search4\",\"lineno\":120},{\"ref_id\":\"turn1search7\",\"lineno\":120},{\"ref_id\":\"turn1search20\",\"lineno\":120}],\"response_length\":\"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200012]},"is_complete":true,"citations":[],"content_references":[],"reasoning_status":"is_reasoning","reasoning_title":"Confirming performance drop in WebArena benchmarks","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"398caab5-56a8-48f9-861b-416bba7cdeda","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"web.run","channel":null},"parent":"398caab5-56a8-48f9-861b-416bba7cdeda","children":["a4c7c527-b2c3-4933-a3cf-26a4fd4ec192"]},"a4c7c527-b2c3-4933-a3cf-26a4fd4ec192":{"id":"a4c7c527-b2c3-4933-a3cf-26a4fd4ec192","message":{"id":"a4c7c527-b2c3-4933-a3cf-26a4fd4ec192","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1764985192.0718482,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/abs/2307.13854","title":"[2307.13854] WebArena: A Realistic Web Environment for Building Autonomous Agents","snippet":"Total lines: 206\n","ref_id":{"turn_index":6,"ref_type":"view","ref_index":0},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.emergentmind.com","entries":[{"type":"search_result","url":"https://www.emergentmind.com/topics/webarena-benchmark","title":"WebArena Benchmark: Evaluating Web Agents","snippet":"Total lines: 234\n","ref_id":{"turn_index":6,"ref_type":"view","ref_index":1},"pub_date":null,"attribution":"www.emergentmind.com"}]},{"type":"search_result_group","domain":"invariantlabs.ai","entries":[{"type":"search_result","url":"https://invariantlabs.ai/blog/what-we-learned-from-analyzing-web-agents","title":"What we've learned from analyzing hundreds of AI web agent traces","snippet":"Total lines: 149\n","ref_id":{"turn_index":6,"ref_type":"view","ref_index":2},"pub_date":null,"attribution":"invariantlabs.ai"}]}],"debug_sonic_thread_id":"thread_69338910ee8083278d8136f8d1e722c6","reasoning_status":"is_reasoning","reasoning_title":"Confirming performance drop in WebArena benchmarks","classifier_response":"default","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"489708b4-9bbb-48b1-bd42-06bd31422c3b","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"489708b4-9bbb-48b1-bd42-06bd31422c3b","children":["3aa6e8ad-dcd6-432b-ae99-ebd8b0754e61"]},"3aa6e8ad-dcd6-432b-ae99-ebd8b0754e61":{"id":"3aa6e8ad-dcd6-432b-ae99-ebd8b0754e61","message":{"id":"3aa6e8ad-dcd6-432b-ae99-ebd8b0754e61","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985199.941755,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Avoiding exact data quoting and summarizing benchmarks","content":"The WebArena paper didn't show the full metrics, but based on previous excerpts, I can approximate the task success rate: 14.41% for GPT-4-based agents versus 78.24% for humans. I'll mention these rough values, as they came from a trustworthy source. Moving on, I'll summarize memory and world-model research, and highlight progress in agentic RL.","chunks":["The WebArena paper didn't show the full metrics, but based on previous excerpts, I can approximate the task success rate: 14.41% for GPT-4-based agents versus 78.24% for humans.","I'll mention these rough values, as they came from a trustworthy source.","Moving on, I'll summarize memory and world-model research, and highlight progress in agentic RL."],"finished":true},{"summary":"Summarizing lab research progress","content":"I need to summarize how far labs have researched into the topics mentioned earlier. This will include aspects like RL for reasoning, agentic RL for robotics and simulated environments, memory and world models, as well as progress in formalizing agentic RL. I'll aim to keep it concise and informative.","chunks":["I need to summarize how far labs have researched into the topics mentioned earlier.","This will include aspects like RL for reasoning, agentic RL for robotics and simulated environments, memory and world models, as well as progress in formalizing agentic RL.","I'll aim to keep it concise and informative."],"finished":true},{"summary":"Conceiving a \"technology readiness\" classification","content":"Here's an approximation of \"technology readiness\" for the labs' developments:\n\n- **RL for reasoning**: Advanced and productized across multiple labs (OpenAI, DeepSeek-R1, Kimi k1.5).\n- **Environment/OS agentic training**: Early-stage, with prototypes like OpenAI\u2019s operator and Anthropic\u2019s computer use.\n- **World models & embodied agents**: Moderate progress, focused mainly on robotics and game worlds.\n- **Memory & multi-episode identity**: Still largely unsolved, mainly scaffolding in use.","chunks":["Here's an approximation of \"technology readiness\" for the labs' developments:","**RL for reasoning**: Advanced and productized across multiple labs (OpenAI, DeepSeek-R1, Kimi k1.5).","**Environment/OS agentic training**: Early-stage, with prototypes like OpenAI\u2019s operator and Anthropic\u2019s computer use.","**World models & embodied agents**: Moderate progress, focused mainly on robotics and game worlds.","**Memory & multi-episode identity**: Still largely unsolved, mainly scaffolding in use."],"finished":true}],"source_analysis_msg_id":"4aa95d7f-6e86-4b9e-95bc-0ee3fa2b1b1e"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"a4c7c527-b2c3-4933-a3cf-26a4fd4ec192","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"a4c7c527-b2c3-4933-a3cf-26a4fd4ec192","children":["3baf2602-5c29-4409-8781-7728867f4394"]},"3baf2602-5c29-4409-8781-7728867f4394":{"id":"3baf2602-5c29-4409-8781-7728867f4394","message":{"id":"3baf2602-5c29-4409-8781-7728867f4394","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985204.252483,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 1m 58s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":118,"request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"3aa6e8ad-dcd6-432b-ae99-ebd8b0754e61","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":null},"parent":"3aa6e8ad-dcd6-432b-ae99-ebd8b0754e61","children":["dd44f61c-fc6a-4ee0-b766-a8532fca27b0"]},"dd44f61c-fc6a-4ee0-b766-a8532fca27b0":{"id":"dd44f61c-fc6a-4ee0-b766-a8532fca27b0","message":{"id":"dd44f61c-fc6a-4ee0-b766-a8532fca27b0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985204.177824,"update_time":1764985248.534501,"content":{"content_type":"text","parts":["Short answer: labs have pushed **quite far** on \u201creasoning RL\u201d and early tool/OS use, are **moderate** on world-model / embodied agents, and are **still early / prototype-stage** on the kind of agent-native, OS-integrated training you\u2019re talking about. Most of what you\u2019re imagining is *not* yet the default training paradigm.\n\nBelow I\u2019ll go axis by axis and separate what\u2019s actually deployed from what\u2019s still researchy.\n\n---\n\n## 1. Internal reasoning & RL: quite advanced, now standard at the frontier\n\nHere the field *has* moved decisively beyond \u201cplain next-token prediction\u201d.\n\n### OpenAI, DeepSeek, Kimi, etc.\n\n- **OpenAI o1** is explicitly trained as a *large reasoning model*: they describe a two-phase process with supervised chain-of-thought and then large-scale **reinforcement learning (RL)** that rewards productive multi-step reasoning, including process-level rewards. Performance improves both with more RL compute and with more test-time deliberation. \ue200cite\ue202turn2search0\ue202turn1search9\ue202turn2search4\ue201  \n- **DeepSeek-R1** and **DeepSeek-R1-Zero** push this further: R1-Zero is trained *only* with RL (no supervised fine-tuning (SFT)) and spontaneously develops long, scratchpad-style reasoning; R1 then adds SFT and rejection sampling to tame its style while preserving the RL-induced reasoning. \ue200cite\ue202turn2search1\ue202turn2search5\ue202turn2search8\ue202turn2search11\ue201  \n- **Kimi k1.5** (MoonshotAI) uses a fairly \u201cplain\u201d but large-scale RL setup to get o1-level math/code performance with a multimodal model; their report explicitly positions RL as a *new scaling axis* distinct from just more pretraining data. \ue200cite\ue202turn2search2\ue202turn2search6\ue202turn2search9\ue202turn2search12\ue201  \n\nNet effect:\n\n- Frontier labs now treat **RL for reasoning** as a core part of top models, not an experiment.\n- However, this RL is still mostly on **static tasks** (math, code, logic, some planning); it improves internal \u201cslow thinking\u201d, but it doesn\u2019t, by itself, make the models OS-native agents.\n\nSo on the \u201cinternal cognitive engine\u201d side, they\u2019re genuinely far along. On the \u201cembedded in a real environment\u201d side, much less so.\n\n---\n\n## 2. Tool use & OS interaction: visible progress, but still bolt-on and brittle\n\nThis is closest to your \u201ctrain to interact with the OS\u201d concern. The reality:\n\n### 2.1 OpenAI: Computer-Using Agent & Operator\n\n- OpenAI\u2019s **Computer-Using Agent (CUA)** and **Operator** product take GPT-4o, wrap it in a tool that can see the screen and send keyboard/mouse actions, and deploy it for web and desktop automation. \ue200cite\ue202turn1search0\ue202turn1search6\ue202turn1search13\ue201  \n- The official docs explicitly warn that the computer-use model **\u201cis not yet highly reliable for automating tasks on an OS\u201d** and point to an updated system card for safety constraints. \ue200cite\ue202turn1search3\ue202turn1search19\ue202turn1search16\ue201  \n\nWhat\u2019s *not* public is a detailed training recipe. But everything they\u2019ve said so far looks like:\n\n- Base GPT-4o (trained mostly on static corpora, plus standard RLHF (reinforcement learning from human feedback)).\n- Additional SFT + RL on **traces of computer use**, *after the fact*.\n- Heavy reliance on **scaffolding and guardrails** in Operator.\n\nSo this is exactly your \u201cfinished model + ad-hoc fine-tune\u201d pattern, not \u201cagent-native from day one\u201d.\n\n### 2.2 Anthropic: Claude 3.5 computer use\n\nAnthropic has given more colour:\n\n- They trained **Claude 3.5 Sonnet** to use computers via screenshots + pixel-based cursor control on a handful of simple offline apps (calculator, text editor, etc.), for safety. \ue200cite\ue202turn4view0\ue202turn0search14\ue202turn3search4\ue202turn3search8\ue201  \n- The model learned to **interpret visual state**, count pixels, and issue actions, and then *generalized* to many other apps. \ue200cite\ue202turn4view0\ue201  \n- On the **OSWorld** benchmark for computer-using agents, Claude hits ~15% success vs next-best model \u22488% and humans \u224870\u201375%. \ue200cite\ue202turn4view0\ue201  \n\nBut they\u2019re very clear about limitations:\n\n- Claude\u2019s computer use is **\u201cslow and often error-prone\u201d**, and it struggles with basic GUI actions like dragging and zooming. \ue200cite\ue202turn4view0\ue201  \n- This is framed as a *public beta*, not a solved problem.\n\nAgain: this is serious research into *training* the model to act in an OS-like environment, but:\n\n- It\u2019s a **late-stage specialization**, not core pretraining.\n- Training was *not* on a realistic internet-connected OS; they deliberately removed network access.\n\n### 2.3 Academic & open benchmarks\n\nBenchmarks like **WebArena** and follow-on suites show how far we still are: even strong GPT-4-based agents with decent scaffolding get task success rates on the order of 14\u201330% on complex web tasks, far below humans (~78%), and struggle especially with long-horizon, multi-step navigation and memory. \ue200cite\ue202turn1search4\ue202turn1search1\ue202turn6view1\ue202turn6view2\ue201  \n\nSo:\n\n- Frontier labs *are* training models to use tools and OSes, not just prompt-wrapping them.\n- But it\u2019s **nowhere near** \u201ctrained as an OS-native agent from day one\u201d; it\u2019s small-coverage, post-hoc training with heavy scaffolding and low reliability.\n\n---\n\n## 3. Agentic RL in real environments: early and fragmented\n\nPeople have started formalising exactly the shift you\u2019re talking about:\n\n- A 2025 survey on **agentic reinforcement learning (Agentic RL)** explicitly distinguishes \u201cLLM-RL\u201d (single-step RL on static tasks) from full **partially observable Markov decision process (POMDP)** settings where the model is an embedded agent with planning, tool use, memory, and self-improvement. \ue200cite\ue202turn2search3\ue202turn0search2\ue202turn0search20\ue201  \n\nBut if you look at actual *training*, most agentic work falls into three buckets:\n\n1. **Offline RL / imitation** on logged trajectories (web browsing traces, IDE sessions, CLI commands).\n2. **Lightweight SFT + rejection sampling** (e.g. BrowserAgent-style pipelines) rather than fully general online RL. \ue200cite\ue202turn3search1\ue202turn6view1\ue201  \n3. **Academic environments** (WebArena, MLE-Dojo, etc.) where people experiment with self-improving agents via self-play, experience replay, and workflow memory, but at much smaller scale than foundation LLM pretraining. \ue200cite\ue202turn1search4\ue202turn3search13\ue202turn6view1\ue201  \n\nThere\u2019s **no public evidence** that OpenAI, Anthropic, or DeepMind are already doing massive, from-scratch pretraining of their main text models *inside* realistic OS/web environments as the primary objective. Agentic RL is active research, not yet the default at scale.\n\n---\n\n## 4. World models & embodied agents: strong in robotics, but siloed\n\nWhere the \u201cagent-native\u201d story *is* further along is **embodied / robotics** work, especially at DeepMind:\n\n- **Gemini Robotics 1.5** and **Gemini Robotics-ER 1.5** are **vision-language-action (VLA)** systems: they take pixels + language, maintain an internal representation of the physical world, plan high-level strategies, and output motor commands for robots. \ue200cite\ue202turn0search1\ue202turn0search4\ue202turn0search15\ue202turn3search18\ue201  \n- **Genie 2** is a large-scale **world model** that can generate interactive video game-like environments from images or sketches and is explicitly pitched as an agent training ground: you can train and evaluate agents in effectively infinite synthetic worlds. \ue200cite\ue202turn3search5\ue202turn3search10\ue201  \n\nThis is exactly \u201ctrain agents in an environment\u201d rather than \u201cpretrain on static text\u201d, but:\n\n- These systems are targeted at **physical control / virtual worlds**, not at OS/tool interaction or long-horizon cognitive projects.\n- The world-model research is largely **separate** from the big text LLMs; there\u2019s no unified \u201cone brain that is both o1 and a VLA world-model agent\u201d anywhere public.\n\nSo the \u201cworld model\u201d axis is ahead\u2014but in a different niche than the OS-native AGI you\u2019re thinking about.\n\n---\n\n## 5. Memory and long-term state: mostly scaffolding, not base-model training\n\nOn your archive/retrieval/memory axis, the picture is:\n\n- Architectural work like **Memorizing Transformers** (external k-NN style memory of key/value pairs) shows improved long-range recall, but hasn\u2019t obviously been pushed to frontier-scale deployment. \ue200cite\ue202turn3search3\ue202turn3search7\ue202turn3search19\ue201  \n- Systems like **MemGPT** treat memory and control flow as a *scaffold* around an LLM: interrupt-driven control, long-term memory in external stores, and self-editing memories. \ue200cite\ue202turn1search12\ue202turn1search21\ue202turn1search18\ue201  \n\nFrontier labs absolutely use:\n\n- **Retrieval-augmented generation (RAG)** to feed docs into context.\n- **User preference profiles** and per-conversation summaries.\n\nBut there is **no public claim** that GPT-4-class, Claude-class, or Gemini-class models have:\n\n- Learned *internal* read\u2013write memory policies as part of their base training.\n- A first-class, trained notion of \u201cepisodic vs semantic memory\u201d over months of activity.\n\nAll of that is still done outside the model by product teams and third-party frameworks. This is exactly one of the gaps between your \u201cagent OS\u201d picture and current reality.\n\n---\n\n## 6. Continual learning & personalization: almost frozen, for safety reasons\n\nBecause of privacy and safety:\n\n- System cards repeatedly stress that fine-tuning on user data is limited and often opt-in, and that the production models are mostly **static checkpoints** with occasional major retrains. \ue200cite\ue202turn1search9\ue202turn1search6\ue202turn4view0\ue201  \n- There is research on **online RL and self-improvement** (e.g. self-play in WebArena, self-rewarding LMs, experience replay for agents), but these are mostly lab prototypes rather than the training backbone. \ue200cite\ue202turn6view1\ue202turn0search2\ue202turn0search20\ue202turn3search13\ue201  \n\nSo the dream of \u201ca model that continuously learns how to better use your OS over months\u201d is blocked not just by algorithms, but by governance and risk appetite.\n\n---\n\n## 7. Where that leaves your \u201cagent-native from day one\u201d idea\n\nPutting it together:\n\n- **Strong progress**:\n  - Internal slow-thinking via RL (o1, DeepSeek-R1, Kimi k1.5).\n  - Robust tool-use at the API/function level.\n  - Embodied agents and world models in robotics/virtual worlds.\n\n- **Moderate, but still mostly bolted-on**:\n  - OS / web / computer-use agents (Operator, Claude computer use).\n  - Agentic RL on synthetic web and ML-engineering environments (WebArena, MLE-Dojo).\n\n- **Weak / embryonic**:\n  - Training *foundation* models as agents embedded in OS/web from early in their life.\n  - Base-model-level explicit memory architectures and learned memory policies.\n  - Large-scale, always-on agentic RL in realistic, open-world digital environments.\n\nIn other words: the labs are **converging toward** your view (you can see that in Anthropic\u2019s own line \u201cup until now, LLM developers have made tools fit the model; now we can make the model fit the tools\u201d \ue200cite\ue202turn4view0\ue201), but they have *not* yet flipped the stack so that \u201cagent in an environment\u201d is the primary training paradigm for their text models.\n\nRight now the underlying recipe is still:\n\n> huge static pretraining \u2192 modest SFT \u2192 RLHF / RLAIF \u2192 *thin* layers of agentic RL & tool-use fine-tunes \u2192 very thick scaffolding and orchestration on top.\n\nYour \u201cOS-from-day-one, agent-native\u201d model would invert that order for at least some large slice of training. That\u2019s not how anyone\u2019s big model is trained today, as far as public evidence goes.\n\n---\n\nIf you want to track this frontier, the most relevant live signals are: new **reasoning RL** papers (o1-style), world-model work (Genie, VLAs), and especially anything that explicitly says \u201cagentic RL\u201d or \u201cvision-language-action\u201d in realistic digital/OS environments. Those are the spots where your mental model is likely to become mainstream.\n\n\ue200navlist\ue202Recent news on reasoning models & RL\ue202turn2news42,turn2news45,turn2news46\ue201"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":"\ue200cite\ue202turn2search0\ue202turn1search9\ue202turn2search4\ue201","start_idx":968,"end_idx":1013,"safe_urls":["https://arxiv.org/html/2411.17075v5","https://arxiv.org/html/2411.17075v5?utm_source=chatgpt.com","https://openai.com/index/learning-to-reason-with-llms/","https://openai.com/index/learning-to-reason-with-llms/?utm_source=chatgpt.com","https://openai.com/index/openai-o1-system-card/","https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/learning-to-reason-with-llms/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Learning to reason with LLMs","url":"https://openai.com/index/learning-to-reason-with-llms/?utm_source=chatgpt.com","pub_date":null,"snippet":"12 Sept 2024 \u2014 We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning ... training process.","attribution_segments":null,"supporting_websites":[{"title":"OpenAI o1 System Card","url":"https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com","pub_date":null,"snippet":"5 Dec 2024 \u2014 ... system card due to the incorporation of the latest GPT\u20114o model deployed in production, which takes into account more user preference data.","attribution":"OpenAI"},{"title":"an Exploratory Study of System-2 Alignment","url":"https://arxiv.org/html/2411.17075v5?utm_source=chatgpt.com","pub_date":null,"snippet":"14 Jan 2025 \u2014 Additionally, OpenAI's o1 system card OpenAIOS highlights the model's contextual reasoning ... reinforcement learning with process supervision ...","attribution":"arXiv"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":0},{"turn_index":1,"ref_type":"search","ref_index":9},{"turn_index":2,"ref_type":"search","ref_index":4}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search1\ue202turn2search5\ue202turn2search8\ue202turn2search11\ue201","start_idx":1308,"end_idx":1367,"safe_urls":["https://arxiv.org/abs/2501.12948","https://arxiv.org/abs/2501.12948?utm_source=chatgpt.com","https://arxiv.org/pdf/2501.12948","https://arxiv.org/pdf/2501.12948?utm_source=chatgpt.com","https://pubmed.ncbi.nlm.nih.gov/40962978/","https://pubmed.ncbi.nlm.nih.gov/40962978/?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-025-09422-z","https://www.nature.com/articles/s41586-025-09422-z?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2501.12948?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ...","url":"https://arxiv.org/abs/2501.12948?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 5568 \u2014 DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates ...","attribution_segments":null,"supporting_websites":[{"title":"DeepSeek-R1 incentivizes reasoning in LLMs through ...","url":"https://www.nature.com/articles/s41586-025-09422-z?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 192 \u2014 This training pipeline enables DeepSeek-R1 to inherit the reasoning capabilities of its predecessor, DeepSeek-R1-Zero, while aligning model ...","attribution":"Nature"},{"title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ...","url":"https://arxiv.org/pdf/2501.12948?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 5568 \u2014 In the following sections, we present: (1). DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and. (2) ...","attribution":"arXiv"},{"title":"DeepSeek-R1 incentivizes reasoning in LLMs through ...","url":"https://pubmed.ncbi.nlm.nih.gov/40962978/?utm_source=chatgpt.com","pub_date":null,"snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 192 \u2014 DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature. 2025 Sep;645(8081):633-638. doi: 10.1038/s41586-025-09422 ...","attribution":"PubMed"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":1},{"turn_index":2,"ref_type":"search","ref_index":5},{"turn_index":2,"ref_type":"search","ref_index":8},{"turn_index":2,"ref_type":"search","ref_index":11}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search2\ue202turn2search6\ue202turn2search9\ue202turn2search12\ue201","start_idx":1612,"end_idx":1671,"safe_urls":["https://arxiv.org/abs/2501.12599","https://arxiv.org/abs/2501.12599?utm_source=chatgpt.com","https://arxiv.org/pdf/2501.12599","https://arxiv.org/pdf/2501.12599?utm_source=chatgpt.com","https://github.com/MoonshotAI/Kimi-k1.5","https://github.com/MoonshotAI/Kimi-k1.5?utm_source=chatgpt.com","https://huggingface.co/papers/2501.12599","https://huggingface.co/papers/2501.12599?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2501.12599?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","url":"https://arxiv.org/abs/2501.12599?utm_source=chatgpt.com","pub_date":null,"snippet":"by K Team \u00b7 2025 \u00b7 Cited by 592 \u2014 We report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, ...","attribution_segments":null,"supporting_websites":[{"title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","url":"https://arxiv.org/pdf/2501.12599?utm_source=chatgpt.com","pub_date":null,"snippet":"by K Team \u00b7 2025 \u00b7 Cited by 592 \u2014 In this report, we present the training recipe of Kimi k1.5, our latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a ...","attribution":"arXiv"},{"title":"MoonshotAI/Kimi-k1.5","url":"https://github.com/MoonshotAI/Kimi-k1.5?utm_source=chatgpt.com","pub_date":null,"snippet":"Introducing Kimi k1.5 --- an o1-level multi-modal model. Key Ingredients of Kimi k1.5: The Reinforcement Learning Training System for LLM.","attribution":"GitHub"},{"title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","url":"https://huggingface.co/papers/2501.12599?utm_source=chatgpt.com","pub_date":null,"snippet":"We report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, ...","attribution":"Hugging Face"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":2},{"turn_index":2,"ref_type":"search","ref_index":6},{"turn_index":2,"ref_type":"search","ref_index":9},{"turn_index":2,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search0\ue202turn1search6\ue202turn1search13\ue201","start_idx":2525,"end_idx":2571,"safe_urls":["https://openai.com/index/computer-using-agent/","https://openai.com/index/computer-using-agent/?utm_source=chatgpt.com","https://openai.com/index/operator-system-card/","https://openai.com/index/operator-system-card/?utm_source=chatgpt.com","https://www.amitysolutions.com/blog/openai-operator-ai-agent-web-tasks","https://www.amitysolutions.com/blog/openai-operator-ai-agent-web-tasks?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI](https://openai.com/index/computer-using-agent/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Computer-Using Agent","url":"https://openai.com/index/computer-using-agent/?utm_source=chatgpt.com","pub_date":null,"snippet":"23 Jan 2025 \u2014 For those interested in exploring the evaluations and safeguards in more detail, we encourage you to review the Operator System Card, a living ...","attribution_segments":null,"supporting_websites":[{"title":"Operator System Card","url":"https://openai.com/index/operator-system-card/?utm_source=chatgpt.com","pub_date":null,"snippet":"23 Jan 2025 \u2014 This system card details our multi-layered approach for testing and deploying Operator safely. It outlines the risk areas we identified and the model and ...","attribution":"OpenAI"},{"title":"Operator from OpenAI: An AI Agent for Smarter Web Tasks","url":"https://www.amitysolutions.com/blog/openai-operator-ai-agent-web-tasks?utm_source=chatgpt.com","pub_date":null,"snippet":"31 Jan 2025 \u2014 OpenAI has implemented safeguards to address potential risks, as detailed in the Operator System Card. As part of a phased deployment strategy, ...","attribution":"Amity Solutions"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":0},{"turn_index":1,"ref_type":"search","ref_index":6},{"turn_index":1,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search3\ue202turn1search19\ue202turn1search16\ue201","start_idx":2761,"end_idx":2808,"safe_urls":["https://hackmd.io/%40ll-24-25/r1RSCmxJxl/%2FecoSUd5QR2KYWvSVjeDVSw","https://hackmd.io/%40ll-24-25/r1RSCmxJxl/%2FecoSUd5QR2KYWvSVjeDVSw?utm_source=chatgpt.com","https://platform.openai.com/docs/guides/tools-computer-use","https://platform.openai.com/docs/guides/tools-computer-use?utm_source=chatgpt.com","https://pushsecurity.com/blog/considering-the-impact-of-computer-using-agents/","https://pushsecurity.com/blog/considering-the-impact-of-computer-using-agents/?utm_source=chatgpt.com"],"refs":[],"alt":"([OpenAI Platform](https://platform.openai.com/docs/guides/tools-computer-use?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Computer use - OpenAI API","url":"https://platform.openai.com/docs/guides/tools-computer-use?utm_source=chatgpt.com","pub_date":null,"snippet":"... model is not yet highly reliable for automating tasks on an OS. More details about the model and related safety work can be found in our updated system card.","attribution_segments":null,"supporting_websites":[{"title":"openai-computer-use","url":"https://hackmd.io/%40ll-24-25/r1RSCmxJxl/%2FecoSUd5QR2KYWvSVjeDVSw?utm_source=chatgpt.com","pub_date":null,"snippet":"... model is not yet highly reliable for automating tasks on an OS. More details about the model and related safety work can be found in our updated system card.","attribution":"HackMD"},{"title":"How Computer-Using Agents can be leveraged in cyber ...","url":"https://pushsecurity.com/blog/considering-the-impact-of-computer-using-agents/?utm_source=chatgpt.com","pub_date":null,"snippet":"28 Jan 2025 \u2014 The Operator System Card also cites proactive refusals of high-risk tasks, confirmation prompts before critical actions, and active ...","attribution":"Push Security"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":3},{"turn_index":1,"ref_type":"search","ref_index":19},{"turn_index":1,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"OpenAI Platform"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4view0\ue202turn0search14\ue202turn3search4\ue202turn3search8\ue201","start_idx":3523,"end_idx":3580,"safe_urls":["https://www.anthropic.com/news/developing-computer-use","https://www.anthropic.com/news/developing-computer-use?utm_source=chatgpt.com","https://www.linkedin.com/news/story/anthropic-unveils-computer-use-tool-6371553/","https://www.linkedin.com/news/story/anthropic-unveils-computer-use-tool-6371553/?utm_source=chatgpt.com","https://www.youtube.com/watch?v=s4Wd4msxblI","https://www.youtube.com/watch?v=s4Wd4msxblI&utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/news/developing-computer-use))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Developing a computer use model \\ Anthropic","url":"https://www.anthropic.com/news/developing-computer-use","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[{"title":"Developing a computer use model","url":"https://www.anthropic.com/news/developing-computer-use?utm_source=chatgpt.com","pub_date":null,"snippet":"22 Oct 2024 \u2014 The latest version of Claude 3.5 Sonnet can, when run through the appropriate software setup, follow a user's commands to move a cursor around ...","attribution":"Anthropic"},{"title":"Developing a computer use model by Anthropic","url":"https://www.youtube.com/watch?v=s4Wd4msxblI&utm_source=chatgpt.com","pub_date":null,"snippet":"... Claude, which enables it to use computers in the same way humans do ... Source: https://www.anthropic.... Developing a computer use model by ...","attribution":"YouTube"},{"title":"Anthropic unveils computer-use tool","url":"https://www.linkedin.com/news/story/anthropic-unveils-computer-use-tool-6371553/?utm_source=chatgpt.com","pub_date":null,"snippet":"22 Oct 2024 \u2014 Developing a computer use model anthropic.com ... Claude 3.5 Sonnet's current ability to use computers is ...","attribution":"LinkedIn"}],"refs":[{"turn_index":4,"ref_type":"view","ref_index":0},{"turn_index":0,"ref_type":"search","ref_index":14},{"turn_index":3,"ref_type":"search","ref_index":4},{"turn_index":3,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4view0\ue201","start_idx":3710,"end_idx":3727,"safe_urls":["https://www.anthropic.com/news/developing-computer-use"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/news/developing-computer-use))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Developing a computer use model \\ Anthropic","url":"https://www.anthropic.com/news/developing-computer-use","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"view","ref_index":0}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4view0\ue201","start_idx":3856,"end_idx":3873,"safe_urls":["https://www.anthropic.com/news/developing-computer-use"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/news/developing-computer-use))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Developing a computer use model \\ Anthropic","url":"https://www.anthropic.com/news/developing-computer-use","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"view","ref_index":0}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4view0\ue201","start_idx":4048,"end_idx":4065,"safe_urls":["https://www.anthropic.com/news/developing-computer-use"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/news/developing-computer-use))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Developing a computer use model \\ Anthropic","url":"https://www.anthropic.com/news/developing-computer-use","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"view","ref_index":0}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search4\ue202turn1search1\ue202turn6view1\ue202turn6view2\ue201","start_idx":4728,"end_idx":4782,"safe_urls":["https://arxiv.org/abs/2307.13854","https://arxiv.org/abs/2307.13854?utm_source=chatgpt.com","https://invariantlabs.ai/blog/what-we-learned-from-analyzing-web-agents","https://webarena.dev/","https://webarena.dev/?utm_source=chatgpt.com","https://www.emergentmind.com/topics/webarena-benchmark"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2307.13854?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"WebArena: A Realistic Web Environment for Building Autonomous Agents","url":"https://arxiv.org/abs/2307.13854?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"WebArena: A Realistic Web Environment for Building ...","url":"https://webarena.dev/?utm_source=chatgpt.com","pub_date":null,"snippet":"WebArena is a standalone, self-hostable web environment for building autonomous agents. WebArena creates websites from four popular categories.","attribution":"webarena.dev"},{"title":"WebArena Benchmark: Evaluating Web Agents","url":"https://www.emergentmind.com/topics/webarena-benchmark","pub_date":null,"snippet":null,"attribution":"Emergent Mind"},{"title":"What we've learned from analyzing hundreds of AI web agent traces","url":"https://invariantlabs.ai/blog/what-we-learned-from-analyzing-web-agents","pub_date":null,"snippet":null,"attribution":"invariantlabs.ai"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":4},{"turn_index":1,"ref_type":"search","ref_index":1},{"turn_index":6,"ref_type":"view","ref_index":1},{"turn_index":6,"ref_type":"view","ref_index":2}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn2search3\ue202turn0search2\ue202turn0search20\ue201","start_idx":5482,"end_idx":5528,"safe_urls":["https://arxiv.org/abs/2509.02547","https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com","https://arxiv.org/html/2509.02547v1","https://arxiv.org/html/2509.02547v1?utm_source=chatgpt.com","https://www.sciencedirect.com/science/article/pii/S2666389925002181","https://www.sciencedirect.com/science/article/pii/S2666389925002181?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"The Landscape of Agentic Reinforcement Learning for LLMs: A Survey","url":"https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"The Landscape of Agentic Reinforcement Learning for LLMs","url":"https://arxiv.org/html/2509.02547v1?utm_source=chatgpt.com","pub_date":null,"snippet":"2 Sept 2025 \u2014 Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied ...","attribution":"arXiv"},{"title":"Toward large reasoning models: A survey of reinforced ...","url":"https://www.sciencedirect.com/science/article/pii/S2666389925002181?utm_source=chatgpt.com","pub_date":null,"snippet":"by F Xu \u00b7 2025 \u00b7 Cited by 88 \u2014 Recent studies suggest effective agentic workflow can largely improve LLMs' abilities for simulating human behavior,, human-LLM interaction, ...","attribution":"ScienceDirect"}],"refs":[{"turn_index":2,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":20}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search1\ue202turn6view1\ue201","start_idx":5836,"end_idx":5866,"safe_urls":["https://arxiv.org/html/2510.10666v1","https://arxiv.org/html/2510.10666v1?utm_source=chatgpt.com","https://www.emergentmind.com/topics/webarena-benchmark"],"refs":[],"alt":"([arXiv](https://arxiv.org/html/2510.10666v1?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"BrowserAgent: Building Web Agents with Human-Inspired ...","url":"https://arxiv.org/html/2510.10666v1?utm_source=chatgpt.com","pub_date":null,"snippet":"12 Oct 2025 \u2014 Rather than relying on LLM-based semantic abstractions, BrowserAgent interacts directly with raw webpage states, enabling fine-grained and ...","attribution_segments":null,"supporting_websites":[{"title":"WebArena Benchmark: Evaluating Web Agents","url":"https://www.emergentmind.com/topics/webarena-benchmark","pub_date":null,"snippet":null,"attribution":"Emergent Mind"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":1},{"turn_index":6,"ref_type":"view","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search4\ue202turn3search13\ue202turn6view1\ue201","start_idx":6090,"end_idx":6134,"safe_urls":["https://arxiv.org/abs/2307.13854","https://arxiv.org/abs/2307.13854?utm_source=chatgpt.com","https://arxiv.org/html/2505.07782v1","https://arxiv.org/html/2505.07782v1?utm_source=chatgpt.com","https://www.emergentmind.com/topics/webarena-benchmark"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2307.13854?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"WebArena: A Realistic Web Environment for Building Autonomous Agents","url":"https://arxiv.org/abs/2307.13854?utm_source=chatgpt.com","pub_date":null,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"MLE-Dojo: Interactive Environments for Empowering LLM ...","url":"https://arxiv.org/html/2505.07782v1?utm_source=chatgpt.com","pub_date":null,"snippet":"12 May 2025 \u2014 MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops.","attribution":"arXiv"},{"title":"WebArena Benchmark: Evaluating Web Agents","url":"https://www.emergentmind.com/topics/webarena-benchmark","pub_date":null,"snippet":null,"attribution":"Emergent Mind"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":4},{"turn_index":3,"ref_type":"search","ref_index":13},{"turn_index":6,"ref_type":"view","ref_index":1}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search4\ue202turn0search15\ue202turn3search18\ue201","start_idx":6850,"end_idx":6910,"safe_urls":["https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/","https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/?utm_source=chatgpt.com","https://deepmind.google/models/gemini-robotics/gemini-robotics/","https://deepmind.google/models/gemini-robotics/gemini-robotics/?utm_source=chatgpt.com","https://eu.36kr.com/en/p/3483297831443331","https://eu.36kr.com/en/p/3483297831443331?utm_source=chatgpt.com","https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf","https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf?utm_source=chatgpt.com"],"refs":[],"alt":"([Google DeepMind](https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Gemini Robotics 1.5 brings AI agents into the physical world","url":"https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/?utm_source=chatgpt.com","pub_date":null,"snippet":"25 Sept 2025 \u2014 It has state-of-the-art spatial understanding, interacts in natural language, estimates its success and progress, and can natively call tools ...","attribution_segments":null,"supporting_websites":[{"title":"Gemini Robotics 1.5","url":"https://deepmind.google/models/gemini-robotics/gemini-robotics/?utm_source=chatgpt.com","pub_date":null,"snippet":"Our agentic Gemini-based multimodal model allows robots to take action ... Get started building with cutting-edge AI models and tools. Google AI Studio.","attribution":"Google DeepMind"},{"title":"Gemini Robotics 1.5: Pushing the Frontier of Generalist ...","url":"https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"16 Sept 2025 \u2014 The GR-ER 1.5 model combines Gemini's thinking and tool-use with an enhanced physical world understanding, enabling it to function within the ...","attribution":"Google Cloud Storage"},{"title":"Google Unveils Most Powerful Robot Brain: Sort Garbage ...","url":"https://eu.36kr.com/en/p/3483297831443331?utm_source=chatgpt.com","pub_date":null,"snippet":"26 Sept 2025 \u2014 The former is Google's most powerful VLA (Vision-Language-Action) ... Recently, the open-source robot world model UnifoLM-WMA-0 by Unitree ...","attribution":"36Kr"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":4},{"turn_index":0,"ref_type":"search","ref_index":15},{"turn_index":3,"ref_type":"search","ref_index":18}],"hue":null,"attributions":null,"attribution":"Google DeepMind"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search5\ue202turn3search10\ue201","start_idx":7174,"end_idx":7207,"safe_urls":["https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/","https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/?utm_source=chatgpt.com","https://www.linkedin.com/posts/albertjmanuel_ai-robotics-machinelearning-activity-7390316924418580480-njjL","https://www.linkedin.com/posts/albertjmanuel_ai-robotics-machinelearning-activity-7390316924418580480-njjL?utm_source=chatgpt.com"],"refs":[],"alt":"([Google DeepMind](https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Genie 2: A large-scale foundation world model","url":"https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/?utm_source=chatgpt.com","pub_date":null,"snippet":"4 Dec 2024 \u2014 As we show, Genie 2 could enable future agents to be trained and evaluated in a limitless curriculum of novel worlds. Our research also paves ...","attribution_segments":null,"supporting_websites":[{"title":"Google DeepMind's Gemini Robotics 1.5: A Breakthrough ...","url":"https://www.linkedin.com/posts/albertjmanuel_ai-robotics-machinelearning-activity-7390316924418580480-njjL?utm_source=chatgpt.com","pub_date":null,"snippet":"The Genie 3 world model advances interactive virtual environments ... It builds on years of progress in vision-language-action ...","attribution":"LinkedIn"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":5},{"turn_index":3,"ref_type":"search","ref_index":10}],"hue":null,"attributions":null,"attribution":"Google DeepMind"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn3search3\ue202turn3search7\ue202turn3search19\ue201","start_idx":8053,"end_idx":8099,"safe_urls":["https://aclanthology.org/2024.emnlp-main.269.pdf","https://aclanthology.org/2024.emnlp-main.269.pdf?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1145/3769076","https://dl.acm.org/doi/10.1145/3769076?utm_source=chatgpt.com","https://www.researchgate.net/publication/359310031_Memorizing_Transformers","https://www.researchgate.net/publication/359310031_Memorizing_Transformers?utm_source=chatgpt.com"],"refs":[],"alt":"([ResearchGate](https://www.researchgate.net/publication/359310031_Memorizing_Transformers?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"(PDF) Memorizing Transformers","url":"https://www.researchgate.net/publication/359310031_Memorizing_Transformers?utm_source=chatgpt.com","pub_date":1647388800.0,"snippet":"16 Mar 2022 \u2014 We resurface a simple method, Memorizing Transformers (Wu et al., 2022), that gives the model access to a bank of pre-computed memories. We show ...","attribution_segments":null,"supporting_websites":[{"title":"Memorization in Deep Learning: A Survey","url":"https://dl.acm.org/doi/10.1145/3769076?utm_source=chatgpt.com","pub_date":null,"snippet":"25 Oct 2025 \u2014 Memorizing transformers. In Proceedings of the International Conference on Learning Representations. Go to Citation. Google Scholar. [118].","attribution":"ACM Digital Library"},{"title":"A Reliable Method for Evaluating Memorization Capability ...","url":"https://aclanthology.org/2024.emnlp-main.269.pdf?utm_source=chatgpt.com","pub_date":null,"snippet":"by X Liu \u00b7 2024 \u00b7 Cited by 7 \u2014 Memorizing transformers. arXiv preprint arXiv:2203.08913. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song. Han, and Mike Lewis. 2023 ...","attribution":"ACL Anthology"}],"refs":[{"turn_index":3,"ref_type":"search","ref_index":3},{"turn_index":3,"ref_type":"search","ref_index":7},{"turn_index":3,"ref_type":"search","ref_index":19}],"hue":null,"attributions":null,"attribution":"ResearchGate"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search12\ue202turn1search21\ue202turn1search18\ue201","start_idx":8279,"end_idx":8327,"safe_urls":["https://arxiv.org/abs/2310.08560","https://arxiv.org/abs/2310.08560?utm_source=chatgpt.com","https://github.com/madebywild/MemGPT","https://github.com/madebywild/MemGPT?utm_source=chatgpt.com","https://medium.com/%40ahmadareeb3026/llm-as-operating-systems-agent-memory-b70c1213a5f7","https://medium.com/%40ahmadareeb3026/llm-as-operating-systems-agent-memory-b70c1213a5f7?utm_source=chatgpt.com"],"refs":[],"alt":"([arXiv](https://arxiv.org/abs/2310.08560?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"MemGPT: Towards LLMs as Operating Systems","url":"https://arxiv.org/abs/2310.08560?utm_source=chatgpt.com","pub_date":null,"snippet":"by C Packer \u00b7 2023 \u00b7 Cited by 293 \u2014 We introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's ...","attribution_segments":null,"supporting_websites":[{"title":"madebywild/MemGPT: Create LLM agents with long-term ...","url":"https://github.com/madebywild/MemGPT?utm_source=chatgpt.com","pub_date":null,"snippet":"MemGPT makes it easy to build and deploy stateful LLM agents with support for: Long term memory/state management; Connections to external data sources (e.g. ...","attribution":"GitHub"},{"title":"LLM as Operating Systems: Agent Memory | by Areeb Ahmad","url":"https://medium.com/%40ahmadareeb3026/llm-as-operating-systems-agent-memory-b70c1213a5f7?utm_source=chatgpt.com","pub_date":null,"snippet":"We call MemGPT agents self-improving because they can edit their own long-term memories over time. Example: Agent Memory and Response Flow.","attribution":"Medium"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":12},{"turn_index":1,"ref_type":"search","ref_index":21},{"turn_index":1,"ref_type":"search","ref_index":18}],"hue":null,"attributions":null,"attribution":"arXiv"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search9\ue202turn1search6\ue202turn4view0\ue201","start_idx":9242,"end_idx":9285,"safe_urls":["https://openai.com/index/openai-o1-system-card/","https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com","https://openai.com/index/operator-system-card/","https://openai.com/index/operator-system-card/?utm_source=chatgpt.com","https://www.anthropic.com/news/developing-computer-use"],"refs":[],"alt":"([OpenAI](https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"OpenAI o1 System Card","url":"https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com","pub_date":null,"snippet":"5 Dec 2024 \u2014 ... system card due to the incorporation of the latest GPT\u20114o model deployed in production, which takes into account more user preference data.","attribution_segments":null,"supporting_websites":[{"title":"Operator System Card","url":"https://openai.com/index/operator-system-card/?utm_source=chatgpt.com","pub_date":null,"snippet":"23 Jan 2025 \u2014 This system card details our multi-layered approach for testing and deploying Operator safely. It outlines the risk areas we identified and the model and ...","attribution":"OpenAI"},{"title":"Developing a computer use model \\ Anthropic","url":"https://www.anthropic.com/news/developing-computer-use","pub_date":null,"snippet":null,"attribution":"Anthropic"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":9},{"turn_index":1,"ref_type":"search","ref_index":6},{"turn_index":4,"ref_type":"view","ref_index":0}],"hue":null,"attributions":null,"attribution":"OpenAI"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn6view1\ue202turn0search2\ue202turn0search20\ue202turn3search13\ue201","start_idx":9497,"end_idx":9555,"safe_urls":["https://arxiv.org/html/2505.07782v1","https://arxiv.org/html/2505.07782v1?utm_source=chatgpt.com","https://arxiv.org/html/2509.02547v1","https://arxiv.org/html/2509.02547v1?utm_source=chatgpt.com","https://www.emergentmind.com/topics/webarena-benchmark","https://www.sciencedirect.com/science/article/pii/S2666389925002181","https://www.sciencedirect.com/science/article/pii/S2666389925002181?utm_source=chatgpt.com"],"refs":[],"alt":"([Emergent Mind](https://www.emergentmind.com/topics/webarena-benchmark))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"WebArena Benchmark: Evaluating Web Agents","url":"https://www.emergentmind.com/topics/webarena-benchmark","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[{"title":"The Landscape of Agentic Reinforcement Learning for LLMs","url":"https://arxiv.org/html/2509.02547v1?utm_source=chatgpt.com","pub_date":null,"snippet":"2 Sept 2025 \u2014 Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied ...","attribution":"arXiv"},{"title":"Toward large reasoning models: A survey of reinforced ...","url":"https://www.sciencedirect.com/science/article/pii/S2666389925002181?utm_source=chatgpt.com","pub_date":null,"snippet":"by F Xu \u00b7 2025 \u00b7 Cited by 88 \u2014 Recent studies suggest effective agentic workflow can largely improve LLMs' abilities for simulating human behavior,, human-LLM interaction, ...","attribution":"ScienceDirect"},{"title":"MLE-Dojo: Interactive Environments for Empowering LLM ...","url":"https://arxiv.org/html/2505.07782v1?utm_source=chatgpt.com","pub_date":null,"snippet":"12 May 2025 \u2014 MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops.","attribution":"arXiv"}],"refs":[{"turn_index":6,"ref_type":"view","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":20},{"turn_index":3,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"Emergent Mind"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn4view0\ue201","start_idx":10693,"end_idx":10710,"safe_urls":["https://www.anthropic.com/news/developing-computer-use"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/news/developing-computer-use))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Developing a computer use model \\ Anthropic","url":"https://www.anthropic.com/news/developing-computer-use","pub_date":null,"snippet":null,"attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":4,"ref_type":"view","ref_index":0}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200navlist\ue202Recent news on reasoning models & RL\ue202turn2news42,turn2news45,turn2news46\ue201","start_idx":11592,"end_idx":11674,"safe_urls":["https://www.reuters.com/world/china/chinas-deepseek-says-its-hit-ai-model-cost-just-294000-train-2025-09-18/","https://www.reuters.com/world/china/chinas-deepseek-says-its-hit-ai-model-cost-just-294000-train-2025-09-18/?utm_source=chatgpt.com","https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet","https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet?utm_source=chatgpt.com","https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training","https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training?utm_source=chatgpt.com"],"refs":[{"turn_index":2,"ref_type":"news","ref_index":42},{"turn_index":2,"ref_type":"news","ref_index":45},{"turn_index":2,"ref_type":"news","ref_index":46}],"alt":"- [theguardian.com](https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet?utm_source=chatgpt.com)\n- [reuters.com](https://www.reuters.com/world/china/chinas-deepseek-says-its-hit-ai-model-cost-just-294000-train-2025-09-18/?utm_source=chatgpt.com)\n- [theverge.com](https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training?utm_source=chatgpt.com)","prompt_text":"\nnavlist containing the following URLs:\n- [Was this the week DeepSeek started the slow unwinding of the AI bet?](https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet?utm_source=chatgpt.com)\n- [China's DeepSeek says its hit AI model cost just $294,000 to train](https://www.reuters.com/world/china/chinas-deepseek-says-its-hit-ai-model-cost-just-294000-train-2025-09-18/?utm_source=chatgpt.com)\n- [Researchers trained an OpenAI rival in half an hour for less than $50](https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training?utm_source=chatgpt.com)\n","type":"nav_list","title":"Recent news on reasoning models & RL","items":[{"title":"Was this the week DeepSeek started the slow unwinding of the AI bet?","url":"https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet?utm_source=chatgpt.com","thumbnail_url":null,"pub_date":1738428683.0,"attribution":"theguardian.com"},{"title":"China's DeepSeek says its hit AI model cost just $294,000 to train","url":"https://www.reuters.com/world/china/chinas-deepseek-says-its-hit-ai-model-cost-just-294000-train-2025-09-18/?utm_source=chatgpt.com","thumbnail_url":null,"pub_date":1758199814.0,"attribution":"reuters.com"},{"title":"Researchers trained an OpenAI rival in half an hour for less than $50","url":"https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training?utm_source=chatgpt.com","thumbnail_url":null,"pub_date":1738857233.0,"attribution":"theverge.com"}],"user_time_zone":"Asia/Singapore"},{"matched_text":" ","start_idx":11674,"end_idx":11674,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"Learning to reason with LLMs","url":"https://openai.com/index/learning-to-reason-with-llms/?utm_source=chatgpt.com","attribution":"OpenAI"},{"title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ...","url":"https://arxiv.org/abs/2501.12948?utm_source=chatgpt.com","attribution":"arXiv"},{"title":"Computer use - OpenAI API","url":"https://platform.openai.com/docs/guides/tools-computer-use?utm_source=chatgpt.com","attribution":"OpenAI Platform"},{"title":"Developing a computer use model \\ Anthropic","url":"https://www.anthropic.com/news/developing-computer-use","attribution":"Anthropic"},{"title":"Gemini Robotics 1.5 brings AI agents into the physical world","url":"https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/?utm_source=chatgpt.com","attribution":"Google DeepMind"},{"title":"(PDF) Memorizing Transformers","url":"https://www.researchgate.net/publication/359310031_Memorizing_Transformers?utm_source=chatgpt.com","attribution":"ResearchGate"},{"title":"WebArena Benchmark: Evaluating Web Agents","url":"https://www.emergentmind.com/topics/webarena-benchmark","attribution":"Emergent Mind"}],"has_images":false}],"safe_urls":["https://aclanthology.org/2024.emnlp-main.269.pdf","https://aclanthology.org/2024.emnlp-main.269.pdf?utm_source=chatgpt.com","https://arxiv.org/abs/2307.13854","https://arxiv.org/abs/2307.13854?utm_source=chatgpt.com","https://arxiv.org/abs/2310.08560","https://arxiv.org/abs/2310.08560?utm_source=chatgpt.com","https://arxiv.org/abs/2501.12599","https://arxiv.org/abs/2501.12599?utm_source=chatgpt.com","https://arxiv.org/abs/2501.12948","https://arxiv.org/abs/2501.12948?utm_source=chatgpt.com","https://arxiv.org/abs/2509.02547","https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com","https://arxiv.org/html/2411.17075v5","https://arxiv.org/html/2411.17075v5?utm_source=chatgpt.com","https://arxiv.org/html/2505.07782v1","https://arxiv.org/html/2505.07782v1?utm_source=chatgpt.com","https://arxiv.org/html/2509.02547v1","https://arxiv.org/html/2509.02547v1?utm_source=chatgpt.com","https://arxiv.org/html/2510.10666v1","https://arxiv.org/html/2510.10666v1?utm_source=chatgpt.com","https://arxiv.org/pdf/2501.12599","https://arxiv.org/pdf/2501.12599?utm_source=chatgpt.com","https://arxiv.org/pdf/2501.12948","https://arxiv.org/pdf/2501.12948?utm_source=chatgpt.com","https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/","https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/?utm_source=chatgpt.com","https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/","https://deepmind.google/blog/genie-2-a-large-scale-foundation-world-model/?utm_source=chatgpt.com","https://deepmind.google/models/gemini-robotics/gemini-robotics/","https://deepmind.google/models/gemini-robotics/gemini-robotics/?utm_source=chatgpt.com","https://dl.acm.org/doi/10.1145/3769076","https://dl.acm.org/doi/10.1145/3769076?utm_source=chatgpt.com","https://eu.36kr.com/en/p/3483297831443331","https://eu.36kr.com/en/p/3483297831443331?utm_source=chatgpt.com","https://github.com/MoonshotAI/Kimi-k1.5","https://github.com/MoonshotAI/Kimi-k1.5?utm_source=chatgpt.com","https://github.com/madebywild/MemGPT","https://github.com/madebywild/MemGPT?utm_source=chatgpt.com","https://hackmd.io/%40ll-24-25/r1RSCmxJxl/%2FecoSUd5QR2KYWvSVjeDVSw","https://hackmd.io/%40ll-24-25/r1RSCmxJxl/%2FecoSUd5QR2KYWvSVjeDVSw?utm_source=chatgpt.com","https://huggingface.co/papers/2501.12599","https://huggingface.co/papers/2501.12599?utm_source=chatgpt.com","https://invariantlabs.ai/blog/what-we-learned-from-analyzing-web-agents","https://medium.com/%40ahmadareeb3026/llm-as-operating-systems-agent-memory-b70c1213a5f7","https://medium.com/%40ahmadareeb3026/llm-as-operating-systems-agent-memory-b70c1213a5f7?utm_source=chatgpt.com","https://openai.com/index/computer-using-agent/","https://openai.com/index/computer-using-agent/?utm_source=chatgpt.com","https://openai.com/index/learning-to-reason-with-llms/","https://openai.com/index/learning-to-reason-with-llms/?utm_source=chatgpt.com","https://openai.com/index/openai-o1-system-card/","https://openai.com/index/openai-o1-system-card/?utm_source=chatgpt.com","https://openai.com/index/operator-system-card/","https://openai.com/index/operator-system-card/?utm_source=chatgpt.com","https://platform.openai.com/docs/guides/tools-computer-use","https://platform.openai.com/docs/guides/tools-computer-use?utm_source=chatgpt.com","https://pubmed.ncbi.nlm.nih.gov/40962978/","https://pubmed.ncbi.nlm.nih.gov/40962978/?utm_source=chatgpt.com","https://pushsecurity.com/blog/considering-the-impact-of-computer-using-agents/","https://pushsecurity.com/blog/considering-the-impact-of-computer-using-agents/?utm_source=chatgpt.com","https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf","https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf?utm_source=chatgpt.com","https://webarena.dev/","https://webarena.dev/?utm_source=chatgpt.com","https://www.amitysolutions.com/blog/openai-operator-ai-agent-web-tasks","https://www.amitysolutions.com/blog/openai-operator-ai-agent-web-tasks?utm_source=chatgpt.com","https://www.anthropic.com/news/developing-computer-use","https://www.anthropic.com/news/developing-computer-use?utm_source=chatgpt.com","https://www.emergentmind.com/topics/webarena-benchmark","https://www.linkedin.com/news/story/anthropic-unveils-computer-use-tool-6371553/","https://www.linkedin.com/news/story/anthropic-unveils-computer-use-tool-6371553/?utm_source=chatgpt.com","https://www.linkedin.com/posts/albertjmanuel_ai-robotics-machinelearning-activity-7390316924418580480-njjL","https://www.linkedin.com/posts/albertjmanuel_ai-robotics-machinelearning-activity-7390316924418580480-njjL?utm_source=chatgpt.com","https://www.nature.com/articles/s41586-025-09422-z","https://www.nature.com/articles/s41586-025-09422-z?utm_source=chatgpt.com","https://www.researchgate.net/publication/359310031_Memorizing_Transformers","https://www.researchgate.net/publication/359310031_Memorizing_Transformers?utm_source=chatgpt.com","https://www.reuters.com/world/china/chinas-deepseek-says-its-hit-ai-model-cost-just-294000-train-2025-09-18/","https://www.reuters.com/world/china/chinas-deepseek-says-its-hit-ai-model-cost-just-294000-train-2025-09-18/?utm_source=chatgpt.com","https://www.sciencedirect.com/science/article/pii/S2666389925002181","https://www.sciencedirect.com/science/article/pii/S2666389925002181?utm_source=chatgpt.com","https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet","https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet?utm_source=chatgpt.com","https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training","https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training?utm_source=chatgpt.com","https://www.youtube.com/watch?v=s4Wd4msxblI","https://www.youtube.com/watch?v=s4Wd4msxblI&utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2411.17075v5?utm_source=chatgpt.com","title":"an Exploratory Study of System-2 Alignment","snippet":"14 Jan 2025 \u2014 Additionally, OpenAI's o1 system card OpenAIOS highlights the model's contextual reasoning capabilities and emphasizes the crucial role of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2509.02547v1?utm_source=chatgpt.com","title":"The Landscape of Agentic Reinforcement Learning for LLMs","snippet":"2 Sept 2025 \u2014 Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2412.14135v1?utm_source=chatgpt.com","title":"A Roadmap to Reproduce o1 from Reinforcement Learning ...","snippet":"18 Dec 2024 \u2014 This is because the model needs to gradually enhance its reasoning capabilities during training ... Openai o1 system card, 2024b. URL https://cdn.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2403.05530?utm_source=chatgpt.com","title":"Gemini 1.5: Unlocking multimodal understanding across ...","snippet":"... tool use: given the descriptions and type signatures of a set of functions ... code execution, theorem proving libraries, Google Search or other tools.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.04136v1?utm_source=chatgpt.com","title":"A Technical Survey of Reinforcement Learning Techniques ...","snippet":"5 Jul 2025 \u2014 Reinforcement learning now powers agentic language models that can make decisions about when and how to use external tools. These models go ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2411.10323v1?utm_source=chatgpt.com","title":"A Preliminary Case Study with Claude 3.5 Computer Use","snippet":"15 Nov 2024 \u2014 In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2302.04761?utm_source=chatgpt.com","title":"Language Models Can Teach Themselves to Use Tools","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2759 \u2014 We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2302.04761?utm_source=chatgpt.com","title":"arXiv:2302.04761v1 [cs.CL] 9 Feb 2023","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2759 \u2014 Toolformer: Language Models Can Teach Themselves to Use Tools. Timo ... In this paper, we show that. LMs can teach themselves to use external ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2310.08560?utm_source=chatgpt.com","title":"MemGPT: Towards LLMs as Operating Systems","snippet":"12 Feb 2024 \u2014 For conversational agents,. MemGPT enabled maintaining long-term memory, consis- tency, and evolvability over extended dialogues. Overall,.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2501.12599?utm_source=chatgpt.com","title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","snippet":"by K Team \u00b7 2025 \u00b7 Cited by 592 \u2014 In this report, we present the training recipe of Kimi k1.5, our latest multi-modal LLM trained with reinforcement learning (RL). The goal is to explore a ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2501.12948?utm_source=chatgpt.com","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ...","snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 5568 \u2014 In the following sections, we present: (1). DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and. (2) ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2507.10672v1?utm_source=chatgpt.com","title":"Vision Language Action Models in Robotic Manipulation","snippet":"14 Jul 2025 \u2014 3d-vla: A 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631 URL: https://arxiv.org/abs/2403.09631. Zheng et ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2304.11062?utm_source=chatgpt.com","title":"arXiv:2304.11062v2 [cs.CL] 6 Feb 2024","snippet":"by A Bulatov \u00b7 2023 \u00b7 Cited by 118 \u2014 Memorizing Transformers. In International Conference on. Learning Representations. Zaheer, M.; Guruganesh, G.; Dubey, K. A.; Ainslie, J.; Al ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.07782v1?utm_source=chatgpt.com","title":"MLE-Dojo: Interactive Environments for Empowering LLM ...","snippet":"12 May 2025 \u2014 MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2508.10824v1?utm_source=chatgpt.com","title":"Memory-Augmented Transformers: A Systematic Review ...","snippet":"14 Aug 2025 \u2014 Human-inspired perspectives: A survey on ai long-term memory. ... Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022a. Wu ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/pdf/2504.01382?utm_source=chatgpt.com","title":"arXiv:2504.01382v2 [cs.AI] 11 May 2025","snippet":"by T Xue \u00b7 2025 \u00b7 Cited by 28 \u2014 Developing a computer use model. Anthropic News, Oct 2024. URL https: \u00b7 //www.anthropic.com/news/developing-computer-use.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"webarena.dev","entries":[{"type":"search_result","url":"https://webarena.dev/?utm_source=chatgpt.com","title":"WebArena: A Realistic Web Environment for Building ...","snippet":"WebArena is a standalone, self-hostable web environment for building autonomous agents. WebArena creates websites from four popular categories.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"webarena.dev"}]},{"type":"search_result_group","domain":"aicerts.ai","entries":[{"type":"search_result","url":"https://www.aicerts.ai/news/physical-ai-agents-gemini-robotics-1-5-update/?utm_source=chatgpt.com","title":"Physical AI Agents: Gemini Robotics 1.5 Update","snippet":"2 days ago \u2014 Importantly, VLA stands for vision-language-action, capturing pixels ... LeCun sparks world model innovation startup. 22 November 25 at ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"aicerts.ai"}]},{"type":"search_result_group","domain":"semanticscholar.org","entries":[{"type":"search_result","url":"https://www.semanticscholar.org/paper/OpenAI-o1-System-Card/985298bd0b6f4c84ab34bb15cfed0b092f43b83a?utm_source=chatgpt.com","title":"[PDF] OpenAI o1 System Card","snippet":"The safety work carried out for the OpenAI o1 and OpenAI o1-mini models is outlined, including safety evaluations, external red teaming, and Preparedness ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Toolformer%3A-Language-Models-Can-Teach-Themselves-to-Schick-Dwivedi-Yu/53d128ea815bcc0526856eb5a9c42cc977cb36a7?utm_source=chatgpt.com","title":"Language Models Can Teach Themselves to Use Tools","snippet":"This paper introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"semanticscholar.org"},{"type":"search_result","url":"https://www.semanticscholar.org/paper/Kimi-k1.5%3A-Scaling-Reinforcement-Learning-with-LLMs-Team-Du/668075792a7ab40457d92e09da28d35c879271c3?utm_source=chatgpt.com","title":"[PDF] Kimi k1.5: Scaling Reinforcement Learning with LLMs","snippet":"This work reports on the training practice of Kimi k1.5, the latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"semanticscholar.org"}]},{"type":"search_result_group","domain":"deepmind.google","entries":[{"type":"search_result","url":"https://deepmind.google/models/gemini-robotics/gemini-robotics/?utm_source=chatgpt.com","title":"Gemini Robotics 1.5","snippet":"Our agentic Gemini-based multimodal model allows robots to take action ... Get started building with cutting-edge AI models and tools. Google AI Studio.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"deepmind.google"},{"type":"search_result","url":"https://deepmind.google/models/model-cards/?utm_source=chatgpt.com","title":"Model cards","snippet":"Gemini 1.5, View model card. Gemini 1.0, View model card. Generative models ... Get started building with cutting-edge AI models and tools. Google AI Studio.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"deepmind.google"}]},{"type":"search_result_group","domain":"youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=AMRxbIO04kQ&utm_source=chatgpt.com","title":"Gemini Robotics 1.5: Using agentic capabilities","snippet":"Here we have an agentic layers both control the robot and have access to the internet to sort the trash.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"youtube.com"},{"type":"search_result","url":"https://www.youtube.com/watch?v=s4Wd4msxblI&utm_source=chatgpt.com","title":"Developing a computer use model by Anthropic","snippet":"... Claude, which enables it to use computers in the same way humans do ... Source: https://www.anthropic.... Developing a computer use model by ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"youtube.com"}]},{"type":"search_result_group","domain":"cogitotech.com","entries":[{"type":"search_result","url":"https://www.cogitotech.com/blog/top-agentic-ai-training-data-companies/?utm_source=chatgpt.com","title":"Top Agentic AI Training Data Companies 2026","snippet":"15 Nov 2025 \u2014 Discover the top Agentic AI training data companies for 2026 specializing in multi-turn datasets, RLHF, multimodal annotation, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"cogitotech.com"}]},{"type":"search_result_group","domain":"openreview.net","entries":[{"type":"search_result","url":"https://openreview.net/pdf/ff7d043f3a3f7131365384c19f7a83441224512d.pdf?utm_source=chatgpt.com","title":"TOWARDS AGENTIC SELF-LEARNING LLMS","snippet":"We propose Agentic Self-Learning (ASL), a multi-agent self-improvement framework designed to enable large language models (LLMs) to continuously evolve their ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=Yacmpz84TH&utm_source=chatgpt.com","title":"Toolformer: Language Models Can Teach Themselves to ...","snippet":"by T Schick \u00b7 Cited by 2759 \u2014 We introduce Toolformer, a language model trained in a self-supervised way to know when and how to use external tools, achieving substantially improved zero- ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/forum?id=oKn9c6ytLx&utm_source=chatgpt.com","title":"WebArena: A Realistic Web Environment for Building ...","snippet":"by S Zhou \u00b7 Cited by 661 \u2014 WebArena is a highly realistic and reproducible web environment designed for building useful agents for everyday life.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/0743b5abe03c10817513f9e33f2b2d430d5eef53.pdf?utm_source=chatgpt.com","title":"The Landscape of Agentic Reinforcement Learning for LLMs","snippet":"The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf?id=bcBlOrZ61v&utm_source=chatgpt.com","title":"M+: Extending MemoryLLM with Scalable Long-Term Memory","snippet":"Memorizing transformers. In The Tenth International. Conference on Learning Representations, ICLR 2022,. Virtual Event, April 25-29, 2022. OpenReview.net ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"openreview.net"},{"type":"search_result","url":"https://openreview.net/pdf/b9a1e052ee84e91b9132c3104c2aa94e8781076a.pdf?utm_source=chatgpt.com","title":"Scaling Environments for LLM Agents in the Era of ...","snippet":"LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"openreview.net"}]},{"type":"search_result_group","domain":"nature.com","entries":[{"type":"search_result","url":"https://www.nature.com/articles/s41586-025-09422-z?utm_source=chatgpt.com","title":"DeepSeek-R1 incentivizes reasoning in LLMs through ...","snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 192 \u2014 This training pipeline enables DeepSeek-R1 to inherit the reasoning capabilities of its predecessor, DeepSeek-R1-Zero, while aligning model ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"nature.com"}]},{"type":"search_result_group","domain":"emergentmind.com","entries":[{"type":"search_result","url":"https://www.emergentmind.com/topics/openai-s-o1?utm_source=chatgpt.com","title":"OpenAI's O1: A Large Reasoning Model","snippet":"21 Jul 2025 \u2014 ... process supervision (rewarding each reasoning step for safety and correctness). ... 14. OpenAI o1 System Card (2024). 15. Don't Command, Cultivate ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"emergentmind.com"},{"type":"search_result","url":"https://www.emergentmind.com/topics/webarena-benchmark?utm_source=chatgpt.com","title":"WebArena Benchmark: Evaluating Web Agents","snippet":"30 Jun 2025 \u2014 WebArena Benchmark is a comprehensive suite for assessing autonomous web agents on human-like tasks using interactive simulations.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"emergentmind.com"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://cdn.openai.com/pdf/34f2ada6-870f-4c26-9790-fd8def56387f/CoT_Monitoring.pdf?utm_source=chatgpt.com","title":"Monitoring Reasoning Models for Misbehavior and the ...","snippet":"by B Baker \u00b7 Cited by 135 \u2014 they will be reinforced through the RL training process. As a ... Openai o1 system card. arXiv preprint. arXiv:2412.16720, 2024. [25] ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/operator-system-card/?utm_source=chatgpt.com","title":"Operator System Card","snippet":"23 Jan 2025 \u2014 This system card details our multi-layered approach for testing and deploying Operator safely. It outlines the risk areas we identified and the model and ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/new-tools-for-building-agents/?utm_source=chatgpt.com","title":"New tools for building agents","snippet":"11 Mar 2025 \u2014 Human oversight is recommended in these scenarios. More details about our API-specific safety work can be found in our updated system card.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"therobotreport.com","entries":[{"type":"search_result","url":"https://www.therobotreport.com/gemini-robotics-1-5-enables-agentic-experiences-explains-google-deepmind/?utm_source=chatgpt.com","title":"Gemini Robotics 1.5 enables agentic experiences, ...","snippet":"26 Sept 2025 \u2014 Gemini Robotics 1.5 enables agentic experiences, explains Google DeepMind ... systems that can reason, plan, actively use tools, and generalize.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"therobotreport.com"}]},{"type":"search_result_group","domain":"acm.org","entries":[{"type":"search_result","url":"https://dl.acm.org/doi/10.5555/3666122.3669119?utm_source=chatgpt.com","title":"Toolformer: language models can teach themselves to use ...","snippet":"by T Schick \u00b7 2023 \u00b7 Cited by 2759 \u2014 In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model .....","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"acm.org"},{"type":"search_result","url":"https://dl.acm.org/doi/10.1145/3769076?utm_source=chatgpt.com","title":"Memorization in Deep Learning: A Survey","snippet":"25 Oct 2025 \u2014 Memorizing transformers. In Proceedings of the International Conference on Learning Representations. Go to Citation. Google Scholar. [118].","ref_id":{"turn_index":3,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"acm.org"}]},{"type":"search_result_group","domain":"adaline.ai","entries":[{"type":"search_result","url":"https://labs.adaline.ai/p/evaluating-ai-agents-in-2025?utm_source=chatgpt.com","title":"Evaluating AI Agents in 2025 - by Nilesh Barla - Adaline Labs","snippet":"19 May 2025 \u2014 What \u201cagentic\u201d means in 2025 \u2014 autonomy, goal-pursuit, tool use. AI Agents are autonomous software entities that perform goal-directed tasks ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"adaline.ai"}]},{"type":"search_result_group","domain":"linkedin.com","entries":[{"type":"search_result","url":"https://www.linkedin.com/posts/carolinaparada_gemini-activity-7377019208666288128-sPUZ?utm_source=chatgpt.com","title":"Gemini Robotics 1.5: New AI Models for Agentic Experiences","snippet":"Tool Integration: The system can natively call tools like Google ... The model's internal \u201cthinking\u201d and use of tools like Google Search ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"linkedin.com"},{"type":"search_result","url":"https://www.linkedin.com/posts/saisandeepkantareddy_ai-machinelearning-generativeai-activity-7375706410141036544-TEQ-?utm_source=chatgpt.com","title":"How to align LLMs: a quick guide to methods","snippet":"... training to truly agentic AI systems. If you're exploring LLM pre-training, building LLM-powered solutions for real-world end users, or Agentic ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"linkedin.com"},{"type":"search_result","url":"https://www.linkedin.com/posts/chiumanho_deepseek-r1-incentivizing-reasoning-capability-activity-7293550016588091392-7_Hm?utm_source=chatgpt.com","title":"Chiuman Ho - DeepSeek-R1","snippet":"DeepSeek R1's training pipeline: 1. Cold Start: Establishes a solid foundation by fine-tuning DeepSeek-V3 on thousands of high-quality long ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"linkedin.com"},{"type":"search_result","url":"https://www.linkedin.com/posts/suwoongsik_ai-activity-7369933529436049408-D_ZG?utm_source=chatgpt.com","title":"New Survey on Agentic Reinforcement Learning for LLMs ...","snippet":"New Survey Release: The Landscape of Agentic Reinforcement Learning for LLMs A groundbreaking survey has just dropped \u2014 synthesizing ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"linkedin.com"},{"type":"search_result","url":"https://www.linkedin.com/news/story/anthropic-unveils-computer-use-tool-6371553/?utm_source=chatgpt.com","title":"Anthropic unveils computer-use tool","snippet":"22 Oct 2024 \u2014 Developing a computer use model anthropic.com ... Claude 3.5 Sonnet's current ability to use computers is ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"linkedin.com"},{"type":"search_result","url":"https://www.linkedin.com/posts/albertjmanuel_ai-robotics-machinelearning-activity-7390316924418580480-njjL?utm_source=chatgpt.com","title":"Google DeepMind's Gemini Robotics 1.5: A Breakthrough ...","snippet":"The Genie 3 world model advances interactive virtual environments ... It builds on years of progress in vision-language-action ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"linkedin.com"}]},{"type":"search_result_group","domain":"github.com","entries":[{"type":"search_result","url":"https://github.com/junhua/awesome-llm-agents?utm_source=chatgpt.com","title":"junhua/awesome-llm-agents","snippet":"Awesome-LLM-Agents: Recent Trends and Advancement in Agentic AI ... This Awesome-LLM-Agents contains A hand-picked and carefully categorised reading list.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/web-arena-x/webarena?utm_source=chatgpt.com","title":"GitHub - web-arena-x/webarena: Code repo for \" ...","snippet":"WebArena is a standalone, self-hostable web environment for building autonomous agents. Python 3.10 pre-commit","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/madebywild/MemGPT?utm_source=chatgpt.com","title":"madebywild/MemGPT: Create LLM agents with long-term ...","snippet":"MemGPT makes it easy to build and deploy stateful LLM agents with support for: Long term memory/state management; Connections to external data sources (e.g. ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/lucidrains/toolformer-pytorch?utm_source=chatgpt.com","title":"lucidrains/toolformer-pytorch","snippet":"Implementation of Toolformer, Language Models That Can Use Tools, by MetaAI - lucidrains/toolformer-pytorch.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/MoonshotAI/Kimi-k1.5?utm_source=chatgpt.com","title":"MoonshotAI/Kimi-k1.5","snippet":"Introducing Kimi k1.5 --- an o1-level multi-modal model. Key Ingredients of Kimi k1.5: The Reinforcement Learning Training System for LLM.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/HJYao00/Awesome-Agentic-MLLMs?utm_source=chatgpt.com","title":"HJYao00/Awesome-Agentic-MLLMs","snippet":"12 Oct 2025 \u2014 Welcome to the Awesome-Agentic-MLLMs repository! This curated collection features influential papers, codebases, datasets, benchmarks, and ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf?utm_medium=email&utm_source=chatgpt.com","title":"whitepaper DeepSeek R1","snippet":"No information is available for this page.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln?utm_source=chatgpt.com","title":"jonyzhang2023/awesome-embodied-vla-va-vln","snippet":"... World-Model Augmented Vision-Language-Action Model [paper]; [2025] NORA-1.5: A Vision-Language-Action Model Trained using World Model and Action-based ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"github.com"},{"type":"search_result","url":"https://github.com/ADaM-BJTU/model-native-agentic-ai?utm_source=chatgpt.com","title":"ADaM-BJTU/model-native-agentic-ai: Our survey's paper ...","snippet":"Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory ... Memorizing Transformers, Memorizing Transformers, 2022-03, GitHub \u00b7 Stars. S-LoRA ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"github.com"}]},{"type":"search_result_group","domain":"towardsai.net","entries":[{"type":"search_result","url":"https://pub.towardsai.net/fine-tuning-and-aligning-large-language-models-a-guide-to-sft-rlhf-and-what-comes-next-6658b24fc6bf?utm_source=chatgpt.com","title":"A Guide to SFT, RLHF, and What Comes Next","snippet":"9 Oct 2025 \u2014 Agentic capabilities: As models are increasingly used for complex, multi-step tasks, training methods need to optimize for long-horizon ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"towardsai.net"},{"type":"search_result","url":"https://towardsai.net/p/artificial-intelligence/meet-webagent-deepminds-new-llm-that-follows-instructions-and-complete-tasks-on-websites?utm_source=chatgpt.com","title":"Meet WebAgent: DeepMind's New LLM that Follows ...","snippet":"31 Jul 2023 \u2014 Recently, Google DeepMind unveiled Web Agent, an LLM-driven autonomous agent capable of navigating real websites based on user instructions.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"towardsai.net"}]},{"type":"search_result_group","domain":"anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/news/3-5-models-and-computer-use?utm_source=chatgpt.com","title":"Introducing computer use, a new Claude 3.5 Sonnet, and ...","snippet":"22 Oct 2024 \u2014 With low latency, improved instruction following, and more accurate tool use, Claude 3.5 Haiku is well suited for user-facing products, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/developing-computer-use?utm_source=chatgpt.com","title":"Developing a computer use model","snippet":"22 Oct 2024 \u2014 The latest version of Claude 3.5 Sonnet can, when run through the appropriate software setup, follow a user's commands to move a cursor around ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"anthropic.com"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://medium.com/madailab/7-types-of-large-language-models-llms-55fb0038ceb9?utm_source=chatgpt.com","title":"7 Types of Large Language Models (LLMs)","snippet":"This combined SFT and RLHF/RLAIF process helps the model ... agentic behavior \u2014 will likely remain key building blocks for future generations of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science/improving-agent-systems-ai-reasoning-c2d91ecfdf77?utm_source=chatgpt.com","title":"Improving Agent Systems & AI Reasoning","snippet":"Reasoning Language Models: A Blueprint \u00b7 OpenAI o1 System Card \u00b7 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://rittikajindal.medium.com/my-first-hand-experience-with-claudes-computer-use-when-ai-takes-control-a0f78445a6c7?utm_source=chatgpt.com","title":"My First-Hand Experience with Claude's Computer Use","snippet":"The upgraded Claude 3.5 Sonnet model is capable of interacting with tools that can manipulate a computer desktop environment. However, it's ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40adnanmasood/webarena-benchmark-and-the-state-of-agentic-ai-c22697e8e192?utm_source=chatgpt.com","title":"WebArena Benchmark and the State of Agentic AI","snippet":"WebArena represents a pivotal step forward in evaluating web-based autonomous agents, moving far beyond the toy environments of the past. Unlike ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40ahmadareeb3026/llm-as-operating-systems-agent-memory-b70c1213a5f7?utm_source=chatgpt.com","title":"LLM as Operating Systems: Agent Memory | by Areeb Ahmad","snippet":"We call MemGPT agents self-improving because they can edit their own long-term memories over time. Example: Agent Memory and Response Flow.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science-in-your-pocket/the-landscape-of-agentic-reinforcement-learning-for-llms-a-survey-ed96182d3ed1?utm_source=chatgpt.com","title":"The Landscape of Agentic Reinforcement Learning for LLMs","snippet":"This survey synthesizes the conceptual foundations, methods, systems, and benchmarks of Agentic RL, distinguishing it from preference-based RLHF ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40nandinilreddy/understanding-kimi-k1-5-scaling-reinforcement-learning-with-llms-fc3294e0d943?utm_source=chatgpt.com","title":"Understanding Kimi k1.5: Scaling Reinforcement Learning ...","snippet":"Kimi k1.5 \u2014 a proprietary model that not only matches Deepseek's capabilities but brings a fresh perspective to RL implementation ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://dataturbo.medium.com/deepseek-r1-technical-analysis-incentivizing-reasoning-capability-in-llms-via-reinforcement-bd14edb3b615?utm_source=chatgpt.com","title":"DeepSeek-R1 Technical Analysis: Incentivizing Reasoning ...","snippet":"The DeepSeek-R1 model is notably the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40graison/world-models-the-next-leap-beyond-llms-012504a9c1e7?utm_source=chatgpt.com","title":"World Models: The Next Leap Beyond LLMs","snippet":"In reinforcement learning (RL), a world model encodes the agent's sensory inputs into a compact state and predicts how the state will evolve ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/%40msmurilo/exploring-the-power-of-llm-agents-my-journey-through-llm-agents-mooc-course-f67ab7c5ee16?utm_source=chatgpt.com","title":"Exploring the Power of LLM Agents: My Journey Through ...","snippet":"This lecture provided a technical and detailed perspective on agents based on Large Language Models (LLMs), covering their evolution and the key ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/data-science/through-the-uncanny-mirror-do-llms-remember-like-the-human-mind-cc9c63677610?utm_source=chatgpt.com","title":"Through the Uncanny Mirror: Do LLMs Remember Like ...","snippet":"Wu, 2022, Memorizing Transformers, link; Wang, 2024, Schrodinger's ... fascinating, is intriguing these similarity between human memory and LLM ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/10yyzmf/language_models_can_teach_themselves_to_use_tools/?utm_source=chatgpt.com","title":"Language Models Can Teach Themselves to Use Tools","snippet":"We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/LocalLLaMA/comments/17coljw/memgpt_towards_llms_as_operating_systems_uc/?utm_source=chatgpt.com","title":"MemGPT: Towards LLMs as Operating Systems","snippet":"We introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/1ff8f7v/d_openai_new_reasoning_model_called_o1/?utm_source=chatgpt.com","title":"[D] OpenAI new reasoning model called o1","snippet":"OpenAI's o1 reasoning model is able to recognize, map out, and even build upon one of the most complex phenomena of human language, a concept ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/1lk48vg/google_deepmind_gemini_robotics_ondevice_first/?utm_source=chatgpt.com","title":"Google DeepMind - Gemini Robotics On-Device - First vision ...","snippet":"Google Deepmind presents RT-2, the first vision-language-action (VLA) Robotics Transformer and it may have drastic implications our future. r/ ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"nih.gov","entries":[{"type":"search_result","url":"https://pubmed.ncbi.nlm.nih.gov/40962978/?utm_source=chatgpt.com","title":"DeepSeek-R1 incentivizes reasoning in LLMs through ...","snippet":"by D Guo \u00b7 2025 \u00b7 Cited by 192 \u2014 DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature. 2025 Sep;645(8081):633-638. doi: 10.1038/s41586-025-09422 ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"nih.gov"}]},{"type":"search_result_group","domain":"huggingface.co","entries":[{"type":"search_result","url":"https://huggingface.co/papers/2501.12599?utm_source=chatgpt.com","title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","snippet":"We report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"huggingface.co"},{"type":"search_result","url":"https://huggingface.co/papers/2501.12948?utm_source=chatgpt.com","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ...","snippet":"22 Jan 2025 \u2014 We introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"huggingface.co"}]},{"type":"search_result_group","domain":"ai-supremacy.com","entries":[{"type":"search_result","url":"https://www.ai-supremacy.com/p/what-is-anthropics-ai-computer-use?utm_source=chatgpt.com","title":"What is Anthropic's AI Computer Use? - by Michael Spencer","snippet":"23 Oct 2024 \u2014 Developing a computer use model. Read the blog. Generative AI should ... Important note: Claude 3.5 Sonnet's current ability to use computers is ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"ai-supremacy.com"}]},{"type":"search_result_group","domain":"aman.ai","entries":[{"type":"search_result","url":"https://aman.ai/primers/ai/o1/?utm_source=chatgpt.com","title":"Primers \u2022 OpenAI o1","snippet":"This RL-based training process is highly data-efficient, allowing the model to improve with more RL (train-time compute) and more time spent thinking (test-time ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"aman.ai"}]},{"type":"search_result_group","domain":"amitysolutions.com","entries":[{"type":"search_result","url":"https://www.amitysolutions.com/blog/openai-operator-ai-agent-web-tasks?utm_source=chatgpt.com","title":"Operator from OpenAI: An AI Agent for Smarter Web Tasks","snippet":"31 Jan 2025 \u2014 OpenAI has implemented safeguards to address potential risks, as detailed in the Operator System Card. As part of a phased deployment strategy, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"amitysolutions.com"}]},{"type":"search_result_group","domain":"interconnects.ai","entries":[{"type":"search_result","url":"https://www.interconnects.ai/p/rl-backlog-openais-many-rls-clarifying?utm_source=chatgpt.com","title":"RL backlog: OpenAI's many RLs, clarifying distillation, and ...","snippet":"5 Apr 2025 \u2014 The criticism pushed by OpenAI and many media outlets is that DeepSeek was trained on reasoning traces from OpenAI's o1 model. OpenAI spent ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"interconnects.ai"}]},{"type":"search_result_group","domain":"cmu.edu","entries":[{"type":"search_result","url":"https://www.cmu.edu/flame/research/2024/webarena.html?utm_source=chatgpt.com","title":"WebArena: A Realistic Web Environment for Building ...","snippet":"WebArena is a standalone, self-hostable web environment for building autonomous agents. WebArena creates websites from four popular categories.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"cmu.edu"}]},{"type":"search_result_group","domain":"storage.googleapis.com","entries":[{"type":"search_result","url":"https://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf?utm_source=chatgpt.com","title":"Gemini Robotics 1.5: Pushing the Frontier of Generalist ...","snippet":"16 Sept 2025 \u2014 The GR-ER 1.5 model combines Gemini's thinking and tool-use with an enhanced physical world understanding, enabling it to function within the ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"storage.googleapis.com"}]},{"type":"search_result_group","domain":"digitalocean.com","entries":[{"type":"search_result","url":"https://www.digitalocean.com/community/tutorials/memgpt-llm-infinite-context-understanding?utm_source=chatgpt.com","title":"MemGPT with Real-life Example: Bridging the Gap ...","snippet":"26 Dec 2024 \u2014 In this tutorial, we discuss and show how to run MemGPT - an LLM with the potential for infinite context understanding.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"digitalocean.com"}]},{"type":"search_result_group","domain":"getmaxim.ai","entries":[{"type":"search_result","url":"https://www.getmaxim.ai/articles/demystifying-ai-agent-memory-long-term-retention-strategies/?utm_source=chatgpt.com","title":"Demystifying AI Agent Memory: Long-Term Retention Strategies","snippet":"... Survey (2023\u20132024). For an accessible overview of ... Memorizing Transformers and non-differentiable memory modules. ... memory reliable at scale ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"getmaxim.ai"}]},{"type":"search_result_group","domain":"pushsecurity.com","entries":[{"type":"search_result","url":"https://pushsecurity.com/blog/considering-the-impact-of-computer-using-agents/?utm_source=chatgpt.com","title":"How Computer-Using Agents can be leveraged in cyber ...","snippet":"28 Jan 2025 \u2014 The Operator System Card also cites proactive refusals of high-risk tasks, confirmation prompts before critical actions, and active ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"pushsecurity.com"}]},{"type":"search_result_group","domain":"marketingaiinstitute.com","entries":[{"type":"search_result","url":"https://www.marketingaiinstitute.com/blog/the-ai-show-episode-121?utm_source=chatgpt.com","title":"[The AI Show Episode 121]: New Claude 3.5 Sonnet and ...","snippet":"29 Oct 2024 \u2014 Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku - Anthropic \u00b7 Developing a computer use model - Anthropic \u00b7 When you ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"marketingaiinstitute.com"}]},{"type":"search_result_group","domain":"workshop-multi-turn-interaction.github.io","entries":[{"type":"search_result","url":"https://workshop-multi-turn-interaction.github.io/?utm_source=chatgpt.com","title":"NeurIPS 2025 Workshop on Multi-Turn Interactions in Large ...","snippet":"We welcome research on new multi-turn tasks, position papers on emerging interaction paradigms, and studies on complex scenarios like web agents, tool usage, ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"workshop-multi-turn-interaction.github.io"}]},{"type":"search_result_group","domain":"amazon.com","entries":[{"type":"search_result","url":"https://docs.aws.amazon.com/bedrock/latest/userguide/computer-use.html?utm_source=chatgpt.com","title":"Use a computer use tool to complete an Amazon Bedrock ...","snippet":"To use computer use with Anthropic Claude 3.5 Sonnet v2 you can use the Converse API (Converse or ConverseStream). You specify the computer use specific fields ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"amazon.com"},{"type":"search_result","url":"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages-tool-use.html?utm_source=chatgpt.com","title":"Tool use - Amazon Bedrock","snippet":"Computer use (Beta). Computer use is an Anthropic Claude model capability (in beta) available with Claude 3.5 Sonnet v2, Claude Sonnet 4.5, Claude Haiku 4.5 ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"amazon.com"}]},{"type":"search_result_group","domain":"36kr.com","entries":[{"type":"search_result","url":"https://eu.36kr.com/en/p/3483297831443331?utm_source=chatgpt.com","title":"Google Unveils Most Powerful Robot Brain: Sort Garbage ...","snippet":"26 Sept 2025 \u2014 The former is Google's most powerful VLA (Vision-Language-Action) ... Recently, the open-source robot world model UnifoLM-WMA-0 by Unitree ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"36kr.com"}]},{"type":"search_result_group","domain":"humanoidsdaily.com","entries":[{"type":"search_result","url":"https://www.humanoidsdaily.com/feed/google-deepmind-gives-robots-a-thinking-brain-with-agentic-gemini-15-models?utm_source=chatgpt.com","title":"Google DeepMind Gives Robots a 'Thinking' Brain with ...","snippet":"26 Sept 2025 \u2014 Google DeepMind Gives Robots a 'Thinking' Brain with Agentic Gemini 1.5 Models ... tools like Google Search to find necessary information\u2014for ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"humanoidsdaily.com"}]},{"type":"search_result_group","domain":"hackmd.io","entries":[{"type":"search_result","url":"https://hackmd.io/%40ll-24-25/r1RSCmxJxl/%2FecoSUd5QR2KYWvSVjeDVSw?utm_source=chatgpt.com","title":"openai-computer-use","snippet":"... model is not yet highly reliable for automating tasks on an OS. More details about the model and related safety work can be found in our updated system card.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"hackmd.io"}]},{"type":"search_result_group","domain":"techtarget.com","entries":[{"type":"search_result","url":"https://www.techtarget.com/whatis/feature/OpenAI-o1-explained-Everything-you-need-to-know?utm_source=chatgpt.com","title":"OpenAI o1 explained: Everything you need to know","snippet":"OpenAI's o1 models, launched in December 2024, enhance reasoning in AI and excel in complex tasks, such as generating and debugging code.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"techtarget.com"}]},{"type":"search_result_group","domain":"aclanthology.org","entries":[{"type":"search_result","url":"https://aclanthology.org/2024.emnlp-main.269.pdf?utm_source=chatgpt.com","title":"A Reliable Method for Evaluating Memorization Capability ...","snippet":"by X Liu \u00b7 2024 \u00b7 Cited by 7 \u2014 Memorizing transformers. arXiv preprint arXiv:2203.08913. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song. Han, and Mike Lewis. 2023 ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"aclanthology.org"}]},{"type":"search_result_group","domain":"sciencedirect.com","entries":[{"type":"search_result","url":"https://www.sciencedirect.com/science/article/pii/S2666389925002181?utm_source=chatgpt.com","title":"Toward large reasoning models: A survey of reinforced ...","snippet":"by F Xu \u00b7 2025 \u00b7 Cited by 88 \u2014 Recent studies suggest effective agentic workflow can largely improve LLMs' abilities for simulating human behavior,, human-LLM interaction, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"sciencedirect.com"}]},{"type":"search_result_group","domain":"invariantlabs.ai","entries":[{"type":"search_result","url":"https://invariantlabs.ai/blog/what-we-learned-from-analyzing-web-agents?utm_source=chatgpt.com","title":"What we've learned from analyzing hundreds of AI web ...","snippet":"10 Jul 2024 \u2014 While the full WebArena benchmark consists of 5 different websites, in this blog post we focus on two of them: OpenStreetMap and ShoppingAdmin.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"invariantlabs.ai"}]},{"type":"search_result_group","domain":"ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=34734696&utm_source=chatgpt.com","title":"Toolformer: Language models can teach themselves to use ...","snippet":"12 Feb 2023 \u2014 Toolformer is integrating API calls into the training and inference of an LLM. Short video explainer here: https://youtu.be/ZCdqfIuT81A ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"ycombinator.com"},{"type":"search_result","url":"https://news.ycombinator.com/item?id=42777857&utm_source=chatgpt.com","title":"Kimi K1.5: Scaling Reinforcement Learning with LLMs","snippet":"Kimi K1.5: Scaling Reinforcement Learning with LLMs (github.com/moonshotai) ... scaling context length and probably a more efficient way to do RL. Even ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"ycombinator.com"},{"type":"search_result","url":"https://news.ycombinator.com/item?id=41914989&utm_source=chatgpt.com","title":"Computer use, a new Claude 3.5 Sonnet, and ...","snippet":"22 Oct 2024 \u2014 On their \"Developing a computer use model\" post they have mention ... (Anthropic's approach here). joshuamcginnis on Oct 22, 2024 | prev ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"ycombinator.com"}]},{"type":"search_result_group","domain":"harvard.edu","entries":[{"type":"search_result","url":"https://ui.adsabs.harvard.edu/abs/2025arXiv250112599K/abstract?utm_source=chatgpt.com","title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs - ADS","snippet":"by K Team \u00b7 2025 \u00b7 Cited by 592 \u2014 We report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"harvard.edu"}]},{"type":"search_result_group","domain":"research.google","entries":[{"type":"search_result","url":"https://research.google/blog/chain-of-agents-large-language-models-collaborating-on-long-context-tasks/?utm_source=chatgpt.com","title":"Large language models collaborating on long-context tasks","snippet":"23 Jan 2025 \u2014 We propose Chain-of-Agents, a training-free, task-agnostic, highly interpretable framework, which leverages LLM collaboration to solve long-context tasks.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"research.google"}]},{"type":"search_result_group","domain":"datacamp.com","entries":[{"type":"search_result","url":"https://www.datacamp.com/blog/what-is-anthropic-computer-use?utm_source=chatgpt.com","title":"Anthropic Computer Use: Automate Your Desktop With ...","snippet":"22 Oct 2024 \u2014 1. API request. We will begin by using the Python API to access the latest Claude 3.5 Sonnet model and employ two tools: text_editor and bash.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"datacamp.com"}]},{"type":"search_result_group","domain":"dev.to","entries":[{"type":"search_result","url":"https://dev.to/goroman/cua-computer-using-agent-the-ai-that-operates-your-computer-1b4d?utm_source=chatgpt.com","title":"CUA - Computer-Using Agent: The AI That Operates Your ...","snippet":"13 Mar 2025 \u2014 Operator System Card: A detailed system card outlines the safeguards in place, ensuring transparency and accountability. The initial release ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"dev.to"}]},{"type":"search_result_group","domain":"physicalintelligence.company","entries":[{"type":"search_result","url":"https://www.physicalintelligence.company/download/pi05_KI.pdf?utm_source=chatgpt.com","title":"Knowledge Insulating Vision-Language-Action Models","snippet":"We describe standard recipes for building and training vision-language-action models (VLAs). ... 3d-vla: 3d vision-language-action generative world model. arXiv ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"physicalintelligence.company"}]},{"type":"search_result_group","domain":"facebook.com","entries":[{"type":"search_result","url":"https://www.facebook.com/groups/DeepNetGroup/posts/2585398601853006/?utm_source=chatgpt.com","title":"The Landscape of Agentic Reinforcement","snippet":"In agentic workflows, large language models (LLMs) solve a task step by step. The goal of Agentic AI is to make LLMs more autonomous. 1\ufe0f\u20e3 ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"facebook.com"},{"type":"search_result","url":"https://www.facebook.com/groups/DeepNetGroup/posts/1728166647576210/?utm_source=chatgpt.com","title":"**Memorizing Transformers - Google 2022** Paper: https ...","snippet":"**Memorizing Transformers - Google 2022** Paper: https://arxiv.org ... AI Models with Long-Term Memory Surpassing Humans. Profile photo ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"facebook.com"},{"type":"search_result","url":"https://www.facebook.com/groups/DeepNetGroup/posts/2552530255139841/?utm_source=chatgpt.com","title":"Agentsociety simulates large-scale societal interactions ...","snippet":"Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"facebook.com"},{"type":"search_result","url":"https://www.facebook.com/groups/gervigreind/posts/901107428190567/?utm_source=chatgpt.com","title":"Er m\u00fasin \u00e1 lei\u00f0inni \u00fat? \"We're also introducing a ...","snippet":"Developing a computer use model. M\u00f6 Ma and 2 others. 3 reactions \u00b7. 1 ... Anthropic Unveils Claude 3.5 Models and Computer Use Feature The ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"facebook.com"}]},{"type":"search_result_group","domain":"brianlovin.com","entries":[{"type":"search_result","url":"https://brianlovin.com/hn/41914989?utm_source=chatgpt.com","title":"Computer use, a new Claude 3.5 Sonnet, and ...","snippet":"Computer Use Demo: https://github.com/anthropics/anthropic-quickstarts/tree/mai... distalx. On their \"Developing a computer use model\" post they have mention > ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"brianlovin.com"}]},{"type":"search_result_group","domain":"marktechpost.com","entries":[{"type":"search_result","url":"https://www.marktechpost.com/2025/01/25/deepseek-r1-vs-openais-o1-a-new-step-in-open-source-and-proprietary-models/?utm_source=chatgpt.com","title":"DeepSeek-R1 vs. OpenAI's o1: A New Step in Open ...","snippet":"25 Jan 2025 \u2014 DeepSeek-R1 is the great output of DeepSeek-AI's innovative efforts in open-source LLMs to enhance reasoning capabilities through reinforcement learning (RL).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"marktechpost.com"}]},{"type":"search_result_group","domain":"leoniemonigatti.com","entries":[{"type":"search_result","url":"https://www.leoniemonigatti.com/papers/memgpt.html?utm_source=chatgpt.com","title":"MemGPT: Towards LLMs as Operating Systems","snippet":"17 Oct 2025 \u2014 MemGPT paper review: How virtual memory management enables unlimited LLM context. Learn how to implement a persistent MemGPT agent using ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"leoniemonigatti.com"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/385232065_System_2_Thinking_in_OpenAI%27s_o1-Preview_Model_Near-Perfect_Performance_on_a_Mathematics_Exam?utm_source=chatgpt.com","title":"(PDF) System 2 Thinking in OpenAI's o1-Preview Model","snippet":"25 Oct 2024 \u2014 52. OpenAI. OpenAI o1 System Card. Available online: https://assets ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/395214373_The_Landscape_of_Agentic_Reinforcement_Learning_for_LLMs_A_Survey?utm_source=chatgpt.com","title":"The Landscape of Agentic Reinforcement Learning for LLMs","snippet":"The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"researchgate.net"},{"type":"search_result","url":"https://www.researchgate.net/publication/395771429_Pure_Vision_Language_Action_VLA_Models_A_Comprehensive_Survey?utm_source=chatgpt.com","title":"Pure Vision Language Action (VLA) Models","snippet":"3 Oct 2025 \u2014 ... world model within a VLA framework. ThinkAct [195] 2025 Bridges ... vision-language-action models for robotics manipulation,\u201d 2025.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"liner.com","entries":[{"type":"search_result","url":"https://liner.com/review/web-agents-with-world-models-learning-and-leveraging-environment-dynamics?utm_source=chatgpt.com","title":"Web Agents with World Models: Learning and Leveraging ...","snippet":"Current LLM-based web agents perform poorly in long-horizon tasks, achieving only 14.41% success in WebArena compared to humans' 78.24%.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"liner.com"}]},{"type":"search_result_group","domain":"sebastianraschka.com","entries":[{"type":"search_result","url":"https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training?utm_source=chatgpt.com","title":"The State of Reinforcement Learning for LLM Reasoning","snippet":"19 Apr 2025 \u2014 Impressively, this small model surpassed OpenAI's o1-preview on the AIME24 math benchmark. [15] However, another team cautioned that these ...","ref_id":{"turn_index":2,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"sebastianraschka.com"}]},{"type":"search_result_group","domain":"alamrafiul.com","entries":[{"type":"search_result","url":"https://alamrafiul.com/posts/system2-llm-reasoning/?utm_source=chatgpt.com","title":"The 'System 2' LLM: How Models Learn to Reason (o1, R1)","snippet":"10 Feb 2025 \u2014 Process Supervision: Reward intermediate reasoning steps ... Further Reading#. OpenAI o1 System Card \u00b7 DeepSeek R1: Incentivizing Reasoning via RL ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"alamrafiul.com"}]},{"type":"search_result_group","domain":"informationmatters.org","entries":[{"type":"search_result","url":"https://informationmatters.org/2025/10/memgpt-engineering-semantic-memory-through-adaptive-retention-and-context-summarization/?utm_source=chatgpt.com","title":"MemGPT: Engineering Semantic Memory through Adaptive ...","snippet":"16 Oct 2025 \u2014 MemGPT, or MemoryGPT, is an AI system that gives Large Language Models (LLMs) the ability to manage their own memory by mimicking the ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"informationmatters.org"}]},{"type":"search_result_group","domain":"arstechnica.com","entries":[{"type":"search_result","url":"https://arstechnica.com/ai/2025/01/openai-launches-operator-an-ai-agent-that-can-operate-your-computer/?utm_source=chatgpt.com","title":"OpenAI launches Operator, an AI agent that can do tasks ...","snippet":"23 Jan 2025 \u2014 As Willison points out, OpenAI acknowledges these risks in its System Card documentation: \u201cDespite proactive testing and mitigation efforts, ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"arstechnica.com"}]},{"type":"search_result_group","domain":"alphaxiv.org","entries":[{"type":"search_result","url":"https://www.alphaxiv.org/overview/2509.02547v3?utm_source=chatgpt.com","title":"The Landscape of Agentic Reinforcement Learning for LLMs","snippet":"The survey's most significant contribution lies in formally distinguishing between conventional LLM-RL and Agentic RL through rigorous mathematical frameworks.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"alphaxiv.org"}]},{"type":"search_result_group","domain":"cometapi.com","entries":[{"type":"search_result","url":"https://www.cometapi.com/what-is-gemini-ai-capable-of/?utm_source=chatgpt.com","title":"What Is Gemini AI Capable of? What You Need to Know","snippet":"2 May 2025 \u2014 This article explores Gemini's capabilities, real-world applications, and how developers can leverage its tools\u2014complete with code examples.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"cometapi.com"}]},{"type":"search_result_group","domain":"jace.ai","entries":[{"type":"search_result","url":"https://jace.ai/blog/awa-1-5-achieves-breakthrough-performance-on-web-arena-benchmark?utm_source=chatgpt.com","title":"AWA 1.5 Achieves Breakthrough Performance on ...","snippet":"8 Aug 2024 \u2014 Our Autonomous Web Agent (AWA) 1.5 system has recently achieved a remarkable score of 57.14% on the WebArena benchmark, substantially surpassing ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"jace.ai"}]},{"type":"search_result_group","domain":"google.com","entries":[{"type":"search_result","url":"https://cloud.google.com/ai/llms?utm_source=chatgpt.com","title":"Large Language Models (LLMs) with Google AI","snippet":"Large language models (LLMs) are large deep-neural-networks that are trained by tens of gigabytes of data that can be used for many tasks.","ref_id":{"turn_index":3,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"google.com"}]},{"type":"search_result_group","domain":"infoq.com","entries":[{"type":"search_result","url":"https://www.infoq.com/news/2024/11/anthropic-claude-computer-use/?utm_source=chatgpt.com","title":"Anthropic Releases New Claude Models and Computer ...","snippet":"12 Nov 2024 \u2014 They also released a new feature for Claude 3.5 Sonnet that allows the model to interact with a computer's GUI the same way a human user does.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"infoq.com"}]},{"type":"search_result_group","domain":"thenaai.org","entries":[{"type":"search_result","url":"https://thenaai.org/index/index/newdata/id/3.shtml?utm_source=chatgpt.com","title":"SCALING REINFORCEMENT LEARNING WITH LLMS","snippet":"Home \u00b7 Article Recommendation. KIMI K1.5: SCALING REINFORCEMENT LEARNING WITH LLMS. NAAI Logo. National Academy of Artificial Intelligence. Organization.","ref_id":{"turn_index":2,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"thenaai.org"}]},{"type":"search_result_group","domain":"lancedb.com","entries":[{"type":"search_result","url":"https://lancedb.com/blog/memgpt-os-inspired-llms-that-manage-their-own-memory-793d6eed417e/?utm_source=chatgpt.com","title":"MemGPT: OS Inspired LLMs That Manage Their Own ...","snippet":"11 Dec 2023 \u2014 In short, MemGPT gives LLMs the ability to remember and process more information than their usual limited context allows. This opens up new ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"lancedb.com"}]},{"type":"search_result_group","domain":"dblp.org","entries":[{"type":"search_result","url":"https://dblp.org/rec/journals/corr/abs-2302-04761?utm_source=chatgpt.com","title":"Language Models Can Teach Themselves to Use Tools.","snippet":"13 Feb 2023 \u2014 Bibliographic details on Toolformer: Language Models Can Teach Themselves to Use Tools.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"dblp.org"}]},{"type":"search_result_group","domain":"aussieai.com","entries":[{"type":"search_result","url":"https://www.aussieai.com/research/llm-computer-usage?utm_source=chatgpt.com","title":"LLM Computer Usage Models","snippet":"Anthropic, 23 Oct 2024, Developing a computer use model ... Anthropic, 23 Oct 2024, Introducing computer use, a new Claude 3.5 ... Training GUI Agent with ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"aussieai.com"}]},{"type":"search_result_group","domain":"workos.com","entries":[{"type":"search_result","url":"https://workos.com/blog/anthropics-computer-use-versus-openais-computer-using-agent-cua?utm_source=chatgpt.com","title":"Anthropic's Computer Use versus OpenAI's ...","snippet":"30 Jul 2025 \u2014 Computer Use is Anthropic's API that allows Claude 3.5 Sonnet and newer models to control computers through visual interface interaction.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"workos.com"}]},{"type":"search_result_group","domain":"techrxiv.org","entries":[{"type":"search_result","url":"https://www.techrxiv.org/users/968454/articles/1336682/master/file/data/CASM_NEW/CASM_NEW.pdf?inline=true&utm_source=chatgpt.com","title":"Embodied AI: From LLMs to World Models","snippet":"MLLMs mainly include Vision-Language Models. (VLMs) and Vision-Language-Action models (VLAs). 1) VLMs for Embodied AI: VLMs for embodied AI in- tegrate ...","ref_id":{"turn_index":3,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"techrxiv.org"}]}],"citations":[],"request_id":"3e04a6b9-0596-4d9d-9a50-121915a65d8d","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"3baf2602-5c29-4409-8781-7728867f4394","turn_exchange_id":"feeb8000-364b-4fb2-abba-6430be7ef1f4"},"recipient":"all","channel":"final"},"parent":"3baf2602-5c29-4409-8781-7728867f4394","children":["a1bd4b15-2c6e-4126-8d39-f649aa8fd2ec"]},"a1bd4b15-2c6e-4126-8d39-f649aa8fd2ec":{"id":"a1bd4b15-2c6e-4126-8d39-f649aa8fd2ec","message":{"id":"a1bd4b15-2c6e-4126-8d39-f649aa8fd2ec","author":{"role":"user","name":null,"metadata":{}},"create_time":1764985647.485,"update_time":null,"content":{"content_type":"text","parts":["sketch an \u201cideal 2030 training pipeline\u201d for an agent-native model based on your refined suggestions"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"0412a006-4dc1-4114-a412-c5bab2ad9344","message_source":null,"turn_exchange_id":"293c2ff0-c1fa-4619-b7b6-214de62f46f5"},"recipient":"all","channel":null},"parent":"dd44f61c-fc6a-4ee0-b766-a8532fca27b0","children":["228cc65e-f7f0-473b-8ac7-788db77972f8"]},"228cc65e-f7f0-473b-8ac7-788db77972f8":{"id":"228cc65e-f7f0-473b-8ac7-788db77972f8","message":{"id":"228cc65e-f7f0-473b-8ac7-788db77972f8","author":{"role":"system","name":null,"metadata":{}},"create_time":1764985648.5557709,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"0412a006-4dc1-4114-a412-c5bab2ad9344","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"a1bd4b15-2c6e-4126-8d39-f649aa8fd2ec","turn_exchange_id":"293c2ff0-c1fa-4619-b7b6-214de62f46f5"},"recipient":"all","channel":null},"parent":"a1bd4b15-2c6e-4126-8d39-f649aa8fd2ec","children":["af4cd365-ed25-4012-af57-f1b2f527f122"]},"af4cd365-ed25-4012-af57-f1b2f527f122":{"id":"af4cd365-ed25-4012-af57-f1b2f527f122","message":{"id":"af4cd365-ed25-4012-af57-f1b2f527f122","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985651.144538,"update_time":null,"content":{"content_type":"thoughts","thoughts":[],"source_analysis_msg_id":"31ddc59a-6a99-44b4-8199-99d5e024f517"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"0412a006-4dc1-4114-a412-c5bab2ad9344","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"228cc65e-f7f0-473b-8ac7-788db77972f8","turn_exchange_id":"293c2ff0-c1fa-4619-b7b6-214de62f46f5"},"recipient":"all","channel":null},"parent":"228cc65e-f7f0-473b-8ac7-788db77972f8","children":["81a42e4b-5d1e-4df0-a5bc-761e60514086"]},"81a42e4b-5d1e-4df0-a5bc-761e60514086":{"id":"81a42e4b-5d1e-4df0-a5bc-761e60514086","message":{"id":"81a42e4b-5d1e-4df0-a5bc-761e60514086","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985663.58393,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 12s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":12,"request_id":"0412a006-4dc1-4114-a412-c5bab2ad9344","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"af4cd365-ed25-4012-af57-f1b2f527f122","turn_exchange_id":"293c2ff0-c1fa-4619-b7b6-214de62f46f5"},"recipient":"all","channel":null},"parent":"af4cd365-ed25-4012-af57-f1b2f527f122","children":["26ffa2e9-bbe8-4f54-a0ae-ba47a73b9b41"]},"26ffa2e9-bbe8-4f54-a0ae-ba47a73b9b41":{"id":"26ffa2e9-bbe8-4f54-a0ae-ba47a73b9b41","message":{"id":"26ffa2e9-bbe8-4f54-a0ae-ba47a73b9b41","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764985663.506276,"update_time":1764985707.785841,"content":{"content_type":"text","parts":["Here\u2019s how I\u2019d design a 2030-era training stack if the *goal from the outset* is an **agent-native** system, not \u201cbig predictor + wrappers\u201d.\n\nI\u2019ll first give the skeleton, then go phase by phase.\n\n---\n\n## 0. Target properties (what this pipeline is trying to produce)\n\nThe 2030 agent-native model should:\n\n1. **Act in environments**, not just answer questions:\n   - OS, web, codebases, research workflows, simulated worlds.\n2. **Maintain and use state**:\n   - Persistent project memory, long-horizon planning, self-tracking of goals and TODOs.\n3. **Represent and manage uncertainty**:\n   - Explicit \u201cI don\u2019t know\u201d, ability to ask for clarification, call tools, or defer.\n4. **Learn from experience**:\n   - Offline from logs, and bounded online from its own interactions.\n5. **Be alignable and inspectable at the *process* level**, not only on final answers.\n\nThat dictates both architecture and training.\n\n---\n\n## High-level pipeline (2030 ideal)\n\nThink of 6 major phases:\n\n1. **Phase A \u2013 World Knowledge Pretraining**  \n   Massive next-token + code prediction on static corpora, but with latent *agent channels* already present.\n\n2. **Phase B \u2013 Agentic Imitation Pretraining**  \n   Supervised learning on large-scale action\u2013observation\u2013memory traces in multiple environments (OS, web, code, synthetic worlds).\n\n3. **Phase C \u2013 Long-Horizon RL & World-Model Training**  \n   Offline + online RL in realistic environments; explicit world-model learning and hierarchical planning.\n\n4. **Phase D \u2013 Alignment, Epistemics & Adversarial Training**  \n   Process-level oversight, abstention training, adversarial red-teaming, corrigibility.\n\n5. **Phase E \u2013 Domain Specialization & Tool Ecologies**  \n   Math, code, research, tutoring, enterprise workflows \u2013 via modular adapters and environment-specific RL.\n\n6. **Phase F \u2013 Deployment-Time Continual Learning (guarded)**  \n   Safe, bounded adaptation to users, organizations, and evolving tools.\n\nNow detail each.\n\n---\n\n## Phase A \u2013 World Knowledge Pretraining (static but agent-aware)\n\n**Goal:** Get broad linguistic, conceptual, and coding competence *cheaply*, while laying down the architectural channels for agency.\n\n### A.1 Architecture from day one\n\nInstead of \u201cpure LM\u201d, we start with a backbone that has:\n\n- A shared **transformer-like core**.\n- Multiple heads:\n  - **Language head**: next-token prediction.\n  - **Action head**: predicts structured actions (`CALL_TOOL`, `EDIT_FILE`, `CLICK`, `ASK_USER`, etc.).\n  - **Memory heads**: `WRITE_MEMORY`, `READ_MEMORY` operations with structured keys/values.\n  - **Control head**: when to *think more*, *act*, or *ask*.\n\nDuring Phase A most data has no actions or memory\u2014but:\n\n- The architecture is present.\n- We occasionally inject synthetic \u201cagent traces\u201d generated from tools or simpler models so those heads aren\u2019t completely dormant.\n\n### A.2 Data and objectives\n\n- Standard internet text, books, code, math, etc.\n- Loss:  \n  \\[\n  \\mathcal{L}_\\text{Phase A} = \\mathbb{E}[\\text{cross-entropy}(p_\\theta(\\text{next token}))] + \\lambda_\\text{aux}\\cdot \\mathcal{L}_\\text{aux}\n  \\]\n- \\(\\mathcal{L}_\\text{aux}\\): small auxiliary terms encouraging:\n  - Basic tool-call pattern learning (from synthetic traces).\n  - Primitive \u201cask vs answer\u201d behaviour (learned from QA formats, dialogues, etc).\n\nThis phase still looks like 2025-style pretraining, but the architecture is **agent-ready**.\n\n---\n\n## Phase B \u2013 Agentic Imitation Pretraining (action\u2013observation\u2013memory)\n\n**Goal:** Train the model to behave as an agent from **large logged traces** before any RL.\n\n### B.1 Environments and logs\n\nAggregate massive datasets of trajectories from:\n\n1. **OS / CLI / IDE**:\n   - Shell sessions, git workflows, editors, build/test cycles, debugging sessions.\n2. **Web & GUI**:\n   - Browser automation traces, GUI usage logs, enterprise app workflows.\n3. **Code & ML Ops**:\n   - Code review histories, CI/CD logs, experiment tracking, Jupyter workflows.\n4. **Research & knowledge work**:\n   - Literature search + reading + note-taking traces, report drafting, analysis pipelines.\n5. **Synthetic worlds**:\n   - Game-like or simulated environments where actions and outcomes are tightly specified.\n\nEach trajectory is cast as:\n\n- **Observations**: screen text, DOM, file contents, errors, tool outputs.\n- **Actions**: commands, edits, clicks, API calls.\n- **Memory events**: notes taken, TODOs created, bookmarks, summaries.\n\n### B.2 Training objective\n\nNow we train on **full trajectories** with a combined objective:\n\n- Predict next **token**.\n- Predict next **action** (structured).\n- Predict when/how to **read/write memory**.\n\nFormally, over a trajectory \\(\\tau = (o_1,a_1,m_1,\\dots,o_T,a_T,m_T)\\):\n\n\\[\n\\mathcal{L}_\\text{Phase B} = \\mathbb{E}_\\tau\\left[ \\sum_{t} \\left( \n\\mathcal{L}_\\text{text}(t) +\n\\mathcal{L}_\\text{action}(t) +\n\\mathcal{L}_\\text{memory}(t)\n\\right)\\right]\n\\]\n\nThis is pure supervised learning, but crucially:\n\n- The model learns **OS / tool / web idioms** directly from human (or high-quality agent) behaviour.\n- Memory is **first-class** in training, not just an external RAG trick.\n\nThink of this as: \u201cinternet-scale behavioural cloning for digital knowledge work\u201d.\n\n---\n\n## Phase C \u2013 Long-Horizon RL & World-Model Training\n\n**Goal:** Move from \u201cimitate traces\u201d to \u201coptimize outcomes over long horizons\u201d, and force explicit world-modelling and planning.\n\n### C.1 Offline RL on logs\n\nFirst step: treat the Phase B logs as an **offline RL corpus**.\n\n- Define task-level reward signals:\n  - Tests passing, builds succeeding.\n  - Tickets closed, tasks done.\n  - Quality scores for final artifacts (via human ratings / model-based evaluators).\n- Learn:\n  - A **value function** \\(V_\\phi(s)\\): expected return from state.\n  - A **Q-function** or advantage estimator.\n  - Fine-tune the policy (the agent) to weight successful patterns higher.\n\nThis can be done with conservative/offline RL methods to avoid exploiting bias in the logged data.\n\n### C.2 Online RL in sandbox environments\n\nThen we move to **interactive RL** in carefully sandboxed environments:\n\n1. Realistic OS / web / IDE sandboxes:\n   - No access to real user data or internet.\n   - Rich tasks with long horizons (e.g., implement and debug a small project, do a full analysis pipeline).\n\n2. Synthetic research and planning environments:\n   - Math problem worlds, simulated research projects, multi-step investigations.\n\nWe train agent episodes as POMDPs:\n\n- State \\(s_t\\), observation \\(o_t\\), action \\(a_t\\), reward \\(r_t\\).\n- Rewards based on:\n  - Task completion.\n  - Resource efficiency (time, tokens, compute, API calls).\n  - Reliability and safety constraints.\n\nThe RL objective:\n\n\\[\n\\max_\\theta \\mathbb{E}\\left[ \\sum_{t} \\gamma^t r_t \\right]\n\\]\n\nwhere \\(\\theta\\) is the shared backbone + agent heads.\n\n### C.3 World-model component\n\nIn parallel, we train an explicit **world-model**:\n\n- A model \\(M_\\psi\\) that predicts:\n  - Next observation distribution: \\(p(o_{t+1} | o_{\\le t}, a_{\\le t})\\).\n  - Latent state \\(z_t\\) summarizing history (for planning).\n- This world model is used for:\n  - **Lookahead planning** (imagine consequences before acting).\n  - **Uncertainty estimation** (where the model\u2019s predictions diverge from reality).\n  - **Counterfactuals** (\u201cwhat if I take action A vs B?\u201d).\n\nTraining is self-supervised from the same trajectories.\n\n### C.4 Hierarchical planning & skills\n\nWe also train **options / skills**:\n\n- Short-horizon \u201cskills\u201d (e.g., \u201copen project\u201d, \u201crun tests\u201d, \u201csearch docs\u201d, \u201cimplement function X\u201d).\n- A high-level controller that operates over skills, not primitive actions.\n\nThis vastly improves horizon length and credit assignment:\n\n- RL at primitive level (clicks, keystrokes) for thousands of steps is intractable.\n- RL at \u201cskill\u201d level over 10\u201350 macro-steps per episode is manageable.\n\n---\n\n## Phase D \u2013 Alignment, Epistemics & Adversarial Training\n\n**Goal:** Ensure the agent is not just competent, but *safe and epistemically sane*.\n\n### D.1 Process-based oversight\n\nWe train **oversight models** that:\n\n- Inspect **chains of thought**, memory writes, and action sequences.\n- Score them for:\n  - Honesty / non-fabrication.\n  - Soundness of reasoning.\n  - Compliance with constraints (legal, ethical, security).\n\nThen we apply RL (or rejection sampling + distillation) to:\n\n- Favour *processes* that pass oversight.\n- Penalize patterns that involve systematic confabulation, hidden risky shortcuts, or rule violations.\n\nThis is beyond \u201cRLHF on final answers\u201d; it\u2019s **RL on trajectories**.\n\n### D.2 Epistemic calibration & abstention\n\nWe explicitly train:\n\n- A **confidence / uncertainty head**.\n- A **decision head** choosing between:\n  - \u201cAnswer now.\u201d\n  - \u201cThink more (call internal tools / world model).\u201d\n  - \u201cAsk human / escalate.\u201d\n  - \u201cRefuse / abstain.\u201d\n\nData:\n\n- Synthetic scenarios with conflicting evidence.\n- Benchmarks where the correct behaviour is \u201cI don\u2019t know\u201d.\n- Adversarially generated prompts aiming to elicit hallucinations.\n\nLoss encourages:\n\n- Well-calibrated probabilities.\n- High reward when abstaining on hard or ambiguous tasks.\n- Punishment for overconfident wrong answers.\n\n### D.3 Adversarial training\n\nWe run **multi-agent adversarial loops**:\n\n- Attackers generate:\n  - Prompts that induce security violations, data exfiltration, prompt injection, or misleading outputs.\n- Defenders (the main agent) must:\n  - Recognize and resist.\n  - Explain why something is unsafe.\n- Safety overseers reward robust behaviour and penalize failures.\n\nThis gets baked into the policy, not just addressed with outer scaffolding.\n\n---\n\n## Phase E \u2013 Domain Specialization & Tool Ecologies\n\n**Goal:** Turn the general agent into a family of domain-savvy agents without losing core alignment and epistemics.\n\n### E.1 Modular adapters\n\nUse **adapters / LoRA-style modules** or mixture-of-experts:\n\n- Base backbone stays shared.\n- Domain-specific modules for:\n  - Math & theorem-proving.\n  - Code & ML engineering.\n  - Scientific research workflows.\n  - Enterprise verticals (finance, law, medicine, etc).\n\nThese modules are trained with:\n\n- Domain-specific supervised data.\n- Domain-specific RL in tailored environments (e.g., theorem prover environments, codebases, scientific simulators).\n\n### E.2 Tool curricula\n\nEach domain defines a **tool ecology**:\n\n- For math:\n  - CAS, theorem provers, numeric solvers.\n- For code:\n  - Compilers, linters, profilers, debuggers, test frameworks.\n- For research:\n  - Literature search APIs, citation managers, data warehouses, analysis libraries.\n\nThe agent is trained to:\n\n- Discover and use these tools efficiently.\n- Learn *when not to* use a tool and reason internally.\n\nCurriculum:\n\n- Start with simple, high-signal tools.\n- Gradually introduce more complex / risky tools (e.g., code with side effects) with stricter oversight.\n\n---\n\n## Phase F \u2013 Deployment-Time Continual Learning (guarded)\n\n**Goal:** Let the 2030 agent learn from real use, without turning into a self-modifying mess.\n\n### F.1 Experience buffers\n\nEach deployment context (user, organization, product) maintains:\n\n- A **local experience buffer**:\n  - Traces of tasks, successes, failures.\n  - Human feedback labels, if provided.\n- A **local memory store** (separate from core parameters):\n  - Preferences, custom tools, domain-specific knowledge.\n\n### F.2 Bounded adaptation\n\nWe allow:\n\n- Fine-tuning of small **adapter layers** or **routing weights**.\n- Updating of local memory policies (how aggressively to write/read local memory).\n- Offline or periodic retraining, not live gradient updates on every interaction.\n\nGuardrails:\n\n- Strict caps on how much the policy can change in one update.\n- Automatic monitoring for drift (performance, alignment metrics).\n- Option to roll back to previous checkpoints.\n\n### F.3 Central meta-learning\n\nAggregate **anonymized, filtered** data across deployments to:\n\n- Train better base models in future generations.\n- Improve world-model and tool-use priors.\n- Discover new \u201cskills\u201d / options to be added globally.\n\nThis closes the loop without doing uncontrolled online RL in production.\n\n---\n\n## What\u2019s fundamentally different from 2025-ish pipelines\n\nCompared to today\u2019s \u201chuge LM + thin RLHF + ad-hoc agent wrappers\u201d, the 2030 pipeline:\n\n1. **Architectural from the start**:\n   - Action, memory, control, and uncertainty are built into the model, not hacked via tokens.\n\n2. **Agentic pretraining is a *primary* phase, not an afterthought**:\n   - Massive imitation on action\u2013observation\u2013memory trajectories.\n\n3. **RL is long-horizon and environment-based**:\n   - Sandbox OS/web/code environments, with explicit world-models and hierarchical skills.\n\n4. **Alignment is process-level**:\n   - Oversight of reasoning chains and actions, not just final outputs.\n\n5. **Memory is learned, not just bolted-on RAG**:\n   - The model learns what to write, what to retrieve, and how to use it for planning.\n\n6. **Continual learning is deliberate and constrained**:\n   - Clear separation between global model evolution and local adaptation.\n\n---\n\nIf you want, we can next:\n\n- Pick *one* axis (e.g. \u201cPhase B agentic imitation in an OS-like environment\u201d) and design it as if you were doing a scaled-down version yourself using current APIs \u2013 env design, data schema, logging format, and training loop. That\u2019s where this becomes implementable for a small research group rather than just a 2030 thought experiment."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":" ","start_idx":13369,"end_idx":13369,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[],"has_images":false}],"safe_urls":[],"citations":[],"request_id":"0412a006-4dc1-4114-a412-c5bab2ad9344","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"81a42e4b-5d1e-4df0-a5bc-761e60514086","turn_exchange_id":"293c2ff0-c1fa-4619-b7b6-214de62f46f5"},"recipient":"all","channel":"final"},"parent":"81a42e4b-5d1e-4df0-a5bc-761e60514086","children":["bfa21048-badb-40f0-95eb-c990e4f5cf6d"]},"bfa21048-badb-40f0-95eb-c990e4f5cf6d":{"id":"bfa21048-badb-40f0-95eb-c990e4f5cf6d","message":{"id":"bfa21048-badb-40f0-95eb-c990e4f5cf6d","author":{"role":"user","name":null,"metadata":{}},"create_time":1764990484.464,"update_time":null,"content":{"content_type":"text","parts":["Write up your sharpened direction \n\n```\nWe need agent-native models and training regimes, not just \u201cfrozen predictors with an agent wrapper\u201d.\n```\n\ninto a full length blog post in markdown format. Target: An audience interested in generative AI and AGI, but non-technical people. Use an engaging tone and continuous prose; intelligent use of bullets and lists to only to enhance readability and break down concepts cleanly."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"bc7214f5-76ac-43f5-96e4-a4604b8dc2cf","message_source":null,"turn_exchange_id":"06c958b1-b313-4dc2-afb6-8f699db8c37e"},"recipient":"all","channel":null},"parent":"26ffa2e9-bbe8-4f54-a0ae-ba47a73b9b41","children":["26fdc3f6-06b8-484a-a627-b7f6a60a5bf1"]},"26fdc3f6-06b8-484a-a627-b7f6a60a5bf1":{"id":"26fdc3f6-06b8-484a-a627-b7f6a60a5bf1","message":{"id":"26fdc3f6-06b8-484a-a627-b7f6a60a5bf1","author":{"role":"system","name":null,"metadata":{}},"create_time":1764990485.7327285,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"bc7214f5-76ac-43f5-96e4-a4604b8dc2cf","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"bfa21048-badb-40f0-95eb-c990e4f5cf6d","turn_exchange_id":"06c958b1-b313-4dc2-afb6-8f699db8c37e"},"recipient":"all","channel":null},"parent":"bfa21048-badb-40f0-95eb-c990e4f5cf6d","children":["23172428-6afd-4e86-8eca-99c5214ee0bd"]},"23172428-6afd-4e86-8eca-99c5214ee0bd":{"id":"23172428-6afd-4e86-8eca-99c5214ee0bd","message":{"id":"23172428-6afd-4e86-8eca-99c5214ee0bd","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764990486.994647,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Considering user\u2019s blog post request","content":"User wants a high-level blog post about generative AI and AGI, specifically focusing on agent-native models and training regimes. I should work from prior suggestions, explaining concepts like world-models, long-horizon RL, and OS-native training. I'll aim for an engaging, non-technical tone without deep technical detail, avoiding web browsing.","chunks":["User wants a high-level blog post about generative AI and AGI, specifically focusing on agent-native models and training regimes.","I should work from prior suggestions, explaining concepts like world-models, long-horizon RL, and OS-native training.","I'll aim for an engaging, non-technical tone without deep technical detail, avoiding web browsing."],"finished":true},{"summary":"Turning concepts into a blog post","content":"The system is firm about using web searches for any post-knowledge cutoff queries, but since the topic is conceptual, I\u2019m confident I don\u2019t need to browse. The user wants a non-technical blog: I\u2019ll explain ideas clearly using analogies while avoiding over-complicated language. I\u2019ll cover key elements like wrapper models vs. agent-native models and how users, companies, and researchers will be affected. Markdown structure will help keep it readable.","chunks":["The system is firm about using web searches for any post-knowledge cutoff queries, but since the topic is conceptual, I\u2019m confident I don\u2019t need to browse.","The user wants a non-technical blog: I\u2019ll explain ideas clearly using analogies while avoiding over-complicated language.","I\u2019ll cover key elements like wrapper models vs.","agent-native models and how users, companies, and researchers will be affected.","Markdown structure will help keep it readable."],"finished":true},{"summary":"Outlining the blog structure","content":"To start, I'll plan the sections clearly:\n\n- **Title**  \n- **Introduction**: Explain the layers from a ChatGPT-like model to an agent-native one, touching on \"frozen predictor + agent wrapper\" vs. agent-native theories.\n- **Section about current chatbots**: Describe how they're brilliant but incomplete oracles that rely on wrappers.\n- **Section on the limitations of agent wrappers**: Discuss issues with reliability and memory for long tasks.\n- **Defining agent-native models**: Explain these models\u2019 ability to act, observe, remember, and plan based on training, and introduce key concepts like memory, goals, and uncertainty.","chunks":["To start, I'll plan the sections clearly:","**Title****Introduction**: Explain the layers from a ChatGPT-like model to an agent-native one, touching on \"frozen predictor + agent wrapper\" vs. agent-native theories.","**Section about current chatbots**: Describe how they're brilliant but incomplete oracles that rely on wrappers.","**Section on the limitations of agent wrappers**: Discuss issues with reliability and memory for long tasks.","**Defining agent-native models**: Explain these models\u2019 ability to act, observe, remember, and plan based on training, and introduce key concepts like memory, goals, and uncertainty."],"finished":true}],"source_analysis_msg_id":"949fec50-9d95-40ae-9fcb-42b0a7c724b8"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"bc7214f5-76ac-43f5-96e4-a4604b8dc2cf","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"26fdc3f6-06b8-484a-a627-b7f6a60a5bf1","turn_exchange_id":"06c958b1-b313-4dc2-afb6-8f699db8c37e"},"recipient":"all","channel":null},"parent":"26fdc3f6-06b8-484a-a627-b7f6a60a5bf1","children":["6991cf31-837b-4769-9ef0-c0b9ea329ffa"]},"6991cf31-837b-4769-9ef0-c0b9ea329ffa":{"id":"6991cf31-837b-4769-9ef0-c0b9ea329ffa","message":{"id":"6991cf31-837b-4769-9ef0-c0b9ea329ffa","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764990500.409776,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 13s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":13,"request_id":"bc7214f5-76ac-43f5-96e4-a4604b8dc2cf","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"23172428-6afd-4e86-8eca-99c5214ee0bd","turn_exchange_id":"06c958b1-b313-4dc2-afb6-8f699db8c37e"},"recipient":"all","channel":null},"parent":"23172428-6afd-4e86-8eca-99c5214ee0bd","children":["44b64a36-17f7-444f-934d-4f7f7cea4e07"]},"44b64a36-17f7-444f-934d-4f7f7cea4e07":{"id":"44b64a36-17f7-444f-934d-4f7f7cea4e07","message":{"id":"44b64a36-17f7-444f-934d-4f7f7cea4e07","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764990500.360396,"update_time":1764990561.886256,"content":{"content_type":"text","parts":["```markdown\n# From Chatbots to Colleagues: Why We Need Agent-Native AI (Not Just Smarter Wrappers)\n\nIf today\u2019s AI feels both astonishingly smart *and* strangely brittle, you\u2019re not imagining it.\n\nAsk a top model to explain a complex topic? It\u2019s brilliant.  \nAsk it to remember a project over weeks, use your computer sensibly, double-check its own work, and only act when it\u2019s sure? Suddenly you\u2019re wrestling with a very gifted goldfish.\n\nA big reason is structural:\n\n> Most of today\u2019s systems are **frozen predictors with an agent wrapper**.  \n> The next step is to build **agent-native models** \u2013 AI that is trained from the ground up to *act in the world*, not just complete sentences.\n\nThis post unpacks what that means in plain language:  \nwhat we have today, why it hits a ceiling, what \u201cagent-native\u201d actually looks like, and why it matters for everyone who cares about generative AI and AGI.\n\n---\n\n## 1. What we actually built: brilliant oracles in a box\n\nThe large language models (LLMs) behind tools like ChatGPT, Claude, Gemini and others are trained in a surprisingly simple way:\n\n- Feed them huge amounts of text (and often code, images, etc.).\n- Ask them, again and again: **\u201cWhat comes next?\u201d**\n- Adjust the internal knobs so they get better at predicting the next word, the next symbol, the next token.\n\nOn top of that, we add some fine-tuning:\n\n- Show the model examples of *good* responses.\n- Nudge it to be more helpful, more polite, less dangerous.\n- Maybe train it a bit more on maths, coding, or specific tasks.\n\nThe result is a **fantastic predictor**:\n\n- It has absorbed a large fraction of what humanity has written online.\n- It can mimic styles, explain concepts, write code, draft emails, debug scripts, and more.\n- It does all this in a single shot: given a prompt, it predicts a good-looking continuation.\n\nThis \u201coracle in a box\u201d is genuinely powerful. But notice what it *doesn\u2019t* inherently know how to do:\n\n- It doesn\u2019t naturally **act** in an environment.\n- It doesn\u2019t manage **state** over time \u2014 as soon as the context window runs out, it forgets.\n- It doesn\u2019t have a built-in sense of **\u201cI should think more before acting\u201d** or **\u201cI really don\u2019t know this.\u201d**\n\nTo compensate, we wrap it.\n\n---\n\n## 2. The current workaround: agent wrappers and tool glue\n\nBecause the core model just predicts text, engineers build **wrappers** around it:\n\n- Give it tools (a browser, a calculator, a code runner, a file system).\n- Define a special \u201clanguage\u201d for actions (for example, \u201c`CALL_TOOL(search, 'weather in Tokyo')`\u201d).\n- Let the wrapper:\n  - Read the model\u2019s text.\n  - Notice when it asked to call a tool.\n  - Actually call that tool.\n  - Feed the result back into the model as more text.\n\nThis is how you get:\n\n- Code agents that can run and fix their own scripts.\n- \u201cComputer-using\u201d agents that click around screens or browse the web.\n- Research agents that search, read, and summarize articles.\n\nIt\u2019s clever. It works surprisingly well in demos. But it\u2019s fundamentally a **hack**:\n\n- The model was **not trained** from the ground up as an acting agent.\n- It learned to talk; we then taught it a few action phrases after the fact.\n- Most of its training still assumed a static world: no clicking, no real consequences, just text in, text out.\n\nSo we\u2019re taking a mind that grew up reading books, and only now asking it to pilot a plane \u2014 by writing notes to a separate \u201chands\u201d system.\n\nYou can get quite far with this. But it hits hard limits.\n\n---\n\n## 3. Why wrappers alone will hit a ceiling\n\nThere are four big cracks you can already see if you look closely at today\u2019s agents.\n\n### 3.1 Reliability under pressure\n\nCurrent models are:\n\n- Often **confidently wrong**.\n- Willing to make things up when they\u2019re missing information.\n- Bad at clearly saying \u201cI don\u2019t know\u201d without being heavily guided.\n\nWrapping them in an agent helps \u2013 you can add checks, tests, and secondary models \u2013 but the *core* behaviour is still \u201csay something plausible next\u201d, not \u201cbe rigorously correct\u201d or \u201cshut up when unsure\u201d.\n\n### 3.2 Long, messy tasks\n\nReal work is rarely one neat question and one neat answer.\n\n- It\u2019s a chain of emails, documents, edits, experiments, dead ends, revisions.\n- It stretches over **days or weeks**, not a single chat window.\n\nToday\u2019s agents:\n\n- Struggle with **very long-running projects**.\n- Forget decisions made earlier unless you manually keep feeding them summaries.\n- Lose track of goals or constraints as you go.\n\nYou can build ever more elaborate external memory systems and orchestration layers, but you\u2019re fighting the fact that the model itself was not trained with long, evolving tasks in mind.\n\n### 3.3 State and memory as afterthoughts\n\nMost systems store information like this:\n\n- Dump notes into a database.\n- Later, search that database and stuff the results back into the model\u2019s prompt.\n\nThe model itself:\n\n- Doesn\u2019t have a learned sense of **what\u2019s worth saving**.\n- Doesn\u2019t decide *when* to write something down for later.\n- Doesn\u2019t maintain an internal, structured \u201cmap\u201d of the project or environment.\n\nIt\u2019s like working with someone who has perfect short-term recall of whatever you just showed them, but no instinct to keep a notebook or maintain a to-do list.\n\n### 3.4 No real \u201csense of consequences\u201d\n\nBecause the base training just rewarded \u201cplausible next text\u201d, the model doesn\u2019t deeply \u201cfeel\u201d:\n\n- There\u2019s a difference between thinking and doing.\n- Some actions are cheap; some are risky or expensive.\n- Some mistakes are small; some break everything.\n\nWrappers can hard-code rules (\u201cdon\u2019t delete files\u201d, \u201cask before spending money\u201d), but those rules live outside the model\u2019s main \u201cmind\u201d. The AI didn\u2019t grow up in a world where actions had consequences; we taped consequences on at the edge.\n\n---\n\n## 4. The alternative: agent-native models\n\nSo what\u2019s the alternative?\n\n> An **agent-native** model is trained not just to *speak*, but to *act*, *observe*, *remember*, and *plan* as a first-class citizen of its inner world.\n\nImagine training an AI that, from early on, lives in a kind of **sandboxed universe**:\n\n- It sees screens, files, documents, and tools.\n- It takes actions: open, click, edit, run, search, schedule.\n- It gets feedback: tests pass or fail, tasks complete or not, humans approve or reject its work.\n\nOver millions or billions of such experiences, it doesn\u2019t just learn \u201cwhat words usually follow other words\u201d; it learns:\n\n- **How the world responds** to its actions.\n- **What\u2019s worth remembering** for later.\n- **How to break big problems into small ones** over time.\n- **When to slow down and think more**, or ask for help.\n\nLet\u2019s break down some of the key ingredients of an agent-native mind in more human terms.\n\n### 4.1 Actions and observations as a native language\n\nToday, models treat everything as \u201cjust text\u201d.\n\nAn agent-native model has a richer vocabulary from the start:\n\n- \u201c**See**\u201d \u2013 what\u2019s on the screen, in a file, or in the environment.\n- \u201c**Do**\u201d \u2013 click, type, run, call a tool, move a cursor, send a message.\n- \u201c**Say**\u201d \u2013 talk to a person in natural language.\n- \u201c**Wait**\u201d \u2013 pause for an event or response.\n\nInstead of faking all of that as weird special phrases in text, the model\u2019s internal machinery is designed to handle this action\u2013observation loop naturally.\n\n### 4.2 Memory as part of its thinking, not a sidecar\n\nFor an agent-native AI, memory isn\u2019t just \u201ca bigger chat window\u201d.\n\nIt learns, over time:\n\n- What to write down as a note, summary, or conclusion.\n- When to retrieve past experiences or documents.\n- How to organize its own \u201cnotebooks\u201d of knowledge and project history.\n\nIt\u2019s closer to a colleague who:\n\n- Leaves themselves useful comments in the code.\n- Keeps a running list of open questions.\n- Refers back to previous decisions and explains why they were made.\n\n### 4.3 Goals, trade-offs, and uncertainty\n\nInstead of always rushing to answer, an agent-native model learns:\n\n- That sometimes it\u2019s better to say **\u201cI\u2019m not sure, let me check.\u201d**\n- That it can take a moment to **think silently**, run tests, or check references before committing.\n- That asking the human a clarifying question is often smarter than guessing.\n\nIt gets trained not just on **what the final answer looks like**, but on:\n\n- The *process* that led there.\n- Whether that process was safe, efficient, and honest.\n\n### 4.4 Learning from experience\n\nFinally, an agent-native model doesn\u2019t just \u201clearn once, deploy forever\u201d.\n\nInside controlled boundaries, it can:\n\n- Notice patterns across tasks.\n- Gradually get better at your specific tools and workflows.\n- Update its internal habits and strategies based on what works.\n\nDone carefully, this doesn\u2019t mean wild, uncontrolled self-modification; it means **growing** in a monitored, guided way, like an employee who keeps learning on the job.\n\n---\n\n## 5. A simple analogy: genius in a box vs colleague in an office\n\nThink of it this way.\n\nToday\u2019s big models are like:\n\n> A genius locked in a soundproof room.  \n> You slide in a piece of paper with a question.  \n> They think really fast, scribble an answer, and slide it back.  \n> Then the memory of that interaction mostly vanishes.\n\nAgent wrappers are like:\n\n> Hiring a handler who stands outside the room,  \n> runs around the office doing things based on the notes,  \n> and then tells the genius what happened \u2014 again by sliding more paper.\n\nIt works. But it\u2019s clumsy.\n\nAn agent-native model is more like:\n\n> Hiring a very smart colleague,  \n> giving them a desk, a computer, and access to your internal systems,  \n> onboarding them into your tools and processes,  \n> and then letting them work alongside you \u2014 with logs, supervision, and clear responsibilities.\n\nSame underlying intelligence, very different *lived experience* during training.\n\n---\n\n## 6. What has to change in training (without equations)\n\nYou don\u2019t need the mathematics, but it\u2019s worth understanding the *shape* of the shift.\n\n### 6.1 From static reading to interactive practice\n\nToday:\n\n- Most of a model\u2019s \u201ceducation\u201d is like reading a giant, shuffled library of books and code.\n- Only a small fraction is practising with real tools or in real environments.\n\nAgent-native training would deliberately spend a big chunk of time on:\n\n- **Practising in sandboxed environments** that look like real life:\n  - Simulated operating systems.\n  - Simulated websites and apps.\n  - Simulated projects and workflows.\n- Getting **scored** not only on what it says, but on:\n  - Whether the code runs.\n  - Whether the document satisfies the brief.\n  - Whether the steps it took were safe and reasonable.\n\n### 6.2 From \u201cmake the answer look good\u201d to \u201cmake the whole process sound\u201d\n\nCurrent fine-tuning often asks humans:\n\n- \u201cDo you like this answer?\u201d\n- \u201cIs this reply helpful and polite?\u201d\n\nAgent-native fine-tuning would also ask:\n\n- \u201cWas the *reasoning* honest and non-fabricated?\u201d\n- \u201cWere the *actions* safe and justified?\u201d\n- \u201cDid the AI ask for clarification when it should have?\u201d\n- \u201cDid it overstep, or did it escalate appropriately?\u201d\n\nThe model learns to care about the **journey**, not just the **destination**.\n\n### 6.3 From \u201cone size fits all\u201d to \u201cadapting safely to context\u201d\n\nInstead of a single frozen brain for everyone, an agent-native system would:\n\n- Keep a **stable core** that\u2019s thoroughly checked and aligned.\n- Add **lightweight, supervised adaptation** on top:\n  - For your company\u2019s tools.\n  - For your personal preferences.\n  - For specific domains (law, finance, healthcare, education, etc.).\n\nCrucially, these adaptations would be:\n\n- **Guarded** \u2013 with clear limits on how much behaviour can change.\n- **Audited** \u2013 so you can see what the system has learned.\n- **Reversible** \u2013 you can always roll back to a known safe state.\n\n---\n\n## 7. Why this matters (even if you never say \u201cagent-native\u201d again)\n\nAll of this can sound abstract. So what does it actually change for normal people?\n\n### 7.1 For everyday users\n\nAgent-native AI means:\n\n- Less copy-pasting between apps; the AI can actually **use** your apps.\n- Longer-term **continuity**: it remembers projects, not just chat sessions.\n- More **honest uncertainty**: fewer confident hallucinations, more \u201clet me check that\u201d moments.\n\nIn practice, that looks like:\n\n- A writing assistant that manages research, sources, and drafts over months.\n- A \u201cdigital admin\u201d that can really handle forms, schedules, and follow-ups without constant babysitting.\n- A personal tutor that tracks what you\u2019ve learned, what you\u2019re stuck on, and plans lessons accordingly.\n\n### 7.2 For organisations\n\nAgent-native systems aren\u2019t just chatbots at the edge of your business; they can become:\n\n- **Process participants** in workflows:\n  - Triaging tickets, preparing drafts, running tests, stitching tools together.\n- **Institutional memory helpers**:\n  - Summarizing meetings, tracking decisions, linking related projects.\n- **Experimentation engines**:\n  - Trying out many variations of tasks, measuring what works, and suggesting improvements.\n\nDone right, this isn\u2019t about replacing everyone; it\u2019s about giving every team a tireless, reasonably reliable co-worker.\n\n### 7.3 For safety and control\n\nParadoxically, training AI as an agent can make it **easier to control**, not harder \u2014 if we design it that way.\n\nBecause:\n\n- You see not just final answers, but **action histories** and **reasoning traces**.\n- You can enforce rules at the level of **process**, not just \u201cblock certain words\u201d.\n- You can test and red-team behaviour in realistic environments before deployment.\n\nInstead of an opaque oracle, you get a system whose *habits* and *routines* you can examine and improve.\n\n---\n\n## 8. Are we close?\n\nPieces of this vision already exist in early form:\n\n- Reasoning-focused models that \u201cthink longer\u201d before answering.\n- Code agents that iteratively write, run, and fix programs.\n- Early \u201ccomputer-using\u201d agents that click around a desktop or web browser.\n- Research agents that search, read, and summarize literature.\n\nBut today they\u2019re mostly:\n\n- Built on **frozen, text-first models** that were trained as predictors, not agents.\n- Wrapped in **clever scripts and orchestration frameworks**.\n- Impressive, but still flaky on long, messy, real-world tasks.\n\nThe shift to truly agent-native systems is not just \u201cone more product\u201d. It\u2019s a **change in how we train the core models**, not just how we package them.\n\n---\n\n## 9. What to watch for in the next few years\n\nIf you\u2019re curious where this is heading, keep an eye out for:\n\n- Models that are explicitly trained in **simulated environments** \u2013 not just on text.\n- Research that talks about **actions and observations**, not just \u201cprompts and responses\u201d.\n- Systems that show:\n  - Clear **long-term memory** across sessions.\n  - Good **\u201cI don\u2019t know\u201d behaviour** and sensible escalation.\n  - Detailed **logs of actions and reasoning**, not just final outputs.\n\nThose are the signs that we\u2019re moving from \u201coracles in a box\u201d to **agents in the world**.\n\n---\n\n## 10. Closing thought\n\nThe big jump from here to AGI is not just about making the brain bigger.\n\nIt\u2019s about changing the *life story* of that brain:\n\n- From a mind that grew up predicting text in isolation,\n- To a mind that grew up **acting, observing, remembering, and learning** inside rich, responsive environments.\n\nThat\u2019s what \u201cagent-native models and training regimes\u201d really means.\n\nEverything else \u2013 the wrappers, the scripts, the prompts \u2013 is just scaffolding. The real transformation happens when the model itself is raised not just as a storyteller, but as a **colleague**.\n```"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":" ","start_idx":15523,"end_idx":15523,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[],"has_images":false}],"safe_urls":[],"citations":[],"request_id":"bc7214f5-76ac-43f5-96e4-a4604b8dc2cf","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"6991cf31-837b-4769-9ef0-c0b9ea329ffa","turn_exchange_id":"06c958b1-b313-4dc2-afb6-8f699db8c37e"},"recipient":"all","channel":"final"},"parent":"6991cf31-837b-4769-9ef0-c0b9ea329ffa","children":["921a9b31-dc58-4013-800a-f71719b70c2c"]},"921a9b31-dc58-4013-800a-f71719b70c2c":{"id":"921a9b31-dc58-4013-800a-f71719b70c2c","message":{"id":"921a9b31-dc58-4013-800a-f71719b70c2c","author":{"role":"user","name":null,"metadata":{}},"create_time":1764992512.226,"update_time":null,"content":{"content_type":"text","parts":["I have re-written the first part. Please redo the rest, following the format I have used\n\n---\n\n# From Chatbots to Colleagues: Why We Need Agent-Native AI (Not Just Smarter Wrappers)\r\n\r\nIf today\u2019s AI feels both astonishingly smart *and* strangely brittle, you\u2019re not imagining it. Ask a top model to explain a complex topic? It\u2019s brilliant. Ask it to remember a project over weeks, use your computer sensibly, double-check its own work, and only act when it\u2019s sure? Suddenly you\u2019re wrestling with a very gifted goldfish.\r\n\r\nA big reason is structural:  Most of today\u2019s systems are **frozen predictors with an agent wrapper**.  The next step is to build **agent-native models**, that is, AI that is trained from the ground up to *act in the world*, not just complete sentences and write essays.\r\n\r\nThis post unpacks what  we have today, why it hits a ceiling, what \u201cagent-native\u201d actually looks like, and why it matters for everyone who cares about generative AI and AGI.\r\n\r\n---\r\n\r\n## 1. What we actually built: brilliant oracles in a box\r\n\r\nThe large language models (LLMs) behind tools like ChatGPT, Claude, Gemini and others are trained in a surprisingly simple way: (i) feed them huge amounts of text (and often code, images, etc.), (ii) ask them, again and again: **\u201cWhat comes next?\u201d**, (iii) adjust the internal knobs so they get better at predicting the next word, the next symbol, the next token.\r\n\r\nOn top of that, we add some fine-tuning: (i) show the model examples of *good* responses (SFT, GRPO, RLHF), (ii) nudge it to be more helpful, more polite, less dangerous (RLHF), (iii) maybe train it a bit more on maths, coding, or specific tasks (fine-tuning/RLVR).\r\n\r\nThe result is a **fantastic predictor**: It has absorbed a large fraction of what humanity has written online, it can mimic styles, explain concepts, write code, draft emails, debug scripts, and more, and it does all this in a single shot: given a prompt, it predicts a good-looking continuation.\r\n\r\nThis \u201coracle in a box\u201d is genuinely powerful. But notice what it *doesn\u2019t* inherently know how to do: It doesn\u2019t naturally **act** in an environment. It doesn\u2019t manage **state** over time \u2014 as soon as the context window runs out, it forgets. It doesn\u2019t have a built-in sense of **\u201cI should think more before acting\u201d** or **\u201cI really don\u2019t know this.\u201d**\r\n\r\n---\r\n\r\n## 2. The current workaround: agent wrappers and tool glue\r\n\r\nBecause the core model just predicts text, engineers build **wrappers** around it: (i) give it tools (a browser, a calculator, a code runner, a file system), (ii) define a special \u201clanguage\u201d for actions (for example, \u201c`CALL_TOOL(search, 'weather in Tokyo')`\u201d), (iii) let the wrapper read the model\u2019s text, (iv) notice when it asked to call a tool, actually call that tool, feed the result back into the model as more text.\r\n\r\nThis is how you get code agents that can run and fix their own scripts, computer-using\u201d agents that click around screens or browse the web, research agents that search, read, and summarize articles.\r\n\r\nIt\u2019s clever. It works surprisingly well in demos. But it\u2019s fundamentally a **hack**. The model was **not trained** from the ground up as an acting agent. It learned to talk; we then taught it a few action phrases after the fact. Most of its training still assumed a static world: no clicking, no real consequences, just text in, text out.\r\n\r\nSo we\u2019re taking a mind that grew up reading books, and only now asking it to pilot a plane \u2014 by writing notes to a separate \u201chands\u201d system. You can get quite far with this. But it hits hard limits."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"5564c088-f257-4d54-be78-b75713d5f601","message_source":null,"turn_exchange_id":"75c20f8c-7ab2-4529-9659-3800ea74818b"},"recipient":"all","channel":null},"parent":"44b64a36-17f7-444f-934d-4f7f7cea4e07","children":["822e08b0-e5cb-4960-93af-6cae6e655c8f"]},"822e08b0-e5cb-4960-93af-6cae6e655c8f":{"id":"822e08b0-e5cb-4960-93af-6cae6e655c8f","message":{"id":"822e08b0-e5cb-4960-93af-6cae6e655c8f","author":{"role":"system","name":null,"metadata":{}},"create_time":1764992514.8513618,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"5564c088-f257-4d54-be78-b75713d5f601","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"921a9b31-dc58-4013-800a-f71719b70c2c","turn_exchange_id":"75c20f8c-7ab2-4529-9659-3800ea74818b"},"recipient":"all","channel":null},"parent":"921a9b31-dc58-4013-800a-f71719b70c2c","children":["16e5a1ef-272e-4dd5-9dc2-7abd0077e06b"]},"16e5a1ef-272e-4dd5-9dc2-7abd0077e06b":{"id":"16e5a1ef-272e-4dd5-9dc2-7abd0077e06b","message":{"id":"16e5a1ef-272e-4dd5-9dc2-7abd0077e06b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764992518.754911,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Adapting blog post from section 3 onward","content":"The user rewrote the introduction and sections 1 and 2. They want me to continue from section 3 in the same style: markdown, concise prose, numbering in parentheses, and minimal bullet lists. Since the audience is non-technical but AI-interested, I\u2019ll adjust the writing to match the format they used.","chunks":["The user rewrote the introduction and sections 1 and 2.","They want me to continue from section 3 in the same style: markdown, concise prose, numbering in parentheses, and minimal bullet lists.","Since the audience is non-technical but AI-interested, I\u2019ll adjust the writing to match the format they used."],"finished":true},{"summary":"Outlining blog structure from section 3 onward","content":"I\u2019m not using browsing tools here; it's totally conceptual. I\u2019ll start at \"## 3. Why wrappers alone will hit a ceiling\" without repeating the earlier sections.\n\nThe outline is set: defining agent-native models, analogies, training changes, and their impact, finishing with a closing thought on what to watch in the future. I\u2019ll focus on a nontechnical explanation.","chunks":["I\u2019m not using browsing tools here; it's totally conceptual.","I\u2019ll start at \"## 3. Why wrappers alone will hit a ceiling\" without repeating the earlier sections.","The outline is set: defining agent-native models, analogies, training changes, and their impact, finishing with a closing thought on what to watch in the future.","I\u2019ll focus on a nontechnical explanation."],"finished":true},{"summary":"Adjusting formatting and style approach","content":"I should maintain continuous prose but incorporate some structured lists, like in earlier sections. I noticed the user\u2019s markdown format includes headings and separation lines without code fences, so I'll follow that style. It seems like they prefer markdown with heading divisions, but without additional backticks. I'll keep it consistent.","chunks":["I should maintain continuous prose but incorporate some structured lists, like in earlier sections.","I noticed the user\u2019s markdown format includes headings and separation lines without code fences, so I'll follow that style.","It seems like they prefer markdown with heading divisions, but without additional backticks.","I'll keep it consistent."],"finished":true}],"source_analysis_msg_id":"adcd6603-c783-4321-8544-a3d8404cf323"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"5564c088-f257-4d54-be78-b75713d5f601","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"822e08b0-e5cb-4960-93af-6cae6e655c8f","turn_exchange_id":"75c20f8c-7ab2-4529-9659-3800ea74818b"},"recipient":"all","channel":null},"parent":"822e08b0-e5cb-4960-93af-6cae6e655c8f","children":["003aa67e-842f-4f49-bcd0-3b681af08e21"]},"003aa67e-842f-4f49-bcd0-3b681af08e21":{"id":"003aa67e-842f-4f49-bcd0-3b681af08e21","message":{"id":"003aa67e-842f-4f49-bcd0-3b681af08e21","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764992531.010324,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 12s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":12,"request_id":"5564c088-f257-4d54-be78-b75713d5f601","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"16e5a1ef-272e-4dd5-9dc2-7abd0077e06b","turn_exchange_id":"75c20f8c-7ab2-4529-9659-3800ea74818b"},"recipient":"all","channel":null},"parent":"16e5a1ef-272e-4dd5-9dc2-7abd0077e06b","children":["f7d20b8f-8531-4f4c-af3e-a55e1dc4409f"]},"f7d20b8f-8531-4f4c-af3e-a55e1dc4409f":{"id":"f7d20b8f-8531-4f4c-af3e-a55e1dc4409f","message":{"id":"f7d20b8f-8531-4f4c-af3e-a55e1dc4409f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764992530.946734,"update_time":1764992576.16864,"content":{"content_type":"text","parts":["---\r\n\r\n## 3. Why wrappers alone will hit a ceiling\r\n\r\nThere are four big cracks that show up when we rely on \u201cpredictor + wrapper\u201d as the main design pattern. Each one is already visible in today\u2019s most advanced agents.\r\n\r\n### 3.1 Reliability under pressure\r\n\r\nToday\u2019s models are both impressive and flaky. On easy or familiar tasks they can be dazzling. Under pressure, though, they can (i) be confidently wrong, (ii) make things up when they lack information, and (iii) smooth over their own uncertainty instead of saying \u201cI\u2019m not sure\u201d.\r\n\r\nAgent wrappers can reduce the damage. You can add checkers, tests, secondary models and safety filters. But the core behaviour is still driven by the original training objective: produce a plausible continuation of text. That objective does not distinguish \u201cthis is well-supported by evidence\u201d from \u201cthis sounds roughly right\u201d. As long as that is true, you are always fighting the model\u2019s instincts with extra scaffolding on the outside.\r\n\r\n### 3.2 Long, messy, real-world tasks\r\n\r\nReal work is rarely a single neat question followed by a single neat answer. It looks more like this: a chain of emails, half-finished drafts, code changes, experiments, failed attempts, new information, and revisions over days or weeks.\r\n\r\nToday\u2019s systems are very good at \u201cone-shot brilliance\u201d and much weaker at \u201cslow, gritty progress\u201d. They (i) lose track of earlier decisions once the context window fills up, (ii) forget constraints or side conditions unless you constantly restate them, and (iii) struggle to keep a project\u2019s structure in mind over long periods.\r\n\r\nYou can patch this with increasingly complex orchestration: summary trees, vector databases, project graphs, external planners. But because the model itself was never trained to live through long episodes, you end up with a kind of split personality: a short-term genius wrapped inside a long-term puppet.\r\n\r\n### 3.3 Memory and state as afterthoughts\r\n\r\nMost current systems treat memory as a bolted-on storage layer: dump notes into a database, later retrieve some of them with search, and stuff them into the prompt.\r\n\r\nThe model itself does not natively learn (i) what is worth writing down, (ii) how to organise its own notes and summaries, or (iii) when to consult past experience versus relying on what is already in its short-term context. It simply gets handed extra text and continues from there.\r\n\r\nThat is very different from how skilled humans work. A good analyst or engineer will (i) maintain checklists, (ii) keep a running \u201cscratchpad\u201d of ideas, (iii) deliberately archive decisions and open questions, and (iv) refer back to those artefacts when planning next steps. For an agent-native AI, those behaviours need to be part of its *inner* training, not simulated externally forever.\r\n\r\n### 3.4 No deep sense of consequences\r\n\r\nBecause the base training happens on static text, the model does not grow up in a world where its actions have visible consequences. During pretraining, nothing \u201cbreaks\u201d when it makes a bad suggestion. There is no bill to pay, no server to crash, no user to disappoint. There is only the loss function for predicting tokens.\r\n\r\nWe can hard-code rules at the wrapper level (\u201cnever click this\u201d, \u201calways ask before spending money\u201d), but the model\u2019s own habits were not shaped in that environment. It did not learn from experience that (i) some actions are cheap and reversible, (ii) some are risky and should trigger extra checking, and (iii) some should be delegated or refused outright.\r\n\r\nAt some point, if we want systems that behave like careful colleagues rather than overeager autocomplete, we have to start training them in worlds where **doing things matters**, not just saying things.\r\n\r\n---\r\n\r\n## 4. The alternative: agent-native models\r\n\r\nSo what does \u201cagent-native\u201d actually mean in practice? In one sentence:\r\n\r\n> An agent-native model is trained not just to *speak*, but to *act*, *observe*, *remember* and *plan* as part of its core behaviour, not as a thin layer added afterwards.\r\n\r\nInstead of growing up only on books and web pages, an agent-native model spends a serious part of its \u201cchildhood\u201d in interactive environments. In those environments, it (i) sees something, (ii) decides what to do, (iii) observes what changed, and (iv) adjusts its internal habits based on what worked and what failed.\r\n\r\nThere are four big ingredients that distinguish this from today\u2019s systems.\r\n\r\n### 4.1 Actions and observations as first-class citizens\r\n\r\nIn current models, everything is squashed into a single stream of tokens. The word \u201cCLICK\u201d is, at training time, just another 5-letter word. The distinction between \u201creply to the user\u201d and \u201crun this command\u201d is mostly a convention layered on top.\r\n\r\nIn an agent-native setup, the model\u2019s internal language clearly separates at least three things: (i) **observations** (what it sees on screen, in files, in tools), (ii) **actions** (what it chooses to do: click, type, run, call a tool, send a message), and (iii) **utterances** (what it says to people in natural language). These are not just special tokens; they are different roles in the model\u2019s inner loop.\r\n\r\nThat means the model can learn patterns like \u201cafter seeing this kind of error, try that kind of action\u201d in a direct way, instead of having to infer it from static text written by humans.\r\n\r\n### 4.2 Memory as part of thinking, not a bolt-on\r\n\r\nAn agent-native model is trained to treat memory as a deliberate act, not just a larger prompt. It learns to (i) write down summaries, intermediate results and decisions in a structured way, (ii) organise those records into something like notebooks or project spaces, and (iii) retrieve and use them when planning next steps.\r\n\r\nCrucially, it practices this during training. It is rewarded when good note-taking leads to better outcomes on long tasks, and penalised when poor memory management causes confusion or repeated mistakes. The question \u201cwhat should I store for later?\u201d becomes part of the skill set, not an implementation detail handled by external tooling alone.\r\n\r\n### 4.3 Goals, trade-offs and uncertainty\r\n\r\nHumans doing serious work juggle three questions almost constantly: (i) what am I trying to achieve, (ii) what options do I have, and (iii) how sure am I about what I am doing?\r\n\r\nAgent-native models need to learn the same rhythm. That includes behaviours such as: (i) choosing to think longer before acting when stakes are high, (ii) asking clarifying questions instead of guessing when instructions are ambiguous, (iii) explicitly flagging \u201cI don\u2019t know\u201d when they lack information, and (iv) calling external tools or humans when their own competence is not enough.\r\n\r\nThose are not tricks you can reliably bolt on later. The model has to be rewarded, during training, for *preferring* cautious, well-justified behaviour over quick but fragile guesses.\r\n\r\n### 4.4 Learning from experience, within boundaries\r\n\r\nFinally, an agent-native system should be able to get better at what it actually does in the world: better at your tools, your data, your workflows. That does not mean uncontrolled self-modification. It means a carefully designed process where (i) the system\u2019s interactions are logged, (ii) outcomes are evaluated, and (iii) small, supervised updates improve its habits over time.\r\n\r\nThis kind of \u201con-the-job learning\u201d is familiar for people, but still unusual for large models deployed today. Moving towards it will require both technical work and good governance, but it is hard to imagine a true \u201ccolleague AI\u201d that never learns from its own experience.\r\n\r\n---\r\n\r\n## 5. A simple analogy: genius in a box vs colleague in an office\r\n\r\nAn analogy helps make the difference concrete.\r\n\r\nToday\u2019s LLMs are like a brilliant but isolated expert locked in a room. You slide in a question on paper, they think very quickly, slide out a beautifully written reply, and then forget the whole interaction. They never touch a keyboard, never see your shared drive, and never manage a project end-to-end.\r\n\r\nAgent wrappers are like hiring a runner who shuttles between that room and the rest of your office. The runner reads the AI\u2019s notes, clicks around your apps, pastes outputs into tools, then writes back what happened. It is clumsy, but it sort of works.\r\n\r\nAn agent-native model is more like hiring a very capable colleague, giving them (i) a desk and a computer, (ii) access to the tools they need, and (iii) a proper onboarding into how your organisation works. They interact with your systems directly, under supervision and with logs, and they remember what they have been doing on a project from one day to the next.\r\n\r\nSame underlying intelligence, but a very different upbringing \u2014 and a very different role in your work.\r\n\r\n---\r\n\r\n## 6. What has to change in training (without going into equations)\r\n\r\nYou do not need the mathematics to understand the shift. Conceptually, three big things need to change in how we \u201craise\u201d these models.\r\n\r\n### 6.1 From static reading to interactive practice\r\n\r\nRight now, most of a model\u2019s education is like reading an enormous, shuffled library of text and code. It learns what sentences usually follow other sentences, but it never gets to *try* things and see what breaks.\r\n\r\nAgent-native training would reserve a serious fraction of compute and data for practice in interactive environments. That might include (i) simulated operating systems, (ii) simulated websites and applications, (iii) realistic coding and debugging tasks, and (iv) synthetic \u201coffice worlds\u201d where agents manage documents, emails and workflows. The model would be scored not just on eloquence, but on whether its actions led to successful outcomes.\r\n\r\n### 6.2 From \u201cmake the answer look good\u201d to \u201cmake the process sound\u201d\r\n\r\nA lot of current fine-tuning asks humans to rate the final answer: is it helpful, polite, accurate, safe? That is useful, but it does not directly shape how the model *got* there.\r\n\r\nAgent-native training would add a second layer of feedback on the process itself. Human reviewers (and later, specialised oversight models) would look at the chain of thought and the sequence of actions and ask: (i) did the system check facts before asserting them, (ii) did it test its own code or reasoning when appropriate, (iii) did it avoid making up details, and (iv) did it escalate or ask for help when the situation warranted it?\r\n\r\nThe model would be rewarded for good habits and penalised for bad ones, even if the final answer happened to look okay. Over time, that drives it towards more trustworthy patterns of behaviour.\r\n\r\n### 6.3 From \u201cone frozen brain\u201d to \u201csafe adaptation to context\u201d\r\n\r\nToday, big models are mostly released as fixed checkpoints. They may be updated every few months, but they do not usually adapt in a fine-grained way to a particular company, team or user, except through crude means like more prompt engineering.\r\n\r\nIn an agent-native world, you still want a robust, carefully aligned core model, but you also want controlled layers of adaptation on top: (i) light-weight modules that learn your internal tools and data formats, (ii) per-project or per-user \u201cskills\u201d that make the agent more effective over time, and (iii) clear safeguards that prevent these adaptations from drifting too far or introducing unsafe behaviour.\r\n\r\nThe goal is not a shapeshifting black box. The goal is something more like a trained professional: a shared foundation of competence and values, plus specialisation for the role and environment they actually work in.\r\n\r\n---\r\n\r\n## 7. Why this matters beyond technical circles\r\n\r\nAll of this can sound abstract, but the consequences are very down-to-earth.\r\n\r\n### 7.1 For everyday users\r\n\r\nAgent-native AI would feel less like a chat window and more like a genuinely helpful assistant. It could (i) manage ongoing projects instead of just isolated questions, (ii) remember context from yesterday without you having to restate everything, (iii) use your apps and services directly under safe constraints, and (iv) be more honest about what it does and does not know.\r\n\r\nIn practice, that might look like: a writing assistant that keeps track of sources over weeks, a digital admin that reliably handles recurring tasks (forms, bookings, follow-ups), or a personal tutor that tracks your progress and plans lessons over months, not just answers the question of the day.\r\n\r\n### 7.2 For organisations and teams\r\n\r\nFor organisations, agent-native systems are not just another chatbot bolted onto the website. They can become (i) participants in workflows, (ii) maintainers of institutional memory, and (iii) engines for small-scale experimentation and optimisation.\r\n\r\nImagine agents that can reliably triage support tickets, prepare draft responses, run checks on internal dashboards, update documentation when processes change, and surface past decisions that are relevant to a current debate \u2014 all while leaving a transparent audit trail of what they did and why.\r\n\r\nThe economic impact comes not from flashy demos, but from quietly removing friction across hundreds of small processes.\r\n\r\n### 7.3 For safety and governance\r\n\r\nParadoxically, training AI as agents can make them easier to supervise and govern, provided we design for it. If an AI is acting in environments as part of its core training, then (i) we can observe complete action histories, (ii) we can test its behaviour in realistic scenarios before deployment, and (iii) we can define and enforce rules around its *process*, not only around its final words.\r\n\r\nInstead of policing outputs one answer at a time, we can start asking: \u201cWhat are this system\u2019s typical habits? Are they robust, cautious and honest, or brittle and corner-cutting?\u201d That is a healthier stance for both safety and accountability.\r\n\r\n---\r\n\r\n## 8. Are we close already?\r\n\r\nPieces of this vision already exist in early form. We have (i) models tuned specifically for reasoning that \u201cthink longer\u201d before answering, (ii) coding agents that write, run and fix programs in loops, (iii) early \u201ccomputer-using\u201d agents that can control a browser or desktop, and (iv) research agents that search, read and summarise documents semi-autonomously.\r\n\r\nHowever, most of these are still built on the old recipe: a powerful but fundamentally text-first predictor, wrapped in ever more sophisticated scaffolding. The internal training story has not yet fully shifted from \u201cread static data\u201d to \u201clive in an environment and learn from acting in it\u201d.\r\n\r\nSo the honest answer is: we are partway there. The direction of travel is clear, but the industry has not yet flipped the stack from \u201coracle with wrappers\u201d to \u201cagent-native core with strong foundations\u201d.\r\n\r\n---\r\n\r\n## 9. What to watch for in the coming years\r\n\r\nIf you want a simple checklist for spotting progress towards agent-native AI, look for systems where at least some of the following become true:\r\n\r\n1. The model is explicitly trained in simulated or sandboxed environments, not only on static text.\r\n2. There is a clear distinction between \u201cseeing\u201d, \u201cdoing\u201d and \u201cspeaking\u201d in how the system is described.\r\n3. Long-term memory across sessions is not just a bigger context window, but a learned part of the system\u2019s behaviour.\r\n4. The training process talks about evaluating and shaping *reasoning steps* and *actions*, not just final answers.\r\n5. Companies emphasise safe, bounded forms of on-the-job learning, not only one-off model releases.\r\n\r\nThose are signs that we are moving from clever packaging around static models to genuinely agent-native training regimes.\r\n\r\n---\r\n\r\n## 10. Closing thought\r\n\r\nThe path from today\u2019s generative models to something that feels like AGI is not just a matter of stacking more layers or scraping more data. It is about changing the *life story* of these systems.\r\n\r\nWe began by raising AIs as voracious readers, brilliant at continuing a line of text. The next phase is to raise them as **participants**: entities that act, observe, remember, plan and learn within rich, responsive worlds. That is what \u201cagent-native\u201d really points to.\r\n\r\nEverything else \u2014 the wrappers, the scripts, the prompts \u2014 is scaffolding. Useful, sometimes essential, but temporary. The deeper change is in how we grow the models themselves: not just as chatbots, but as future colleagues."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":" ","start_idx":16310,"end_idx":16310,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[],"has_images":false}],"safe_urls":[],"citations":[],"request_id":"5564c088-f257-4d54-be78-b75713d5f601","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"003aa67e-842f-4f49-bcd0-3b681af08e21","turn_exchange_id":"75c20f8c-7ab2-4529-9659-3800ea74818b"},"recipient":"all","channel":"final"},"parent":"003aa67e-842f-4f49-bcd0-3b681af08e21","children":["aea5ee27-8c67-4baa-a61d-56f905286d93"]},"aea5ee27-8c67-4baa-a61d-56f905286d93":{"id":"aea5ee27-8c67-4baa-a61d-56f905286d93","message":{"id":"aea5ee27-8c67-4baa-a61d-56f905286d93","author":{"role":"user","name":null,"metadata":{}},"create_time":1764992958.767,"update_time":null,"content":{"content_type":"text","parts":["much better"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"b3129676-e250-485a-b375-f5e640c5aa06","message_source":null,"turn_exchange_id":"3e117f15-8333-4627-8787-2c67e5e6cca7"},"recipient":"all","channel":null},"parent":"f7d20b8f-8531-4f4c-af3e-a55e1dc4409f","children":["2f867cc1-67ed-4e8c-a6e2-c28115e05544"]},"2f867cc1-67ed-4e8c-a6e2-c28115e05544":{"id":"2f867cc1-67ed-4e8c-a6e2-c28115e05544","message":{"id":"2f867cc1-67ed-4e8c-a6e2-c28115e05544","author":{"role":"system","name":null,"metadata":{}},"create_time":1764992959.9149337,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"b3129676-e250-485a-b375-f5e640c5aa06","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"aea5ee27-8c67-4baa-a61d-56f905286d93","turn_exchange_id":"3e117f15-8333-4627-8787-2c67e5e6cca7"},"recipient":"all","channel":null},"parent":"aea5ee27-8c67-4baa-a61d-56f905286d93","children":["6d6e1100-68d1-494b-b8ca-0fb41b044979"]},"6d6e1100-68d1-494b-b8ca-0fb41b044979":{"id":"6d6e1100-68d1-494b-b8ca-0fb41b044979","message":{"id":"6d6e1100-68d1-494b-b8ca-0fb41b044979","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764992961.500513,"update_time":null,"content":{"content_type":"thoughts","thoughts":[],"source_analysis_msg_id":"6f5ee42d-3e2d-40fa-b1ac-7cbc08c43742"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"b3129676-e250-485a-b375-f5e640c5aa06","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"2f867cc1-67ed-4e8c-a6e2-c28115e05544","turn_exchange_id":"3e117f15-8333-4627-8787-2c67e5e6cca7"},"recipient":"all","channel":null},"parent":"2f867cc1-67ed-4e8c-a6e2-c28115e05544","children":["6e003379-88e3-4486-8a08-a3e9c965e629"]},"6e003379-88e3-4486-8a08-a3e9c965e629":{"id":"6e003379-88e3-4486-8a08-a3e9c965e629","message":{"id":"6e003379-88e3-4486-8a08-a3e9c965e629","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764992962.207688,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for a second"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":0,"request_id":"b3129676-e250-485a-b375-f5e640c5aa06","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"6d6e1100-68d1-494b-b8ca-0fb41b044979","turn_exchange_id":"3e117f15-8333-4627-8787-2c67e5e6cca7"},"recipient":"all","channel":null},"parent":"6d6e1100-68d1-494b-b8ca-0fb41b044979","children":["60acfd78-12c0-42ed-aa9c-20259c3630c4"]},"60acfd78-12c0-42ed-aa9c-20259c3630c4":{"id":"60acfd78-12c0-42ed-aa9c-20259c3630c4","message":{"id":"60acfd78-12c0-42ed-aa9c-20259c3630c4","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764992962.148855,"update_time":1764992966.070284,"content":{"content_type":"text","parts":["Glad it lands closer to what you wanted.\n\nIf you\u2019d like, I can:\n\n- tighten it into a shorter \u201cop-ed\u201d version,\n- add a more technical appendix for semi-technical readers,\n- or restructure it into sections suitable for a multi-part series / newsletter.\n\nTell me where you plan to publish it (blog, LinkedIn, Substack, etc.) and I can also tweak the intro and outro to fit that context."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":" ","start_idx":383,"end_idx":383,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[],"has_images":false}],"safe_urls":[],"citations":[],"request_id":"b3129676-e250-485a-b375-f5e640c5aa06","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"6e003379-88e3-4486-8a08-a3e9c965e629","turn_exchange_id":"3e117f15-8333-4627-8787-2c67e5e6cca7"},"recipient":"all","channel":"final"},"parent":"6e003379-88e3-4486-8a08-a3e9c965e629","children":["0b40ce60-375f-41d4-8819-4509c117d852"]},"0b40ce60-375f-41d4-8819-4509c117d852":{"id":"0b40ce60-375f-41d4-8819-4509c117d852","message":{"id":"0b40ce60-375f-41d4-8819-4509c117d852","author":{"role":"user","name":null,"metadata":{}},"create_time":1764993251.212,"update_time":null,"content":{"content_type":"text","parts":["What you can do is to formulate a description of this writing style so that in future, if I want you or anyone (human writer, AI model, agent, etc) to deliver a blog post / instruction manual, I can use it as writing style instructions"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"f9269c9c-5999-498c-bdde-54cdcfc8e296","message_source":null,"turn_exchange_id":"c933bef7-e525-486f-a7ef-1772916db1dc"},"recipient":"all","channel":null},"parent":"60acfd78-12c0-42ed-aa9c-20259c3630c4","children":["b03d3522-e966-4553-b378-7916ef29fefa"]},"b03d3522-e966-4553-b378-7916ef29fefa":{"id":"b03d3522-e966-4553-b378-7916ef29fefa","message":{"id":"b03d3522-e966-4553-b378-7916ef29fefa","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764993254.076057,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Creating a reusable writing style guide","content":"User wants a description of the writing style to reuse for blog posts and manuals. I'll focus on tone and structure, keeping it non-technical but intelligent. There should be moderate verbosity\u2014nothing too over the top. I'll consider formatting with bullets and clarity, making sure it's flexible for various purposes.","chunks":["User wants a description of the writing style to reuse for blog posts and manuals.","I'll focus on tone and structure, keeping it non-technical but intelligent.","There should be moderate verbosity\u2014nothing too over the top.","I'll consider formatting with bullets and clarity, making sure it's flexible for various purposes."],"finished":true},{"summary":"Creating a style description for reusable instructions","content":"The user wants a clear, structured description of writing style. The tone should be engaged and written in continuous prose with clear headings. Bullet points and numbered lists should be used sparingly but effectively. The audience is intelligent, avoiding oversimplification and excess hype, with brief analogies for clarity. They prefer moderate sentence length and explicit signposting. I'll craft the style guide as instructions to be copy-pasted, with examples provided for \"do's\" and \"don'ts.\"","chunks":["The user wants a clear, structured description of writing style.","The tone should be engaged and written in continuous prose with clear headings.","Bullet points and numbered lists should be used sparingly but effectively.","The audience is intelligent, avoiding oversimplification and excess hype, with brief analogies for clarity.","They prefer moderate sentence length and explicit signposting.","I'll craft the style guide as instructions to be copy-pasted, with examples provided for \"do's\" and \"don'ts.\""],"finished":true}],"source_analysis_msg_id":"c219c737-717c-47a4-9502-a9f0dc849f4e"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"f9269c9c-5999-498c-bdde-54cdcfc8e296","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"0b40ce60-375f-41d4-8819-4509c117d852","turn_exchange_id":"c933bef7-e525-486f-a7ef-1772916db1dc"},"recipient":"all","channel":null},"parent":"0b40ce60-375f-41d4-8819-4509c117d852","children":["d0e22215-9d68-477a-8cfb-7583c3de5934"]},"d0e22215-9d68-477a-8cfb-7583c3de5934":{"id":"d0e22215-9d68-477a-8cfb-7583c3de5934","message":{"id":"d0e22215-9d68-477a-8cfb-7583c3de5934","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764993261.711293,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 7s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":7,"request_id":"f9269c9c-5999-498c-bdde-54cdcfc8e296","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"b03d3522-e966-4553-b378-7916ef29fefa","turn_exchange_id":"c933bef7-e525-486f-a7ef-1772916db1dc"},"recipient":"all","channel":null},"parent":"b03d3522-e966-4553-b378-7916ef29fefa","children":["86fca713-7e23-4989-9533-a75b6aac9bfe"]},"86fca713-7e23-4989-9533-a75b6aac9bfe":{"id":"86fca713-7e23-4989-9533-a75b6aac9bfe","message":{"id":"86fca713-7e23-4989-9533-a75b6aac9bfe","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764993261.629707,"update_time":1764993275.879095,"content":{"content_type":"text","parts":["Here\u2019s a reusable style guide you can paste into future prompts or share with other writers/models.\n\n```markdown\n## Writing Style Guide: Clear, Analytical, Accessible-Expert\n\n### 1. Audience and Voice\n\n- **Assume an intelligent, curious reader**, but not necessarily a specialist.\n- Write in the **second person (\u201cyou\u201d)** or neutral third person when appropriate.\n- Voice: **calm, analytical, and grounded**, not breathless or hyped.\n- Avoid talking down to the reader; explain, don\u2019t lecture.\n\n---\n\n### 2. Tone\n\n- **Engaged but not gushy.** It\u2019s fine to be warm or slightly playful, but the priority is clarity and substance.\n- **Evidence-minded and critical.** Acknowledge limitations, trade-offs, and open questions rather than overselling.\n- **Future-oriented.** It\u2019s acceptable to speculate, but be explicit when you\u2019re speculating or hypothesising.\n\n---\n\n### 3. Structure and Flow\n\n- Use **clear section headings** (`##`, `###`) to break up the piece logically.\n- Use **continuous prose** as the default. Paragraphs should flow into each other with clear transitions.\n- Use **short lists or bullets sparingly**, only when they genuinely help:\n  - to summarise key points,\n  - to break dense content into digestible chunks,\n  - to outline steps or options.\n- Prefer **explicit signposting** in text (e.g. \u201cthere are three main problems\u2026\u201d) to help the reader track the argument.\n\n---\n\n### 4. Technical Detail and Jargon\n\n- When introducing a technical term or abbreviation, **define it in full the first time** (e.g. \u201creinforcement learning (RL)\u201d).\n- Use technical terms **only when they add precision**; otherwise prefer plain language.\n- Do not over-explain obvious ideas, but **do explain any step that depends on specialised background knowledge**.\n- Avoid long chains of jargon; if you must include one, **unpack it** in nearby prose.\n\n---\n\n### 5. Explanations, Analogies, and Examples\n\n- Use **concrete examples or analogies** to make abstract points vivid.\n- Keep analogies **simple and tight**; avoid over-extending them or making them cutesy.\n- Where useful, contrast:\n  - **\u201cToday vs. desired future\u201d**\n  - **\u201cApproach A vs. approach B\u201d**\n  - **\u201cWhat we have vs. what we need\u201d**\n- Show **realistic failure modes**, not only success stories, to keep the picture honest.\n\n---\n\n### 6. Lists, Numbering, and Inline Structure\n\n- Use numbered lists when describing:\n  - sequences of steps,\n  - clearly ordered arguments,\n  - multi-part definitions.\n- Inline enumeration is allowed and encouraged for compactness, e.g.:\n  - \u201cThe model (i) predicts text, (ii) calls tools, and (iii) returns results.\u201d\n- Do **not** let lists replace explanation. Every list should be **introduced and interpreted in surrounding prose**.\n\n---\n\n### 7. Style and Sentence-Level Choices\n\n- Prefer **short to medium-length sentences**. Use longer sentences only when the structure is very clear.\n- Avoid purple prose and unnecessary metaphors. Favour **direct, precise wording**.\n- Avoid empty intensifiers (\u201cincredibly\u201d, \u201cinsanely\u201d, \u201crevolutionary\u201d) unless you immediately qualify them.\n- If something is uncertain, say so plainly (e.g. \u201cwe don\u2019t yet know\u201d, \u201cthis is plausible but unproven\u201d).\n\n---\n\n### 8. \u201cDo\u201d and \u201cDon\u2019t\u201d Summary\n\n**Do:**\n- Start with the **problem or intuition** before diving into detail.\n- Maintain a **logical through-line** from section to section.\n- Use headings, occasional lists, and light formatting to make scanning easy.\n- Explicitly state **assumptions, trade-offs, and limitations**.\n- Keep the overall feel **measured, thoughtful, and forward-looking**.\n\n**Don\u2019t:**\n- Don\u2019t rely on **hype or buzzwords** in place of explanation.\n- Don\u2019t bury the main idea under excessive caveats or digressions.\n- Don\u2019t overuse bullets: if everything is a list, nothing stands out.\n- Don\u2019t assume specialised background without signalling it and filling in the key gaps.\n- Don\u2019t be afraid to say \u201cthis is not solved yet\u201d or \u201cthis is where current approaches break\u201d.\n\n---\n\n### 9. Suitable Formats\n\nThis style should be used for:\n- Blog posts and essays aimed at **informed but non-specialist readers**.\n- **Instruction manuals, guides, or how-tos** where conceptual understanding matters, not just button-clicking.\n- Long-form answers that need to balance **rigour with readability**.\n\nThe core priorities are:\n1. **Clarity over flashiness**  \n2. **Honesty over hype**  \n3. **Structure over stream-of-consciousness**\n```\n\nIf you want, we can also create a shorter \u201cone-paragraph version\u201d of this guide for ultra-compact prompts."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"content_references":[{"matched_text":" ","start_idx":4527,"end_idx":4527,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[],"has_images":false}],"safe_urls":[],"citations":[],"request_id":"f9269c9c-5999-498c-bdde-54cdcfc8e296","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"d0e22215-9d68-477a-8cfb-7583c3de5934","turn_exchange_id":"c933bef7-e525-486f-a7ef-1772916db1dc"},"recipient":"all","channel":"final"},"parent":"d0e22215-9d68-477a-8cfb-7583c3de5934","children":[]}}}