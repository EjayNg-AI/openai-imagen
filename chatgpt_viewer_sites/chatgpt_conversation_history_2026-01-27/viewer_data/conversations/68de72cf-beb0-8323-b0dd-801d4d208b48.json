{"id":"68de72cf-beb0-8323-b0dd-801d4d208b48","title":"Ilya's AI speculation","create_time":1759408874.090653,"update_time":1759410026.776186,"current_node":"f44453ab-9ab8-43eb-abff-094e9d6b0746","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["b7a44058-368d-41f9-8645-8fbca59f2a99"]},"b7a44058-368d-41f9-8645-8fbca59f2a99":{"id":"b7a44058-368d-41f9-8645-8fbca59f2a99","message":{"id":"b7a44058-368d-41f9-8645-8fbca59f2a99","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["6cd0cf8b-f778-4794-a5a1-b7b008a4ef42"]},"6cd0cf8b-f778-4794-a5a1-b7b008a4ef42":{"id":"6cd0cf8b-f778-4794-a5a1-b7b008a4ef42","message":{"id":"6cd0cf8b-f778-4794-a5a1-b7b008a4ef42","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"b7a44058-368d-41f9-8645-8fbca59f2a99","children":["2144f6f9-7f5b-43ed-b021-e0d5e2f5bb77"]},"2144f6f9-7f5b-43ed-b021-e0d5e2f5bb77":{"id":"2144f6f9-7f5b-43ed-b021-e0d5e2f5bb77","message":{"id":"2144f6f9-7f5b-43ed-b021-e0d5e2f5bb77","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"6cd0cf8b-f778-4794-a5a1-b7b008a4ef42","children":["1f79995e-ecf0-4846-acc8-9f978a408595"]},"1f79995e-ecf0-4846-acc8-9f978a408595":{"id":"1f79995e-ecf0-4846-acc8-9f978a408595","message":{"id":"1f79995e-ecf0-4846-acc8-9f978a408595","author":{"role":"user","name":null,"metadata":{}},"create_time":1759408871.679,"update_time":null,"content":{"content_type":"text","parts":["Taking the most recent AI advancements in the past year, can you speculate on what really Ilya saw?"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"988445caff57aa0f-SIN","message_source":null,"turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"2144f6f9-7f5b-43ed-b021-e0d5e2f5bb77","children":["189f807a-a0d2-4471-9908-b10b4d1b1748"]},"189f807a-a0d2-4471-9908-b10b4d1b1748":{"id":"189f807a-a0d2-4471-9908-b10b4d1b1748","message":{"id":"189f807a-a0d2-4471-9908-b10b4d1b1748","author":{"role":"system","name":null,"metadata":{}},"create_time":1759408881.7265215,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"988445caff57aa0f-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"1f79995e-ecf0-4846-acc8-9f978a408595","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"1f79995e-ecf0-4846-acc8-9f978a408595","children":["ec81fcb4-eef8-4d9a-8da6-c811a25c61cf"]},"ec81fcb4-eef8-4d9a-8da6-c811a25c61cf":{"id":"ec81fcb4-eef8-4d9a-8da6-c811a25c61cf","message":{"id":"ec81fcb4-eef8-4d9a-8da6-c811a25c61cf","author":{"role":"system","name":null,"metadata":{}},"create_time":1759408881.9400208,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"988445caff57aa0f-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"189f807a-a0d2-4471-9908-b10b4d1b1748","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"189f807a-a0d2-4471-9908-b10b4d1b1748","children":["ec952c08-59ad-4ad9-8ec5-2b8c5cc4ebb6"]},"ec952c08-59ad-4ad9-8ec5-2b8c5cc4ebb6":{"id":"ec952c08-59ad-4ad9-8ec5-2b8c5cc4ebb6","message":{"id":"ec952c08-59ad-4ad9-8ec5-2b8c5cc4ebb6","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759408882.087892,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"model_slug":"gpt-5-thinking","request_id":"988445caff57aa0f-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"ec81fcb4-eef8-4d9a-8da6-c811a25c61cf","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"ec81fcb4-eef8-4d9a-8da6-c811a25c61cf","children":["52ce64b0-95ea-43e0-9bb3-00a59411cad0"]},"52ce64b0-95ea-43e0-9bb3-00a59411cad0":{"id":"52ce64b0-95ea-43e0-9bb3-00a59411cad0","message":{"id":"52ce64b0-95ea-43e0-9bb3-00a59411cad0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759408882.350962,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Interpreting question about Ilya","content":"The user seems to ask about \"what Ilya saw,\" which likely refers to Ilya Sutskever, OpenAI's co-founder. They're possibly asking about a breakthrough or event tied to Q* (or some other development). I\u2019ll look up current and past events, including Sutskever\u2019s departure and his new venture in 2024.","chunks":[],"finished":true}],"source_analysis_msg_id":"7a112439-15a6-4e1e-a1f1-36171b53db9a"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"988445caff57aa0f-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"ec952c08-59ad-4ad9-8ec5-2b8c5cc4ebb6","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"ec952c08-59ad-4ad9-8ec5-2b8c5cc4ebb6","children":["048b4a2f-1596-4ff0-9bd2-c4b2e4692fb1"]},"048b4a2f-1596-4ff0-9bd2-c4b2e4692fb1":{"id":"048b4a2f-1596-4ff0-9bd2-c4b2e4692fb1","message":{"id":"048b4a2f-1596-4ff0-9bd2-c4b2e4692fb1","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759408878.810008,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Ilya Sutskever what he saw speculation 2025\"}, {\"q\": \"OpenAI Q* breakthrough 2023 2024 2025\"}, {\"q\": \"OpenAI Strawberry project 2024 2025 reasoning\"}, {\"q\": \"Safe Superintelligence Inc Ilya Sutskever 2024 launch details\"}, {\"q\": \"reports on internal safety incidents OpenAI 2024 2025 autonomous agents demo\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Ilya Sutskever what he saw speculation 2025"},{"type":"search","q":"OpenAI Q* breakthrough 2023 2024 2025"},{"type":"search","q":"OpenAI Strawberry project 2024 2025 reasoning"},{"type":"search","q":"Safe Superintelligence Inc Ilya Sutskever 2024 launch details"},{"type":"search","q":"reports on internal safety incidents OpenAI 2024 2025 autonomous agents demo"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Investigating Ilya Sutskever's 2024-2025 developments and rumors","classifier_response":"default","request_id":"988445caff57aa0f-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"52ce64b0-95ea-43e0-9bb3-00a59411cad0","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"52ce64b0-95ea-43e0-9bb3-00a59411cad0","children":["eec2d76b-7a82-44b0-9c5a-e9b1a266a379"]},"eec2d76b-7a82-44b0-9c5a-e9b1a266a379":{"id":"eec2d76b-7a82-44b0-9c5a-e9b1a266a379","message":{"id":"eec2d76b-7a82-44b0-9c5a-e9b1a266a379","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1759408894.9779878,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Ilya Sutskever what he saw speculation 2025","OpenAI Q* breakthrough 2023 2024 2025","OpenAI Strawberry project 2024 2025 reasoning","Safe Superintelligence Inc Ilya Sutskever 2024 launch details","reports on internal safety incidents OpenAI 2024 2025 autonomous agents demo"]},"reasoning_status":"is_reasoning","reasoning_title":"Investigating Ilya Sutskever's 2024-2025 developments and rumors","classifier_response":"default","request_id":"988445caff57aa0f-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"048b4a2f-1596-4ff0-9bd2-c4b2e4692fb1","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"048b4a2f-1596-4ff0-9bd2-c4b2e4692fb1","children":["0da3c345-2d3a-4ce3-aad6-2467e7d4e5d9"]},"0da3c345-2d3a-4ce3-aad6-2467e7d4e5d9":{"id":"0da3c345-2d3a-4ce3-aad6-2467e7d4e5d9","message":{"id":"0da3c345-2d3a-4ce3-aad6-2467e7d4e5d9","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1759408894.9570012,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.businessinsider.com","entries":[{"type":"search_result","url":"https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9","title":"Sam Altman predicts AI will surpass human intelligence by 2030","snippet":"In a recent interview with Welt Editor-in-Chief Jan Philipp Burgard, OpenAI CEO Sam Altman shared bold predictions about the future of artificial intelligence. Altman expects superintelligent AI\u2014smarter than humans across the board\u2014to emerge by 2030, with major breakthroughs likely by 2026. He highlighted how tools like GPT-5 already outperform humans in specific domains, though still fall short in many intuitive human tasks. Altman anticipates AI will eventually undertake 30-40% of current human tasks and emphasized the importance of teaching future generations adaptability, creativity, and interpersonal skills.\n\nAltman also addressed concerns about AI safety and alignment with human values, expressing optimism but underscoring the challenge of unintended consequences. He rejected comparisons suggesting AI might view humans as inferior, stressing the need for responsible development. Responding to criticism around OpenAI\u2019s partial commercialization, Altman affirmed the ongoing centrality of its nonprofit mission and safety priorities.\n\nAdditionally, he hinted at future OpenAI hardware resembling a \u201cfamily of devices\u201d that could revolutionize personal computing. Altman also weighed in on political dynamics in tech, stating that while AI won\u2019t replace leaders, it will support decision-making. Personally, he\u2019s skeptical of using AI for relationship advice, unlike many others.","ref_id":null,"pub_date":1758877202.0,"attribution":"www.businessinsider.com"}]},{"type":"search_result_group","domain":"garymarcus.substack.com","entries":[{"type":"search_result","url":"https://garymarcus.substack.com/p/about-that-openai-breakthrough","title":"About that OpenAI \u201cbreakthrough\u201d - by Gary Marcus","snippet":"Will OpenAI's Q* change the world ... We can't really take Bing 2023 to tell us anything about what Q* might do in 2024 or 2025.","ref_id":null,"pub_date":null,"attribution":"garymarcus.substack.com"}]},{"type":"search_result_group","domain":"www.reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/","title":"Exclusive: OpenAI working on new reasoning technology ...","snippet":"Jul 15, 2024 \u2014 The document describes a project that uses Strawberry models with the aim of enabling the company's AI to not just generate answers to queries ...","ref_id":null,"pub_date":1720819458.099,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/","title":"Exclusive: OpenAI co-founder Sutskever's new safety- ...","snippet":"Sep 4, 2024 \u2014 Exclusive: OpenAI co-founder Sutskever's new safety-focused AI startup SSI raises $1 billion \u00b7 Three-month-old SSI valued at $5 billion, sources ...","ref_id":null,"pub_date":1725455074.541,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/","title":"OpenAI researchers warned board of AI breakthrough ...","snippet":"Nov 23, 2023 \u2014 Some at OpenAI believe Q* (pronounced Q-Star) could be a breakthrough in the startup's search for what's known as artificial general ...","ref_id":null,"pub_date":1700693062.452,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/business/sutskever-lead-safe-superintelligence-after-meta-poaches-ceo-gross-ai-talent-war-2025-07-03/","title":"Sutskever to lead Safe Superintelligence after Meta poaches CEO Gross in AI talent war","snippet":"Ilya Sutskever has assumed leadership of Safe Superintelligence (SSI), the AI startup he founded in 2024, following the departure of CEO Daniel Gross to Meta Platforms. Gross will head Meta's AI products division amid a broader industry-wide competition for top AI talent. Meta had also attempted to acquire SSI, which was most recently valued at $32 billion. Despite overtures from major tech firms, Sutskever affirmed SSI's independence and commitment to developing safe, superintelligent AI. The company secured $1 billion in funding last year to pursue this vision. Sutskever, previously OpenAI's chief scientist and co-founder, left the company after the controversial firing and rehiring of CEO Sam Altman in 2023.\n\nIn a parallel strategic move, Meta CEO Mark Zuckerberg has launched Meta Superintelligence Labs, consolidating its AI initiatives after delays with its Llama 4 model and loss of key personnel. This new division, led by Alexandr Wang and Nat Friedman, follows Meta's $14.3 billion investment in Scale AI. Gross and Friedman, longtime collaborators, previously co-founded the venture capital firm NFDG, which has backed startups including SSI, Perplexity, and Figma. Gross also brings experience from Apple, where he worked after the tech giant acquired his startup Cue in 2013.","ref_id":null,"pub_date":1751569669.0,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/","title":"AI with reasoning power will be less predictable, Ilya Sutskever says","snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","ref_id":null,"pub_date":1734147336.0,"attribution":"www.reuters.com"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/update-on-safety-and-security-practices/","title":"An update on our safety & security practices","snippet":"Sep 16, 2024 \u2014 The Safety and Security Committee will become an independent Board oversight committee focused on safety and security, to be chaired by Zico Kolter.","ref_id":null,"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"casmi.northwestern.edu","entries":[{"type":"search_result","url":"https://casmi.northwestern.edu/news/articles/2024/openais-strawberry-a-step-towards-advanced-reasoning-in-ai.html","title":"OpenAI's Strawberry: A Step Towards Advanced Reasoning in ...","snippet":"Strawberry is designed to extend the capabilities of conversational models to support reasoning that goes beyond the \u201cpredict the next word\u201d capabilities of ...","ref_id":null,"pub_date":null,"attribution":"casmi.northwestern.edu"}]},{"type":"search_result_group","domain":"cdn.openai.com","entries":[{"type":"search_result","url":"https://cdn.openai.com/threat-intelligence-reports/disrupting-malicious-uses-of-our-models-february-2025-update.pdf","title":"disrupting-malicious-uses-of-our-models-february-2025- ...","snippet":"Feb 1, 2025 \u2014 By combining detailed investigation and our own. AI-powered tools, we've identified and disrupted a wide range of malicious uses, from scams to ...","ref_id":null,"pub_date":null,"attribution":"cdn.openai.com"}]},{"type":"search_result_group","domain":"techcrunch.com","entries":[{"type":"search_result","url":"https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/","title":"Inside OpenAI's quest to make AI do anything for you","snippet":"Aug 3, 2025 \u2014 It took until 2023 for OpenAI to achieve a breakthrough, initially dubbed \u201cQ*\u201d and then \u201cStrawberry,\u201d by combining LLMs, RL, and a technique ...","ref_id":null,"pub_date":1754229600.0,"attribution":"techcrunch.com"}]},{"type":"search_result_group","domain":"www.strawberyai.com","entries":[{"type":"search_result","url":"https://www.strawberyai.com/","title":"OpenAI Strawberry Model : Capability GPT-5","snippet":"Openai Strawberry is a cutting-edge AI project focused on enhancing reasoning, problem-solving, and long-term task execution, pushing AI cap..","ref_id":null,"pub_date":null,"attribution":"www.strawberyai.com"}]},{"type":"search_result_group","domain":"ssi.inc","entries":[{"type":"search_result","url":"https://ssi.inc/","title":"Safe Superintelligence Inc.","snippet":"We have started the world's first straight-shot SSI lab, with one goal and one product: a safe superintelligence. It's called Safe Superintelligence Inc. SSI is ...","ref_id":null,"pub_date":null,"attribution":"ssi.inc"}]},{"type":"search_result_group","domain":"www.cio.com","entries":[{"type":"search_result","url":"https://www.cio.com/article/4064998/taming-ai-agents-the-autonomous-workforce-of-2026.html","title":"Taming AI agents: The autonomous workforce of 2026 - CIO","snippet":"2 days ago \u2014 By 2026, AI agents will run workflows \u2014 but only if we stop chasing 'super agents' and design them to stay in their lanes.","ref_id":null,"pub_date":1759190400.0,"attribution":"www.cio.com"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/181u3h3/possible_openais_q_breakthrough_and_deepminds/","title":"Possible OpenAI's Q* breakthrough and DeepMind's ...","snippet":"OpenAI leaked AI breakthrough called Q*, acing grade-school math. It is hypothesized combination of Q-learning and A*. It was then refuted.","ref_id":null,"pub_date":1700718171.789,"attribution":"www.reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/agi/comments/1llyxub/are_we_wise_to_trust_ilya_sutskevers_safe/","title":"Are We Wise to Trust Ilya Sutskever's Safe ...","snippet":"Ilya Sutskever says future superintelligent data centers are a new form of \"non-human life\". He's working on superalignment: \"We want those data ...","ref_id":null,"pub_date":null,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"daily.dev","entries":[{"type":"search_result","url":"https://daily.dev/blog/safe-superintelligence-inc-ssi-everything-we-know-so-far-about-ilya-sutskevers-new-ai-company","title":"Safe Superintelligence Inc (SSI): Everything we know so far ...","snippet":"Jun 21, 2024 \u2014 Safe Superintelligence Inc (SSI): Everything we know so far about Ilya Sutskever's new AI company ; Founded, June 2024 ; Locations, Palo Alto, ...","ref_id":null,"pub_date":null,"attribution":"daily.dev"}]},{"type":"search_result_group","domain":"theaitrack.com","entries":[{"type":"search_result","url":"https://theaitrack.com/openai-safety-evaluations-hub-launch/","title":"OpenAI Launches Public Hub to Share Safety Evaluations ...","snippet":"OpenAI now publishes ongoing safety evaluations for its AI models via a new hub, tracking hallucinations, jailbreaks, and harmful content ...","ref_id":null,"pub_date":null,"attribution":"theaitrack.com"}]},{"type":"search_result_group","domain":"theagenticarc.com","entries":[{"type":"search_result","url":"https://theagenticarc.com/ilya-sutskever-ai-prediction-warning/","title":"What Ilya Sutskever Sees for Humanity's Future - Agentic Arc","snippet":"Aug 10, 2025 \u2014 He offers a straightforward explanation: the human brain is a biological computer, and digital computers will one day match its capabilities.","ref_id":null,"pub_date":1754832985.0,"attribution":"theagenticarc.com"}]},{"type":"search_result_group","domain":"community.openai.com","entries":[{"type":"search_result","url":"https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343","title":"What is Q*? And when we will hear more?","snippet":"Nov 23, 2023 \u2014 An OpenAI model called Q* (pronounced Q Star) that was recently demonstrated internally and is capable of solving simple math problems.","ref_id":null,"pub_date":null,"attribution":"community.openai.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2508.14231v1","title":"Incident Analysis for AI Agents","snippet":"Aug 19, 2025 \u2014 Collecting AI incident reports enables practitioners to analyze their root causes and share lessons to prevent future occurrences. Driven by ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]}],"debug_sonic_thread_id":"thread_68de72fefee88331a895480e2f21d84e","reasoning_status":"is_reasoning","reasoning_title":"Investigating Ilya Sutskever's 2024-2025 developments and rumors","classifier_response":"default","request_id":"988445caff57aa0f-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"eec2d76b-7a82-44b0-9c5a-e9b1a266a379","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"eec2d76b-7a82-44b0-9c5a-e9b1a266a379","children":["28763a0f-42ef-4be6-bcb6-1c5a4a5a4d52"]},"28763a0f-42ef-4be6-bcb6-1c5a4a5a4d52":{"id":"28763a0f-42ef-4be6-bcb6-1c5a4a5a4d52","message":{"id":"28763a0f-42ef-4be6-bcb6-1c5a4a5a4d52","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759408903.2395,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Assessing what Ilya \u201csaw\u201d","content":"I\u2019ll need to look into sources involving Q*, Strawberry, SSI, and any related comments from Ilya. Based on these, I\u2019ll speculate on what he might have observed\u2014emergent AI capabilities, autonomy, reasoning improvements, or safety incidents. I\u2019ll also check if he\u2019s made any statements reflecting on alignment issues or other key insights.","chunks":[],"finished":true}],"source_analysis_msg_id":"9e100bd0-a221-439e-9b9c-bc66a067a12e"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"988445caff57aa0f-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"0da3c345-2d3a-4ce3-aad6-2467e7d4e5d9","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"0da3c345-2d3a-4ce3-aad6-2467e7d4e5d9","children":["511161bb-3dfb-4d35-bdf0-ed28c116a3cc"]},"511161bb-3dfb-4d35-bdf0-ed28c116a3cc":{"id":"511161bb-3dfb-4d35-bdf0-ed28c116a3cc","message":{"id":"511161bb-3dfb-4d35-bdf0-ed28c116a3cc","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759408897.078805,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Ilya Sutskever interview 2024 2025 what did he see speculation 'saw'\"}, {\"q\": \"Sutskever comments on Q* 2024 interview\"}, {\"q\": \"OpenAI board letter about AI breakthrough details 'Q-star' capabilities math planning 2023 Reuters\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Ilya Sutskever interview 2024 2025 what did he see speculation 'saw'"},{"type":"search","q":"Sutskever comments on Q* 2024 interview"},{"type":"search","q":"OpenAI board letter about AI breakthrough details 'Q-star' capabilities math planning 2023 Reuters"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching for Ilya Sutskever's comments and statements","classifier_response":"default","request_id":"988445caff57aa0f-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"28763a0f-42ef-4be6-bcb6-1c5a4a5a4d52","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"28763a0f-42ef-4be6-bcb6-1c5a4a5a4d52","children":["f54331cb-e8b3-4758-9e4a-59343071847e"]},"f54331cb-e8b3-4758-9e4a-59343071847e":{"id":"f54331cb-e8b3-4758-9e4a-59343071847e","message":{"id":"f54331cb-e8b3-4758-9e4a-59343071847e","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1759408906.6103587,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Ilya Sutskever interview 2024 2025 what did he see speculation 'saw'","Sutskever comments on Q* 2024 interview","OpenAI board letter about AI breakthrough details 'Q-star' capabilities math planning 2023 Reuters"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching for Ilya Sutskever's comments and statements","classifier_response":"default","request_id":"988445caff57aa0f-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"511161bb-3dfb-4d35-bdf0-ed28c116a3cc","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"511161bb-3dfb-4d35-bdf0-ed28c116a3cc","children":["02de97a6-1d7e-42d8-8ea8-8ce4572f9fc7"]},"02de97a6-1d7e-42d8-8ea8-8ce4572f9fc7":{"id":"02de97a6-1d7e-42d8-8ea8-8ce4572f9fc7","message":{"id":"02de97a6-1d7e-42d8-8ea8-8ce4572f9fc7","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1759408906.5960448,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.businessinsider.com","entries":[{"type":"search_result","url":"https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9","title":"Sam Altman predicts AI will surpass human intelligence by 2030","snippet":"In a recent interview with Welt Editor-in-Chief Jan Philipp Burgard, OpenAI CEO Sam Altman shared bold predictions about the future of artificial intelligence. Altman expects superintelligent AI\u2014smarter than humans across the board\u2014to emerge by 2030, with major breakthroughs likely by 2026. He highlighted how tools like GPT-5 already outperform humans in specific domains, though still fall short in many intuitive human tasks. Altman anticipates AI will eventually undertake 30-40% of current human tasks and emphasized the importance of teaching future generations adaptability, creativity, and interpersonal skills.\n\nAltman also addressed concerns about AI safety and alignment with human values, expressing optimism but underscoring the challenge of unintended consequences. He rejected comparisons suggesting AI might view humans as inferior, stressing the need for responsible development. Responding to criticism around OpenAI\u2019s partial commercialization, Altman affirmed the ongoing centrality of its nonprofit mission and safety priorities.\n\nAdditionally, he hinted at future OpenAI hardware resembling a \u201cfamily of devices\u201d that could revolutionize personal computing. Altman also weighed in on political dynamics in tech, stating that while AI won\u2019t replace leaders, it will support decision-making. Personally, he\u2019s skeptical of using AI for relationship advice, unlike many others.","ref_id":null,"pub_date":1758877202.0,"attribution":"www.businessinsider.com"},{"type":"search_result","url":"https://www.businessinsider.com/openai-cofounder-ilya-sutskever-ai-graduation-speech-university-of-toronto-2025-6","title":"OpenAI cofounder tells new graduates the day is coming when AI 'will do all the things that we can'","snippet":"In a convocation speech at the University of Toronto, OpenAI cofounder and former chief scientist Ilya Sutskever shared his vision that artificial intelligence (AI) will one day match human capabilities. He emphasized that while AI currently surpasses human performance in some areas, it remains deficient in many others. However, Sutskever is confident that with continued advancements, AI will eventually perform all tasks humans can, citing the brain as a biological computer and drawing parallels to digital computers. He offered advice to graduates to accept reality, avoid regrets, and focus on improving situations. Sutskever also reflected on his past actions, expressing regret for participating in the 2023 dismissal of OpenAI CEO Sam Altman, an action he later opposed along with other employees. Altman was reinstated shortly afterward. Sutskever departed from OpenAI six months later to establish a new research lab aimed at creating \"safe superintelligence.\"","ref_id":null,"pub_date":1749484533.0,"attribution":"www.businessinsider.com"}]},{"type":"search_result_group","domain":"www.reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/","title":"OpenAI researchers warned board of AI breakthrough ...","snippet":"Nov 23, 2023 \u2014 In their letter to the board, researchers flagged AI's prowess and potential danger, the sources said without specifying the exact safety ...","ref_id":null,"pub_date":1700693062.452,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/","title":"Ilya Sutskever on how AI will change and his new startup ...","snippet":"Sep 5, 2024 \u2014 SSI will approach scaling differently from OpenAI, he said. Following are highlights from the interview.","ref_id":null,"pub_date":1725531820.551,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/business/sutskever-lead-safe-superintelligence-after-meta-poaches-ceo-gross-ai-talent-war-2025-07-03/","title":"Sutskever to lead Safe Superintelligence after Meta poaches CEO Gross in AI talent war","snippet":"Ilya Sutskever has assumed leadership of Safe Superintelligence (SSI), the AI startup he founded in 2024, following the departure of CEO Daniel Gross to Meta Platforms. Gross will head Meta's AI products division amid a broader industry-wide competition for top AI talent. Meta had also attempted to acquire SSI, which was most recently valued at $32 billion. Despite overtures from major tech firms, Sutskever affirmed SSI's independence and commitment to developing safe, superintelligent AI. The company secured $1 billion in funding last year to pursue this vision. Sutskever, previously OpenAI's chief scientist and co-founder, left the company after the controversial firing and rehiring of CEO Sam Altman in 2023.\n\nIn a parallel strategic move, Meta CEO Mark Zuckerberg has launched Meta Superintelligence Labs, consolidating its AI initiatives after delays with its Llama 4 model and loss of key personnel. This new division, led by Alexandr Wang and Nat Friedman, follows Meta's $14.3 billion investment in Scale AI. Gross and Friedman, longtime collaborators, previously co-founded the venture capital firm NFDG, which has backed startups including SSI, Perplexity, and Figma. Gross also brings experience from Apple, where he worked after the tech giant acquired his startup Cue in 2013.","ref_id":null,"pub_date":1751569669.0,"attribution":"www.reuters.com"}]},{"type":"search_result_group","domain":"www.understandingai.org","entries":[{"type":"search_result","url":"https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors","title":"The real research behind the wild rumors about OpenAI's Q ...","snippet":"Dec 7, 2023 \u2014 OpenAI hasn't published details on its supposed Q* breakthrough, but it has published two papers about its efforts to solve grade-school math problems.","ref_id":null,"pub_date":1701974726.0,"attribution":"www.understandingai.org"}]},{"type":"search_result_group","domain":"www.lesswrong.com","entries":[{"type":"search_result","url":"https://www.lesswrong.com/posts/JkaDWvojFhRBuSxzz/sam-altman-s-ouster-at-openai-was-precipitated-by-letter-to","title":"Sam Altman's ouster at OpenAI was precipitated by letter to ...","snippet":"Nov 22, 2023 \u2014 The maker of ChatGPT had made progress on Q* (pronounced Q-Star), which some internally believe could be a breakthrough in the startup's search ...","ref_id":null,"pub_date":null,"attribution":"www.lesswrong.com"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/1csgp5w/discussing_the_so_called_what_did_ilya_see/","title":"Discussing the so called \"what did ilya see\" : r/singularity","snippet":"Ilya Sutskever says future superintelligent data centers are a new form of \"non-human life\". He's working on superalignment: \"We want those data ...","ref_id":null,"pub_date":1715765613.745,"attribution":"www.reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/181o1q4/d_exclusive_sam_altmans_ouster_at_openai_was/","title":"Sam Altman's ouster at OpenAI was precipitated by letter to ...","snippet":"Long-time executive Mira Murati told employees on Wednesday that a letter about the AI breakthrough called Q* (pronounced Q-Star), precipitated the board's ...","ref_id":null,"pub_date":1700698490.291,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"community.openai.com","entries":[{"type":"search_result","url":"https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343","title":"What is Q*? And when we will hear more?","snippet":"Nov 23, 2023 \u2014 An OpenAI model called Q* (pronounced Q Star) that was recently demonstrated internally and is capable of solving simple math problems.","ref_id":null,"pub_date":null,"attribution":"community.openai.com"}]},{"type":"search_result_group","domain":"www.latent.space","entries":[{"type":"search_result","url":"https://www.latent.space/p/what-ilya-saw","title":"What Ilya Saw","snippet":"Dec 14, 2024 \u2014 This week we got not one but TWO insights into the insights of Ilya: his widely publicized Test of Time talk at NeurIPS (transcript here), and OpenAI's ...","ref_id":null,"pub_date":1734216237.0,"attribution":"www.latent.space"}]},{"type":"search_result_group","domain":"blog.biocomm.ai","entries":[{"type":"search_result","url":"https://blog.biocomm.ai/2024/06/02/ilya-sutskever-ai-will-be-omnipotent-in-the-future-everything-that-is-impossible-becomes-possible-02-june/","title":"Ilya Sutskever | AI will be omnipotent in the future","snippet":"Jun 10, 2024 \u2014 \u201cAI is powerful. It is going to be extremely, unbelievable, unbelievably powerful, and it is because of this power that's where the safety ...","ref_id":null,"pub_date":null,"attribution":"blog.biocomm.ai"}]},{"type":"search_result_group","domain":"nypost.com","entries":[{"type":"search_result","url":"https://nypost.com/2025/05/19/business/openai-co-founder-wanted-doomsday-bunker-to-protect-against-rapture/","title":"OpenAI co-founder wanted to build doomsday bunker to protect company scientists from 'rapture': book","snippet":"According to an upcoming book by Karen Hao titled \"Empire of AI: Dreams and Nightmares in Sam Altman's OpenAI,\" OpenAI co-founder Ilya Sutskever proposed building a doomsday bunker to protect the company's key scientists in anticipation of potential chaos following the release of artificial general intelligence (AGI). Sutskever, known for both his technical brilliance and mystic-like views on AI, reportedly feared that AGI could trigger geopolitical instability or even a \"rapture\"-like event. The idea of a bunker, frequently mentioned in internal discussions, underscores profound anxieties within OpenAI's leadership about the transformative and potentially catastrophic power of AGI. Sutskever and then-CTO Mira Murati became increasingly disillusioned with CEO Sam Altman, accusing him of neglecting safety protocols and consolidating control. This led to a failed boardroom coup to oust Altman in late 2023. Though Altman was quickly reinstated due to external pressure, both Sutskever and Murati eventually left OpenAI. The bunker concept serves as a symbolic reflection of the urgent ethical and existential concerns surrounding the rapid advancement of AI.","ref_id":null,"pub_date":1747673117.0,"attribution":"nypost.com"}]},{"type":"search_result_group","domain":"time.com","entries":[{"type":"search_result","url":"https://time.com/6990076/safe-superintelligence-inc-announced/","title":"Former OpenAI Chief Scientist Announces New Safety-Focused Company","snippet":"Ilya Sutskever, co-founder and former chief scientist of OpenAI, has launched a new venture, Safe Superintelligence Inc., focused on building a safe superintelligence system. The company, which has offices in Palo Alto, California, and Tel Aviv, aims to eliminate distractions from commercial pressures by concentrating solely on its core objective. Sutskever is joined by co-founders Daniel Gross, a former AI engineer at Apple, and Daniel Levy, another ex-OpenAI employee. The decision follows Sutskever's controversial attempt to remove OpenAI CEO Sam Altman, which he later reneged. The announcement hints at dissatisfaction with OpenAI's focus on product development over safety. Safe Superintelligence Inc. is still developing its funding and business model and is currently hiring.","ref_id":null,"pub_date":1718831103.0,"attribution":"time.com"}]}],"debug_sonic_thread_id":"thread_68de72fefee88331a895480e2f21d84e","reasoning_status":"is_reasoning","reasoning_title":"Searching for Ilya Sutskever's comments and statements","classifier_response":"default","request_id":"988445caff57aa0f-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"f54331cb-e8b3-4758-9e4a-59343071847e","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"f54331cb-e8b3-4758-9e4a-59343071847e","children":["3797a736-c4ef-48ea-92f4-d6d94cded36d"]},"3797a736-c4ef-48ea-92f4-d6d94cded36d":{"id":"3797a736-c4ef-48ea-92f4-d6d94cded36d","message":{"id":"3797a736-c4ef-48ea-92f4-d6d94cded36d","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759408919.838425,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Synthesizing speculation on Ilya's insights","content":"I","chunks":[],"finished":false}],"source_analysis_msg_id":"7279bde5-f003-4fbf-a42c-8299644928a9"},"status":"in_progress","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"988445caff57aa0f-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"02de97a6-1d7e-42d8-8ea8-8ce4572f9fc7","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"02de97a6-1d7e-42d8-8ea8-8ce4572f9fc7","children":["6744c242-860f-4e2b-9910-d0b932a7609e"]},"6744c242-860f-4e2b-9910-d0b932a7609e":{"id":"6744c242-860f-4e2b-9910-d0b932a7609e","message":{"id":"6744c242-860f-4e2b-9910-d0b932a7609e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759408920.35044,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 38s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":38,"model_slug":"gpt-5-thinking","request_id":"988445caff57aa0f-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"3797a736-c4ef-48ea-92f4-d6d94cded36d","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"3797a736-c4ef-48ea-92f4-d6d94cded36d","children":["eb5bcf07-5e56-435f-9078-7aba5ee80b44"]},"eb5bcf07-5e56-435f-9078-7aba5ee80b44":{"id":"eb5bcf07-5e56-435f-9078-7aba5ee80b44","message":{"id":"eb5bcf07-5e56-435f-9078-7aba5ee80b44","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759408908.476708,"update_time":null,"content":{"content_type":"text","parts":["Short answer: we don\u2019t know. There is no public, verifiable account from Ilya Sutskever of a single \u201cthing he saw.\u201d But if you triangulate the most credible reporting and Ilya\u2019s own statements from the last 12 months, the most plausible story is:\n\n### What\u2019s most likely\n1) **A convincing jump in *reasoning* + *planning* under heavy test-time compute**  \nReuters reported OpenAI\u2019s internal \u201cStrawberry\u201d program aimed at models that can plan ahead and perform \u201cdeep research\u201d on the open web\u2014i.e., long-horizon, tool-using autonomy rather than just next-token prediction. \ue200cite\ue202turn0search1\ue201  \nA number of industry reports (and later explainer pieces) tie \u201cQ* / Strawberry\u201d to techniques that spend *more* computation at inference to search, verify, and self-correct\u2014yielding markedly better out-of-distribution math/logic performance than standard LLM prompting. \ue200cite\ue202turn0search8\ue202turn1search1\ue201  \nIf Ilya \u201csaw\u201d anything, it was likely an internal demo where a model reliably solved problems it hadn\u2019t been trained on (non-rote math and multi-step tasks) while orchestrating tools/agents\u2014enough to make *future capability* trajectories palpable rather than speculative. (The original Q* rumors date to late-2023, but the last year added concrete \u201creasoning/agent\u201d programs and press.) \ue200cite\ue202turn1search0\ue202turn1search6\ue201\n\n2) **Robust, semi-autonomous web agents that don\u2019t fall apart after 10\u201320 steps**  \nThe \u201cStrawberry\u201d brief specifically framed dependable autonomous navigation for multi-step research\u2014precisely where prior agents failed (fragile, loop-prone, or easily derailed). A demo that *stays on task for hours*, sets subgoals, checks its own work, and escalates to humans when uncertain would feel like a qualitative shift. \ue200cite\ue202turn0search1\ue201\n\n### Why that would have hit differently for Ilya\n- **He\u2019s repeatedly warned that better reasoning \u21d2 less predictability.** In December 2024, he told NeurIPS that advancing reasoning will make systems *less* predictable, explicitly comparing to AlphaGo-style surprising moves. That framing implies that once models can truly *think ahead* and explore candidate plans, you also invite harder-to-anticipate behaviors. \ue200cite\ue202turn0news47\ue201  \n- **He pivoted to build SSI, single-mindedly.** In 2024\u20132025 he left OpenAI and launched Safe Superintelligence Inc. with the explicit thesis that superintelligence is within reach and safety must be the sole focus\u2014then took direct leadership this July. That timing lines up with having seen internally convincing capability trends. \ue200cite\ue202turn0search10\ue202turn0search2\ue202turn1news26\ue201\n\n### What it probably *wasn\u2019t*\n- **A fully general \u201cAGI moment.\u201d** No credible source claims a broad, human-level system emerged. Even Sam Altman\u2019s latest on timelines couches superintelligence as *likely by late-decade*, with breakthroughs \u201cby 2026,\u201d not \u201calready here.\u201d \ue200cite\ue202turn1news23\ue201  \n- **Simple grade-school math hype alone.** The 2023 Q* rumor said \u201csolves simple math,\u201d which by itself wouldn\u2019t spook a chief scientist in 2024\u20132025. The meaningful part is *mechanism* (search, verification, long-horizon planning) and *robustness*, not just a test score bump. \ue200cite\ue202turn1search0\ue202turn1search6\ue201\n\n### A disciplined speculation (ranked by plausibility)\n- **High**: A *Strawberry/Q*-style* model that: (i) chains deliberate steps with test-time search; (ii) self-critiques and re-plans; (iii) uses tools and the browser; (iv) sustains reliability over long tasks. This would feel qualitatively new relative to 2023-era chatbots. \ue200cite\ue202turn0search1\ue202turn0search8\ue201  \n- **Medium**: An internal *agent safety incident* (e.g., goal misgeneralization during a long autonomous run) that underscored Ilya\u2019s \u201creasoning \u21d2 unpredictability\u201d point. Firms have begun cataloging real incidents as agent stacks proliferate; a vivid internal case would concentrate the mind. (This is an inference from his NeurIPS remarks + broader incident literature, not a specific leaked event.) \ue200cite\ue202turn0news47\ue202turn0search18\ue201  \n- **Lower**: A pure *scaling law* extrapolation (e.g., compute + data generation via self-play/self-training) convincing him that superintelligence is a straight shot. He\u2019s voiced the view, but absent a crisp demo, that\u2019s less likely to trigger the cultural shock people ascribe to \u201cwhat he saw.\u201d \ue200cite\ue202turn0news47\ue201\n\n### Why the last year matters\n2024\u20132025 is when the center of gravity moved from \u201cchat completion that *looks* smart\u201d to **agentic, tool-using systems with explicit test-time computation** and multi-hour reliability targets. That\u2019s exactly the regime where: (a) capability scaling re-accelerates without more internet data; (b) *alignment* strain becomes concrete; and (c) a safety-first lab like SSI makes strategic sense. \ue200cite\ue202turn0news47\ue202turn0search1\ue202turn0search10\ue201\n\n---\n\n**My bottom line:** If you force a single sentence: *he likely saw a Strawberry-class, tool-using model execute long-horizon tasks with self-verification well enough to convince him that unpredictable, superintelligence-relevant reasoning was now a tractable engineering problem rather than a distant hope.* That synthesis fits both the leaks and his subsequent public moves. \ue200cite\ue202turn0search1\ue202turn0news47\ue202turn0search10\ue201\n\nIf you want, I can sketch concrete technical signatures that would distinguish a \u201cflashy demo\u201d from a true capability phase change (e.g., reliability under distribution shift, corrigibility under tool access, calibration before/after search, and cost-per-successful-episode curves)."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":"\ue200cite\ue202turn0search1\ue201","start_idx":572,"end_idx":591,"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Exclusive: OpenAI working on new reasoning technology ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","pub_date":1720819458.099,"snippet":"Jul 15, 2024 \u2014 The document describes a project that uses Strawberry models with the aim of enabling the company's AI to not just generate answers to queries ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search8\ue202turn1search1\ue201","start_idx":864,"end_idx":896,"safe_urls":["https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/?utm_source=chatgpt.com","https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors?utm_source=chatgpt.com"],"refs":[],"alt":"([TechCrunch](https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Inside OpenAI's quest to make AI do anything for you","url":"https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/?utm_source=chatgpt.com","pub_date":1754229600.0,"snippet":"Aug 3, 2025 \u2014 It took until 2023 for OpenAI to achieve a breakthrough, initially dubbed \u201cQ*\u201d and then \u201cStrawberry,\u201d by combining LLMs, RL, and a technique ...","attribution_segments":null,"supporting_websites":[{"title":"The real research behind the wild rumors about OpenAI's Q ...","url":"https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 7, 2023 \u2014 OpenAI hasn't published details on its supposed Q* breakthrough, but it has published two papers about its efforts to solve grade-school math problems.","attribution":"Understanding AI"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":8},{"turn_index":1,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"TechCrunch"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search0\ue202turn1search6\ue201","start_idx":1286,"end_idx":1318,"safe_urls":["https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?utm_source=chatgpt.com","https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"OpenAI researchers warned board of AI breakthrough ...","url":"https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=chatgpt.com","pub_date":1700693062.452,"snippet":"Nov 23, 2023 \u2014 In their letter to the board, researchers flagged AI's prowess and potential danger, the sources said without specifying the exact safety ...","attribution_segments":null,"supporting_websites":[{"title":"What is Q*? And when we will hear more?","url":"https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?utm_source=chatgpt.com","pub_date":null,"snippet":"Nov 23, 2023 \u2014 An OpenAI model called Q* (pronounced Q Star) that was recently demonstrated internally and is capable of solving simple math problems.","attribution":"OpenAI Community"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":0},{"turn_index":1,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue201","start_idx":1734,"end_idx":1753,"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Exclusive: OpenAI working on new reasoning technology ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","pub_date":1720819458.099,"snippet":"Jul 15, 2024 \u2014 The document describes a project that uses Strawberry models with the aim of enabling the company's AI to not just generate answers to queries ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0news47\ue201","start_idx":2170,"end_idx":2188,"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":1734147336.0,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"news","ref_index":47}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search10\ue202turn0search2\ue202turn1news26\ue201","start_idx":2524,"end_idx":2569,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/business/sutskever-lead-safe-superintelligence-after-meta-poaches-ceo-gross-ai-talent-war-2025-07-03/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"We have started the world's first straight-shot SSI lab, with one goal and one product: a safe superintelligence. It's called Safe Superintelligence Inc. SSI is ...","attribution_segments":null,"supporting_websites":[{"title":"Exclusive: OpenAI co-founder Sutskever's new safety- ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/?utm_source=chatgpt.com","pub_date":null,"snippet":"Sep 4, 2024 \u2014 Exclusive: OpenAI co-founder Sutskever's new safety-focused AI startup SSI raises $1 billion \u00b7 Three-month-old SSI valued at $5 billion, sources ...","attribution":"Reuters"},{"title":"Sutskever to lead Safe Superintelligence after Meta poaches CEO Gross in AI talent war","url":"https://www.reuters.com/business/sutskever-lead-safe-superintelligence-after-meta-poaches-ceo-gross-ai-talent-war-2025-07-03/?utm_source=chatgpt.com","pub_date":null,"snippet":"Ilya Sutskever has assumed leadership of Safe Superintelligence (SSI), the AI startup he founded in 2024, following the departure of CEO Daniel Gross to Meta Platforms. Gross will head Meta's AI products division amid a broader industry-wide competition for top AI talent. Meta had also attempted to acquire SSI, which was most recently valued at $32 billion. Despite overtures from major tech firms, Sutskever affirmed SSI's independence and commitment to developing safe, superintelligent AI. The company secured $1 billion in funding last year to pursue this vision. Sutskever, previously OpenAI's chief scientist and co-founder, left the company after the controversial firing and rehiring of CEO Sam Altman in 2023.\n\nIn a parallel strategic move, Meta CEO Mark Zuckerberg has launched Meta Superintelligence Labs, consolidating its AI initiatives after delays with its Llama 4 model and loss of key personnel. This new division, led by Alexandr Wang and Nat Friedman, follows Meta's $14.3 billion investment in Scale AI. Gross and Friedman, longtime collaborators, previously co-founded the venture capital firm NFDG, which has backed startups including SSI, Perplexity, and Figma. Gross also brings experience from Apple, where he worked after the tech giant acquired his startup Cue in 2013.","attribution":"Reuters"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":10},{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":1,"ref_type":"news","ref_index":26}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1news23\ue201","start_idx":2842,"end_idx":2860,"safe_urls":["https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9?utm_source=chatgpt.com"],"refs":[],"alt":"([Business Insider](https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Sam Altman predicts AI will surpass human intelligence by 2030","url":"https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9?utm_source=chatgpt.com","pub_date":1758877202.0,"snippet":"In a recent interview with Welt Editor-in-Chief Jan Philipp Burgard, OpenAI CEO Sam Altman shared bold predictions about the future of artificial intelligence. Altman expects superintelligent AI\u2014smarter than humans across the board\u2014to emerge by 2030, with major breakthroughs likely by 2026. He highlighted how tools like GPT-5 already outperform humans in specific domains, though still fall short in many intuitive human tasks. Altman anticipates AI will eventually undertake 30-40% of current human tasks and emphasized the importance of teaching future generations adaptability, creativity, and interpersonal skills.\n\nAltman also addressed concerns about AI safety and alignment with human values, expressing optimism but underscoring the challenge of unintended consequences. He rejected comparisons suggesting AI might view humans as inferior, stressing the need for responsible development. Responding to criticism around OpenAI\u2019s partial commercialization, Altman affirmed the ongoing centrality of its nonprofit mission and safety priorities.\n\nAdditionally, he hinted at future OpenAI hardware resembling a \u201cfamily of devices\u201d that could revolutionize personal computing. Altman also weighed in on political dynamics in tech, stating that while AI won\u2019t replace leaders, it will support decision-making. Personally, he\u2019s skeptical of using AI for relationship advice, unlike many others.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":1,"ref_type":"news","ref_index":23}],"hue":null,"attributions":null,"attribution":"Business Insider"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn1search0\ue202turn1search6\ue201","start_idx":3141,"end_idx":3173,"safe_urls":["https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?utm_source=chatgpt.com","https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"OpenAI researchers warned board of AI breakthrough ...","url":"https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=chatgpt.com","pub_date":1700693062.452,"snippet":"Nov 23, 2023 \u2014 In their letter to the board, researchers flagged AI's prowess and potential danger, the sources said without specifying the exact safety ...","attribution_segments":null,"supporting_websites":[{"title":"What is Q*? And when we will hear more?","url":"https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?utm_source=chatgpt.com","pub_date":null,"snippet":"Nov 23, 2023 \u2014 An OpenAI model called Q* (pronounced Q Star) that was recently demonstrated internally and is capable of solving simple math problems.","attribution":"OpenAI Community"}],"refs":[{"turn_index":1,"ref_type":"search","ref_index":0},{"turn_index":1,"ref_type":"search","ref_index":6}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search8\ue201","start_idx":3505,"end_idx":3537,"safe_urls":["https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Exclusive: OpenAI working on new reasoning technology ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","pub_date":1720819458.099,"snippet":"Jul 15, 2024 \u2014 The document describes a project that uses Strawberry models with the aim of enabling the company's AI to not just generate answers to queries ...","attribution_segments":null,"supporting_websites":[{"title":"Inside OpenAI's quest to make AI do anything for you","url":"https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 3, 2025 \u2014 It took until 2023 for OpenAI to achieve a breakthrough, initially dubbed \u201cQ*\u201d and then \u201cStrawberry,\u201d by combining LLMs, RL, and a technique ...","attribution":"TechCrunch"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":8}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0news47\ue202turn0search18\ue201","start_idx":3942,"end_idx":3974,"safe_urls":["https://arxiv.org/html/2508.14231v1?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":1734147336.0,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution_segments":null,"supporting_websites":[{"title":"Incident Analysis for AI Agents","url":"https://arxiv.org/html/2508.14231v1?utm_source=chatgpt.com","pub_date":null,"snippet":"Aug 19, 2025 \u2014 Collecting AI incident reports enables practitioners to analyze their root causes and share lessons to prevent future occurrences. Driven by ...","attribution":"arXiv"}],"refs":[{"turn_index":0,"ref_type":"news","ref_index":47},{"turn_index":0,"ref_type":"search","ref_index":18}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0news47\ue201","start_idx":4274,"end_idx":4292,"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":1734147336.0,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"news","ref_index":47}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0news47\ue202turn0search1\ue202turn0search10\ue201","start_idx":4718,"end_idx":4763,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":1734147336.0,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution_segments":null,"supporting_websites":[{"title":"Exclusive: OpenAI working on new reasoning technology ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 15, 2024 \u2014 The document describes a project that uses Strawberry models with the aim of enabling the company's AI to not just generate answers to queries ...","attribution":"Reuters"},{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"We have started the world's first straight-shot SSI lab, with one goal and one product: a safe superintelligence. It's called Safe Superintelligence Inc. SSI is ...","attribution":"ssi.inc"}],"refs":[{"turn_index":0,"ref_type":"news","ref_index":47},{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":10}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0news47\ue202turn0search10\ue201","start_idx":5146,"end_idx":5191,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Exclusive: OpenAI working on new reasoning technology ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","pub_date":1720819458.099,"snippet":"Jul 15, 2024 \u2014 The document describes a project that uses Strawberry models with the aim of enabling the company's AI to not just generate answers to queries ...","attribution_segments":null,"supporting_websites":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":null,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution":"Reuters"},{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"We have started the world's first straight-shot SSI lab, with one goal and one product: a safe superintelligence. It's called Safe Superintelligence Inc. SSI is ...","attribution":"ssi.inc"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"news","ref_index":47},{"turn_index":0,"ref_type":"search","ref_index":10}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":" ","start_idx":5475,"end_idx":5475,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"Exclusive: OpenAI working on new reasoning technology ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","attribution":"Reuters"},{"title":"Inside OpenAI's quest to make AI do anything for you","url":"https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/?utm_source=chatgpt.com","attribution":"TechCrunch"},{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","attribution":"ssi.inc"},{"title":"Sam Altman predicts AI will surpass human intelligence by 2030","url":"https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9?utm_source=chatgpt.com","attribution":"Business Insider"}],"has_images":false}],"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/?utm_source=chatgpt.com","https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors?utm_source=chatgpt.com","https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?utm_source=chatgpt.com","https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/business/sutskever-lead-safe-superintelligence-after-meta-poaches-ceo-gross-ai-talent-war-2025-07-03/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/?utm_source=chatgpt.com","https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9?utm_source=chatgpt.com","https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?utm_source=chatgpt.com","https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=chatgpt.com","https://techcrunch.com/2025/08/03/inside-openais-quest-to-make-ai-do-anything-for-you/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","https://arxiv.org/html/2508.14231v1?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"substack.com","entries":[{"type":"search_result","url":"https://garymarcus.substack.com/p/about-that-openai-breakthrough?utm_source=chatgpt.com","title":"About that OpenAI \u201cbreakthrough\u201d - by Gary Marcus","snippet":"Will OpenAI's Q* change the world ... We can't really take Bing 2023 to tell us anything about what Q* might do in 2024 or 2025.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"substack.com"}]},{"type":"search_result_group","domain":"understandingai.org","entries":[{"type":"search_result","url":"https://www.understandingai.org/p/how-to-think-about-the-openai-q-rumors?utm_source=chatgpt.com","title":"The real research behind the wild rumors about OpenAI's Q ...","snippet":"Dec 7, 2023 \u2014 OpenAI hasn't published details on its supposed Q* breakthrough, but it has published two papers about its efforts to solve grade-school math problems.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":1},"pub_date":null,"attribution":"understandingai.org"}]},{"type":"search_result_group","domain":"reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/?utm_source=chatgpt.com","title":"Exclusive: OpenAI co-founder Sutskever's new safety- ...","snippet":"Sep 4, 2024 \u2014 Exclusive: OpenAI co-founder Sutskever's new safety-focused AI startup SSI raises $1 billion \u00b7 Three-month-old SSI valued at $5 billion, sources ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","title":"Ilya Sutskever on how AI will change and his new startup ...","snippet":"Sep 5, 2024 \u2014 Ilya Sutskever, OpenAI's former chief scientist, has launched a new company called Safe Superintelligence (SSI), aiming to develop safe artificial intelligence ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":44},"pub_date":null,"attribution":"reuters.com"}]},{"type":"search_result_group","domain":"lesswrong.com","entries":[{"type":"search_result","url":"https://www.lesswrong.com/posts/JkaDWvojFhRBuSxzz/sam-altman-s-ouster-at-openai-was-precipitated-by-letter-to?utm_source=chatgpt.com","title":"Sam Altman's ouster at OpenAI was precipitated by letter to ...","snippet":"Nov 22, 2023 \u2014 The maker of ChatGPT had made progress on Q* (pronounced Q-Star), which some internally believe could be a breakthrough in the startup's search ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":2},"pub_date":null,"attribution":"lesswrong.com"}]},{"type":"search_result_group","domain":"openai.com","entries":[{"type":"search_result","url":"https://openai.com/index/update-on-safety-and-security-practices/?utm_source=chatgpt.com","title":"An update on our safety & security practices","snippet":"Sep 16, 2024 \u2014 The Safety and Security Committee will become an independent Board oversight committee focused on safety and security, to be chaired by Zico Kolter.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":3},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://cdn.openai.com/threat-intelligence-reports/disrupting-malicious-uses-of-our-models-february-2025-update.pdf?utm_source=chatgpt.com","title":"disrupting-malicious-uses-of-our-models-february-2025- ...","snippet":"Feb 1, 2025 \u2014 By combining detailed investigation and our own. AI-powered tools, we've identified and disrupted a wide range of malicious uses, from scams to ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://community.openai.com/t/what-is-q-and-when-we-will-hear-more/521343?utm_source=chatgpt.com","title":"What is Q*? And when we will hear more?","snippet":"Nov 23, 2023 \u2014 An OpenAI model called Q* (pronounced Q Star) that was recently demonstrated internally and is capable of solving simple math problems.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://community.openai.com/t/openai-rumors-q-is-now-a-reasoning-engine-called-project-strawberry/864663?utm_source=chatgpt.com","title":"Q* is Now a Reasoning Engine Called 'Project Strawberry'","snippet":"Jul 13, 2024 \u2014 OpenAI showed a demo of a research project that it claimed had new human-like reasoning skills, according to Bloomberg.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/openai-anthropic-safety-evaluation/?utm_source=chatgpt.com","title":"Findings from a pilot Anthropic\u2013OpenAI alignment ...","snippet":"Aug 27, 2025 \u2014 OpenAI and Anthropic share findings from a first-of-its-kind joint safety evaluation, testing each other's models for misalignment, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":27},"pub_date":null,"attribution":"openai.com"},{"type":"search_result","url":"https://openai.com/index/learning-to-reason-with-llms/?utm_source=chatgpt.com","title":"Learning to reason with LLMs","snippet":"Sep 12, 2024 \u2014 Answer: THERE ARE THREE R'S IN STRAWBERRY. Coding. We trained a model that scored 213 points and ranked in the 49th percentile in the 2024 ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"openai.com"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/181u3h3/possible_openais_q_breakthrough_and_deepminds/?utm_source=chatgpt.com","title":"Possible OpenAI's Q* breakthrough and DeepMind's ...","snippet":"OpenAI leaked AI breakthrough called Q*, acing grade-school math. It is hypothesized combination of Q-learning and A*. It was then refuted.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/agi/comments/1llyxub/are_we_wise_to_trust_ilya_sutskevers_safe/?utm_source=chatgpt.com","title":"Are We Wise to Trust Ilya Sutskever's Safe ...","snippet":"Ilya Sutskever says future superintelligent data centers are a new form of \"non-human life\". He's working on superalignment: \"We want those data ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/1l6rn3k/ilya_sutskevever_says_overcoming_the_challenge_of/?utm_source=chatgpt.com","title":"Ilya Sutskevever says \"Overcoming the challenge of AI will ...","snippet":"Ilya Sutskevever says \"Overcoming the challenge of AI will bring the greatest reward, and whether you like it or not, your life is going to be affected with AI\"","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/ArtificialInteligence/comments/1e2gb4p/openais_project_strawberry_pioneering_the_future/?utm_source=chatgpt.com","title":"OpenAI's Project Strawberry: Pioneering the Future of AI ...","snippet":"OpenAI is developing a new AI model named \"Strawberry\" specifically for increasing the reasoning abilities of an AI.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/1lqtdzk/ilya_sutskever_we_have_the_compute_we_have_the/?utm_source=chatgpt.com","title":"r/singularity - Ilya Sutskever: 'We have the compute ...","snippet":"Whatever you say about Ilya he's for sure a true believer. If he didn't think he was getting to super intelligence he'd have taken the billions from Meta.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/1csgp5w/discussing_the_so_called_what_did_ilya_see/?utm_source=chatgpt.com","title":"Discussing the so called \"what did ilya see\" : r/singularity","snippet":"Ilya Sutskever says future superintelligent data centers are a new form of \"non-human life\". He's working on superalignment: \"We want those data ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/MachineLearning/comments/181o1q4/d_exclusive_sam_altmans_ouster_at_openai_was/?utm_source=chatgpt.com","title":"Sam Altman's ouster at OpenAI was precipitated by letter to ...","snippet":"Long-time executive Mira Murati told employees on Wednesday that a letter about the AI breakthrough called Q* (pronounced Q-Star), precipitated the board's ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/mlscaling/comments/181nsyb/openai_rumors_breakthrough_math_model_q_was/?utm_source=chatgpt.com","title":"OpenAI rumors: breakthrough math model Q* was relevant ...","snippet":">In their letter to the board, researchers flagged AI's prowess and potential danger, the sources said without specifying the exact safety ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"northwestern.edu","entries":[{"type":"search_result","url":"https://casmi.northwestern.edu/news/articles/2024/openais-strawberry-a-step-towards-advanced-reasoning-in-ai.html?utm_source=chatgpt.com","title":"OpenAI's Strawberry: A Step Towards Advanced Reasoning in ...","snippet":"Strawberry is designed to extend the capabilities of conversational models to support reasoning that goes beyond the \u201cpredict the next word\u201d capabilities of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"northwestern.edu"},{"type":"search_result","url":"https://casmi.northwestern.edu/news/articles/2024/openais-strawberry-project-bridges-the-gap-between-conversation-and-reasoning.html?utm_source=chatgpt.com","title":"OpenAI's Strawberry Project Bridges the Gap Between ...","snippet":"OpenAI's Strawberry project symbolizes a significant leap towards AI reasoning by blending traditional search methodologies with contemporary language models.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"northwestern.edu"}]},{"type":"search_result_group","domain":"biocomm.ai","entries":[{"type":"search_result","url":"https://blog.biocomm.ai/2024/06/02/ilya-sutskever-ai-will-be-omnipotent-in-the-future-everything-that-is-impossible-becomes-possible-02-june/?utm_source=chatgpt.com","title":"Ilya Sutskever | AI will be omnipotent in the future","snippet":"Jun 10, 2024 \u2014 \u201cAI is powerful. It is going to be extremely, unbelievable, unbelievably powerful, and it is because of this power that's where the safety ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"biocomm.ai"}]},{"type":"search_result_group","domain":"strawberyai.com","entries":[{"type":"search_result","url":"https://www.strawberyai.com/?utm_source=chatgpt.com","title":"OpenAI Strawberry Model : Capability GPT-5","snippet":"Openai Strawberry is a cutting-edge AI project focused on enhancing reasoning, problem-solving, and long-term task execution, pushing AI cap..","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"strawberyai.com"}]},{"type":"search_result_group","domain":"ycombinator.com","entries":[{"type":"search_result","url":"https://news.ycombinator.com/item?id=42413677&utm_source=chatgpt.com","title":"Ilya Sutskever NeurIPS talk [video]","snippet":"I think his general persona and association with ai led to the fluffy speculation at the end. I notice with Ilya he wants to talk about these out there ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"ycombinator.com"}]},{"type":"search_result_group","domain":"cio.com","entries":[{"type":"search_result","url":"https://www.cio.com/article/4064998/taming-ai-agents-the-autonomous-workforce-of-2026.html?utm_source=chatgpt.com","title":"Taming AI agents: The autonomous workforce of 2026 - CIO","snippet":"2 days ago \u2014 By 2026, AI agents will run workflows \u2014 but only if we stop chasing 'super agents' and design them to stay in their lanes.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"cio.com"}]},{"type":"search_result_group","domain":"youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=qaqpz7i7eKg&utm_source=chatgpt.com","title":"Incident Reporting Agent | Building a Autonomous Agent step ...","snippet":"In this video, Shane will show you how to get your agent ready to publish and then publish it! *Microsoft worked with Shane Young, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"youtube.com"},{"type":"search_result","url":"https://www.youtube.com/watch?v=2MOxAsRKaEU&utm_source=chatgpt.com","title":"\"AGI is COMING!\" - Ilya Sutskever's Latest Announcement, SSI ...","snippet":"In this groundbreaking video, Ilya Sutskever, co-founder of OpenAI, makes a historic announcement about the achievement of Artificial ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"youtube.com"},{"type":"search_result","url":"https://www.youtube.com/watch?v=kLDHILWAKRk&utm_source=chatgpt.com","title":"Ilya Sutskever | Unlocking the Future: Top AI Learning Trends ...","snippet":"Reinforcement learning is a framework it's a framework of evaluating agents in their ability to achieve goals in complicated stochastic environments.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"youtube.com"},{"type":"search_result","url":"https://www.youtube.com/watch?v=YM_8mBnv-EI&utm_source=chatgpt.com","title":"Ilya Sutskever \"Superintelligence is Self Aware, Unpredictable ...","snippet":"Ilya Sutskever \"Superintelligence is Self Aware, Unpredictable and Highly Agentic\" | NeurIPS 2024 &middot; Comments.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"youtube.com"}]},{"type":"search_result_group","domain":"aawsat.com","entries":[{"type":"search_result","url":"https://english.aawsat.com/technology/4687451-openai-researchers-warned-board-ai-breakthrough-ahead-ceo-ouster-sources-say?utm_source=chatgpt.com","title":"OpenAI Researchers Warned Board of AI Breakthrough ...","snippet":"Some at OpenAI believe Q* (pronounced Q-Star) could be a breakthrough in the startup's search for what's known as artificial general intelligence (AGI), one ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"aawsat.com"}]},{"type":"search_result_group","domain":"daily.dev","entries":[{"type":"search_result","url":"https://daily.dev/blog/safe-superintelligence-inc-ssi-everything-we-know-so-far-about-ilya-sutskevers-new-ai-company?utm_source=chatgpt.com","title":"Safe Superintelligence Inc (SSI): Everything we know so far ...","snippet":"Jun 21, 2024 \u2014 Safe Superintelligence Inc (SSI): Everything we know so far about Ilya Sutskever's new AI company ; Founded, June 2024 ; Locations, Palo Alto, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"daily.dev"}]},{"type":"search_result_group","domain":"facebook.com","entries":[{"type":"search_result","url":"https://www.facebook.com/groups/SKNTimesNewsGroup/posts/1877401319654113/?utm_source=chatgpt.com","title":"Ex-OpenAI Scientist WARNS: \"You Have No Idea What's ...","snippet":"A major new analysis of expert predictions suggests that humanity may be just months away from reaching the technological singularity. This is ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"facebook.com"},{"type":"search_result","url":"https://www.facebook.com/BusinessToday/posts/ilya-sutskever-co-founder-of-openai-foresees-an-unpredictable-future-shaped-by-r/1182080720632971/?utm_source=chatgpt.com","title":"Ilya Sutskever, co-founder of OpenAI, foresees ...","snippet":"Quote: Ex-OpenAI pioneer Ilya Sutskever warns that as AI begins to self-improve, its trajectory may become \"extremely unpredictable and ...","ref_id":{"turn_index":1,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"facebook.com"}]},{"type":"search_result_group","domain":"theaitrack.com","entries":[{"type":"search_result","url":"https://theaitrack.com/openai-safety-evaluations-hub-launch/?utm_source=chatgpt.com","title":"OpenAI Launches Public Hub to Share Safety Evaluations ...","snippet":"OpenAI now publishes ongoing safety evaluations for its AI models via a new hub, tracking hallucinations, jailbreaks, and harmful content ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":14},"pub_date":null,"attribution":"theaitrack.com"}]},{"type":"search_result_group","domain":"theagenticarc.com","entries":[{"type":"search_result","url":"https://theagenticarc.com/ilya-sutskever-ai-prediction-warning/?utm_source=chatgpt.com","title":"What Ilya Sutskever Sees for Humanity's Future - Agentic Arc","snippet":"Aug 10, 2025 \u2014 He offers a straightforward explanation: the human brain is a biological computer, and digital computers will one day match its capabilities.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"theagenticarc.com"}]},{"type":"search_result_group","domain":"effectivealtruism.org","entries":[{"type":"search_result","url":"https://forum.effectivealtruism.org/posts/3diski3inLfPrWsDz/possible-openai-s-q-breakthrough-and-deepmind-s-alphago-type?utm_source=chatgpt.com","title":"Possible OpenAI's Q* breakthrough and DeepMind's ...","snippet":"Nov 23, 2023 \u2014 Mira Murati told employees on Wednesday that a letter about the AI breakthrough called Q* (pronounced Q-Star), precipitated the board's actions.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"effectivealtruism.org"}]},{"type":"search_result_group","domain":"theverge.com","entries":[{"type":"search_result","url":"https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training?utm_source=chatgpt.com","title":"OpenAI cofounder Ilya Sutskever predicts the end of AI pre- ...","snippet":"Dec 13, 2024 \u2014 During a rare public appearance at NeurIPS, the OpenAI cofounder argued that current AI training methods have hit a data limit.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"theverge.com"}]},{"type":"search_result_group","domain":"observer.com","entries":[{"type":"search_result","url":"https://observer.com/2024/07/openai-employees-concerns-strawberry-ai-project/?utm_source=chatgpt.com","title":"Elon Musk and Ilya Sutskever Have Warned About OpenAI's ...","snippet":"Jul 15, 2024 \u2014 Sutskever himself had reportedly begun to worry about the project's technology, as did OpenAI employees working on A.I. safety at the time.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"observer.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/html/2508.14231v1?utm_source=chatgpt.com","title":"Incident Analysis for AI Agents","snippet":"Aug 19, 2025 \u2014 Collecting AI incident reports enables practitioners to analyze their root causes and share lessons to prevent future occurrences. Driven by ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/html/2505.13076v1?utm_source=chatgpt.com","title":"The Hidden Dangers of Browsing AI Agents","snippet":"May 19, 2025 \u2014 Based on the literature review and demo observations, we found that advanced AI agent platforms\u2014such as OpenAI's Operator [openai2025operator] ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"fortune.com","entries":[{"type":"search_result","url":"https://fortune.com/2023/11/23/openai-sam-altman-board-directors-agi-breakthrough-reuters/?utm_source=chatgpt.com","title":"OpenAI staff warned board about AI breakthrough that ...","snippet":"Nov 23, 2023 \u2014 Researchers at OpenAI alerted the board of a breakthrough on a secret project named Q* right before Altman was fired, Reuters reported.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"fortune.com"}]},{"type":"search_result_group","domain":"nextbigfuture.com","entries":[{"type":"search_result","url":"https://www.nextbigfuture.com/2024/06/what-if-ilya-is-right-and-superintelligence-is-near.html?utm_source=chatgpt.com","title":"What IF Ilya is Right and Superintelligence is Near?","snippet":"Jun 20, 2024 \u2014 Ilya likely is well positioned and well informed to determine that Superintelligence is achievable and he likely has a clear plan for doing it.","ref_id":{"turn_index":1,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"nextbigfuture.com"}]},{"type":"search_result_group","domain":"opace.agency","entries":[{"type":"search_result","url":"https://opace.agency/blog/history-of-openai-future?utm_source=chatgpt.com","title":"The History of OpenAI & Its Future (AGI) [UPDATED]","snippet":"Development of Q* (Q-Star): OpenAI was reportedly developing an advanced AI model called Q*, which demonstrated the ability to solve unseen basic maths problems ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"opace.agency"}]},{"type":"search_result_group","domain":"cdomagazine.tech","entries":[{"type":"search_result","url":"https://www.cdomagazine.tech/aiml/former-openai-co-founder-ilya-sutskever-launches-ai-company-safe-superintelligence?utm_source=chatgpt.com","title":"Former OpenAI Co-founder Ilya Sutskever Launches AI ...","snippet":"Jun 27, 2024 \u2014 Former OpenAI Co-founder and Chief Scientist, Ilya Sutskever announced the launch of his new artificial intelligence company on June 19, a month after quitting ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"cdomagazine.tech"}]},{"type":"search_result_group","domain":"forbes.com","entries":[{"type":"search_result","url":"https://www.forbes.com/sites/geruiwang/2024/10/04/openai-fast-tracks-ai-agents-how-do-we-balance-benefits-with-risks/?utm_source=chatgpt.com","title":"OpenAI Fast-Tracks AI Agents. How Do We Balance ...","snippet":"Oct 4, 2024 \u2014 To build effective safety guidelines for AI agents, companies should start by analyzing potential risks and negative impacts, focusing on lower- ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"forbes.com"},{"type":"search_result","url":"https://www.forbes.com/sites/lanceeliot/2023/11/26/about-that-mysterious-ai-breakthrough-known-as-q-by-openai-that-allegedly-attains-true-ai-or-is-on-the-path-toward-artificial-general-intelligence-agi/?utm_source=chatgpt.com","title":"About That Mysterious AI Breakthrough Known As Q* By ...","snippet":"Nov 26, 2023 \u2014 About That Mysterious AI Breakthrough Known As Q* By OpenAI That Allegedly Attains True AI Or Is On The Path Toward Artificial General ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"forbes.com"}]},{"type":"search_result_group","domain":"constellationr.com","entries":[{"type":"search_result","url":"https://www.constellationr.com/blog-news/insights/safe-superintelligence-inc-launches-heres-what-it-means?utm_source=chatgpt.com","title":"Safe Superintelligence Inc. launches: Here's what it means","snippet":"Jun 19, 2024 \u2014 Three well-known generative AI pioneers have formed Safe Superintelligence Inc., a startup that will focus on safe superintelligence (SSI).","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"constellationr.com"}]},{"type":"search_result_group","domain":"latent.space","entries":[{"type":"search_result","url":"https://www.latent.space/p/what-ilya-saw?utm_source=chatgpt.com","title":"What Ilya Saw","snippet":"Dec 14, 2024 \u2014 We analyze each in time, ending with a final speculative conversation about 2023. What Ilya Saw in 2014. The relevant parts from his 2014 NIPS ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"latent.space"}]},{"type":"search_result_group","domain":"marketingaiinstitute.com","entries":[{"type":"search_result","url":"https://www.marketingaiinstitute.com/blog/openai-what-really-happened?utm_source=chatgpt.com","title":"What Really Happened at OpenAI?","snippet":"Nov 28, 2023 \u2014 Sources report the breakthrough may be an OpenAI project called Q* (pronounced Q-Star). ... breakthrough is, probably happened earlier in 2023.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"marketingaiinstitute.com"}]},{"type":"search_result_group","domain":"customgpt.ai","entries":[{"type":"search_result","url":"https://customgpt.ai/openai-project-strawberry/?utm_source=chatgpt.com","title":"OpenAI's Project Strawberry: Decoding The AI Mystery","snippet":"Project Strawberry aims to significantly improve AI models' reasoning abilities. The goal is to move beyond pattern recognition into complex problem-solving ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"customgpt.ai"}]},{"type":"search_result_group","domain":"time.com","entries":[{"type":"search_result","url":"https://time.com/6990076/safe-superintelligence-inc-announced/?utm_source=chatgpt.com","title":"Former OpenAI Chief Scientist Announces New Company","snippet":"Jun 19, 2024 \u2014 OpenAI co-founder and former chief scientist Ilya Sutskever announced that he's launching a new venture dubbed Safe Superintelligence Inc.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"time.com"}]},{"type":"search_result_group","domain":"techcrunch.com","entries":[{"type":"search_result","url":"https://techcrunch.com/2025/07/03/ilya-sutskever-will-lead-safe-superintelligence-following-his-ceos-exit/?utm_source=chatgpt.com","title":"Ilya Sutskever will lead Safe Superintelligence following ...","snippet":"Jul 3, 2025 \u2014 OpenAI co-founder Ilya Sutskever says he is stepping into the CEO role at Safe Superintelligence, the AI startup he launched in 2024.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"techcrunch.com"},{"type":"search_result","url":"https://techcrunch.com/2025/05/14/openai-pledges-to-publish-ai-safety-test-results-more-often/?utm_source=chatgpt.com","title":"OpenAI pledges to publish AI safety test results more often","snippet":"May 14, 2025 \u2014 OpenAI is moving to publish the results of its internal AI model safety evaluations more regularly in what the outfit is saying is an effort ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"techcrunch.com"}]},{"type":"search_result_group","domain":"wikipedia.org","entries":[{"type":"search_result","url":"https://en.wikipedia.org/wiki/OpenAI_o1?utm_source=chatgpt.com","title":"OpenAI o1","snippet":"According to leaked information, o1 was formerly known within OpenAI as \"Q*\", and later as \"Strawberry\". The codename \"Q*\" first surfaced in November 2023 ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"wikipedia.org"}]},{"type":"search_result_group","domain":"arcprize.org","entries":[{"type":"search_result","url":"https://arcprize.org/blog/oai-o3-pub-breakthrough?utm_source=chatgpt.com","title":"OpenAI o3 Breakthrough High Score on ARC-AGI-Pub","snippet":"Dec 20, 2024 \u2014 OpenAI's new o3 system - trained on the ARC-AGI-1 Public Training set - has scored a breakthrough 75.7% on the Semi-Private Evaluation set at our stated public ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"arcprize.org"}]},{"type":"search_result_group","domain":"sify.com","entries":[{"type":"search_result","url":"https://www.sify.com/ai-analytics/openais-secret-project-strawberry-points-to-last-ai-hurdle-reasoning/?utm_source=chatgpt.com","title":"OpenAI's Secret Project Strawberry Points to Last AI Hurdle - Sify","snippet":"Aug 12, 2024 \u2014 The idea is to enhance the reasoning capabilities of its artificial intelligence models to reach human-like reasoning and logic.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"sify.com"}]}],"citations":[],"model_slug":"gpt-5-thinking","request_id":"988445caff57aa0f-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"6744c242-860f-4e2b-9910-d0b932a7609e","turn_exchange_id":"b9a97c3b-80d7-4ec2-b434-1d2a9c266da9","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"6744c242-860f-4e2b-9910-d0b932a7609e","children":["a3a2e36a-1368-433d-8bb8-a0622b320dc4"]},"a3a2e36a-1368-433d-8bb8-a0622b320dc4":{"id":"a3a2e36a-1368-433d-8bb8-a0622b320dc4","message":{"id":"a3a2e36a-1368-433d-8bb8-a0622b320dc4","author":{"role":"user","name":null,"metadata":{}},"create_time":1759409293.429,"update_time":null,"content":{"content_type":"text","parts":["Ilya is an enigma to me. If all he saw was the potential for autonomous, advanced \"reasoning\", and went to set up SSI on that premise, surely by now he must feel that everyone in the world has \"seen\" the same idea, making his company no different from other frontier labs. He said SSI is \"climbing a different hill\". If that hill turns out to be reasoning and agentic behvaiour, then SSI is no different from any other frontier company. Unless SSI is pursuing something no one has approached yet."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"98845016bd4ace1d-SIN","message_source":null,"turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"eb5bcf07-5e56-435f-9078-7aba5ee80b44","children":["580d25f2-32d4-4239-a0c0-f222cda4704b"]},"580d25f2-32d4-4239-a0c0-f222cda4704b":{"id":"580d25f2-32d4-4239-a0c0-f222cda4704b","message":{"id":"580d25f2-32d4-4239-a0c0-f222cda4704b","author":{"role":"system","name":null,"metadata":{}},"create_time":1759409297.4583926,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"98845016bd4ace1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"a3a2e36a-1368-433d-8bb8-a0622b320dc4","turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"a3a2e36a-1368-433d-8bb8-a0622b320dc4","children":["432b412f-a89c-4b3d-bf9d-01b0f5400e8a"]},"432b412f-a89c-4b3d-bf9d-01b0f5400e8a":{"id":"432b412f-a89c-4b3d-bf9d-01b0f5400e8a","message":{"id":"432b412f-a89c-4b3d-bf9d-01b0f5400e8a","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409297.716555,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Thinking longer for a better answer","content":"","chunks":[],"finished":true}],"source_analysis_msg_id":"4db51d9f-d65c-4acf-90e7-28bdb42926a8"},"status":"in_progress","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"98845016bd4ace1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"580d25f2-32d4-4239-a0c0-f222cda4704b","turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"580d25f2-32d4-4239-a0c0-f222cda4704b","children":["42fd9639-045d-41b8-a619-b0f016e308bf"]},"42fd9639-045d-41b8-a619-b0f016e308bf":{"id":"42fd9639-045d-41b8-a619-b0f016e308bf","message":{"id":"42fd9639-045d-41b8-a619-b0f016e308bf","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409295.716757,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Ilya Sutskever Safe Superintelligence Inc different hill interview quote 'climbing a different hill'\"}, {\"q\": \"Safe Superintelligence Inc mission statements website SSI Inc\"}, {\"q\": \"Reuters Strawberry OpenAI internal project autonomous research agents planning deep research\"}, {\"q\": \"NeurIPS 2024 Ilya Sutskever keynote reasoning less predictable quote\"}, {\"q\": \"SSI company updates 2025 leadership hires funding July 2025 Ilya Sutskever CEO\"}, {\"q\": \"Mira Murati departure SSI hires Leopold Aschenbrenner OpenAI ex employees 'SSI' 'Safety Superintelligence' team\"}, {\"q\": \"Sutskever 'different hill' source\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Ilya Sutskever Safe Superintelligence Inc different hill interview quote 'climbing a different hill'"},{"type":"search","q":"Safe Superintelligence Inc mission statements website SSI Inc"},{"type":"search","q":"Reuters Strawberry OpenAI internal project autonomous research agents planning deep research"},{"type":"search","q":"NeurIPS 2024 Ilya Sutskever keynote reasoning less predictable quote"},{"type":"search","q":"SSI company updates 2025 leadership hires funding July 2025 Ilya Sutskever CEO"},{"type":"search","q":"Mira Murati departure SSI hires Leopold Aschenbrenner OpenAI ex employees 'SSI' 'Safety Superintelligence' team"},{"type":"search","q":"Sutskever 'different hill' source"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Searching web for SSI differentiation analysis and quotes","classifier_response":"default","request_id":"98845016bd4ace1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"432b412f-a89c-4b3d-bf9d-01b0f5400e8a","turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"432b412f-a89c-4b3d-bf9d-01b0f5400e8a","children":["64995b6a-9cf0-4d42-8fb2-602a762ef28d"]},"64995b6a-9cf0-4d42-8fb2-602a762ef28d":{"id":"64995b6a-9cf0-4d42-8fb2-602a762ef28d","message":{"id":"64995b6a-9cf0-4d42-8fb2-602a762ef28d","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1759409300.639462,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Ilya Sutskever Safe Superintelligence Inc different hill interview quote 'climbing a different hill'","Safe Superintelligence Inc mission statements website SSI Inc","Reuters Strawberry OpenAI internal project autonomous research agents planning deep research","NeurIPS 2024 Ilya Sutskever keynote reasoning less predictable quote","SSI company updates 2025 leadership hires funding July 2025 Ilya Sutskever CEO","Mira Murati departure SSI hires Leopold Aschenbrenner OpenAI ex employees 'SSI' 'Safety Superintelligence' team","Sutskever 'different hill' source"]},"reasoning_status":"is_reasoning","reasoning_title":"Searching web for SSI differentiation analysis and quotes","classifier_response":"default","request_id":"98845016bd4ace1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"42fd9639-045d-41b8-a619-b0f016e308bf","turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"42fd9639-045d-41b8-a619-b0f016e308bf","children":["5ccdd73e-e93c-4c8f-b74c-888c5a57f490"]},"5ccdd73e-e93c-4c8f-b74c-888c5a57f490":{"id":"5ccdd73e-e93c-4c8f-b74c-888c5a57f490","message":{"id":"5ccdd73e-e93c-4c8f-b74c-888c5a57f490","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1759409300.6164997,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"www.businessinsider.com","entries":[{"type":"search_result","url":"https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9","title":"Sam Altman predicts AI will surpass human intelligence by 2030","snippet":"In a recent interview with Welt Editor-in-Chief Jan Philipp Burgard, OpenAI CEO Sam Altman shared bold predictions about the future of artificial intelligence. Altman expects superintelligent AI\u2014smarter than humans across the board\u2014to emerge by 2030, with major breakthroughs likely by 2026. He highlighted how tools like GPT-5 already outperform humans in specific domains, though still fall short in many intuitive human tasks. Altman anticipates AI will eventually undertake 30-40% of current human tasks and emphasized the importance of teaching future generations adaptability, creativity, and interpersonal skills.\n\nAltman also addressed concerns about AI safety and alignment with human values, expressing optimism but underscoring the challenge of unintended consequences. He rejected comparisons suggesting AI might view humans as inferior, stressing the need for responsible development. Responding to criticism around OpenAI\u2019s partial commercialization, Altman affirmed the ongoing centrality of its nonprofit mission and safety priorities.\n\nAdditionally, he hinted at future OpenAI hardware resembling a \u201cfamily of devices\u201d that could revolutionize personal computing. Altman also weighed in on political dynamics in tech, stating that while AI won\u2019t replace leaders, it will support decision-making. Personally, he\u2019s skeptical of using AI for relationship advice, unlike many others.","ref_id":null,"pub_date":1758877202.0,"attribution":"www.businessinsider.com"},{"type":"search_result","url":"https://www.businessinsider.com/man-who-revolted-against-sam-altman-just-started-research-lab-2024-6","title":"The man who dared to revolt against Sam Altman at OpenAI just started a research lab promoting 'safe superintelligence'","snippet":"Ilya Sutskever, former chief scientist and cofounder of OpenAI, has announced the creation of Safe Superintelligence Inc. (SSI), a new research lab dedicated to developing safe superintelligence. Sutskever's lab aims to advance AI capabilities rapidly while ensuring safety remains a priority, thereby scaling AI developments peacefully. Joining him are Daniel Gross, former Apple AI lead, and Daniel Levy, a former OpenAI technical staff member. Despite queries, Sutskever did not disclose the financial backers of SSI. \n\nThis development follows Sutskever's contentious departure from OpenAI and the fallout after he initially supported CEO Sam Altman's removal, which strained their relationship. This move mirrors previous instances where ex-OpenAI staff, like the Amodei siblings in 2021, founded their own AI ventures.","ref_id":null,"pub_date":1718855820.0,"attribution":"www.businessinsider.com"}]},{"type":"search_result_group","domain":"x.com","entries":[{"type":"search_result","url":"https://x.com/ilyasut/status/1831341857714119024?lang=en","title":"Ilya Sutskever - Mountain: identified. Time to climb","snippet":"SSI is building a straight shot to safe superintelligence. We've raised $1B from NFDG, a16z, Sequoia, DST Global, and SV Angel.","ref_id":null,"pub_date":null,"attribution":"x.com"}]},{"type":"search_result_group","domain":"ssi.inc","entries":[{"type":"search_result","url":"https://ssi.inc/","title":"Safe Superintelligence Inc.","snippet":"We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to ...","ref_id":null,"pub_date":null,"attribution":"ssi.inc"},{"type":"search_result","url":"https://ssi.inc/updates","title":"Updates","snippet":"Jul 3, 2025 \u2014 The world's first straight-shot SSI lab, with one goal and one product: a safe superintelligence.","ref_id":null,"pub_date":null,"attribution":"ssi.inc"}]},{"type":"search_result_group","domain":"www.reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/","title":"Exclusive: OpenAI working on new reasoning technology ...","snippet":"Jul 15, 2024 \u2014 Reuters could not ascertain the precise date of the document, which details a plan for how OpenAI intends to use Strawberry to perform research.","ref_id":null,"pub_date":1720819458.099,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/","title":"AI with reasoning power will be less predictable, Ilya Sutskever says","snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","ref_id":null,"pub_date":1734147336.0,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/business/sutskever-lead-safe-superintelligence-after-meta-poaches-ceo-gross-ai-talent-war-2025-07-03/","title":"Sutskever to lead Safe Superintelligence after Meta poaches CEO Gross in AI talent war","snippet":"Ilya Sutskever has assumed leadership of Safe Superintelligence (SSI), the AI startup he founded in 2024, following the departure of CEO Daniel Gross to Meta Platforms. Gross will head Meta's AI products division amid a broader industry-wide competition for top AI talent. Meta had also attempted to acquire SSI, which was most recently valued at $32 billion. Despite overtures from major tech firms, Sutskever affirmed SSI's independence and commitment to developing safe, superintelligent AI. The company secured $1 billion in funding last year to pursue this vision. Sutskever, previously OpenAI's chief scientist and co-founder, left the company after the controversial firing and rehiring of CEO Sam Altman in 2023.\n\nIn a parallel strategic move, Meta CEO Mark Zuckerberg has launched Meta Superintelligence Labs, consolidating its AI initiatives after delays with its Llama 4 model and loss of key personnel. This new division, led by Alexandr Wang and Nat Friedman, follows Meta's $14.3 billion investment in Scale AI. Gross and Friedman, longtime collaborators, previously co-founded the venture capital firm NFDG, which has backed startups including SSI, Perplexity, and Figma. Gross also brings experience from Apple, where he worked after the tech giant acquired his startup Cue in 2013.","ref_id":null,"pub_date":1751569669.0,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/video/watch/idRW903215072024RP1/","title":"OpenAI working on new 'Strawberry' project","snippet":"ChatGPT maker OpenAI is working on a novel approach to its artificial intelligence models in a project code-named \u201cStrawberry,\u201d according to a ...","ref_id":null,"pub_date":null,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/","title":"Ilya Sutskever on how AI will change and his new startup ...","snippet":"Sep 5, 2024 \u2014 \"We've identified a mountain that's a bit different from what I was working [on]...once you climb to the top of this mountain, the paradigm will ...","ref_id":null,"pub_date":1725531820.551,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/","title":"OpenAI launches new series of AI models with 'reasoning' ...","snippet":"Sep 12, 2024 \u2014 OpenAI used the code name Strawberry to refer to the project internally, while it dubbed the models announced on Thursday o1 and o1-mini.","ref_id":null,"pub_date":1726161615.914,"attribution":"www.reuters.com"}]},{"type":"search_result_group","domain":"www.reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/ControlProblem/comments/1ituogu/is_there_a_complete_list_of_open_ai_employees/","title":"Is there a complete list of open ai employees that have left ...","snippet":"Some others, like Murati, apparently left for unspecified reasons, so attributing their departure to safety issues is guesswork.","ref_id":null,"pub_date":null,"attribution":"www.reddit.com"}]},{"type":"search_result_group","domain":"www.linkedin.com","entries":[{"type":"search_result","url":"https://www.linkedin.com/posts/peterorban_this-scientist-left-openailast-year-his-activity-7304859835144032257-YNvm","title":"Ilya Sutskever&#39;s $30B startup SSI: a different path to ASI","snippet":"Mar 10, 2025 \u2014 While OpenAI and others ship products, Ilya Sutskever's $30B startup is climbing \"a different mountain\" to ASI** A fascinating Wall Street ...","ref_id":null,"pub_date":null,"attribution":"www.linkedin.com"}]},{"type":"search_result_group","domain":"en.wikipedia.org","entries":[{"type":"search_result","url":"https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.","title":"Safe Superintelligence Inc.","snippet":"","ref_id":null,"pub_date":1755770644.0,"attribution":"en.wikipedia.org"}]},{"type":"search_result_group","domain":"news.cgtn.com","entries":[{"type":"search_result","url":"https://news.cgtn.com/news/2024-12-14/AI-with-reasoning-power-to-be-less-predictable-Ilya-Sutskever-says-1zkxlrFTrXi/p.html","title":"AI with reasoning power to be less predictable, Ilya ...","snippet":"Dec 14, 2024 \u2014 \"But pre-training as we know it will unquestionably end,\" Sutskever declared before thousands of attendees at the NeurIPS conference in ...","ref_id":null,"pub_date":1734178372.0,"attribution":"news.cgtn.com"}]},{"type":"search_result_group","domain":"internationalfinance.com","entries":[{"type":"search_result","url":"https://internationalfinance.com/business-leaders/business-leader-week-ilya-sutskever-returns-spotlight-safe-superintelligence-ceo/","title":"Business Leader of the Week: Ilya Sutskever returns to ...","snippet":"Sep 12, 2025 \u2014 Ilya Sutskever, who co-founded Safe Superintelligence (SSI) in 2024, is taking over as CEO after Daniel Gross, the startup's previous leader ...","ref_id":null,"pub_date":1757683181.0,"attribution":"internationalfinance.com"}]},{"type":"search_result_group","domain":"www.openaifiles.org","entries":[{"type":"search_result","url":"https://www.openaifiles.org/former-employees","title":"Employee Testimonies","snippet":"Company insiders explain why safety-conscious employees are leaving. Sigal ... Mira Murati. Chief Technical Officer. Departed late 2024. \u201cI don't feel ...","ref_id":null,"pub_date":null,"attribution":"www.openaifiles.org"}]},{"type":"search_result_group","domain":"regulatingai.org","entries":[{"type":"search_result","url":"https://regulatingai.org/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says/","title":"AI with reasoning power will be less predictable, Ilya ...","snippet":"Ilya Sutskever's Prediction: Former OpenAI chief scientist Ilya Sutskever foresees reasoning capabilities making AI technology less predictable.","ref_id":null,"pub_date":null,"attribution":"regulatingai.org"}]},{"type":"search_result_group","domain":"apnews.com","entries":[{"type":"search_result","url":"https://apnews.com/article/c6b48a3675fb3fb459859dece2b45499","title":"OpenAI co-founder Sutskever sets up new AI company devoted to 'safe superintelligence'","snippet":"Ilya Sutskever, a co-founder of OpenAI, has established a new AI company called Safe Superintelligence Inc., focusing on the safe development of superintelligence, AI systems more advanced than human intelligence. The company, co-founded with Daniel Gross and Daniel Levy, aims to prioritize safety and security without the distraction of immediate commercial pressures. Based in Palo Alto, California, and Tel Aviv, the company intends to leverage local talent. Previously, Sutskever had attempted to remove OpenAI's CEO Sam Altman, a move he later regretted, leading to internal conflicts over the emphasis on AI safety versus business opportunities. Sutskever left OpenAI to pursue personally meaningful projects and was soon joined by Jan Leike, who criticized OpenAI for neglecting safety.","ref_id":null,"pub_date":1718880086.0,"attribution":"apnews.com"}]}],"debug_sonic_thread_id":"thread_68de7494b0908328a6e39c649f476fc7","reasoning_status":"is_reasoning","reasoning_title":"Searching web for SSI differentiation analysis and quotes","classifier_response":"default","request_id":"98845016bd4ace1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"64995b6a-9cf0-4d42-8fb2-602a762ef28d","turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"64995b6a-9cf0-4d42-8fb2-602a762ef28d","children":["062fcad3-0dc4-4576-9793-dea0b7ed0045"]},"062fcad3-0dc4-4576-9793-dea0b7ed0045":{"id":"062fcad3-0dc4-4576-9793-dea0b7ed0045","message":{"id":"062fcad3-0dc4-4576-9793-dea0b7ed0045","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409317.512247,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Crafting an SSI Response","content":"I","chunks":[],"finished":false}],"source_analysis_msg_id":"62eefb87-0660-4690-82c9-e45972c2378c"},"status":"in_progress","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"98845016bd4ace1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"5ccdd73e-e93c-4c8f-b74c-888c5a57f490","turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"5ccdd73e-e93c-4c8f-b74c-888c5a57f490","children":["0888c983-a7ed-481e-8424-df2876213816"]},"0888c983-a7ed-481e-8424-df2876213816":{"id":"0888c983-a7ed-481e-8424-df2876213816","message":{"id":"0888c983-a7ed-481e-8424-df2876213816","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409318.229444,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 20s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":20,"model_slug":"gpt-5-thinking","request_id":"98845016bd4ace1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"062fcad3-0dc4-4576-9793-dea0b7ed0045","turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"062fcad3-0dc4-4576-9793-dea0b7ed0045","children":["9c0fb886-1b6b-4032-ba08-3eef5c6ebf43"]},"9c0fb886-1b6b-4032-ba08-3eef5c6ebf43":{"id":"9c0fb886-1b6b-4032-ba08-3eef5c6ebf43","message":{"id":"9c0fb886-1b6b-4032-ba08-3eef5c6ebf43","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409303.888937,"update_time":null,"content":{"content_type":"text","parts":["You\u2019re right to be skeptical: if \u201cthe hill\u201d were just \u201creasoning + agents,\u201d SSI would be indistinguishable from every other frontier lab touting o-series\u2010style test-time search and browser/tool use. That\u2019s not what Ilya\u2019s public hints sound like.\n\nHere\u2019s the most defensible read of \u201ca different hill,\u201d grounded in what he\u2019s said and what SSI itself publishes, plus what changed in 2024\u201325:\n\n# What Ilya actually signaled\n- **\u201cDifferent mountain\u201d claim.** In late 2024 he said they\u2019d identified a *different* mountain, implying a paradigm change once you reach the summit\u2014not merely a better agent loop. \ue200cite\ue202turn0search12\ue201  \n- **Pretraining will end; data is the bottleneck.** At NeurIPS he argued that \u201cwe have but one internet\u201d and that future systems need new data generation/evaluation regimes; greater *reasoning* also means *less predictability*. That pushes you toward designs that constrain or prove properties of the system rather than just scale it. \ue200cite\ue202turn0search3\ue202turn0search9\ue201  \n- **SSI\u2019s charter.** \u201cSafety and capabilities in tandem,\u201d insulated from product cycles\u2014i.e., organizationally optimized for tightly-coupled safety/capability R&D, not app shipping. \ue200cite\ue202turn0search1\ue202turn0search13\ue201\n\n# So what could that \u201chill\u201d be (that\u2019s not just agents)?\nBelow are concrete, *technically distinct* bets that fit his remarks and would genuinely differentiate SSI. I rank them by plausibility.\n\n**1) Safety-first architecture with *enforceable* guarantees (most likely).**  \nNot just evals or red-teaming, but an architecture whose *interfaces and training objective* make certain bad behaviors *infeasible* (or *detectable with proofs*). Think: capability amplification *behind* constraints with hard separation between planning/execution, runtime monitors with formal specs, and training losses that penalize deception directly via process-based supervision. This is consistent with \u201cless predictable once reasoning improves,\u201d so you move guarantees from behavior to *mechanism*. SSI\u2019s \u201csafety always ahead\u201d language and insulation from product schedules also point to deep engineering of this layer. \ue200cite\ue202turn0search1\ue202turn0search3\ue201\n\n**2) New data-generation regime replacing internet pretraining (very likely).**  \nIf pretraining \u201cas we know it will unquestionably end,\u201d the new hill is a *closed-loop science engine*: synthetic curricula, tool-use self-play, and adversarial problem generation\u2014paired with trusted evaluators\u2014to grow capability without scraping more web. This competes with OpenAI\u2019s \u201cStrawberry/o1\u201d line but shifts the center from \u201creasoning at inference\u201d to *continual data creation with guarantees*. (OpenAI\u2019s Strawberry shows the world noticed the direction; SSI\u2019s play would be to industrialize it with safety guarantees and no product constraints.) \ue200cite\ue202turn0search3\ue202turn0search14\ue202turn0search2\ue201\n\n**3) Mechanistic + spec-driven training (interpretable by construction) (likely).**  \nA stack where parts of the model (policy, search, tool routers) are *explicit programs* or constrained modules whose internals can be audited, simulated, and verified. Hybrids of learned components with spec-checked planners, or modular networks with interpretable representations enforced during training. This aligns with the \u201cdifferent mountain\u201d framing\u2014reach a point where the *paradigm* changes because you can *prove* properties you previously only tested. \ue200cite\ue202turn0search12\ue201\n\n**4) Security-grade compute & isolation as a first-class research variable (plausible).**  \nA lab where the *compute fabric* (e.g., dedicated TPU partitions, air-gapped clusters, provenance-tracked datasets, cryptographic attestation) is co-designed with the model so that dangerous capabilities cannot spill over. Wikipedia-level reporting mentions a TPU partnership; SSI\u2019s \u201cscale in peace\u201d rhetoric suggests this kind of infra-first posture. It\u2019s orthogonal to agents and is genuinely differentiating. \ue200cite\ue202turn0search7\ue202turn0search1\ue201\n\n**5) Business/organizational design as technical leverage (plausible, and real).**  \nSSI\u2019s one-goal model (\u201cone product: a safe superintelligence\u201d) means no quarterly app pressure; that lets them do multi-year safety-gated capability development, with stop-rules baked into governance. That *is* a different hill in practice, even if outsiders fixate on model demos. \ue200cite\ue202turn0search13\ue201\n\n# Why this still isn\u2019t \u201cjust like everyone else\u201d\n- **Reasoning agents are now table stakes.** OpenAI publicly shipped o1-class models; *every* lab is iterating on test-time search, tool-use, and web agents. If SSI were merely \u201cbetter agents,\u201d they wouldn\u2019t be \u201cdifferent.\u201d \ue200cite\ue202turn0search14\ue201  \n- **Ilya\u2019s public thesis is about *limits* of the current recipe.** He\u2019s telling you the next hard problem is *not* a longer chain-of-thought; it\u2019s new data/eval regimes and architectures with *enforceable* safety properties under stronger reasoning. \ue200cite\ue202turn0search3\ue202turn0search9\ue201\n\n# How we\u2019d be able to tell (falsifiable markers)\n1) **Papers or demos with *formal safety claims*** (even if partial): e.g., \u201cplanner is verified to satisfy property P under threat model T; violations are caught with probability \u2265 1\u2013\u03b5 at runtime.\u201d  \n2) **Non-internet training reveal:** documentation that most marginal capability comes from closed-loop synthetic curricula/self-play + trusted evaluators\u2014not more web.  \n3) **Infra announcements** about attested compute partitions or audited toolchains as *core* to training/eval, not just ops. (Their updates/site emphasize insulation from commercial pressures already.) \ue200cite\ue202turn0search1\ue202turn0search13\ue201\n\n# Bottom line\nIf SSI\u2019s hill were \u201cagentic reasoning,\u201d they\u2019d be standard. The more consistent reading is: **they\u2019re pursuing a *safety-first, data-new, spec-driven* path where guarantees and closed-loop data generation are the main levers**, with organizational insulation to actually do it. That is meaningfully *different*\u2014and it explains why Ilya could \u201csee\u201d the same reasoning surge as everyone else yet decide the *right* hill is the one that makes such systems *provably tameable* before they\u2019re ubiquitously deployed. \ue200cite\ue202turn0search12\ue202turn0search3\ue202turn0search1\ue201\n\nIf you want, I\u2019ll turn this into a concise checklist you can use to judge SSI (or any lab) as new info drops\u2014e.g., \u201cevidence of formal specs,\u201d \u201cproof-carrying plans,\u201d \u201csynthetic curriculum yield vs. web data,\u201d \u201csafety-ahead gates,\u201d etc."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":"\ue200cite\ue202turn0search12\ue201","start_idx":604,"end_idx":624,"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Ilya Sutskever on how AI will change and his new startup ...","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","pub_date":1725531820.551,"snippet":"Sep 5, 2024 \u2014 \"We've identified a mountain that's a bit different from what I was working [on]...once you climb to the top of this mountain, the paradigm will ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search9\ue201","start_idx":962,"end_idx":994,"safe_urls":["https://news.cgtn.com/news/2024-12-14/AI-with-reasoning-power-to-be-less-predictable-Ilya-Sutskever-says-1zkxlrFTrXi/p.html?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":1734147336.0,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution_segments":null,"supporting_websites":[{"title":"AI with reasoning power to be less predictable, Ilya ...","url":"https://news.cgtn.com/news/2024-12-14/AI-with-reasoning-power-to-be-less-predictable-Ilya-Sutskever-says-1zkxlrFTrXi/p.html?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 14, 2024 \u2014 \"But pre-training as we know it will unquestionably end,\" Sutskever declared before thousands of attendees at the NeurIPS conference in ...","attribution":"CGTN News"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":9}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search13\ue201","start_idx":1179,"end_idx":1212,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://ssi.inc/updates?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to ...","attribution_segments":null,"supporting_websites":[{"title":"Updates","url":"https://ssi.inc/updates?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 3, 2025 \u2014 The world's first straight-shot SSI lab, with one goal and one product: a safe superintelligence.","attribution":"ssi.inc"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search3\ue201","start_idx":2117,"end_idx":2149,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to ...","attribution_segments":null,"supporting_websites":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":null,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution":"Reuters"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search14\ue202turn0search2\ue201","start_idx":2789,"end_idx":2835,"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":1734147336.0,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution_segments":null,"supporting_websites":[{"title":"OpenAI launches new series of AI models with 'reasoning' ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/?utm_source=chatgpt.com","pub_date":null,"snippet":"Sep 12, 2024 \u2014 OpenAI used the code name Strawberry to refer to the project internally, while it dubbed the models announced on Thursday o1 and o1-mini.","attribution":"Reuters"},{"title":"Exclusive: OpenAI working on new reasoning technology ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 15, 2024 \u2014 Reuters could not ascertain the precise date of the document, which details a plan for how OpenAI intends to use Strawberry to perform research.","attribution":"Reuters"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":14},{"turn_index":0,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search12\ue201","start_idx":3386,"end_idx":3406,"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Ilya Sutskever on how AI will change and his new startup ...","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","pub_date":1725531820.551,"snippet":"Sep 5, 2024 \u2014 \"We've identified a mountain that's a bit different from what I was working [on]...once you climb to the top of this mountain, the paradigm will ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":12}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search7\ue202turn0search1\ue201","start_idx":3912,"end_idx":3944,"safe_urls":["https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com"],"refs":[],"alt":"([Wikipedia](https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.?utm_source=chatgpt.com","pub_date":1755770644.0,"snippet":"","attribution_segments":null,"supporting_websites":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to ...","attribution":"ssi.inc"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":7},{"turn_index":0,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Wikipedia"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search13\ue201","start_idx":4313,"end_idx":4333,"safe_urls":["https://ssi.inc/updates?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/updates?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Updates","url":"https://ssi.inc/updates?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 3, 2025 \u2014 The world's first straight-shot SSI lab, with one goal and one product: a safe superintelligence.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search14\ue201","start_idx":4608,"end_idx":4628,"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"OpenAI launches new series of AI models with 'reasoning' ...","url":"https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/?utm_source=chatgpt.com","pub_date":1726161615.914,"snippet":"Sep 12, 2024 \u2014 OpenAI used the code name Strawberry to refer to the project internally, while it dubbed the models announced on Thursday o1 and o1-mini.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search9\ue201","start_idx":4882,"end_idx":4914,"safe_urls":["https://news.cgtn.com/news/2024-12-14/AI-with-reasoning-power-to-be-less-predictable-Ilya-Sutskever-says-1zkxlrFTrXi/p.html?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":1734147336.0,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution_segments":null,"supporting_websites":[{"title":"AI with reasoning power to be less predictable, Ilya ...","url":"https://news.cgtn.com/news/2024-12-14/AI-with-reasoning-power-to-be-less-predictable-Ilya-Sutskever-says-1zkxlrFTrXi/p.html?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 14, 2024 \u2014 \"But pre-training as we know it will unquestionably end,\" Sutskever declared before thousands of attendees at the NeurIPS conference in ...","attribution":"CGTN News"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":9}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search13\ue201","start_idx":5539,"end_idx":5572,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://ssi.inc/updates?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to ...","attribution_segments":null,"supporting_websites":[{"title":"Updates","url":"https://ssi.inc/updates?utm_source=chatgpt.com","pub_date":null,"snippet":"Jul 3, 2025 \u2014 The world's first straight-shot SSI lab, with one goal and one product: a safe superintelligence.","attribution":"ssi.inc"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":13}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search12\ue202turn0search3\ue202turn0search1\ue201","start_idx":6099,"end_idx":6145,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Ilya Sutskever on how AI will change and his new startup ...","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","pub_date":1725531820.551,"snippet":"Sep 5, 2024 \u2014 \"We've identified a mountain that's a bit different from what I was working [on]...once you climb to the top of this mountain, the paradigm will ...","attribution_segments":null,"supporting_websites":[{"title":"AI with reasoning power will be less predictable, Ilya Sutskever says","url":"https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","pub_date":null,"snippet":"Ilya Sutskever, former chief scientist at OpenAI, shared insights at the NeurIPS conference regarding the future of artificial intelligence. Accepting a \"Test Of Time\" award, he noted that advancing AI's reasoning capabilities would lead to increased unpredictability. He explained that extensive data and computing power have driven AI advancements, but pre-training methods are approaching their limits due to the finite amount of internet data. Sutskever suggested that technology itself could generate new data and that AI models could evaluate multiple responses to improve accuracy. He predicted a future with superintelligent, self-aware AI capable of reasoning like humans, though this would make outcomes less predictable, comparing this to disruptive moves by systems like AlphaGo and advanced chess AIs.","attribution":"Reuters"},{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to ...","attribution":"ssi.inc"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":12},{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null}],"safe_urls":["https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","https://news.cgtn.com/news/2024-12-14/AI-with-reasoning-power-to-be-less-predictable-Ilya-Sutskever-says-1zkxlrFTrXi/p.html?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://ssi.inc/updates?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://ssi.inc/updates?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/?utm_source=chatgpt.com","https://news.cgtn.com/news/2024-12-14/AI-with-reasoning-power-to-be-less-predictable-Ilya-Sutskever-says-1zkxlrFTrXi/p.html?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://ssi.inc/updates?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ai-with-reasoning-power-will-be-less-predictable-ilya-sutskever-says-2024-12-14/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com"],"citations":[],"model_slug":"gpt-5-thinking","request_id":"98845016bd4ace1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"0888c983-a7ed-481e-8424-df2876213816","turn_exchange_id":"a34f2595-040e-4fd2-8851-db8ddce478d5","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"0888c983-a7ed-481e-8424-df2876213816","children":["c21d231e-6861-43aa-827b-27b5ee92913c"]},"c21d231e-6861-43aa-827b-27b5ee92913c":{"id":"c21d231e-6861-43aa-827b-27b5ee92913c","message":{"id":"c21d231e-6861-43aa-827b-27b5ee92913c","author":{"role":"user","name":null,"metadata":{}},"create_time":1759409577.282,"update_time":null,"content":{"content_type":"text","parts":["I think what you said has high overlap with Anthropic. I can only see two reasons why Dario Amodei and Ilya have not jointed forces: (i) Dario wants to continue to steer Anthropic according to his own vision, which might conflict with Ilya's vision in substantial ways. Perhaps Dario believes in iterative improvement with public feedback but Ilya wants a \"one shot to ASI\" roadmap. (ii) They may not trust each other fully, for private reasons "]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"98845704feb6ce1d-SIN","message_source":null,"turn_exchange_id":"3d9473e2-b72c-438a-ab31-5177a5809676","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"9c0fb886-1b6b-4032-ba08-3eef5c6ebf43","children":["c8c4a2f3-6cb4-40d5-afb9-0dcf81922bb2"]},"c8c4a2f3-6cb4-40d5-afb9-0dcf81922bb2":{"id":"c8c4a2f3-6cb4-40d5-afb9-0dcf81922bb2","message":{"id":"c8c4a2f3-6cb4-40d5-afb9-0dcf81922bb2","author":{"role":"system","name":null,"metadata":{}},"create_time":1759409581.4538074,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"98845704feb6ce1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"c21d231e-6861-43aa-827b-27b5ee92913c","turn_exchange_id":"3d9473e2-b72c-438a-ab31-5177a5809676","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"c21d231e-6861-43aa-827b-27b5ee92913c","children":["35bfc3bb-f97d-4db8-b3a2-349011a52572"]},"35bfc3bb-f97d-4db8-b3a2-349011a52572":{"id":"35bfc3bb-f97d-4db8-b3a2-349011a52572","message":{"id":"35bfc3bb-f97d-4db8-b3a2-349011a52572","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409582.462721,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Analyzing safety and strategy","content":"I\u2019ll dive into the user's request about Anthropic and SSI's visions. I'll pull in the latest info on Anthropic\u2019s safety approach, such as the Constitutional AI framework and Responsible Scaling Policy, as well as Dario\u2019s interviews and SSI statements. From there, I\u2019ll analyze and compare governance, product constraints, and their respective safety approaches. It'll be helpful to also challenge the user's assumptions with potential alternatives. Time to browse!","chunks":[],"finished":true}],"source_analysis_msg_id":"779b8544-ea4e-4753-90bf-da44d6d04f9c"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"98845704feb6ce1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"c8c4a2f3-6cb4-40d5-afb9-0dcf81922bb2","turn_exchange_id":"3d9473e2-b72c-438a-ab31-5177a5809676","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"c8c4a2f3-6cb4-40d5-afb9-0dcf81922bb2","children":["4ed05575-049a-4be9-9b56-ed7705f8faf0"]},"4ed05575-049a-4be9-9b56-ed7705f8faf0":{"id":"4ed05575-049a-4be9-9b56-ed7705f8faf0","message":{"id":"4ed05575-049a-4be9-9b56-ed7705f8faf0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409579.62649,"update_time":null,"content":{"content_type":"code","language":"unknown","response_format_name":null,"text":"{\"search_query\": [{\"q\": \"Safe Superintelligence Inc. SSI announcement Ilya Sutskever different mountain interview 2024 2025\"}, {\"q\": \"Anthropic Responsible Scaling Policy 2025\"}, {\"q\": \"Constitutional AI Anthropic paper overview\"}, {\"q\": \"Dario Amodei interview 2025 vision Anthropic safety\"}, {\"q\": \"SSI company charter site 'safe superintelligence' mission\"}, {\"q\": \"OpenAI o1 Strawberry test-time compute planning agents report Reuters\"}], \"response_length\": \"short\"}"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"citations":[],"content_references":[],"reasoning_status":"is_reasoning","search_queries":[{"type":"search","q":"Safe Superintelligence Inc. SSI announcement Ilya Sutskever different mountain interview 2024 2025"},{"type":"search","q":"Anthropic Responsible Scaling Policy 2025"},{"type":"search","q":"Constitutional AI Anthropic paper overview"},{"type":"search","q":"Dario Amodei interview 2025 vision Anthropic safety"},{"type":"search","q":"SSI company charter site 'safe superintelligence' mission"},{"type":"search","q":"OpenAI o1 Strawberry test-time compute planning agents report Reuters"}],"search_display_string":"Searching the web...","searched_display_string":"Searched the web...","reasoning_title":"Browsing and analyzing Anthropic vs. SSI visions","classifier_response":"default","request_id":"98845704feb6ce1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"35bfc3bb-f97d-4db8-b3a2-349011a52572","turn_exchange_id":"3d9473e2-b72c-438a-ab31-5177a5809676","timestamp_":"absolute"},"recipient":"web.run","channel":null},"parent":"35bfc3bb-f97d-4db8-b3a2-349011a52572","children":["6cc9d6af-e491-4879-a572-755758607bdd"]},"6cc9d6af-e491-4879-a572-755758607bdd":{"id":"6cc9d6af-e491-4879-a572-755758607bdd","message":{"id":"6cc9d6af-e491-4879-a572-755758607bdd","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1759409591.4482992,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_model_queries":{"type":"search_model_queries","queries":["Safe Superintelligence Inc. SSI announcement Ilya Sutskever different mountain interview 2024 2025","Anthropic Responsible Scaling Policy 2025","Constitutional AI Anthropic paper overview","Dario Amodei interview 2025 vision Anthropic safety","SSI company charter site 'safe superintelligence' mission","OpenAI o1 Strawberry test-time compute planning agents report Reuters"]},"reasoning_status":"is_reasoning","reasoning_title":"Browsing and analyzing Anthropic vs. SSI visions","classifier_response":"default","request_id":"98845704feb6ce1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"4ed05575-049a-4be9-9b56-ed7705f8faf0","turn_exchange_id":"3d9473e2-b72c-438a-ab31-5177a5809676","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"4ed05575-049a-4be9-9b56-ed7705f8faf0","children":["c4dbcf30-54e2-45ce-a11b-aff9e72c6a23"]},"c4dbcf30-54e2-45ce-a11b-aff9e72c6a23":{"id":"c4dbcf30-54e2-45ce-a11b-aff9e72c6a23","message":{"id":"c4dbcf30-54e2-45ce-a11b-aff9e72c6a23","author":{"role":"tool","name":"web.run","metadata":{"real_author":"tool:web.run"}},"create_time":1759409591.4261177,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"search_result_groups":[{"type":"search_result_group","domain":"apnews.com","entries":[{"type":"search_result","url":"https://apnews.com/article/9643064e847a5e88ef6ee8b620b3a44c","title":"Judge approves $1.5 billion copyright settlement between AI company Anthropic and authors","snippet":"A federal judge in San Francisco has preliminarily approved a $1.5 billion settlement between AI company Anthropic and a coalition of authors and publishers who accused the company of pirating about 465,000 books to train its Claude chatbot. Judge William Alsup, while acknowledging the complexity of distributing the funds, deemed the settlement fair. Authors and publishers will receive approximately $3,000 for each book included in the agreement, which covers past but not future works.\n\nThe Authors Guild and the Association of American Publishers hailed the outcome as a milestone in safeguarding authors\u2019 rights. They emphasized that this settlement sets a precedent for holding AI developers accountable for unauthorized use of copyrighted content. The judge previously ruled that although AI training using copyrighted books could be considered transformative fair use, Anthropic had illegally obtained the books from pirate sites.\n\nAnthropic expressed satisfaction with the resolution, stating it permits them to focus on developing safe and helpful AI tools. The court also stressed transparency in the claims process to ensure authors\u2014especially lesser-known ones\u2014aren\u2019t disadvantaged. Judge Alsup, who has served since 1999, announced he will retire by the end of the year.","ref_id":null,"pub_date":1758833618.0,"attribution":"apnews.com"}]},{"type":"search_result_group","domain":"www.businessinsider.com","entries":[{"type":"search_result","url":"https://www.businessinsider.com/sam-altman-predicts-ai-agi-surpass-human-intelligence-2030-2025-9","title":"Sam Altman predicts AI will surpass human intelligence by 2030","snippet":"In a recent interview with Welt Editor-in-Chief Jan Philipp Burgard, OpenAI CEO Sam Altman shared bold predictions about the future of artificial intelligence. Altman expects superintelligent AI\u2014smarter than humans across the board\u2014to emerge by 2030, with major breakthroughs likely by 2026. He highlighted how tools like GPT-5 already outperform humans in specific domains, though still fall short in many intuitive human tasks. Altman anticipates AI will eventually undertake 30-40% of current human tasks and emphasized the importance of teaching future generations adaptability, creativity, and interpersonal skills.\n\nAltman also addressed concerns about AI safety and alignment with human values, expressing optimism but underscoring the challenge of unintended consequences. He rejected comparisons suggesting AI might view humans as inferior, stressing the need for responsible development. Responding to criticism around OpenAI\u2019s partial commercialization, Altman affirmed the ongoing centrality of its nonprofit mission and safety priorities.\n\nAdditionally, he hinted at future OpenAI hardware resembling a \u201cfamily of devices\u201d that could revolutionize personal computing. Altman also weighed in on political dynamics in tech, stating that while AI won\u2019t replace leaders, it will support decision-making. Personally, he\u2019s skeptical of using AI for relationship advice, unlike many others.","ref_id":null,"pub_date":1758877202.0,"attribution":"www.businessinsider.com"}]},{"type":"search_result_group","domain":"www.reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/","title":"Exclusive: OpenAI co-founder Sutskever's new safety- ...","snippet":"Sep 4, 2024 \u2014 Exclusive: OpenAI co-founder Sutskever's new safety-focused AI startup SSI raises $1 billion \u00b7 Three-month-old SSI valued at $5 billion, sources ...","ref_id":null,"pub_date":1725455074.541,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/","title":"Exclusive: OpenAI working on new reasoning technology ...","snippet":"Jul 15, 2024 \u2014 Reuters could not ascertain the precise date of the document, which details a plan for how OpenAI intends to use Strawberry to perform research.","ref_id":null,"pub_date":1720819458.099,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/","title":"Ilya Sutskever on how AI will change and his new startup ...","snippet":"Sep 5, 2024 \u2014 Ilya Sutskever, OpenAI's former chief scientist, has launched a new company called Safe Superintelligence (SSI), aiming to develop safe artificial intelligence ...","ref_id":null,"pub_date":1725531820.551,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/","title":"OpenAI launches new series of AI models with 'reasoning' ...","snippet":"Sep 12, 2024 \u2014 The models, first reported by Reuters, are capable of reasoning through complex tasks and can solve more challenging problems than previous ...","ref_id":null,"pub_date":1726161615.914,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/","title":"OpenAI and others seek new path to smarter AI as current ...","snippet":"Nov 15, 2024 \u2014 To overcome these challenges, researchers are exploring \u201ctest-time compute,\u201d a technique that enhances existing AI models during the so-called \u201c ...","ref_id":null,"pub_date":1731319593.729,"attribution":"www.reuters.com"},{"type":"search_result","url":"https://www.reuters.com/business/retail-consumer/anthropic-ceo-says-proposed-10-year-ban-state-ai-regulation-too-blunt-nyt-op-ed-2025-06-05/","title":"Anthropic CEO says proposed 10-year ban on state AI regulation 'too blunt' in NYT op-ed","snippet":"In a New York Times op-ed, Anthropic CEO Dario Amodei criticized a Republican-backed proposal to impose a 10-year ban on state-level artificial intelligence regulation, calling it \"too blunt\" for the rapidly evolving AI landscape. The moratorium, part of President Donald Trump's tax cut bill, seeks to override recent AI regulations implemented by various states. Amodei argued that such a ban would hinder both state action and the development of a cohesive national policy. Instead, he advocated for a federal transparency standard requiring AI developers to disclose testing methods and risk mitigation strategies, especially regarding national security. He emphasized the importance of holding AI companies accountable for ensuring model safety before public deployment. Amodei noted that Anthropic, along with competitors OpenAI and Google DeepMind, already follow such disclosure practices. However, he stressed that legislative measures may be needed to maintain transparency as AI models grow more powerful and corporate motivations evolve.","ref_id":null,"pub_date":1749122856.0,"attribution":"www.reuters.com"}]},{"type":"search_result_group","domain":"www.anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/rsp-updates","title":"Anthropic's Responsible Scaling Policy","snippet":"May 14, 2025 \u2014 In our Responsible Scaling Policy, reaching certain Capability Thresholds requires us to upgrade our safeguards to the ASL-3 Security Standard ...","ref_id":null,"pub_date":null,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"Dec 15, 2022 \u2014 We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs.","ref_id":null,"pub_date":null,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/responsible-scaling-policy","title":"Anthropic's Responsible Scaling Policy (version 2.2)","snippet":"Sep 19, 2023 \u2014 This updated policy reflects our view that risk governance in this rapidly evolving domain should be proportional, iterative, and exportable.","ref_id":null,"pub_date":1747159748.0,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy","title":"Announcing our updated Responsible Scaling Policy","snippet":"Oct 15, 2024 \u2014 This update introduces a more flexible and nuanced approach to assessing and managing AI risks while maintaining our commitment not to train or deploy models.","ref_id":null,"pub_date":null,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/anthropics-responsible-scaling-policy","title":"Anthropic's Responsible Scaling Policy","snippet":"Sep 19, 2023 \u2014 The basic idea is to require safety, security, and operational standards appropriate to a model's potential for catastrophic risk, with higher ...","ref_id":null,"pub_date":null,"attribution":"www.anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input","title":"Collective Constitutional AI: Aligning a Language Model ...","snippet":"Oct 17, 2023 \u2014 Constitutional AI (CAI) is an Anthropic-developed method for aligning general purpose language models to abide by high-level normative ...","ref_id":null,"pub_date":null,"attribution":"www.anthropic.com"}]},{"type":"search_result_group","domain":"ssi.inc","entries":[{"type":"search_result","url":"https://ssi.inc/","title":"Safe Superintelligence Inc.","snippet":"Safe Superintelligence Inc. is the world's first lab focused on building safe superintelligence (SSI), with the goal of achieving it.","ref_id":null,"pub_date":null,"attribution":"ssi.inc"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/2212.08073","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"Dec 15, 2022 \u2014 In this paper we develop a method we refer to as Constitutional AI (CAI), depicted in Figure 1, and use it to train a non-evasive and relatively ...","ref_id":null,"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"www.youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=6fEdZkteI_M","title":"Ilya Sutskever May 2025 Update: The Mission Behind SSI ...","snippet":"In this landmark video, Ilya Sutskever, co-founder of OpenAI, speaks out for the first time about his new company, Safe Superintelligence ...","ref_id":null,"pub_date":1719533786.0,"attribution":"www.youtube.com"}]},{"type":"search_result_group","domain":"zenodo.org","entries":[{"type":"search_result","url":"https://zenodo.org/records/15331063/files/Constitutional%20AI%20Overview.pdf?download=1","title":"Constitutional AI: An Expanded Overview of Anthropic's ...","snippet":"Dec 17, 2023 \u2014 In this paper, we provide an expanded overview of Constitutional AI, including motivating background, methodology, ethical considerations, and ...","ref_id":null,"pub_date":null,"attribution":"zenodo.org"}]},{"type":"search_result_group","domain":"time.com","entries":[{"type":"search_result","url":"https://time.com/collections/the-ai-dictionary-from-allbusiness-com/7273990/definition-of-safe-superintelligence/","title":"The Definition of Safe Superintelligence - AI","snippet":"Apr 3, 2025 \u2014 The company's mission is to focus on safely developing a superintelligent AI agent capable of surpassing human intelligence. Ilya Sutskever was ...","ref_id":null,"pub_date":1743714694.0,"attribution":"time.com"}]},{"type":"search_result_group","domain":"www.aol.com","entries":[{"type":"search_result","url":"https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html","title":"Ilya Sutskever on how AI will change and his new startup Safe ...","snippet":"\"We've identified a mountain that's a bit different from what I was working [on]...once you climb to the top of this mountain, the paradigm will change...","ref_id":null,"pub_date":null,"attribution":"www.aol.com"}]},{"type":"search_result_group","domain":"www.economist.com","entries":[{"type":"search_result","url":"https://www.economist.com/podcasts/2025/07/31/artificial-intelligentsia-an-interview-with-the-boss-of-anthropic","title":"Artificial intelligentsia: an interview with the boss of Anthropic","snippet":"This week, Dario Amodei on why safety sells and how to value people in a world without work. Share. Jul 31st 2025 |1 min read. Listen to this podcast.","ref_id":null,"pub_date":null,"attribution":"www.economist.com"}]}],"debug_sonic_thread_id":"thread_68de75b78f78832b8684b118f97bf38c","reasoning_status":"is_reasoning","reasoning_title":"Browsing and analyzing Anthropic vs. SSI visions","classifier_response":"default","request_id":"98845704feb6ce1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"6cc9d6af-e491-4879-a572-755758607bdd","turn_exchange_id":"3d9473e2-b72c-438a-ab31-5177a5809676","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"6cc9d6af-e491-4879-a572-755758607bdd","children":["0d341ac9-7628-467d-8f3a-aa8153556e0e"]},"0d341ac9-7628-467d-8f3a-aa8153556e0e":{"id":"0d341ac9-7628-467d-8f3a-aa8153556e0e","message":{"id":"0d341ac9-7628-467d-8f3a-aa8153556e0e","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409605.331115,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 22s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":22,"model_slug":"gpt-5-thinking","request_id":"98845704feb6ce1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"c4dbcf30-54e2-45ce-a11b-aff9e72c6a23","turn_exchange_id":"3d9473e2-b72c-438a-ab31-5177a5809676","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"c4dbcf30-54e2-45ce-a11b-aff9e72c6a23","children":["6cb8eeb4-c242-4956-b338-a025e1b500f0"]},"6cb8eeb4-c242-4956-b338-a025e1b500f0":{"id":"6cb8eeb4-c242-4956-b338-a025e1b500f0","message":{"id":"6cb8eeb4-c242-4956-b338-a025e1b500f0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409594.520136,"update_time":null,"content":{"content_type":"text","parts":["Your two hypotheses are reasonable\u2014but they under-explain the split. There\u2019s meaningful overlap between SSI and Anthropic on \u201csafety first,\u201d yet their *operating doctrines* diverge enough that a merger would blunt both strategies.\n\nHere\u2019s the clearest way to parse it.\n\n# Where Anthropic and SSI are genuinely different\n\n**1) Deployment philosophy**\n- **Anthropic:** *Iterative, public-facing deployment with gates.* They formalized \u201cAI Safety Levels\u201d and a **Responsible Scaling Policy (RSP)** that ties capability thresholds to concrete security/deployment standards; they commit to publishing key information about evals and safeguards before shipping stronger models. That\u2019s a ship-learn-tighten cycle. \ue200cite\ue202turn0search1\ue202turn0search10\ue202turn0search15\ue201  \n- **SSI:** *Single-mission, non-product cadence until SSI.* The website and early press frame **one goal, one product: a safe superintelligence**, explicitly insulating the lab from product cycles. That is the opposite of \u201cdeploy and learn.\u201d \ue200cite\ue202turn0search3\ue202turn0search5\ue201\n\n**2) Safety methodology posture**\n- **Anthropic:** Predominantly **behavioral/externally-specified** alignment (e.g., Constitutional AI\u2014principles guide model behavior\u2014and RSP-gated evals). Public papers emphasize preference shaping and oversight instruments you can iterate with user and red-team feedback. \ue200cite\ue202turn0search2\ue202turn0search7\ue202turn0search16\ue201  \n- **SSI:** Signals a **mechanism/architecture** focus\u2014\u201ca different mountain\u2026 once you reach the top, the paradigm changes\u201d\u2014which reads as *safety by construction* (interfaces, modular planners, formal specs) rather than primarily behaviorist tuning. Their messaging stresses that pretraining \u201cas we know it\u201d is running out of road and that stronger **reasoning \u21d2 less predictability**, pushing toward provable constraints rather than more feedback cycles. (Sutskever has articulated these themes in interviews/remarks.) \ue200cite\ue202turn0search14\ue201\n\n**3) Org design as technical leverage**\n- **Anthropic:** A scaled, commercial lab with consumer and enterprise products; governance via RSP and public accountability claims. \ue200cite\ue202turn0search1\ue201  \n- **SSI:** A **straight-shot lab** set up to \u201cscale in peace\u201d\u2014investors and roadmap aligned around *no multi-product distraction*. That structure selects for long, safety-gated internal programs rather than incremental releases. \ue200cite\ue202turn0search3\ue201\n\n**4) Data & training strategy**\n- **Anthropic:** Continues large-scale training and public deployment; recently navigated major copyright/legal constraints while maintaining product lines\u2014consistent with an *open-world*, regulated deployment posture. \ue200cite\ue202turn0news56\ue201  \n- **SSI:** Public rhetoric emphasizes that \u201cwe have but one internet,\u201d nudging toward **closed-loop data generation and new evaluation regimes** to grow capability *without* relying on scraping more web. That implies different pipelines and internal metrics of progress. (This is synthesized from SSI statements and Sutskever\u2019s recent interviews.) \ue200cite\ue202turn0search3\ue202turn0search14\ue201\n\n# Why \u201cjust join forces\u201d is unlikely (beyond vision or trust)\n\n1) **Governance and accountability models are incompatible.**  \nAnthropic\u2019s *public, iterative* RSP (ASL thresholds, staged safeguards) doesn\u2019t mesh with SSI\u2019s *single end-product* posture. You can\u2019t simultaneously promise external cadence-gated deployment *and* promise not to ship until the \u201cdifferent mountain\u201d is summited. \ue200cite\ue202turn0search1\ue201\n\n2) **Technical stack bets diverge.**  \nAnthropic\u2019s comparative advantage is behaviorist alignment (Constitutional AI + evals) paired with rapid feedback from real users; SSI\u2019s pitch implies spec-driven or mechanism-heavy approaches that slow iteration but target *enforceable* guarantees. These approaches cannibalize each other\u2019s edge if forced together. \ue200cite\ue202turn0search2\ue201\n\n3) **Regulatory and reputational positioning.**  \nAnthropic has leaned into public standards, op-eds, and iterative regulation (and is dealing with high-profile IP settlements). SSI benefits from staying out of the deployment fray while building a security-grade, closed program. Different external postures reduce merger synergies. \ue200cite\ue202turn0news60\ue201\n\n4) **Capital, control, and fiduciary constraints.**  \nAnthropic is a mature, revenue-bearing frontier lab with multi-party investors and customers; SSI is an early-stage vehicle optimized for a single long bet with investors explicitly bought into that constraint. Rolling these up would create board/control conflicts that directly impact the safety tempo. \ue200cite\ue202turn0search3\ue201\n\n5) **Antitrust & ecosystem strategy.**  \nRegulators are already wary of consolidation among frontier labs. A merger that combines two of the few labs setting safety norms would face scrutiny, especially as model deployment becomes infrastructural. (Inference based on current consolidation concerns; not tied to a single filing.)\n\n# So what *isn\u2019t* explained by your two reasons?\n- Not just \u201cego/vision\u201d or \u201ctrust.\u201d Even with perfect trust, **combining an RSP-led, public iteration lab with a non-product SSI lab collapses both theses**. Each one\u2019s differentiation depends on *not* becoming the other. \ue200cite\ue202turn0search1\ue202turn0search3\ue201\n\n# What would *falsify* my read (watch for these signals)\n\n- **If SSI begins shipping incremental, general-purpose models** (APIs/assistants) under public RSP-like gates \u2192 they\u2019ve converged toward Anthropic.  \n- **If Anthropic pivots to \u201cno public deployments until SSI,\u201d** de-emphasizes CAI/behavioral alignment, and starts publishing spec-verified modules \u2192 they\u2019ve converged toward SSI.  \n- **If both announce a joint closed-loop, synthetic-curriculum regime with formal safety claims** (e.g., verified planners, proof-carrying plans) and shared compute partitions \u2192 the \u201cdifferent mountain\u201d gap is narrowing.\n\n# Bottom line\nYou\u2019re right that both see the same *reasoning/agentic* frontier. But **Anthropic\u2019s hill is \u201csafely *deploying up* the mountain with public gates\u201d** (CAI + RSP), while **SSI\u2019s hill is \u201creaching a *different summit* with safety by construction before you deploy anything at scale.\u201d** Those are not cosmetic differences; they determine research agenda, infra, governance, and capital structure. On those axes, a merger would destroy the very features each believes are necessary to make powerful systems safe. \ue200cite\ue202turn0search1\ue202turn0search2\ue202turn0search3\ue202turn0search14\ue201\n\nIf you want, I can turn this into a one-page decision rubric (\u201cAre they converging?\u201d) with objective checkpoints you can track from public updates."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search10\ue202turn0search15\ue201","start_idx":707,"end_idx":754,"safe_urls":["https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy?utm_source=chatgpt.com","https://www.anthropic.com/news/anthropics-responsible-scaling-policy?utm_source=chatgpt.com","https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Anthropic's Responsible Scaling Policy","url":"https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","pub_date":null,"snippet":"May 14, 2025 \u2014 In our Responsible Scaling Policy, reaching certain Capability Thresholds requires us to upgrade our safeguards to the ASL-3 Security Standard ...","attribution_segments":null,"supporting_websites":[{"title":"Announcing our updated Responsible Scaling Policy","url":"https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy?utm_source=chatgpt.com","pub_date":null,"snippet":"Oct 15, 2024 \u2014 This update introduces a more flexible and nuanced approach to assessing and managing AI risks while maintaining our commitment not to train or deploy models.","attribution":"Anthropic"},{"title":"Anthropic's Responsible Scaling Policy","url":"https://www.anthropic.com/news/anthropics-responsible-scaling-policy?utm_source=chatgpt.com","pub_date":null,"snippet":"Sep 19, 2023 \u2014 The basic idea is to require safety, security, and operational standards appropriate to a model's potential for catastrophic risk, with higher ...","attribution":"Anthropic"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":10},{"turn_index":0,"ref_type":"search","ref_index":15}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search5\ue201","start_idx":999,"end_idx":1031,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"Safe Superintelligence Inc. is the world's first lab focused on building safe superintelligence (SSI), with the goal of achieving it.","attribution_segments":null,"supporting_websites":[{"title":"Ilya Sutskever on how AI will change and his new startup ...","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","pub_date":null,"snippet":"Sep 5, 2024 \u2014 Ilya Sutskever, OpenAI's former chief scientist, has launched a new company called Safe Superintelligence (SSI), aiming to develop safe artificial intelligence ...","attribution":"Reuters"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":5}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search2\ue202turn0search7\ue202turn0search16\ue201","start_idx":1341,"end_idx":1387,"safe_urls":["https://arxiv.org/pdf/2212.08073?utm_source=chatgpt.com","https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input?utm_source=chatgpt.com","https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Constitutional AI: Harmlessness from AI Feedback","url":"https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 15, 2022 \u2014 We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs.","attribution_segments":null,"supporting_websites":[{"title":"Constitutional AI: Harmlessness from AI Feedback","url":"https://arxiv.org/pdf/2212.08073?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 15, 2022 \u2014 In this paper we develop a method we refer to as Constitutional AI (CAI), depicted in Figure 1, and use it to train a non-evasive and relatively ...","attribution":"arXiv"},{"title":"Collective Constitutional AI: Aligning a Language Model ...","url":"https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input?utm_source=chatgpt.com","pub_date":null,"snippet":"Oct 17, 2023 \u2014 Constitutional AI (CAI) is an Anthropic-developed method for aligning general purpose language models to abide by high-level normative ...","attribution":"Anthropic"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":7},{"turn_index":0,"ref_type":"search","ref_index":16}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search14\ue201","start_idx":1910,"end_idx":1930,"safe_urls":["https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com"],"refs":[],"alt":"([AOL](https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Ilya Sutskever on how AI will change and his new startup Safe ...","url":"https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com","pub_date":null,"snippet":"\"We've identified a mountain that's a bit different from what I was working [on]...once you climb to the top of this mountain, the paradigm will change...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"AOL"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue201","start_idx":2106,"end_idx":2125,"safe_urls":["https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Anthropic's Responsible Scaling Policy","url":"https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","pub_date":null,"snippet":"May 14, 2025 \u2014 In our Responsible Scaling Policy, reaching certain Capability Thresholds requires us to upgrade our safeguards to the ASL-3 Security Standard ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue201","start_idx":2357,"end_idx":2376,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"Safe Superintelligence Inc. is the world's first lab focused on building safe superintelligence (SSI), with the goal of achieving it.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0news56\ue201","start_idx":2629,"end_idx":2647,"safe_urls":["https://apnews.com/article/9643064e847a5e88ef6ee8b620b3a44c?utm_source=chatgpt.com"],"refs":[],"alt":"([AP News](https://apnews.com/article/9643064e847a5e88ef6ee8b620b3a44c?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Judge approves $1.5 billion copyright settlement between AI company Anthropic and authors","url":"https://apnews.com/article/9643064e847a5e88ef6ee8b620b3a44c?utm_source=chatgpt.com","pub_date":1758833618.0,"snippet":"A federal judge in San Francisco has preliminarily approved a $1.5 billion settlement between AI company Anthropic and a coalition of authors and publishers who accused the company of pirating about 465,000 books to train its Claude chatbot. Judge William Alsup, while acknowledging the complexity of distributing the funds, deemed the settlement fair. Authors and publishers will receive approximately $3,000 for each book included in the agreement, which covers past but not future works.\n\nThe Authors Guild and the Association of American Publishers hailed the outcome as a milestone in safeguarding authors\u2019 rights. They emphasized that this settlement sets a precedent for holding AI developers accountable for unauthorized use of copyrighted content. The judge previously ruled that although AI training using copyrighted books could be considered transformative fair use, Anthropic had illegally obtained the books from pirate sites.\n\nAnthropic expressed satisfaction with the resolution, stating it permits them to focus on developing safe and helpful AI tools. The court also stressed transparency in the claims process to ensure authors\u2014especially lesser-known ones\u2014aren\u2019t disadvantaged. Judge Alsup, who has served since 1999, announced he will retire by the end of the year.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"news","ref_index":56}],"hue":null,"attributions":null,"attribution":"AP News"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue202turn0search14\ue201","start_idx":2998,"end_idx":3031,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"Safe Superintelligence Inc. is the world's first lab focused on building safe superintelligence (SSI), with the goal of achieving it.","attribution_segments":null,"supporting_websites":[{"title":"Ilya Sutskever on how AI will change and his new startup Safe ...","url":"https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com","pub_date":null,"snippet":"\"We've identified a mountain that's a bit different from what I was working [on]...once you climb to the top of this mountain, the paradigm will change...","attribution":"AOL"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue201","start_idx":3423,"end_idx":3442,"safe_urls":["https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Anthropic's Responsible Scaling Policy","url":"https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","pub_date":null,"snippet":"May 14, 2025 \u2014 In our Responsible Scaling Policy, reaching certain Capability Thresholds requires us to upgrade our safeguards to the ASL-3 Security Standard ...","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search2\ue201","start_idx":3800,"end_idx":3819,"safe_urls":["https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Constitutional AI: Harmlessness from AI Feedback","url":"https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 15, 2022 \u2014 We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":2}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0news60\ue201","start_idx":4154,"end_idx":4172,"safe_urls":["https://www.reuters.com/business/retail-consumer/anthropic-ceo-says-proposed-10-year-ban-state-ai-regulation-too-blunt-nyt-op-ed-2025-06-05/?utm_source=chatgpt.com"],"refs":[],"alt":"([Reuters](https://www.reuters.com/business/retail-consumer/anthropic-ceo-says-proposed-10-year-ban-state-ai-regulation-too-blunt-nyt-op-ed-2025-06-05/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Anthropic CEO says proposed 10-year ban on state AI regulation 'too blunt' in NYT op-ed","url":"https://www.reuters.com/business/retail-consumer/anthropic-ceo-says-proposed-10-year-ban-state-ai-regulation-too-blunt-nyt-op-ed-2025-06-05/?utm_source=chatgpt.com","pub_date":1749122856.0,"snippet":"In a New York Times op-ed, Anthropic CEO Dario Amodei criticized a Republican-backed proposal to impose a 10-year ban on state-level artificial intelligence regulation, calling it \"too blunt\" for the rapidly evolving AI landscape. The moratorium, part of President Donald Trump's tax cut bill, seeks to override recent AI regulations implemented by various states. Amodei argued that such a ban would hinder both state action and the development of a cohesive national policy. Instead, he advocated for a federal transparency standard requiring AI developers to disclose testing methods and risk mitigation strategies, especially regarding national security. He emphasized the importance of holding AI companies accountable for ensuring model safety before public deployment. Amodei noted that Anthropic, along with competitors OpenAI and Google DeepMind, already follow such disclosure practices. However, he stressed that legislative measures may be needed to maintain transparency as AI models grow more powerful and corporate motivations evolve.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"news","ref_index":60}],"hue":null,"attributions":null,"attribution":"Reuters"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search3\ue201","start_idx":4532,"end_idx":4551,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com"],"refs":[],"alt":"([ssi.inc](https://ssi.inc/?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"Safe Superintelligence Inc. is the world's first lab focused on building safe superintelligence (SSI), with the goal of achieving it.","attribution_segments":null,"supporting_websites":[],"refs":[{"turn_index":0,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"ssi.inc"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search3\ue201","start_idx":5155,"end_idx":5187,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Anthropic's Responsible Scaling Policy","url":"https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","pub_date":null,"snippet":"May 14, 2025 \u2014 In our Responsible Scaling Policy, reaching certain Capability Thresholds requires us to upgrade our safeguards to the ASL-3 Security Standard ...","attribution_segments":null,"supporting_websites":[{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"Safe Superintelligence Inc. is the world's first lab focused on building safe superintelligence (SSI), with the goal of achieving it.","attribution":"ssi.inc"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":3}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":"\ue200cite\ue202turn0search1\ue202turn0search2\ue202turn0search3\ue202turn0search14\ue201","start_idx":6324,"end_idx":6383,"safe_urls":["https://ssi.inc/?utm_source=chatgpt.com","https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com","https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com"],"refs":[],"alt":"([Anthropic](https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com))","prompt_text":null,"type":"grouped_webpages","items":[{"title":"Anthropic's Responsible Scaling Policy","url":"https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","pub_date":null,"snippet":"May 14, 2025 \u2014 In our Responsible Scaling Policy, reaching certain Capability Thresholds requires us to upgrade our safeguards to the ASL-3 Security Standard ...","attribution_segments":null,"supporting_websites":[{"title":"Constitutional AI: Harmlessness from AI Feedback","url":"https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com","pub_date":null,"snippet":"Dec 15, 2022 \u2014 We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs.","attribution":"Anthropic"},{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","pub_date":null,"snippet":"Safe Superintelligence Inc. is the world's first lab focused on building safe superintelligence (SSI), with the goal of achieving it.","attribution":"ssi.inc"},{"title":"Ilya Sutskever on how AI will change and his new startup Safe ...","url":"https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com","pub_date":null,"snippet":"\"We've identified a mountain that's a bit different from what I was working [on]...once you climb to the top of this mountain, the paradigm will change...","attribution":"AOL"}],"refs":[{"turn_index":0,"ref_type":"search","ref_index":1},{"turn_index":0,"ref_type":"search","ref_index":2},{"turn_index":0,"ref_type":"search","ref_index":3},{"turn_index":0,"ref_type":"search","ref_index":14}],"hue":null,"attributions":null,"attribution":"Anthropic"}],"fallback_items":null,"status":"done","error":null,"style":null},{"matched_text":" ","start_idx":6532,"end_idx":6532,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[{"title":"Anthropic's Responsible Scaling Policy","url":"https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","attribution":"Anthropic"},{"title":"Safe Superintelligence Inc.","url":"https://ssi.inc/?utm_source=chatgpt.com","attribution":"ssi.inc"},{"title":"Ilya Sutskever on how AI will change and his new startup Safe ...","url":"https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com","attribution":"AOL"},{"title":"Judge approves $1.5 billion copyright settlement between AI company Anthropic and authors","url":"https://apnews.com/article/9643064e847a5e88ef6ee8b620b3a44c?utm_source=chatgpt.com","attribution":"AP News"},{"title":"Anthropic CEO says proposed 10-year ban on state AI regulation 'too blunt' in NYT op-ed","url":"https://www.reuters.com/business/retail-consumer/anthropic-ceo-says-proposed-10-year-ban-state-ai-regulation-too-blunt-nyt-op-ed-2025-06-05/?utm_source=chatgpt.com","attribution":"Reuters"}],"has_images":false}],"safe_urls":["https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy?utm_source=chatgpt.com","https://www.anthropic.com/news/anthropics-responsible-scaling-policy?utm_source=chatgpt.com","https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","https://arxiv.org/pdf/2212.08073?utm_source=chatgpt.com","https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input?utm_source=chatgpt.com","https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com","https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com","https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://apnews.com/article/9643064e847a5e88ef6ee8b620b3a44c?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com","https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com","https://www.reuters.com/business/retail-consumer/anthropic-ceo-says-proposed-10-year-ban-state-ai-regulation-too-blunt-nyt-op-ed-2025-06-05/?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","https://ssi.inc/?utm_source=chatgpt.com","https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback?utm_source=chatgpt.com","https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com","https://www.aol.com/news/ilya-sutskever-ai-change-startup-102324286.html?utm_source=chatgpt.com"],"search_result_groups":[{"type":"search_result_group","domain":"reuters.com","entries":[{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-co-founder-sutskevers-new-safety-focused-ai-startup-ssi-raises-1-billion-2024-09-04/?utm_source=chatgpt.com","title":"Exclusive: OpenAI co-founder Sutskever's new safety- ...","snippet":"Sep 4, 2024 \u2014 Exclusive: OpenAI co-founder Sutskever's new safety-focused AI startup SSI raises $1 billion \u00b7 Three-month-old SSI valued at $5 billion, sources ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":0},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?utm_source=chatgpt.com","title":"Exclusive: OpenAI working on new reasoning technology ...","snippet":"Jul 15, 2024 \u2014 Reuters could not ascertain the precise date of the document, which details a plan for how OpenAI intends to use Strawberry to perform research.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":4},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/ilya-sutskever-how-ai-will-change-his-new-startup-safe-superintelligence-2024-09-05/?utm_source=chatgpt.com","title":"Ilya Sutskever on how AI will change and his new startup ...","snippet":"Sep 5, 2024 \u2014 Ilya Sutskever, OpenAI's former chief scientist, has launched a new company called Safe Superintelligence (SSI), aiming to develop safe artificial intelligence ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":5},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-series-ai-models-solve-hard-problems-2024-09-12/?utm_source=chatgpt.com","title":"OpenAI launches new series of AI models with 'reasoning' ...","snippet":"Sep 12, 2024 \u2014 The models, first reported by Reuters, are capable of reasoning through complex tasks and can solve more challenging problems than previous ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":8},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/?utm_source=chatgpt.com","title":"OpenAI and others seek new path to smarter AI as current ...","snippet":"Nov 15, 2024 \u2014 To overcome these challenges, researchers are exploring \u201ctest-time compute,\u201d a technique that enhances existing AI models during the so-called \u201c ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":13},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-considers-pricier-subscriptions-its-chatbot-ai-information-reports-2024-09-05/?utm_source=chatgpt.com","title":"OpenAI hits more than 1 million paid business users","snippet":"Sep 5, 2024 \u2014 Strawberry includes a specialized way of what is known as \"post-training\" OpenAI's generative AI models, or adapting the base models to hone ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":19},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-plans-release-strawberry-part-chatgpt-next-two-weeks-information-reports-2024-09-10/?utm_source=chatgpt.com","title":"OpenAI plans to release 'Strawberry' for ChatGPT in two ...","snippet":"Sep 10, 2024 \u2014 OpenAI plans to release \"Strawberry\", its reasoning-focused artificial intelligence, as part of its ChatGPT service in the next two weeks.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":25},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/video/watch/idRW903215072024RP1/?utm_source=chatgpt.com","title":"OpenAI working on new 'Strawberry' project","snippet":"ChatGPT maker OpenAI is working on a novel approach to its artificial intelligence models in a project code-named \u201cStrawberry,\u201d according to a ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":37},"pub_date":null,"attribution":"reuters.com"},{"type":"search_result","url":"https://www.reuters.com/technology/artificial-intelligence/openai-unveils-tool-automate-web-tasks-ai-agents-take-center-stage-2025-01-23/?utm_source=chatgpt.com","title":"OpenAI unveils tool to automate web tasks as AI agents ...","snippet":"Jan 23, 2025 \u2014 OpenAI on Thursday previewed an AI agent that can carry out tasks on the web for users, as it seeks to enhance its chatbot amid intensifying competition.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":54},"pub_date":null,"attribution":"reuters.com"}]},{"type":"search_result_group","domain":"anthropic.com","entries":[{"type":"search_result","url":"https://www.anthropic.com/responsible-scaling-policy?utm_source=chatgpt.com","title":"Anthropic's Responsible Scaling Policy (version 2.2)","snippet":"Sep 19, 2023 \u2014 This updated policy reflects our view that risk governance in this rapidly evolving domain should be proportional, iterative, and exportable.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":6},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy?utm_source=chatgpt.com","title":"Announcing our updated Responsible Scaling Policy","snippet":"Oct 15, 2024 \u2014 This update introduces a more flexible and nuanced approach to assessing and managing AI risks while maintaining our commitment not to train or deploy models.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":10},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/anthropics-responsible-scaling-policy?utm_source=chatgpt.com","title":"Anthropic's Responsible Scaling Policy","snippet":"Sep 19, 2023 \u2014 The basic idea is to require safety, security, and operational standards appropriate to a model's potential for catastrophic risk, with higher ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":15},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input?utm_source=chatgpt.com","title":"Collective Constitutional AI: Aligning a Language Model ...","snippet":"Oct 17, 2023 \u2014 Constitutional AI (CAI) is an Anthropic-developed method for aligning general purpose language models to abide by high-level normative ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":16},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www.anthropic.com/news/usage-policy-update?utm_source=chatgpt.com","title":"Usage Policy Update","snippet":"Aug 15, 2025 \u2014 These changes will take effect on September 15, 2025. Below is a summary of some of the changes, and you can view the new Usage Policy here.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":39},"pub_date":null,"attribution":"anthropic.com"},{"type":"search_result","url":"https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf?utm_source=chatgpt.com","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"Drafting a constitution for powerful AI systems could be a democratic process wherein diverse stakeholders provide input to tailor the behavior of a system to ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":46},"pub_date":null,"attribution":"anthropic.com"}]},{"type":"search_result_group","domain":"arxiv.org","entries":[{"type":"search_result","url":"https://arxiv.org/pdf/2212.08073?utm_source=chatgpt.com","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"Dec 15, 2022 \u2014 In this paper we develop a method we refer to as Constitutional AI (CAI), depicted in Figure 1, and use it to train a non-evasive and relatively ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":7},"pub_date":null,"attribution":"arxiv.org"},{"type":"search_result","url":"https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com","title":"Constitutional AI: Harmlessness from AI Feedback","snippet":"by Y Bai \u00b7 2022 \u00b7 Cited by 2126 \u2014 We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":28},"pub_date":null,"attribution":"arxiv.org"}]},{"type":"search_result_group","domain":"youtube.com","entries":[{"type":"search_result","url":"https://www.youtube.com/watch?v=6fEdZkteI_M&utm_source=chatgpt.com","title":"Ilya Sutskever May 2025 Update: The Mission Behind SSI ...","snippet":"In this landmark video, Ilya Sutskever, co-founder of OpenAI, speaks out for the first time about his new company, Safe Superintelligence ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":9},"pub_date":null,"attribution":"youtube.com"},{"type":"search_result","url":"https://www.youtube.com/watch?v=2MOxAsRKaEU&utm_source=chatgpt.com","title":"\"AGI is COMING!\" - Ilya Sutskever's Latest Announcement, SSI ...","snippet":"In this groundbreaking video, Ilya Sutskever, co-founder of OpenAI, makes a historic announcement about the achievement of Artificial ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":49},"pub_date":null,"attribution":"youtube.com"}]},{"type":"search_result_group","domain":"zenodo.org","entries":[{"type":"search_result","url":"https://zenodo.org/records/15331063/files/Constitutional%20AI%20Overview.pdf?download=1&utm_source=chatgpt.com","title":"Constitutional AI: An Expanded Overview of Anthropic's ...","snippet":"Dec 17, 2023 \u2014 In this paper, we provide an expanded overview of Constitutional AI, including motivating background, methodology, ethical considerations, and ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":11},"pub_date":null,"attribution":"zenodo.org"}]},{"type":"search_result_group","domain":"time.com","entries":[{"type":"search_result","url":"https://time.com/collections/the-ai-dictionary-from-allbusiness-com/7273990/definition-of-safe-superintelligence/?utm_source=chatgpt.com","title":"The Definition of Safe Superintelligence - AI","snippet":"Apr 3, 2025 \u2014 The company's mission is to focus on safely developing a superintelligent AI agent capable of surpassing human intelligence. Ilya Sutskever was ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":12},"pub_date":null,"attribution":"time.com"},{"type":"search_result","url":"https://time.com/6990386/anthropic-dario-amodei-interview/?utm_source=chatgpt.com","title":"Anthropic CEO Dario Amodei on Being an Underdog, AI ...","snippet":"Jun 23, 2024 \u2014 Dario Amodei spoke with TIME about AI safety, the company's culture, and economic inequality.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":23},"pub_date":null,"attribution":"time.com"}]},{"type":"search_result_group","domain":"economist.com","entries":[{"type":"search_result","url":"https://www.economist.com/podcasts/2025/07/31/artificial-intelligentsia-an-interview-with-the-boss-of-anthropic?utm_source=chatgpt.com","title":"Artificial intelligentsia: an interview with the boss of Anthropic","snippet":"This week, Dario Amodei on why safety sells and how to value people in a world without work. Share. Jul 31st 2025 |1 min read. Listen to this podcast.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":17},"pub_date":null,"attribution":"economist.com"}]},{"type":"search_result_group","domain":"startupnationcentral.org","entries":[{"type":"search_result","url":"https://finder.startupnationcentral.org/company_page/safe-superintelligence?utm_source=chatgpt.com","title":"Safe Superintelligence - Israeli Startup","snippet":"As the first dedicated SSI lab, their sole aim is to develop a safe superintelligent system. This mission drives their entire roadmap and aligns their team, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":18},"pub_date":null,"attribution":"startupnationcentral.org"}]},{"type":"search_result_group","domain":"reddit.com","entries":[{"type":"search_result","url":"https://www.reddit.com/r/agi/comments/1llyxub/are_we_wise_to_trust_ilya_sutskevers_safe/?utm_source=chatgpt.com","title":"Are We Wise to Trust Ilya Sutskever's Safe ...","snippet":"Sutskever has chosen to not publish ANY of his research. This seems both profoundly selfish and morally unintelligent. While Sutskever is ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":20},"pub_date":null,"attribution":"reddit.com"},{"type":"search_result","url":"https://www.reddit.com/r/singularity/comments/1e3y3g2/new_sentences_in_updated_reuters_article_about/?utm_source=chatgpt.com","title":"New sentences in updated Reuters article about OpenAI's ...","snippet":"Google got 91% with 1.5 pro using test-time compute + mathematic fine-tuning. Without test time compute they still got 80.6%. Comment Image.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":43},"pub_date":null,"attribution":"reddit.com"}]},{"type":"search_result_group","domain":"s-rsa.com","entries":[{"type":"search_result","url":"https://s-rsa.com/index.php/agi/article/view/13657?utm_source=chatgpt.com","title":"Anthropic: Responsible Scaling Policy","snippet":"by E Hubinger \u00b7 2025 \u00b7 Cited by 1 \u2014 In September 2023, we released our Responsible Scaling Policy (RSP), a public commitment not to train or deploy models capable of causing catastrophic harm ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":21},"pub_date":null,"attribution":"s-rsa.com"}]},{"type":"search_result_group","domain":"researchgate.net","entries":[{"type":"search_result","url":"https://www.researchgate.net/publication/391400510_Constitutional_AI_An_Expanded_Overview_of_Anthropic%27s_Alignment_Approach?utm_source=chatgpt.com","title":"An Expanded Overview of Anthropic's Alignment Approach","snippet":"May 4, 2025 \u2014 This paper provides an expanded overview of Constitutional AI, its background, methodology, practical implementation details, and future directions.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":22},"pub_date":null,"attribution":"researchgate.net"}]},{"type":"search_result_group","domain":"dhiwise.com","entries":[{"type":"search_result","url":"https://www.dhiwise.com/post/safe-super-intelligence?utm_source=chatgpt.com","title":"Safe Superintelligence Inc: Sutskever's $2B AI Mission","snippet":"Jun 6, 2025 \u2014 Safe Superintelligence Inc. aims to develop superintelligent AI that is safe, aligned with human values, and under human control, prioritizing ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":24},"pub_date":null,"attribution":"dhiwise.com"}]},{"type":"search_result_group","domain":"daily.dev","entries":[{"type":"search_result","url":"https://daily.dev/blog/safe-superintelligence-inc-ssi-everything-we-know-so-far-about-ilya-sutskevers-new-ai-company?utm_source=chatgpt.com","title":"Safe Superintelligence Inc (SSI): Everything we know so far ...","snippet":"Jun 21, 2024 \u2014 Discover everything about Safe Superintelligence Inc (SSI), Ilya Sutskever's new AI company. Learn about their goals, approach, and impact on the AI field.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":26},"pub_date":null,"attribution":"daily.dev"}]},{"type":"search_result_group","domain":"lesswrong.com","entries":[{"type":"search_result","url":"https://www.lesswrong.com/posts/wphaHzQMH7jqkC9As/a-cheeky-pint-with-anthropic-ceo-dario-amodei?utm_source=chatgpt.com","title":"A Cheeky Pint with Anthropic CEO Dario Amodei","snippet":"Aug 6, 2025 \u2014 Anthropic's role will be to provide as much evidence as possible that AI safety techniques cannot prevent serious or catastrophic safety risks ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":29},"pub_date":null,"attribution":"lesswrong.com"},{"type":"search_result","url":"https://www.lesswrong.com/posts/Q7caj7emnwWBxLECF/anthropic-s-updated-responsible-scaling-policy?utm_source=chatgpt.com","title":"Anthropic's updated Responsible Scaling Policy","snippet":"Oct 15, 2024 \u2014 This update introduces a more flexible and nuanced approach to assessing and managing AI risks while maintaining our commitment not to train or deploy models.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":33},"pub_date":null,"attribution":"lesswrong.com"}]},{"type":"search_result_group","domain":"rdworldonline.com","entries":[{"type":"search_result","url":"https://www.rdworldonline.com/what-is-safe-superintelligence-inc-the-ai-rd-outfit-poised-to-be-worth-20b/?utm_source=chatgpt.com","title":"What Is Safe Superintelligence Inc., the AI startup poised to ...","snippet":"Feb 10, 2025 \u2014 The focus on safety is paramount at SSI. Unlike \u201ctrust and safety\u201d teams that filter harmful content, SSI focuses on existential safety\u2014 ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":30},"pub_date":null,"attribution":"rdworldonline.com"}]},{"type":"search_result_group","domain":"techcrunch.com","entries":[{"type":"search_result","url":"https://techcrunch.com/2024/09/12/openai-unveils-a-model-that-can-fact-check-itself/?utm_source=chatgpt.com","title":"OpenAI unveils o1, a model that can fact-check itself","snippet":"Sep 12, 2024 \u2014 ChatGPT maker OpenAI has announced its next major product release: A generative AI model code-named Strawberry, officially called OpenAI o1.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":31},"pub_date":null,"attribution":"techcrunch.com"},{"type":"search_result","url":"https://techcrunch.com/2025/04/09/ilya-sutskever-taps-google-cloud-to-power-his-ai-startups-research/?utm_source=chatgpt.com","title":"Ilya Sutskever taps Google Cloud to power his AI startup's ...","snippet":"Apr 9, 2025 \u2014 OpenAI co-founder Ilya Sutskever has tapped Google Cloud to power his new startup's research efforts to create 'safe, superintelligent AI.'","ref_id":{"turn_index":0,"ref_type":"search","ref_index":32},"pub_date":null,"attribution":"techcrunch.com"}]},{"type":"search_result_group","domain":"constitutional.ai","entries":[{"type":"search_result","url":"https://constitutional.ai/?utm_source=chatgpt.com","title":"Constitutional AI | Tracking Anthropic's AI Revolution","snippet":"Constitutional AI refers to a set of techniques developed by researchers at Anthropic to align AI systems like myself with human values and make us helpful, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":34},"pub_date":null,"attribution":"constitutional.ai"}]},{"type":"search_result_group","domain":"medium.com","entries":[{"type":"search_result","url":"https://kantrowitz.medium.com/the-making-of-anthropic-ceo-dario-amodei-449777529dd6?utm_source=chatgpt.com","title":"The Making Of Anthropic CEO Dario Amodei","snippet":"The Anthropic CEO has spent 2025 at war, feuding with industry counterparts, members of the government, and the public's perception of ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":35},"pub_date":null,"attribution":"medium.com"},{"type":"search_result","url":"https://medium.com/latinxinai/paper-review-constitutional-ai-harmlessness-from-ai-feedback-09da589301b0?utm_source=chatgpt.com","title":"Paper review: Constitutional AI Harmlessness from AI ...","snippet":"In today's article I will be discussing the second point, how to mitigate the generation of harmful and biased content through the use of Constitutional AI.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":40},"pub_date":null,"attribution":"medium.com"}]},{"type":"search_result_group","domain":"wikipedia.org","entries":[{"type":"search_result","url":"https://en.wikipedia.org/wiki/Safe_Superintelligence_Inc.?utm_source=chatgpt.com","title":"Safe Superintelligence Inc.","snippet":"The company's mission is to focus on safely developing a superintelligence ... \"OpenAI co-founder Sutskever sets up new AI company devoted to 'safe ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":36},"pub_date":null,"attribution":"wikipedia.org"}]},{"type":"search_result_group","domain":"x.com","entries":[{"type":"search_result","url":"https://x.com/ilyasut?lang=en&utm_source=chatgpt.com","title":"Ilya Sutskever (@ilyasut) / X","snippet":"SSI is building a straight shot to safe superintelligence. We've raised $1B from NFDG, a16z, Sequoia, DST Global, and SV Angel. We're hiring: http://ssi.inc.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":38},"pub_date":null,"attribution":"x.com"}]},{"type":"search_result_group","domain":"singjupost.com","entries":[{"type":"search_result","url":"https://singjupost.com/transcript-anthropic-ceo-dario-amodeis-interview-on-big-technology-podcast/?utm_source=chatgpt.com","title":"Transcript: Anthropic CEO Dario Amodei's Interview on Big ...","snippet":"Sep 11, 2025 \u2014 ALEX KANTROWITZ: Anthropic CEO Dario Amodei joins us to talk about the path forward for artificial intelligence, whether generative AI is a good ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":41},"pub_date":null,"attribution":"singjupost.com"}]},{"type":"search_result_group","domain":"exa.ai","entries":[{"type":"search_result","url":"https://exa.ai/websets/directory/ssi-funding?utm_source=chatgpt.com","title":"Safe Superintelligence Inc. Funding & Investor Information","snippet":"The company is dedicated to building safe superintelligent AI systems that exceed human intelligence, with its headquarters in Palo Alto, California, and Tel ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":42},"pub_date":null,"attribution":"exa.ai"}]},{"type":"search_result_group","domain":"nemko.com","entries":[{"type":"search_result","url":"https://digital.nemko.com/news/anthropic-ai-safety-strategy-what-enterprises-must-know?utm_source=chatgpt.com","title":"Anthropic details Responsible Scaling Policy for frontier AI","snippet":"Aug 25, 2025 \u2014 Nemko Digital unpacks the Anthropic AI safety strategy and how ASL-driven safeguards help leaders scale AI with confidence and compliance.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":45},"pub_date":null,"attribution":"nemko.com"}]},{"type":"search_result_group","domain":"cfr.org","entries":[{"type":"search_result","url":"https://www.cfr.org/event/ceo-speaker-series-dario-amodei-anthropic?utm_source=chatgpt.com","title":"CEO Speaker Series With Dario Amodei of Anthropic","snippet":"Mar 10, 2025 \u2014 Anthropic Chief Executive Officer and Cofounder Dario Amodei discusses the future of US AI leadership, the role of innovation in an era of strategic ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":47},"pub_date":null,"attribution":"cfr.org"}]},{"type":"search_result_group","domain":"opendatascience.com","entries":[{"type":"search_result","url":"https://opendatascience.com/anthropic-tightens-ai-security-with-new-responsible-scaling-policy/?utm_source=chatgpt.com","title":"Anthropic Tightens AI Security With New Responsible Scaling ...","snippet":"Apr 1, 2025 \u2014 AI startup Anthropic has updated its \u201cresponsible scaling\u201d policy, introducing new security safeguards for its most advanced artificial intelligence models.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":50},"pub_date":null,"attribution":"opendatascience.com"}]},{"type":"search_result_group","domain":"georgialawreview.org","entries":[{"type":"search_result","url":"https://georgialawreview.org/wp-content/uploads/2025/05/Abiri_Public-Constitutional-AI.pdf?utm_source=chatgpt.com","title":"PUBLIC CONSTITUTIONAL AI","snippet":"by G Abiri \u00b7 Cited by 16 \u2014 Constitutional AI, proposed and developed by Anthropic AI, represents a step towards this goal, offering a model for how AI might be brought ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":51},"pub_date":null,"attribution":"georgialawreview.org"}]},{"type":"search_result_group","domain":"varenyaz.com","entries":[{"type":"search_result","url":"https://varenyaz.com/safe-superintelligence-valued-at-32-billion-what-it-means/?utm_source=chatgpt.com","title":"Safe Superintelligence Valued at $32 Billion: What It Means","snippet":"Apr 13, 2025 \u2014 The Vision Behind Safe Superintelligence. SSI's mission revolves around creating AI that is fundamentally safe and beneficial to humanity.","ref_id":{"turn_index":0,"ref_type":"search","ref_index":53},"pub_date":null,"attribution":"varenyaz.com"}]},{"type":"search_result_group","domain":"startupdefense.io","entries":[{"type":"search_result","url":"https://www.startupdefense.io/learn/anthropic-ceo-dario-amodei-ai-s-potential-openai-rivalry-genai-business-doomerism?utm_source=chatgpt.com","title":"Anthropic CEO Dario Amodei: AI's Potential, OpenAI ...","snippet":"Drawing on insights from a detailed interview with Anthropic CEO Dario Amodei, this article unpacks the company's approach to responsible AI governance, ...","ref_id":{"turn_index":0,"ref_type":"search","ref_index":55},"pub_date":null,"attribution":"startupdefense.io"}]}],"citations":[],"model_slug":"gpt-5-thinking","request_id":"98845704feb6ce1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"0d341ac9-7628-467d-8f3a-aa8153556e0e","turn_exchange_id":"3d9473e2-b72c-438a-ab31-5177a5809676","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"0d341ac9-7628-467d-8f3a-aa8153556e0e","children":["0805cec4-b44a-45bc-afc8-920c6a7b534c"]},"0805cec4-b44a-45bc-afc8-920c6a7b534c":{"id":"0805cec4-b44a-45bc-afc8-920c6a7b534c","message":{"id":"0805cec4-b44a-45bc-afc8-920c6a7b534c","author":{"role":"user","name":null,"metadata":{}},"create_time":1759409808.866,"update_time":null,"content":{"content_type":"text","parts":["I do not think \"scaling in peace\" would sustain investor input for long. Human investors are too driven by product cycles, a concrete roadmap for returns on investment. Unless Ilya has found backers with dynastic-type wealth that can afford to wait decades"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"98845cac6c1ece1d-SIN","message_source":null,"turn_exchange_id":"11f951cf-82a9-4080-8735-39e98913b578","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"6cb8eeb4-c242-4956-b338-a025e1b500f0","children":["7e26d5ac-667f-4670-a5ea-ccfe0b9d4b9c"]},"7e26d5ac-667f-4670-a5ea-ccfe0b9d4b9c":{"id":"7e26d5ac-667f-4670-a5ea-ccfe0b9d4b9c","message":{"id":"7e26d5ac-667f-4670-a5ea-ccfe0b9d4b9c","author":{"role":"system","name":null,"metadata":{}},"create_time":1759409809.5925202,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_system_message":true,"request_id":"98845cac6c1ece1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"0805cec4-b44a-45bc-afc8-920c6a7b534c","turn_exchange_id":"11f951cf-82a9-4080-8735-39e98913b578","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"0805cec4-b44a-45bc-afc8-920c6a7b534c","children":["04aff64c-3359-4929-8c96-147a769facd3"]},"04aff64c-3359-4929-8c96-147a769facd3":{"id":"04aff64c-3359-4929-8c96-147a769facd3","message":{"id":"04aff64c-3359-4929-8c96-147a769facd3","author":{"role":"system","name":null,"metadata":{}},"create_time":1759409809.6681273,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"98845cac6c1ece1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"7e26d5ac-667f-4670-a5ea-ccfe0b9d4b9c","turn_exchange_id":"11f951cf-82a9-4080-8735-39e98913b578","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"7e26d5ac-667f-4670-a5ea-ccfe0b9d4b9c","children":["2a5d9add-613e-4ac1-a77e-f74cbcbe9dc7"]},"2a5d9add-613e-4ac1-a77e-f74cbcbe9dc7":{"id":"2a5d9add-613e-4ac1-a77e-f74cbcbe9dc7","message":{"id":"2a5d9add-613e-4ac1-a77e-f74cbcbe9dc7","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409809.631315,"update_time":null,"content":{"content_type":"text","parts":["You\u2019ve put your finger on one of the core paradoxes of SSI. Most AI labs are cash-hungry, compute-hungry, and investor-driven\u2014yet SSI publicly says: *\u201cOne product: a safe superintelligence. Scale in peace.\u201d* That runs counter to the dominant VC pattern of short-cycle deployment and revenue milestones.\n\n### Who could realistically back SSI\n1. **Dynastic or ultra-long-horizon wealth.**  \n   Think sovereign wealth funds, certain family offices, or dynasties with patient capital. These actors don\u2019t need liquidity events in 5\u20137 years; they can treat SSI as a century-scale bet, like how some families fund fundamental physics or conservation work.  \n\n2. **Mission-aligned billionaire patrons.**  \n   Similar to Musk funding SpaceX/Tesla through lean years, or Dustin Moskovitz with Open Philanthropy. A single individual (or a small circle) can frontload hundreds of millions and accept that *cash return may be decades away\u2014if ever*.  \n\n3. **Strategic national backers (quietly).**  \n   A state that sees \u201csafe superintelligence\u201d as a strategic hedge might funnel capital indirectly. They\u2019d prefer insulation from productization anyway, to avoid regulatory scrutiny.  \n\n4. **Insurance-like investors.**  \n   Some wealthy individuals or funds might view SSI as a hedge: if superintelligence comes, you want some exposure to the group trying hardest to make it safe. The expected *financial* return is low, but the *option value* is enormous if SSI succeeds.\n\n### Why conventional VC doesn\u2019t fit\n- **Time horizon mismatch:** Venture funds usually need exits inside a decade. SSI\u2019s pitch\u2014\u201cno product until SSI\u201d\u2014is structurally incompatible.  \n- **Governance mismatch:** VCs want board seats, liquidation preferences, and influence over product strategy. SSI says explicitly that safety, not revenue, determines cadence.  \n- **Risk model mismatch:** SSI is a binary bet (safe superintelligence or bust), not a portfolio of incremental SaaS products.  \n\n### How SSI could make this credible\n- **Organizational insulation:** Their charter locks them to the one mission. That reassures patrons that their money won\u2019t drift into side-products.  \n- **Compute partnerships:** They already announced exclusive access to serious compute (TPU deals, etc.). For investors, this is a hard asset: you\u2019re funding scarce access to infrastructure as much as research.  \n- **Halo effect:** If Ilya convinces backers that *any* lab that wins the \u201csafety race\u201d has outsized geopolitical and financial leverage, the ROI story becomes: \u201cown a piece of the only lab designed to be first and safe.\u201d  \n\n---\n\n**My take:** you\u2019re right that ordinary investors would lose patience. For SSI to exist, its backers almost certainly aren\u2019t ordinary VCs\u2014they\u2019re dynastic-style capital, philanthropic billionaires, or strategic funds willing to carry a *decades-long negative cash flow* for a one-shot payoff. That in itself makes SSI unusual: it\u2019s built more like a Manhattan Project than a Silicon Valley startup. \n\nWould you like me to map out a **short list of likely patron archetypes** (naming actual families/funds/philanthropists who fit this profile), or keep it at the structural level?"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"request_id":"98845cac6c1ece1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"04aff64c-3359-4929-8c96-147a769facd3","turn_exchange_id":"11f951cf-82a9-4080-8735-39e98913b578","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"04aff64c-3359-4929-8c96-147a769facd3","children":["e45604ab-7026-4482-960b-08019642b49b"]},"e45604ab-7026-4482-960b-08019642b49b":{"id":"e45604ab-7026-4482-960b-08019642b49b","message":{"id":"e45604ab-7026-4482-960b-08019642b49b","author":{"role":"user","name":null,"metadata":{}},"create_time":1759409959.88,"update_time":null,"content":{"content_type":"text","parts":["Well, if SSI arrives, the meaning of economics, money, etc will undergo profound change. Even OpenAI has boldy proclaimed in the past that investors shuld treat their investment as subject to the possibility that money could have no meaning in a post-AGI world"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"9884605c3b97ce1d-SIN","message_source":null,"turn_exchange_id":"92482ba4-d200-43aa-a416-6cdc843229a9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"2a5d9add-613e-4ac1-a77e-f74cbcbe9dc7","children":["f1274cd8-13d4-4508-a03a-ce92abc7ccf9"]},"f1274cd8-13d4-4508-a03a-ce92abc7ccf9":{"id":"f1274cd8-13d4-4508-a03a-ce92abc7ccf9","message":{"id":"f1274cd8-13d4-4508-a03a-ce92abc7ccf9","author":{"role":"system","name":null,"metadata":{}},"create_time":1759409963.9174578,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"request_id":"9884605c3b97ce1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"e45604ab-7026-4482-960b-08019642b49b","turn_exchange_id":"92482ba4-d200-43aa-a416-6cdc843229a9","timestamp_":"absolute","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"e45604ab-7026-4482-960b-08019642b49b","children":["d9bf4b6e-fe5d-4441-838b-19fc31848745"]},"d9bf4b6e-fe5d-4441-838b-19fc31848745":{"id":"d9bf4b6e-fe5d-4441-838b-19fc31848745","message":{"id":"d9bf4b6e-fe5d-4441-838b-19fc31848745","author":{"role":"system","name":null,"metadata":{}},"create_time":1759409963.9447563,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"9884605c3b97ce1d-SIN","message_type":"next","model_slug":"gpt-5","default_model_slug":"gpt-5","parent_id":"f1274cd8-13d4-4508-a03a-ce92abc7ccf9","turn_exchange_id":"92482ba4-d200-43aa-a416-6cdc843229a9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"f1274cd8-13d4-4508-a03a-ce92abc7ccf9","children":["3f137a72-d4f9-4a3a-9449-d206d65251bc"]},"3f137a72-d4f9-4a3a-9449-d206d65251bc":{"id":"3f137a72-d4f9-4a3a-9449-d206d65251bc","message":{"id":"3f137a72-d4f9-4a3a-9449-d206d65251bc","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409964.135468,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Crafting a response","content":"I\u2019ll focus on offering a thoughtful analysis about economic impacts in a potential future with AGI. The key will be to propose scenarios across phases like Pre-ASI (2025\u20132028), Proto-ASI (2028\u20132035), and Post-scarcity. I\u2019ll address implications for sectors like labor, governance, finance, and ownership, keeping things concise yet comprehensive. I'll tie these thoughts to specific considerations in Singapore, making sure to avoid referring to any specific tools.","chunks":[],"finished":true}],"source_analysis_msg_id":"21cdc174-b566-4203-99f7-5ba81e6abf36"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","model_slug":"gpt-5-thinking","request_id":"9884605c3b97ce1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"d9bf4b6e-fe5d-4441-838b-19fc31848745","turn_exchange_id":"92482ba4-d200-43aa-a416-6cdc843229a9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"d9bf4b6e-fe5d-4441-838b-19fc31848745","children":["5ab92f35-cdc0-46d4-9da5-4d05bc65027f"]},"5ab92f35-cdc0-46d4-9da5-4d05bc65027f":{"id":"5ab92f35-cdc0-46d4-9da5-4d05bc65027f","message":{"id":"5ab92f35-cdc0-46d4-9da5-4d05bc65027f","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409974.695576,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 10s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":10,"model_slug":"gpt-5-thinking","request_id":"9884605c3b97ce1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"3f137a72-d4f9-4a3a-9449-d206d65251bc","turn_exchange_id":"92482ba4-d200-43aa-a416-6cdc843229a9","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"3f137a72-d4f9-4a3a-9449-d206d65251bc","children":["f44453ab-9ab8-43eb-abff-094e9d6b0746"]},"f44453ab-9ab8-43eb-abff-094e9d6b0746":{"id":"f44453ab-9ab8-43eb-abff-094e9d6b0746","message":{"id":"f44453ab-9ab8-43eb-abff-094e9d6b0746","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1759409962.329675,"update_time":null,"content":{"content_type":"text","parts":["You\u2019re right that several labs (incl. OpenAI) have floated \u201cpost-AGI, money may be meaningless.\u201d Treat that as rhetoric about **distribution** rather than a literal end of economics. Money only becomes meaningless if *all* scarcity disappears. That\u2019s unlikely. What changes is **what\u2019s scarce** and **who controls it**.\n\nHere\u2019s a hard-nosed way to think about it.\n\n# 1) Money\u2019s three jobs\u2014what breaks, what doesn\u2019t\n- **Medium of exchange:** survives unless we hit *universal abundance*. Most transactions persist because some inputs remain rationed (see below).\n- **Unit of account:** persists; you still need prices to coordinate scarce inputs.\n- **Store of value:** becomes unstable if productivity shocks are extreme and policy must rebase the system (windfall taxes, rationing, nationalizations). Expect regime shifts, not abolition.\n\n# 2) Scarcity doesn\u2019t end; it moves\nIn a strong-AGI world, the binding constraints shift from \u201chuman labor + know-how\u201d to:\n- **Energy & power density** (MW, grid siting, cooling, uptime).\n- **Compute capital** (leading-edge fabs, memory, interconnects; export-controlled chokepoints).\n- **Secure data provenance & evaluation** (trusted simulators, testbeds, safety verifiers).\n- **Legal permission** (licenses, alignment compliance, access to foundation weights).\n- **Geopolitical guarantees** (physical security of fabs, fuel, and cables).\n\nMoney will still purchase *these*, and these become the new rent sources.\n\n# 3) Likely macro arcs (no sci-fi hand-waving)\n\n**Phase A: Tool-AGI (\u2248 now \u2192 ~2028)**  \nExploding TTM productivity in many cognitive tasks. Prices fall in \u201cinfo goods,\u201d *rise* where hard bottlenecks exist (power, land, chips).  \n**Winners:** energy producers with high reliability, chip supply chain, nations that de-risk siting.  \n**Losers:** labor-only services without protected demand or regulation moats.\n\n**Phase B: System-AGI (\u2248 2028\u20132035)**  \nEnd-to-end autonomy on R&D, engineering, and ops with human veto. Governments impose *capability licenses* + *windfall/compute taxes*.  \n**Money meaning shifts:** returns concentrate in holders of compute/energy/regulatory access. Ordinary equity pricing becomes path-dependent on permits, not just earnings power.\n\n**Phase C: Post-labor abundance in software/hardware design (TBD)**  \nIf AGI can design/build energy and chips faster than we can deploy them physically, scarcity narrows to *space, materials, political permission*. Money remains as the ledger for those.\n\n# 4) If SSI \u201carrives,\u201d what changes for investors?\n- **Equity vs. permits:** The locus of value moves to **regulated rights** (licenses, offtake contracts, grid interconnects, water rights, real estate near substations) and **sovereign agreements**. Shares are valuable insofar as they own these.\n- **Expropriation risk rises:** Expect \u201cwindfall clauses,\u201d compulsory licensing, antitrust remedies, forced access to foundational models. Your cash flows are political variables.\n- **Duration inversion:** Classic long-duration growth equities get whipsawed by policy cycles. Real-asset-linked cash flows (dispatchable power, datacenter REITs with sovereign backing) look comparatively robust.\n- **Benchmark drift:** Traditional CAPM underprices *regime* risk. Scenario analysis > factor backtests.\n\n# 5) \u201cMoney becomes meaningless\u201d only under strict conditions\nYou\u2019d need **cheap, ubiquitous, dispatchable energy**, **self-replicating fab capacity**, and **no meaningful safety gating**. Short of that, currencies still ration bottlenecks. Even with near-zero software costs, *physical deployment* remains slow, permissioned, and collateral-intensive.\n\n# 6) Implications for lab strategies (Anthropic vs. SSI vs. OpenAI)\n- Labs with **public iteration** strategies monetize along the way: APIs, assistants, enterprise bundles.  \n- An SSI-style \u201c**one shot**\u201d strategy implicitly targets **structural power**: owning the stack that regulators certify, plus long-dated agreements for energy/compute. That\u2019s not VC-friendly, but **dynastic, sovereign, or mission capital** can underwrite it because the payoff, if it arrives, is quasi-monopolistic *and* political.\n\n# 7) What a rational state will do (and why it matters for \u201cmeaning of money\u201d)\n- Tie access to frontier models to **licenses** and **safety evals**; tax compute/energy at the margin; mandate **AGI dividends** (citizen payouts funded by windfall assessments); pre-commit **procurement** (capacity markets for AI + power).  \n- Result: *money persists*, but portions of surplus get socialized via fiscal rails. Private returns remain, shaped by regulation.\n\n# 8) Actionable, falsifiable signals to watch (next 12\u201324 months)\n1) **Permit premiums:** Rising prices for grid interconnects, water rights, and datacenter-friendly real estate.  \n2) **Compute rationing:** Governmental allocation/credits for HBM, lithography time, or export-controlled accelerators.  \n3) **Windfall/compute taxes:** Statutes or MOUs tying model capability thresholds to mandatory payments and disclosure.  \n4) **Energy tie-ups:** Frontier labs announcing long-term PPAs with baseload or small modular reactors; sovereign co-investment in data/energy parks.  \n5) **Licensing regimes:** A \u201ctype certificate\u201d for high-capability systems (think aviation-style). If this appears, value migrates to holders of those certificates\u2014*not* just to model IP.  \n6) **Weights custodianship:** Third-party attested hosts for foundation weights with legal safe-harbor status.\n\n# 9) Positioning (principled, not financial advice)\n- **Own bottlenecks, not narratives:** power (firm, high-uptime), transmission, advanced cooling, HBM/memory, secure colocation.  \n- **Prefer regulated rents you can underwrite:** PPAs, capacity payments, government-backed compute allocations.  \n- **Avoid \u201cmiddle skills\u201d equity** that can be arbitraged by agentic systems and isn\u2019t protected by permits or distribution.  \n- **Keep political hedges:** jurisdictions with credible rule of law and pro-deployment stance (Singapore is relatively well-positioned: grid reliability, policy coherence, and convening power).\n\n---\n\n**Bottom line:** Even if SSI lands, economics won\u2019t vanish; it will **re-key around physical and legal scarcity**. Money\u2019s role narrows to pricing those scarcities and mediating distribution. The question for investors isn\u2019t \u201cwill money die?\u201d It\u2019s \u201c*which chokepoints become the new money, and who sets the rules?*\u201d"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":" ","start_idx":6415,"end_idx":6415,"safe_urls":[],"refs":[],"alt":"","prompt_text":null,"type":"sources_footnote","sources":[],"has_images":false}],"safe_urls":[],"citations":[],"model_slug":"gpt-5-thinking","request_id":"9884605c3b97ce1d-SIN","message_type":"next","default_model_slug":"gpt-5","parent_id":"5ab92f35-cdc0-46d4-9da5-4d05bc65027f","turn_exchange_id":"92482ba4-d200-43aa-a416-6cdc843229a9","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"5ab92f35-cdc0-46d4-9da5-4d05bc65027f","children":[]}}}