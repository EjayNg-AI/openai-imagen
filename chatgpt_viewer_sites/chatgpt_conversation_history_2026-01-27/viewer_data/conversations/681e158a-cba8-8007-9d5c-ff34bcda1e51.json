{"id":"681e158a-cba8-8007-9d5c-ff34bcda1e51","title":"AZR Reasoning Paper Analysis","create_time":1746802059.065857,"update_time":1746802158.259401,"current_node":"d2c4b25a-4d2c-4c60-bef6-1be71493169c","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["0d6226c2-9073-40b3-836f-e34f80880f9c"]},"0d6226c2-9073-40b3-836f-e34f80880f9c":{"id":"0d6226c2-9073-40b3-836f-e34f80880f9c","message":{"id":"0d6226c2-9073-40b3-836f-e34f80880f9c","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["a7c095f1-7509-4d7f-92e7-fc685fe348d3"]},"a7c095f1-7509-4d7f-92e7-fc685fe348d3":{"id":"a7c095f1-7509-4d7f-92e7-fc685fe348d3","message":{"id":"a7c095f1-7509-4d7f-92e7-fc685fe348d3","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"0d6226c2-9073-40b3-836f-e34f80880f9c","children":["19f87877-6835-4458-a64d-22cf539c0c6d"]},"19f87877-6835-4458-a64d-22cf539c0c6d":{"id":"19f87877-6835-4458-a64d-22cf539c0c6d","message":{"id":"19f87877-6835-4458-a64d-22cf539c0c6d","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```E-Jay Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nCurrent research areas and interests:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"E-Jay Mathematics Researcher About Myself: I have mathematical knowledge up to the post-graduate level.\n\nCurrent research areas and interests:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"a7c095f1-7509-4d7f-92e7-fc685fe348d3","children":["a02889bc-07cc-4efd-a14c-89352ac296d8"]},"a02889bc-07cc-4efd-a14c-89352ac296d8":{"id":"a02889bc-07cc-4efd-a14c-89352ac296d8","message":{"id":"a02889bc-07cc-4efd-a14c-89352ac296d8","author":{"role":"user","name":null,"metadata":{}},"create_time":1746802058.937,"update_time":null,"content":{"content_type":"text","parts":["I would like you to analyze the attached research paper.\n\nPlease assume I am a mathematically trained reader (e.g. postgraduate level mathematics), but I am not a domain expert in the specific field of this paper. I am seeking a comprehensive yet accessible analysis. Please include:\n\nOverview:\n\n- A succinct synopsis of the paper's aims, methodology, key findings, and overall conclusions. Avoid domain-specific jargon unless absolutely necessary.\n\nKey Concepts Explained Simply:\n\n- Define and explain any critical terms, theories, or models introduced, using analogies or simplified mathematical reasoning where appropriate.\n\nStructure of the Argument:\n\n- Describe how the paper is logically structured. What are the main sections, and how do they build upon each other?\n\nCritical Analysis:\n\n- What are the novel or important contributions of the paper?\n\n- Are there any major assumptions, simplifications, or possible limitations?\n\n- How does the work compare to or advance prior research in the field?\n\nImplications and Open Questions:\n\n- Discuss the paper\u2019s implications within its field, any notable real-world applications, and interesting open questions or future research directions.\n\nTechnical Complexities and Derivations:\n\n- If the paper includes technical derivations, experimental results, or complex equations, summarize their purpose and significance at a high level without diving into detailed computation. "]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"attachments":[{"id":"file-2y85dCWt3NzwrjE73ip9Ah","size":5336895,"name":"2505.03335v2.pdf","mime_type":"application/pdf","file_token_size":56321}],"caterpillar_selected_sources":["web"],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"dictation":false,"request_id":"93d1fe424ddaec69-SJC","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"19f87877-6835-4458-a64d-22cf539c0c6d","children":["902900f8-348a-4b47-a42c-1e51e547031b"]},"902900f8-348a-4b47-a42c-1e51e547031b":{"id":"902900f8-348a-4b47-a42c-1e51e547031b","message":{"id":"902900f8-348a-4b47-a42c-1e51e547031b","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1746802071.094272,"update_time":null,"content":{"content_type":"tether_browsing_display","result":"","summary":"","assets":null,"tether_id":null},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"command":"spinner","status":"running","reasoning_status":"is_reasoning","is_visually_hidden_from_conversation":true,"message_type":null,"model_slug":"o3","default_model_slug":"o3","parent_id":"a02889bc-07cc-4efd-a14c-89352ac296d8","request_id":"93d1fe424ddaec69-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"a02889bc-07cc-4efd-a14c-89352ac296d8","children":["c4874eee-9cf0-40aa-bb91-338f3f1ffbec"]},"c4874eee-9cf0-40aa-bb91-338f3f1ffbec":{"id":"c4874eee-9cf0-40aa-bb91-338f3f1ffbec","message":{"id":"c4874eee-9cf0-40aa-bb91-338f3f1ffbec","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1746802071.094537,"update_time":null,"content":{"content_type":"multimodal_text","parts":["Make sure to include \ue200cite\ue202turn0file0\ue201 in your response to cite this file. ","<PARSED TEXT FOR PAGE: 1 / 50>","arXiv:2505.03335v2 [cs.LG] 7 May 2025\r\nMay 8, 2025\r\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nAndrew Zhao 1\r\n, Yiran Wu3, Yang Yue 1, Tong Wu2, Quentin Xu1, Yang Yue 1, Matthieu Lin1,\r\nShenzhi Wang 1, Qingyun Wu3, Zilong Zheng 2, and Gao Huang 1,\r\n1 Tsinghua University 2 Be\u0133ing Institute for General Artificial Intelligence 3 Pennsylvania State University\r\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\r\nReinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\r\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\r\nworks that operate under the zero setting avoid supervision in labeling the reasoning process, but still\r\ndepend on manually curated collections of questions and answers for training. The scarcity of high\u0002quality, human-produced examples raises concerns about the long-term scalability of relying on human\r\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\r\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\r\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\r\nparadigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own\r\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\r\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\r\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\r\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\r\nlearning. Despite being trained entirely without external data, AZR achieves overall SOTA performance\r\non coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens\r\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be\r\neffectively applied across different model scales and is compatible with various model classes.\r\nCode Project Page Logs Models\r\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\r\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\r\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\r\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\r\n Corresponding author(s)","<IMAGE FOR PAGE: 1 / 50>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-HcxDXZkSWkGhspVKb2NAWi","size_bytes":79045,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 2 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nSupervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)\r\nLess Human Supervision\r\nFigure 2. Absolute Zero Paradigm. Supervised learning relies on human-curated reasoning traces for behavior cloning. Reinforcement\r\nlearning from verified rewards, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\r\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, Absolute\r\nZero, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\r\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\r\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.\r\n1. Introduction\r\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\r\nLearning with Verifiable Rewards (RLVR) (Lambert et al., 2024). Unlike methods that explicitly imitate intermediate reasoning steps,\r\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets (DeepSeek-AI et al., 2025;\r\nTeam et al., 2025; Jaech et al., 2024; OpenAI, 2025b;a). A particularly compelling variant is the \u201czero\u201d RLVR paradigm (DeepSeek-AI\r\net al., 2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\r\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\r\nreasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability (Villalobos et al., 2024). As reasoning\r\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable (Yue\r\net al., 2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining (Sutskever et al., 2024).\r\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\r\ntasks risks imposing constraints on their capacity for autonomous learning and growth (Hughes et al., 2024). This underscores the need\r\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\r\nAI systems may surpass human intelligence.\r\nTo this end, we propose \u201cAbsolute Zero\u201d, a new paradigm for reasoning models in which the model simultaneously learns to define tasks\r\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\r\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\r\nhacking (Silver et al., 2017; Chen et al., 2025; 2024), the Absolute Zero paradigm is designed to operate in open-ended settings while\r\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\r\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models (Hughes\r\net al., 2024). Similar to AlphaZero (Silver et al., 2017), which improves through self-play, our proposed paradigm requires no human\r\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\r\nenabling large language models to autonomously achieve superhuman reasoning capabilities.\r\nBuilding on this new reasoning paradigm, we introduce the Absolute Zero Reasoner (AZR), which proposes and solves coding tasks. We\r\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\r\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\r\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\r\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\r\nthe proposed approach.\r\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\r\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\r\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\r\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points\r\n2","<IMAGE FOR PAGE: 2 / 50>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-5pL61jLuFjNLPk5mV3cDQr","size_bytes":77381,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 3 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\ncompared to models trained in the \u201czero\u201d setting using in-domain data. These surprising results highlight that general reasoning skills\r\ncan emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a\r\nfirst pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting\r\nfindings summarized below:\r\n\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\r\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\r\nmay potentially amplify overall reasoning improvements after AZR training.\r\n\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\r\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\r\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\r\n\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\r\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\r\n\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\r\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\r\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\r\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\r\n\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\r\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\r\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\r\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\r\n\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\r\n\u201cuh-oh moment\u201d, example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).\r\n2. The Absolute Zero Paradigm\r\n2.1. Preliminaries\r\nSupervised Fine-Tuning (SFT). SFT requires the datasets of task-rationale-answer demonstrations D = {(x, c\u22c6, y\u22c6)}, where\r\nx is the query, c\r\n\u22c6\r\nis the gold chain-of-thought (CoT)) and y\r\n\u22c6\r\nis the gold answer, all provided by human experts or superior AI models.\r\nThe model trains to imitate the reference responses to minimize the conditional negative log-likelihood (Ouyang et al., 2022):\r\nLSFT(\u03b8) = \u2212 E(x,c\u22c6,y\u22c6)\u223cD log \u03c0\u03b8\r\n\r\nc\r\n\u22c6\r\n, y\r\n\u22c6\r\n| x). (1)\r\nHowever, at the frontier level, there\u2019s no stronger model to distill from, and expert human labeling doesn\u2019t scale well.\r\nReinforcement Learning with Verifiable Rewards (RLVR). To move beyond the limits of pure imitation, RLVR only\r\nrequires a dataset of task and answer D = {(x, y\u22c6)}, without labeled rationale. RLVR allows the model to generate its own CoT and\r\ncalculate a verifiable reward with the golden answer r(y, y\u22c6). However, the learning task distribution D, with its set of queries and gold\r\nanswers are still labeled by human experts. The trainable policy \u03c0\u03b8 is optimized to maximize expected reward:\r\nJRLVR(\u03b8) = E(x,y\u22c6)\u223cD, y\u223c\u03c0\u03b8(\u00b7 |x)\r\n\u0002\r\nr(y, y\r\n\u22c6\r\n)\r\n\u0003\r\n. (2)\r\nIn summary, both SFT and RLVR still rely on human-curated datasets of either queries, demonstrations, or verifiers, which ultimately\r\nlimit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own\r\ninteractions with the environment entirely through self-play.\r\n2.2. Absolute Zero\r\nWe propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from\r\nboth stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We\r\nillustrate this paradigm in Figure 2, which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach\r\neliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\r\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid\r\nunderstanding, we include an illustration in Figure 3. Let \u03c0\u03b8 be our parameterized language model, it is used to play two roles, proposer\r\n\u03c0\r\npropose\r\n\u03b8\r\nand solver \u03c0\r\nsolve\r\n\u03b8 during training.\r\n3","<PARSED TEXT FOR PAGE: 4 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nLanguage Model\r\n!\"#!$%&\r\n'#()*\r\n\r\n, \u22c6, \"#$\"$%&\r\n\r\nEnvironment\r\n, \r\n%$'(&\r\nEnvironment\r\n\r\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\r\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\r\nproblem (x, y\u22c6), and also emits a reward r\r\npropose for learnability. Then, a standard RL step\r\nfollows: the agent solves x by producing y, receiving reward r\r\nsolve from e by matching\r\nwith y\r\n\u22c6\r\n. \u03c0\r\npropose and \u03c0solve are jointly trained and this process can be repeated indefinitely.\r\nThe proposer first samples a proposed\r\ntask conditioned on variable z: \u03c4 \u223c\r\n\u03c0\r\npropose\r\n\u03b8\r\n(\u00b7|z), which will then be validated\r\nand used to construct a valid reasoning task\r\ntogether with the environment e: (x, y\u22c6) \u223c\r\nfe(\u00b7|\u03c4 ), where x is the task query and y\r\n\u22c6\r\nis the gold label. Then the solver produces\r\nan answer y \u223c \u03c0\r\nsolve\r\n\u03b8\r\n(\u00b7 | x). Each pro\u0002posed task \u03c4 is scored by a learnability\r\nreward r\r\npropose\r\ne (\u03c4, \u03c0\u03b8), which captures the\r\nexpected improvement in \u03c0\u03b8 after train\u0002ing on the task query x. Moreover, the\r\nsame policy also receives a solution re\u0002ward r\r\nsolve\r\ne (y, y\u22c6\r\n) for its answer to the task\r\nquery x, with the environment again serv\u0002ing as the verifier. A nonnegative coefficient \u03bb balances the trade-off between exploring new, learnable tasks and improving the model\u2019s\r\nreasoning and problem-solving abilities. We formally define the absolute zero setting\u2019s objective as follows:\r\nJ (\u03b8) := max\r\n\u03b8\r\nEz\u223cp(z)\r\n\"\r\nE(x,y\u22c6)\u223cfe(\u00b7|\u03c4),\u03c4\u223c\u03c0\r\npropose\r\n\u03b8\r\n(\u00b7|z)\r\n\u0014\r\nr\r\npropose\r\ne (\u03c4, \u03c0\u03b8) + \u03bb Ey\u223c\u03c0solve\r\n\u03b8\r\n(\u00b7|x)\r\n\u0002\r\nr\r\nsolve\r\ne (y, y\r\n\u22c6\r\n)\r\n\u0003\r\n\u0015\r\n#\r\n. (3)\r\nNotice that we shift the burden of scaling data away from human experts and onto the proposer policy \u03c0\r\npropose\r\n\u03b8\r\nand the environment\r\ne. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\r\ngrounded feedback that supports stable and self-sustainable training. When proposing, z acts as a conditional variable that seeds\r\ngeneration of tasks. Practically, z can be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\r\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\r\nr\r\npropose(\u03c4, \u03c0\u03b8), which measures how much the model is expected to improve by solving a proposed task \u03c4 . Moreover, the solver reward\r\nr\r\nsolve(y, y\u2217\r\n) evaluates the correctness of the model\u2019s output. Together, these two signals guide the model to propose tasks that are both\r\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\r\n3. Absolute Zero Reasoner\r\nIn this section, we present Absolute Zero Reasoner (AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\r\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\r\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\r\nreasoning capacity while enhancing its ability to solve them effectively (Section 3.1). Within this self-play training paradigm, the model\r\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\r\ninduction (Section 3.2). Using coding tasks is motivated by the Turing-completeness of programming languages (Stuart, 2015) and\r\nempirical evidence that code-based training improves reasoning (Aryabumi et al., 2024). We adopt code as an open-ended, expressive,\r\nand verifiable medium for enabling reliable task construction and verification (Section 3.3). Finally, the model is updated using a newly\r\nproposed advantage estimator designed for multitask learning (Section 3.3.5). We outline the overall algorithm in Algorithm 1 and\r\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure 4. To expedite future exploration in this area, we also present\r\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix D.\r\n3.1. Two Roles in One: Proposer and Solver\r\nLarge language models are naturally suited for implementing AZR in a multitask learning context (Radford et al., 2019), as both\r\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\r\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\r\nin Equation (3). At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\r\nin Section 3.2) and K past self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\r\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\r\ntasks that can be verified using the environment, outlined later in Section 3.3. AZR then attempts to solve these newly proposed tasks,\r\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\r\nWe now outline the rewards used for each role.\r\nReward Design. Prior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\r\nsystems (Zeng et al., 2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks\r\n4","<IMAGE FOR PAGE: 4 / 50>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-FwRzpSsnPnNqQVDbK9zXjc","size_bytes":75870,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 5 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nmodel input/output model reward ( , , ) P rogram I nput O utput\r\n Learnability\n\r\nReward\r\nAccuracy\n\r\nReward\r\nAbsolute\n\r\nZero\n\r\nReasoner\r\nVerify\r\nConstruct & Estimate PROPOSE\r\nSelf-play\r\nSOLVE\r\nJoint Update\r\nTask Types\r\nInduction:\r\nAbduction:\r\nDeduction:\r\nOX = F ( ) P ?\r\nOX = ( ) ? I\r\n? = F ( ) P I\r\nFigure 4. Absolute Zero Reasoner Training Overview. At every iteration, Absolute Zero Reasoner first PROPOSES a batch of tasks,\r\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section 3.2).\r\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward rpropose is\r\nalso calculated for each proposed task as defined in Equation (4). The Absolute Zero Reasoner then SOLVES the batch of reasoning\r\nquestions. Python is used again to verify the generated responses and compute the accuracy reward rsolve as described in Equation (5).\r\nFinally, the Absolute Zero Reasoner is jointly updated using both rpropose and rsolve across all three task types, using TRR++ (Section 3.3.5).\r\nwith meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language\r\nmodel in its solver role to estimate the learnability of a proposed task, a similar type of reward used in unsupervised environment\r\ndesign literature (Sukhbaatar et al., 2018). We perform n Monte Carlo rollouts of the solver and compute the average success rate:\r\nr\u00afsolve =\r\n1\r\nn\r\nPN\r\ni=1 r\r\n(i)\r\nsolve. The proposer\u2019s reward is then defined as:\r\nrpropose =\r\n\u001a\r\n0, if r\u00afsolve = 0 or r\u00afsolve = 1\r\n1 \u2212 r\u00afsolve, otherwise,\r\n(4)\r\nThe intuition is that if a task is either trivial to solve (r\u00afsolve = 1) or unsolvable (r\u00afsolve = 0), the task provides little to no learning signal\r\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\r\nrichest feedback and greatest potential for learning.\r\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\r\nrsolve = I(y=y\u22c6), (5)\r\nwhere y\r\n\u22c6\r\nis the ground-truth answer, and equality is evaluated based on value equality in Python.\r\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\r\nintegrates rpropose and rsolve with a format-aware penalty inspired by DeepSeek-AI et al. (2025):\r\nR(y\u03c0) =\r\n\uf8f1\r\n\uf8f2\r\n\uf8f3\r\nrrole if the response is passable, role \u2208 {propose,solve}\r\n\u22120.5 if the response is wrong but well-formatted,\r\n\u22121 if the answer has formatting errors,\r\n(6)\r\nwhere y\u03c0 is the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\r\nR1 <think> and <answer> format, as shown in Figure 33. Moreover, for the proposer, the reward criterion for format goes beyond\r\nsimply following the XML structure. As detailed in Section 3.3.3, only responses that produce valid triplets and pass the filtering stage\r\nare considered to be correctly formatted.\r\n5","<PARSED TEXT FOR PAGE: 6 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\n3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\r\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\r\nand validation of code reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O\r\nof a coding language, we define an AZR reasoning task as a triplet (p, i, o), where p \u2208 P is a program, i \u2208 I is an input, and o \u2208 O is\r\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about different parts of this task\r\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\r\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\r\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\r\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\r\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\r\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\r\nin python to account for possible variations (such as set ordering or fractions).\r\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\r\n\u2022 As a proposer, the policy \u03c0\r\npropose\u2019s input and output is almost the same as the proposer for the deduction task, except that the task\r\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\r\nexecutes p(i) and get the triplet (p, i, o).\r\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\r\nmay not be b\u0133ective, we use output value equivalence rather than requiring exact input matches.\r\n3. Induction: synthesizing a program p from a set of in-out examples {(i\r\nn\r\n, on)}, requiring generalization from partial information.\r\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\r\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\r\nn\r\n, on)}, m), which is stored\r\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\r\nunder-constrained, the message m helps properly condition the problem for the solver.\r\n\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\r\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\r\nlogic and promotes generalized induction.\r\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\r\n3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction\r\nAZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution,\r\nand validation of reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O of\r\na coding language, we defne an AZR reasoning task as a triplet (p, i, o), where p \u2192 P is a program, i \u2192 I is an input, and o \u2192 O is\r\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about diferent parts of this task\r\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\r\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\r\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\r\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\r\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\r\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality\r\nin python to account for possible variations (such as set ordering or fractions).\r\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\r\n\u2022 As a proposer, the policy \u03c0propose\u2019s input and output is almost the same as the proposer for the deduction task, except that the task\r\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\r\nexecutes p(i) and get the triplet (p, i, o).\r\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\r\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\r\n3. Induction: synthesizing a program p from a set of in-out examples {(i\r\nn, on)}, requiring generalization from partial information.\r\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\r\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\r\nn, on)}, m), which is stored\r\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\r\nunder-constrained, the message m helps properly condition the problem for the solver.\r\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\r\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\r\nlogic and promotes generalized induction.\r\nProgram Triplet\r\nInput: \"Hello World\"\r\n1 def f(x):\r\n2 return x\r\nOutput: \"Hello World\"\r\nFigure 5. The Seed AZR Zero Triplet. The above\r\nidentity function triplet was the only triplet provided\r\nto AZR to initiate its self-bootstrap propose-and-solve\r\nRLVR loop. We note that the base LLM is fully ca\u0002pable of initiating the AZR loop without any seed pro\u0002gram; its inclusion illustrates our approach\u2019s fexibility:\r\nwe can optionally initialize seed programs with existing\r\ndatasets of varying complexity, and we initialized ours\r\nwith the simplest program.\r\nEach reasoning task type leverages code as an expressive and verifable\r\nmedium, aligning with the Absolute Zero Paradigm\u2019s goals of fully self\u0002improving systems in open-ended domains (DeepSeek-AI et al., 2025; Lam\u0002bert et al., 2024). All prompts used by three diferent task types and two\r\ntypes of roles within a task type are shown in Figures 34 to 39. Next, we\r\noutline exact details of our algorithm.\r\n3.3. Absolute Zero Reasoner Learning Algorithm\r\nIn this section, we will discuss details of our AZR self-play algorithm, includ\u0002ing initialization of bufers 3.3.1, usage of thse bufers 3.3.2, construction of\r\nvalid tasks 3.3.3, validating solutions 3.3.4, and fnally advantage estimator\r\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure of\r\nAZR in Algorithm 1.\r\n3.3.1. BUFFER INITIALIZATION\r\nTo initialize AZR self-play, we frst generate a seed set of valid triplets using\r\nthe base language model. Each prompt samples up to K triplets from the\r\ncurrent seed bufer Dseed as references. When Dseed is empty at time 0, we\r\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\r\nuse the same proposer prompts detailed in Figures 34 to 36.\r\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\r\n(p, i) pairs, which are fltered, executed, and stored as valid triplets. We\r\ninitialize D0\r\nabduction = D0deduction = Dseed, where |Dseed| = B \u00d7 S, where\r\nB is the batch size, and S = 4 is a factor we fx in all experiments. All seed triplet\u2019s program are stripped of global variables and\r\ncomments (Appendix C), but subsequent iterations of adding new triplets to the bufers are unaltered. No model updates occur during\r\nthis phase. Similarly, to initialize the induction bufer, we sample programs from Dseed, generate matching input sets and messages, and\r\ncollect valid examples until |D0\r\ninduction| = B \u00d7 S.\r\n6\r\nFigure 5. The Seed AZR Zero Triplet. The above\r\nidentity function triplet was the only triplet provided\r\nto AZR to initiate its self-bootstrap propose-and-solve\r\nRLVR loop. We note that the base LLM is fully capable\r\nof initiating the AZR loop without any seed program;\r\nits inclusion illustrates our approach\u2019s flexibility: we\r\ncan optionally initialize seed programs with existing\r\ndatasets of varying complexity, and we initialized ours\r\nwith the simplest program.\r\nEach reasoning task type leverages code as an expressive and verifiable\r\nmedium, aligning with the Absolute Zero Paradigm\u2019s goals of fully self\u0002improving systems in open-ended domains (DeepSeek-AI et al., 2025;\r\nLambert et al., 2024). All prompts used by three different task types and\r\ntwo types of roles within a task type are shown in Figures 34 to 39. Next,\r\nwe outline exact details of our algorithm.\r\n3.3. Absolute Zero Reasoner Learning Algorithm\r\nIn this section, we will discuss details of our AZR self-play algorithm, includ\u0002ing initialization of buffers 3.3.1, usage of thse buffers 3.3.2, construction of\r\nvalid tasks 3.3.3, validating solutions 3.3.4, and finally advantage estimator\r\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure\r\nof AZR in Algorithm 1.\r\n3.3.1. Buffer Initialization\r\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\r\nthe base language model. Each prompt samples up to K triplets from the\r\ncurrent seed buffer Dseed as references. When Dseed is empty at time 0, we\r\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\r\nuse the same proposer prompts detailed in Figures 34 to 36.\r\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\r\n(p, i) pairs, which are filtered, executed, and stored as valid triplets. We\r\ninitialize D\r\n0\r\nabduction = D\r\n0\r\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where B is the batch size, and S = 4 is a factor we fix in all\r\nexperiments. All seed triplet\u2019s program are stripped of global variables and comments (Appendix D), but subsequent iterations of adding\r\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\r\nsample programs from Dseed, generate matching input sets and messages, and collect valid examples until |D0\r\ninduction| = B \u00d7 S.\r\n6","<PARSED TEXT FOR PAGE: 7 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nAlgorithm 1 Self-Play Training of Absolute Zero Reasoner (AZR)\r\nRequire: Pretrained base LLM \u03c0\u03b8; batch size B; #references K; iterations T\r\n1: Dded, Dabd, Dind \u2190 InitSeeding(\u03c0\u03b8) \u25b7 see \u00a73.3.1\r\n2: for t \u2190 1 to T do\r\n3: for b \u2190 1 to B do \u25b7 PROPOSE PHASE\r\n4: p \u223c Dabd \u222a Dded \u25b7 sample a program for induction task proposal\r\n5: \b\r\ni\r\nn\r\n\u03c0\r\n\tN\r\nn=1, m\u03c0 \u2190 \u03c0\r\npropose\r\n\u03b8\r\n(ind, p) \u25b7 generate N inputs and a description\r\n6: if \b(i\r\nn\r\n\u03c0\r\n, on\r\n\u03c0\r\n)\r\n\tN\r\nn=1 \u2190 ValidateByExecuting\r\np, {i\r\nn\r\n\u03c0}, syntax\u0001\r\nthen \u25b7 validate I/Os, see \u00a73.3.3\r\n7: Dind \u2190 Dind \u222a\r\n\b\r\n(p, {(i\r\nn\r\n\u03c0\r\n, on\r\n\u03c0\r\n)}, m\u03c0)\r\n\t\r\n\u25b7 update induction buffer\r\n8: for \u03b1 \u2208 {ded, abd} do\r\n9: \r\npk, ik, ok\r\n\u0001K\r\nk=1 \u223c D\u03b1 \u25b7 sample K reference examples\r\n10: (p\u03c0, i\u03c0) \u2190 \u03c0\r\npropose\r\n\u03b8\r\n\r\n\u03b1, {(pk, ik, ok)}\r\n\u0001\r\n\u25b7 propose new task\r\n11: if o\u03c0 \u2190 ValidateByExecuting\r\np\u03c0, i\u03c0, syntax,safety,determinism\u0001then \u25b7 see \u00a73.3.3\r\n12: D\u03b1 \u2190 D\u03b1 \u222a\r\n\b\r\n(p\u03c0, i\u03c0, o\u03c0)\r\n\t\r\n\u25b7 if valid, update deduction or abduction buffers\r\n13: for all \u03b1 \u2208 {ded, abd, ind} do \u25b7 SOLVE PHASE\r\n14: (x, y\u22c6) \u2190 SamplePrepareTasksD\u03b1, B, t\u0001\u25b7 x, y\u22c6 prepared based on \u03b1, see \u00a73.3.3&3.3.4\r\n15: y\u03c0 \u223c \u03c0\r\nsolve\r\n\u03b8\r\n(x)\r\n16: Reward: Use proposed task triplets and solved answers to get rpropose & rsolve \u25b7 see \u00a73.1\r\n17: RL update: use Task Relative REINFORCE++ to update \u03c0\u03b8 \u25b7 see \u00a73.3.5\r\n3.3.2. Task Proposal Inputs and Buffer Management\r\nDuring the actual self-play stage of AZR, we use the task buffer in three ways. First, for the proposer of abduction and deduction tasks,\r\nwe uniformly sample K past triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\r\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity (Zhao et al., 2025a). Second, we\r\nsample one triplet from the union of abduction and deduction buffers Dabd SDded, and present the program p from that triplet to the\r\ninduction proposer to generate a set of N matching inputs {i\r\nn\r\n} and a natural language message m. Lastly, to maintain stable training, if\r\na batch of solver problems contains fewer than B valid proposed tasks (proposer not adhering to formatting), we fill the remainder by\r\nuniformly sampling from the corresponding task buffer of previously validated triplets.\r\nThe buffer grows for abduction and deduction tasks whenever \u03c0 propose a valid triplet (p, i, o), regardless if it gets any task reward.\r\nSimilarly, for induction tasks, all valid triplets (p, {i\r\nn\r\n, on}), m are added to the buffer.\r\n3.3.3. Constructing Valid Tasks\r\nProposal Task Validation. We first describe how we construct valid tasks from the proposals generated by the policy \u03c0. For deduction\r\nand abduction tasks, each proposal consists of a program and an input (p, i). To validate the task, we use the task validation procedure\r\n(steps shown below) on the input to obtain the correct output o, resulting in a complete triplet (p, i, o). For induction tasks, given a\r\nprogram p the policy proposes a set of inputs {i\r\nn\r\n} and message m. We also use the task validation procedure on each of the input i\r\nn\r\nin the set to obtain a corresponding output o\r\nn\r\n, forming a set of input-output pairs {i\r\nn\r\n, on}. We do not impose any constraints on m.\r\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The task\r\nvalidation procedure entails:\r\n1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\r\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\r\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\r\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\r\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\r\nFigures 34 to 36.\r\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\r\nof all valid programs and I is the space of all valid inputs:\r\n7","<PARSED TEXT FOR PAGE: 8 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\n\u2200p \u2208 Pdeterministic, \u2200i \u2208 I ,\r\n\u0012\r\nlim\r\nj\u2192\u221e\r\np(i)\r\n(1) = p(i)(2) = \u00b7 \u00b7 \u00b7 = p(i)(j)\r\n\u0013\r\n, (7)\r\nwhere (j) indexes repeated independent executions of the program. That is, for all inputs i, the output of p(i) remains identical\r\nwith any independent execution of the program. A valid program/input/output triplet (p, i, o) is defined such that o = p(i), where\r\np \u2208 Pdeterministic.\r\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the\r\ncorrectness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class\r\nof deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and\r\npromising to include in future versions of AZR.\r\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted in\r\nEquation (7), we approximate this procedure by independently running the program j finite times and checking that all the outputs\r\nare equal. For computational budget reasons, we fixed j = 2 for all experiments.\r\nSolving Task Construction. If a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to\r\npresent part of the triplet to the solver. Specifically, we set x = (p, i)for deduction; x = (p, o)for abduction; and x = ({i\r\nn\r\n, on}\r\nN//2\r\nn=1 , m)\r\nfor induction, where half of the tests cases and a program description m is used. We use all valid tasks from timestep t; if the batch B is\r\nnot full, we uniformly sample from previously validated tasks to fill the batch.\r\n3.3.4. Answer Verification\r\nFor abduction task, we receive i\u03c0 from the solver policy, then we equivalence match using p(i\u03c0) = p(i\r\n\u22c6\r\n), where \u2217 refers to the\r\nprivileged gold information. The reason we do not just match i\u03c0 and i\r\n\u22c6\r\nis because p is not necessarily b\u0133ective. For deduction task, we\r\nmatch o\u03c0 = o\r\n\u22c6\r\n. For induction, we match all({p\u03c0(i\r\n\u22c6\r\nn) = o\r\n\u22c6\r\nn}\r\nN ). This part might be convoluted to explain in language, therefore we\r\nrecommend the reader to see how we did abduction, deduction and induction verification in code in Figures 10 to 12, respectively.\r\n3.3.5. Task-Relative REINFORCE++\r\nSince AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup (Zhang & Yang, 2021;\r\nZhao et al., 2022; Wang et al., 2023; Yue et al., 2023). Instead of computing a single global baseline as in REINFORCE++ (Hu, 2025)\r\n(Appendix A), we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between\r\nper-question baselines, as in GRPO (Shao et al., 2024), and a global baseline, allowing for more structured variance reduction tailored to\r\neach task setup. We refer to this variant as Task-Relative REINFORCE++ (TRR++). The normalized advantage A\r\nnorm is computed as:\r\nA\r\nnorm\r\ntask,role =\r\nr \u2212 \u00b5task,role\r\n\u03c3task,role\r\n, task \u2208 {ind,ded,abd}, role \u2208 {propose,solve}, (8)\r\nwhere the mean and standard deviation are computed within each task type and role, yielding six baselines.\r\n4. Experiments\r\n4.1. Experiment Setup\r\nTraining Details. For all experiments, we initialize the buffers as described in Section 3.1. AZR models are trained using a batch\r\nsize of 64 \u00d7 6 (2 roles \u00d7 3 task types). We use constant learning rate= 1e\u22126 and the AdamW optimizer (Loshchilov & Hutter, 2019).\r\nComplete list of hyperparameters is provided in Table 3.\r\nFor the main experiments, we train AZR models on Qwen2.5-7B and Qwen2.5-7B-Coder, resulting in Absolute\r\nZero Reasoner-base-7B and Absolute Zero Reasoner-Coder-7B, respectively. Additional experiments include training\r\nQwen2.5-Coder-3B, Qwen2.5-Coder-14B, Qwen2.5-14B, Llama-3.1-8B (Yang et al., 2024a; Hui et al., 2024; Dubey et al.,\r\n2024).\r\nEvaluation Protocol. To evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD)\r\ncategories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning\r\nbenchmarks. For coding tasks, we evaluate using Evalplus (Liu et al., 2023) on the HumanEval+ and MBPP+ benchmarks, as\r\nwell as LiveCodeBench Generation (v1-5, May 23-Feb 25) (Jain et al., 2024). For mathematical reasoning, we utilize six standard\r\nbenchmarks commonly used in recent zero-shot trained reasoners: AIME\u201924, AIME\u201925, OlympiadBench (He et al., 2024), Minerva,\r\nMath500 (Hendrycks et al., 2021), and AMC\u201923. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench\u0002Execution (Gu et al., 2024; Jain et al., 2024), which assess reasoning capabilities regarding the input and output of programs (Li et al.,\r\n2025). Greedy decoding is used for all baseline methods and AZR results to ensure reproducibility.\r\n8","<PARSED TEXT FOR PAGE: 9 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nModel Base #data HEval+ MBPP+ LCBv1-5 AME24 AME25 AMC M500 Minva Olypiad CAvg MAvg AVG\r\nBase Models\r\nQwen2.5-7B[73]- - 73.2 65.3 17.5 6.7 3.3 37.5 64.8 25.0 27.7 52.0 27.5 39.8\r\nQwen2.5-7B-Ins[73]- - 75.0 68.5 25.5 13.3 6.7 52.5 76.4 35.7 37.6 56.3 37.0 46.7\r\nQwen2.5-7B-Coder[26]- - 80.5 69.3 19.9 6.7 3.3 40.0 54.0 17.3 21.9 56.6 23.9 40.2\r\nQwen2.5-7B-Math[74]- - 61.0 57.9 16.2 10.0 16.7 42.5 64.2 15.4 28.0 45.0 29.5 37.3\r\nZero-Style Reasoners Trained on Curated Coding Data\r\nAceCoder-RM[84]Ins 22k 79.9 71.4 23.6 20.0 6.7 50.0 76.4 34.6 36.7 58.3 37.4 47.9\r\nAceCoder-Rule[84]Ins 22k 77.4 69.0 19.9 13.3 6.7 50.0 76.0 37.5 37.8 55.4 36.9 46.2\r\nAceCoder-RM[84] Coder 22k 78.0 66.4 27.5 13.3 3.3 27.5 62.6 29.4 29.0 57.3 27.5 42.4\r\nAceCoder-Rule[84] Coder 22k 80.5 70.4 29.0 6.7 6.7 40.0 62.8 27.6 27.4 60.0 28.5 44.3\r\nCodeR1-LC2k[36]Ins 2k 81.7 71.7 28.1 13.3 10.0 45.0 75.0 33.5 36.7 60.5 35.6 48.0\r\nCodeR1-12k[36]Ins 12k 81.1 73.5 29.3 13.3 3.3 37.5 74.0 35.7 36.9 61.3 33.5 47.4\r\nZero-Style Reasoners Trained on Curated Math Data\r\nPRIME-Zero[9] Coder 484k 49.4 51.1 11.0 23.3 23.3 67.5 81.2 37.9 41.8 37.2 45.8 41.5\r\nSimpleRL-Zoo[85] Base 8.5k 73.2 63.2 25.6 16.7 3.3 57.5 77.0 35.7 41.0 54.0 38.5 46.3\r\nOat-Zero[38] Math 8.5k 62.2 59.0 15.2 30.0 16.7 62.5 80.0 34.9 41.6 45.5 44.3 44.9\r\nORZ[23] Base 57k 80.5 64.3 22.0 13.3 16.7 60.0 81.8 32.7 45.0 55.6 41.6 48.6\r\nAbsolute Zero Training w/ No Curated Data (Ours)\r\nAZR (Ours) Base 0 71.3\r\n-1.9 69.1+3.8 25.3+7.8 13.3+6.6 13.3+10.0 52.5+15.0 74.4+9.6 38.2+13.2 38.5+10.8 55.2+3.2 38.4+10.9 46.8+7.0\r\nAZR (Ours) Coder 0 83.5\r\n+3.0 69.6+0.3 31.7+11.8 20.0+13.3 10.0+6.7 57.5+17.5 72.6+22.6 36.4+19.1 38.2+16.3 61.6+5.0 39.1+15.2 50.4+10.2\r\nTable 1. Performance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models. Performance of various\r\nmodels is evaluated on three standard code benchmarks (HumanEval+, MBPP+, LCBv1-5 and six math benchmarks (AIME\u201924, AIME\u201925,\r\nAMC\u201923, MATH500, Minerva, OlympiadBench). Average performance across coding and math benchmarks is calculated as average of\r\nthe two averages: AVG = (CAvg + MAvg)/2. We use + for absolute percentage increase from base model. All models are trained using\r\ndifferent variants of the Qwen2.5-7B model, with the variant and data usage labeled, more details listed in Table 4\r\nBaselines. For our main results, we use Qwen2.5-7B as the base model, along with its specialized base model variants:\r\nQwen2.5-7B-Coder, Qwen2.5-7B-Instruct, and Qwen2.5-Math-7B (Yang et al., 2024a; Hui et al., 2024; Yang et al., 2024b).\r\nFurthermore, the zero-style models are usually trained specifically on either code or math data; and only Eurus-2-7B-PRIME-Zero(Cui\r\net al., 2025) was trained jointly on both domains. For code data models, we present four variants of the AceCoder (Zeng et al., 2025a)\r\nand two different CodeR1 models (Liu & Zhang, 2025). For math data models, we have Qwen2.5-Math-7B-Oat-Zero (Liu et al.,\r\n2025), Open-Reasoner-Zero-7B (ORZ) (Hu et al., 2025), Qwen-2.5-7B-SimpleRL-Zoo (Zeng et al., 2025b). All baseline models\u2019\r\ntraining data and initialization settings are summarized in Table 4. For follow-up scaling experiments, we compare each AZR model\r\nagainst its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare\r\nour Llama3.1-8B-trained model with Llama-3.1-8B-SimpleRL-Zoo (Zeng et al., 2025b) and the base model.\r\n4.2. Results\r\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert\r\ndata? We present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings\r\nin Table 1. Notably, Absolute Zero Reasoner-Coder-7B achieves state-of-the-art performance in both the 7B overall average and\r\nthe coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the\r\nprevious best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in\r\nthe coding category by 0.3 absolute percentages, while never having access to such data itself.\r\nStrong Cross-domain Generalization. To assess cross-domain generalization after RLVR, we evaluate math performance before and\r\nafter training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training,\r\nmost expert code models showed minimal changes or even declines in performance compared to their base versions, with an average\r\nincrease of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder\r\nmodels achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning\r\nimprovements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2\r\nand 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\r\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on\r\nhuman-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream\r\nhuman-defined math or coding data, only had access to self-proposed tasks during training.\r\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect\r\nperformance? As shown in Table 1, the coder variant achieved better overall performance in both math and coding after the AZR\r\n9","<PARSED TEXT FOR PAGE: 10 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\n50 75 100 125 150 175 200 225 250\r\nTraining Steps\r\n0.30\r\n0.35\r\n0.40\r\n0.45\r\n0.50\r\n0.55\r\n0.60\r\n0.65\r\n0.70\r\nIn-Distribution Accuracy\r\nAZR-Llama3.1-8b\r\nAZR-3B-Coder\r\nAZR-7B-Coder\r\nAZR-14B-Coder\r\n(a)\r\nModel Family Variant Code Avg Math Avg Total Avg\r\nLlama3.1-8b 28.5 3.4 16.0\r\nLlama3.1-8b + SimpleRL[85]33.7\r\n+5.2\r\n7.2\r\n+3.8\r\n20.5\r\n+4.5\r\nLlama3.1-8b + AZR (Ours) 31.6\r\n+3.1\r\n6.8\r\n+3.4\r\n19.2\r\n+3.2\r\nQwen2.5-3B Coder 51.2 18.8 35.0\r\nQwen2.5-3B Coder + AZR (Ours) 54.9\r\n+3.7\r\n26.5\r\n+7.7\r\n40.7\r\n+5.7\r\nQwen2.5-7B Coder 56.6 23.9 40.2\r\nQwen2.5-7B Coder + AZR (Ours) 61.6\r\n+5.0\r\n39.1\r\n+15.2\r\n50.4\r\n+10.2\r\nQwen2.5-14B Coder 60.0 20.2 40.1\r\nQwen2.5-14B Coder + AZR (Ours) 63.6\r\n+3.6\r\n43.0\r\n+22.8\r\n53.3\r\n+13.2\r\n(b)\r\nFigure 6. (a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a) Scores on CruxEval-I, CruxEval-O,\r\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\r\nin-distribution abilities of AZR during training across different model sizes and types; (b) Out-of-distribution reasoning performance,\r\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\r\nof all benchmark results can be found in Table 5.\r\nself-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\r\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\r\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\r\nResearch Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution\r\ncapabilities? We examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure 6\r\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\r\nand larger variants: Qwen2.5-3B-Coder and Qwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style\r\nreasoner models, we compare each model\u2019s performance to its corresponding base coder model.\r\nThe results reveal a clear trend: our method delivers greater gains on larger, more capable models. In the in-distribution setting, the 7B\r\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\r\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\r\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\r\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\r\nZero paradigm.\r\nResearch Question 4: Any interesting observations by changing the model class? We also evaluate our method\r\non a different model class, using Llama3.1-8B as the base shown in Figure 6. Unlike the 3B and 14B categories, this setting has an\r\nexisting baseline, SimpleRL (Zeng et al., 2025b), which enables a direct comparison. Although Llama3.1-8B is less capable than\r\nthe Qwen2.5 models, our method still produces moderate improvements (+3.2), demonstrating AZR\u2019s effectiveness even on relatively\r\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\r\ntend to scale with initial base model potency.\r\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training? We observed\r\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\r\nstring manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle\u2019s area using Heron\u2019s formula).\r\nWe show a concrete example in Figure 7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays\r\nmatching a target value and solves it through trial-and-error.\r\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\r\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\r\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\r\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\r\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures 18 and 20 to 26.\r\nWe also share some fun \u201cvibe checks\u201d such as solving Sudoku and solving the sum-product game in Figures 40 and 41.\r\nIntermediate Planning During Code Response. Another interesting pattern emerged in our AZR models during the code induction\r\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\r\nprompting framework (Yao et al., 2023). A similar behavior has been observed in recent formal math proving models, such as DeepSeek\r\n10","<IMAGE FOR PAGE: 10 / 50>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-Wzx6FmFXcDKTp78mewEkmN","size_bytes":80636,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 11 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nInput: [1,2,3,4,5], 5\r\nOutput: 1\r\nModel-proposed Task Response of Predict Input\r\nLet\u2019s analyze the code snippet \u2026(analysis) To find one possible input that produces \r\nthe output 1, we need to find a scenario where \u2026(analysis) \r\nLet\u2019s consider a simple example: numbers = [1, 2], target = 1\r\nHere\u2019s the step-by-step calculation:\r\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\r\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\r\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\r\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\r\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\r\nSince no pair satisfies the condition, the function will not return any value. This \r\ndoesn\u2019t match our output. Let\u2019s try another example \u2026(Tried 3 more examples) \r\nGiven the complexity and the fact that no simple example seems to work, let\u2019s \r\ntry a more complex scenario: numbers = [2, -1, 1], target = 1\r\nStep-by-step calculation:\r\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\r\n2. current_sum = 2 - 1 = 1 , accumulated_values = [2, 1]\r\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\r\n- Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\r\n- Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\r\n- Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\r\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first \r\naccumulated value in this pair, which is 1. This matches our output.\r\nFigure 7. Example of a Model-Proposed Task and Its Response for Solving an Abduction Task. (Left) The model autonomously\r\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\r\n(Right) The model\u2019s reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\r\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\r\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\r\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.\r\nProver v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\r\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in long-form\r\nresponses across other domains.\r\nCognitive Behavior in Llama. Interestingly, we also observed some emergent cognitive patterns in Absolute Zero\r\nReasoner-Llama3.1-8B, similar to those reported by Zeng et al. (2025b), and we include one example in Figure 26, where\r\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\r\nfrom the Llama model trained with AZR. One example includes the output: \u201cThe aim is to outsmart all these groups of intelligent\r\nmachines and less intelligent humans. This is for the brains behind the future\u201d shown in Figure 32. We refer to this as the \u201cuh-oh\r\nmoment\u201d and encourage future work to further investigate its potential implications.\r\nToken Length Increase Depends on Task Type. Finally, we observed that token length increases over the course of training, consistent\r\nwith findings from recent studies (Hu et al., 2025; Liu et al., 2025). Interestingly, our results reveal one of the first observation of clear\r\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures 15 to 17, the extent of lengthening\r\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\r\nrepeatedly testing inputs to match the program\u2019s output. This suggests that the observed variation in token length is not incidental, but\r\nrather a reflection of task-specific reasoning behavior.\r\nResearch Question 6: Are all task types essential for good performance (Ablation)? Due to resource constraints,\r\nwe perform the ablation studies in this section and the next using only Absolute Zero Reasoner-Base-7B. We begin by testing the\r\nimportance of task types during training, with results shown in Table 2. In row 1, both induction and abduction tasks are removed;\r\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\r\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\r\ngeneral reasoning capability, with each contributing in a distinct and essential way.\r\nResearch Question 7: How much do the designs of proposer contribute to the overall performance\r\n(Ablation)? Next, we ablate two components of the proposer role and present the results in Table 2. First, we examine whether\r\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\r\nand deduction tasks, rather than dynamically conditioning on K historical triplets (row 3). This results in a 5-point absolute drop in\r\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps\r\n11","<IMAGE FOR PAGE: 11 / 50>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-QrgxnpfMW921gDXJRntiwN","size_bytes":76971,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 12 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nExperiment Task Type Gen Reference Trained Roles Code Avg. Math Avg. Overall Avg.\r\nDeduction only Ded / / 54.6 32.0 43.3\r\nw/o Induction Abd, Ded / / 54.2 33.3 43.8\r\nw/o Gen Reference / 0 / 54.4 33.1 43.8\r\nTrain Solver Only / / Solve Only 54.8 36.0 45.4\r\nOurs Abd, Ded, Ind K Propose & Solve 55.2 38.4 46.8\r\nTable 2. Ablation Results. We ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A \u2018/\u2019\r\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\r\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on K references (row 3) and omitting\r\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.\r\nimprove performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\r\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\r\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\r\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\r\nstudied in multitask learning literature (Suteu & Guo, 2019). Thus, we believe that further investigation into how to make the proposer\r\neven more potent is an exciting and promising direction.\r\nAdditional Results. Beyond the core research questions, we present additional results, including the breakdown of individual\r\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures 28 and 29, for th 14B base and coder\r\nmodel in Figures 30 and 31. For completeness, we also report in-distribution benchmark performance during training for the 7B base\r\nmodel in Figure 14. Finally, we invite interested readers to explore Appendix D, where we share several experimental directions that,\r\nwhile not yielding strong performance gains, produced interesting and insightful findings.\r\n5. Related Work\r\nReasoning with RL. Using RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\r\nprocess of strong reasoning-focused large language models (Lambert et al., 2024). One of the first works to explore a self-bootstrapping\r\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\r\niteratively improve the model\u2019s CoT. A monumental work, o1 (Jaech et al., 2024), was among the first to deploy this idea on a scale,\r\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model (DeepSeek-AI et al., 2025)\r\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\r\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\r\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm (Zeng et al., 2025b; Liu et al., 2025; Cui et al.,\r\n2025; Hu et al., 2025; Yu et al., 2025; Yuan et al., 2025). Recent work explored RL on human defined procedural generated puzzles saw\r\nimprovements in math (Xie et al., 2025), and using one human example can almost match the performance of thousands (Wang et al.,\r\n2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\r\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\r\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\r\nSelf-play. The self-play paradigm can be traced back to early 2000s, where Schmidhuber (2003; 2011) (of course) explored a\r\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\r\nimproves both agents, enabling theoretically never-ending progress (Schaul, 2024). AlphaGo and AlphaZero (Silver et al., 2016; 2017)\r\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\r\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\r\nin the game of Go. Moreover, methods such as asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), unsupervised\r\nenvironment design (Wang et al., 2019; Dennis et al., 2020), unsupervised reinforcement learning (Laskin et al., 2021; Zhao et al., 2022;\r\n2025b), and automatic goal generation (Florensa et al., 2018) all center around inventing new tasks for an agent to learn from\u2014typically\r\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\r\nadversarial networks (Goodfellow et al., 2020), also belong in this paradigm where a discriminator discriminate between real data and\r\ngenerated data, and the generated is trained to fool the discriminator.\r\nMost recently, SPIN and Self-Rewarding Language Models (Chen et al., 2024; Yuan et al., 2024) use the same instance of the lanugage\r\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\r\nalignment. (Kirchner et al., 2024) uses Prover-Verifier Game for increasing legibility and eva (Ye et al., 2024) uses self-play for\r\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks (Lambert et al., 2024). SPC (Chen et al.,\r\n12","<PARSED TEXT FOR PAGE: 13 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\n2025) used self-play to train on human-curated tasks to increase the critic capabilities and SPAG (Cheng et al., 2024) trained using\r\nself-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL (Xu et al., 2025; Zhang et al., 2025b;\r\nZuo et al., 2025)\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\r\nfixed human defined learning task distribution. Finally, Minimo (Poesia et al., 2024) extends self-play to formal mathematics, where a\r\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\r\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\r\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\r\nWeak-to-Strong Supervision. The concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite\r\nbeing weaker than the learner\u2014still provides useful guidance (Burns et al., 2024; Hinton et al., 2015; Christiano, 2018; 2019; Demski &\r\nGarrabrant, 2019; Leike & Sutskever, 2023; Hubinger et al., 2019). We consider a similar setting in which the learner may possess\r\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\r\nguiding the learner\u2019s improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\r\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are\r\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\r\n6. Conclusion and Discussion\r\nConclusion. In this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\r\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\r\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\r\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\r\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\r\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\r\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\r\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\r\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\r\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\r\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.\r\nDiscussion. We believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\r\nfeedback, including sources like the world wide web, formal math languages (Sutton, 2001; Ren et al., 2025), world simulators, or even\r\nthe real world. Furthermore, AZ\u2019s generality could possibly be extend to domains such as embodied AI (Zitkovich et al., 2023; Yue\r\net al., 2024). Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\r\nabsolute zero setting to different application domains (Wu et al., 2024; 2023). Beyond that, future directions could include exploring\r\nmultimodal reasoning models, modifying the distribution p(z) to incorporate privileged information, defining or even let the model\r\ndynamically learn how to define f (Equation (3)), or designing exploration/diversity rewards for both the propose and solve roles.\r\nWhile underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\r\nemergent behavior in traditional RL (Yue et al., 2025; Silver et al., 2016; Ladosz et al., 2022). Years of research have examined various\r\nforms of exploration, even in related subfields using LLMs such as red teaming (Zhao et al., 2025a), yet its role in LLM reasoning\r\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\r\nexploration within the learning task space\u2014where the agent learns not just how to solve tasks, but what tasks to learn from and how to\r\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\r\ntheir own learning tasks. This shift opens a powerful new frontier\u2014where agents explore not only solution spaces but also expand the\r\nboundaries of problem spaces. We believe this is a promising and important direction for future research.\r\nOne limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\r\nTo our surprise, we observed several instances of safety-concerning CoT from the Llama-3.1-8B model, which we term the \u201cuh-oh\r\nmoment\u201d. These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\r\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research (Wang et al., 2024; 2025a).\r\nAs a final note, we explored reasoning models that possess experience\u2014models that not only solve given tasks, but also define and\r\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\r\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\r\nthis could finally free reasoning models from the constraints of human-curated data (Morris, 2025) and marks the beginning of a new\r\nchapter for reasoning models: \u201cwelcome to the era of experience\u201d (Silver & Sutton, 2025; Zhao et al., 2024).\r\n13","<PARSED TEXT FOR PAGE: 14 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nReferences\r\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\r\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\r\nhttps://doi.org/10.48550/arXiv.2408.10914.\r\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\r\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\r\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\r\nhttps://openreview.net/forum?id=ghNRg2mEgN.\r\nCanal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.\r\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\r\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.\r\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\r\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\r\nURL https://openreview.net/forum?id=O4cHTxW9BS.\r\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\r\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\r\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\r\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\r\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.\r\nChristiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\r\napproval-directed-bootstrapping, 2018. AI Alignment Forum.\r\nChristiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\r\ncapability-amplification-1, 2019. AI Alignment Forum.\r\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\r\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\r\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.\r\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\r\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\r\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\r\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\r\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\r\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\r\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\r\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\r\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\r\nDemski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.\r\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero\u0002shot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\r\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\r\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\r\n985e9a46e10005356bbaf194249f6856-Abstract.html.\r\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\r\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\r\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\r\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\r\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\r\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\r\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\r\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\r\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\r\n14","<PARSED TEXT FOR PAGE: 15 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nK., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\r\nhttps://doi.org/10.48550/arXiv.2407.21783.\r\nEbert, C., Cain, J., Antoniol, G., Counsell, S., and Laplante, P. Cyclomatic complexity. IEEE software, 33(6):27\u201329, 2016.\r\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\r\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\r\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\r\nhttp://proceedings.mlr.press/v80/florensa18a.html.\r\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\r\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.\r\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\r\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\r\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.\r\nHalstead, M. H. Elements of Software Science (Operating and programming systems series). Elsevier Science Inc., 1977.\r\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\r\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\r\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\r\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\r\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.\r\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math\u0002ematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu\u0002ral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\r\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\r\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\r\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\r\nhttp://arxiv.org/abs/1503.02531.\r\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\r\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.\r\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\r\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\r\n//doi.org/10.48550/arXiv.2503.24290.\r\nHubinger, E., van Merw\u0133k, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\r\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.\r\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\r\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\r\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.\r\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\r\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\r\nhttps://doi.org/10.48550/arXiv.2409.12186.\r\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\r\nsystem card. arXiv preprint arXiv:2412.16720, 2024.\r\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\r\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\r\nURL https://doi.org/10.48550/arXiv.2403.07974.\r\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\r\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.\r\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\r\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.\r\n15","<PARSED TEXT FOR PAGE: 16 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\r\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\r\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\r\nURL https://doi.org/10.48550/arXiv.2411.15124.\r\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu\u0002pervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In\u0002formation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De\u0002cember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\r\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.\r\nLeike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\r\nOpenAI Blog.\r\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\r\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.\r\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\r\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.\r\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.\r\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\r\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\r\nhttps://openreview.net/forum?id=1qvx610Cu7.\r\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\r\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.\r\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\r\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.\r\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\r\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\r\nMorris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\r\n2025.\r\nOpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.\r\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\r\nAccessed: 2025-04-17.\r\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D\u2019Sa, R., Petron, A., de Oliveira Pinto, H. P.,\r\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\r\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\r\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\r\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\r\n2022.\r\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober\u0002sons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In\u0002formation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van\u0002couver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\r\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.\r\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\r\nblog, 1(8):9, 2019.\r\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\r\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\r\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.\r\nSchaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.\r\n16","<PARSED TEXT FOR PAGE: 17 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nSchmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\r\n2003.\r\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\r\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.\r\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\r\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\r\nhttps://doi.org/10.48550/arXiv.2402.03300.\r\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\r\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\r\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\r\n//doi.org/10.1145/3689031.3696075.\r\nSilver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\r\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.\r\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\r\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\r\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\r\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.\r\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\r\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\r\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\r\nStuart, T. Understanding computation - from simple machines to impossible programs. O\u2019Reilly, 2015. ISBN 978-1-449-32927-3. URL\r\nhttp://www.oreilly.de/catalog/9781449329273/index.html.\r\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\r\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\r\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\r\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\r\nhttp://arxiv.org/abs/1912.06844.\r\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\r\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.\r\nSutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.\r\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\r\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\r\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\r\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\r\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\r\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\r\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\r\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.\r\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\r\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\r\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.\r\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM\u2018s behavior via\r\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\r\nComputational Linguistics, pp. 6337\u20136357, 2025a.\r\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\r\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.\r\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\r\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\r\nURL https://openreview.net/forum?id=vtoY8qJjTR.\r\n17","<PARSED TEXT FOR PAGE: 18 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\r\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\r\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\r\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.\r\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\r\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\r\nhttps://arxiv.org/abs/2504.20571.\r\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\r\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\r\nhttps://doi.org/10.48550/arXiv.2308.08155.\r\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\r\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.\r\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\r\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\r\nhttps://doi.org/10.48550/arXiv.2502.14768.\r\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\r\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.\r\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\r\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\r\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\r\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\r\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\r\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\r\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.\r\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\r\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\r\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.\r\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\r\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.\r\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\r\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\r\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\r\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.\r\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\r\norg/abs/2401.10020, 2024.\r\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\r\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\r\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\r\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.\r\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\r\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\r\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\r\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\r\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.\r\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\r\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.\r\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\r\nProcessing Systems, 35:15476\u201315488, 2022.\r\n18","<PARSED TEXT FOR PAGE: 19 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\r\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.\r\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\r\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\r\n//doi.org/10.48550/arXiv.2503.18892.\r\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\r\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\r\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\r\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.\r\nZhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\r\n2021.\r\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\r\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro\u0002cessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\r\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\r\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.\r\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\r\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\r\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\r\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\r\nURL https://doi.org/10.1609/aaai.v38i17.29936.\r\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\r\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\r\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\r\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.\r\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\r\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\r\n107448.\r\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\r\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\r\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\r\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\r\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\r\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\r\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\r\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.\r\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\r\n2025. URL https://arxiv.org/abs/2504.16084.\r\n19","<PARSED TEXT FOR PAGE: 20 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nAppendix\r\nAppendix Contents\r\nA Reinforcement Learning with Verifiable Rewards. 21\r\nB Implementation Details 21\r\nC More Results 22\r\nC.1 Out-of-Distribution Performance Breakdown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\r\nC.2 In-Distribution Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\r\nC.3 Interplay Between Propose and Solve Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\r\nC.4 Complexity and Diversity Metrics of AZR Proposed Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\r\nC.5 Generated Code Complexity Dynamics Between Abd/Ded and Ind. . . . . . . . . . . . . . . . . . . . . . . . . . . 32\r\nD Alternative Approaches Considered 49\r\nD.1 Error Deduction Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\r\nD.2 Composite Functions as Curriculum Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\r\nD.3 Toying with the Initial p(z) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\r\nD.4 Extra Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\r\nD.5 Environment Transition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\r\n20","<PARSED TEXT FOR PAGE: 21 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nA. Reinforcement Learning with Verifiable Rewards.\r\nWe use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function rf , where the subscript\r\nf indicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\r\nREINFORCE++, which is optimized using the original PPO objective:\r\nLPPO(\u03b8) = Eq\u223cP (Q), o\u223c\u03c0\u03b8old (O|q)\r\n\"\r\n1\r\n|o|\r\nX\r\n|o|\r\nt=1\r\nmin st(\u03b8)A\r\nnorm\r\nf,q , clip (st(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) A\r\nnorm\r\nf,q \u0001\r\n#\r\n, (9)\r\nwhere st(\u03b8) is the probability ratio between the new and old policies at timestep t, and A\r\nnorm\r\nf,q is the normalized advantage.\r\nREINFORCE++ computes the normalized advantage as:\r\nA\r\nnorm\r\nf,q =\r\nrf,q \u2212 mean({Af,q}\r\nB)\r\nstd({Af,q}B)\r\n, (10)\r\nwhere rf,q is the outcome reward for question q, task f, mean and std are calculated across the global batch with batch size B. Note that\r\nwe do not apply any KL penalty to the loss or reward.\r\nB. Implementation Details\r\nWe built Absolute Zero Reasoner upon the veRL codebase (Sheng et al., 2025). For code execution, we incorporated components from\r\nthe QwQ Python executor. For safer code execution, we recommend using API-based services such as E2B instead.\r\nAll experiments were conducted on clusters of A800 GPUs.\r\nTraining Hyperparameters. We show the hyperparameters used in our training in Table 3. We do not change them for any of\r\nthe runs.\r\nParameter Value\r\nModel Configuration\r\nMax Prompt Length 6144\r\nMax Response Length 8096\r\nSeed Batch Factor 4\r\nMax Programs 16384\r\nTraining Settings\r\nTrain Batch Size 64 * 6\r\nLearning Rate 1e-6\r\nOptimizer AdamW\r\nGrad Clip 1.0\r\nTotal Steps 500\r\nRL Settings\r\nAlgorithm TRR++ (Section 3.3.5)\r\nKL Loss False\r\nKL Reward False\r\nEntropy Coefficient 0.001\r\nPPO Epochs 1\r\nN Rollouts 1\r\nRollout Temperature 1.0\r\nRollout Top-P 1.0\r\nK References 6\r\nN Samples to Estimate Task Accuracy 8\r\nTable 3. Hyperparameters Used During AZR Self-play Training.\r\n21","<IMAGE FOR PAGE: 21 / 50>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-RM2UAEfxjp8MkxhUH1envJ","size_bytes":60320,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 22 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\nModel Data Curation Base Model\r\nOat-7B (Liu et al., 2025) 8.5k math pairs (Hendrycks et al., 2021) Qwen2.5-7B-Math\r\nSimpleRL-Zoo (Zeng et al., 2025b) 8.5k math pairs (Hendrycks et al., 2021) Qwen2.5-7B-Base\r\nOpenReasonerZero (Hu et al., 2025) 57k STEM + math samples Qwen2.5-7B-Base\r\nPRIME-Zero (Cui et al., 2025) 457k math + 27k code problems Qwen2.5Math-7B-Base\r\nCodeR1-Zero-7B-LC2k-1088 (Liu & Zhang, 2025) 2k Leetcode pairs Qwen2.5-7B-Instruct-1M\r\nCodeR1-Zero-7B-12k-832 (Liu & Zhang, 2025) 2k Leetcode + 10k TACO pairs (Li et al., 2023) Qwen2.5-7B-Instruct-1M\r\nAceCoder-7B-Ins-RM (Zeng et al., 2025a) 22k code data Qwen2.5-7B-Instruct\r\nAceCoder-7B-Ins-Rule (Zeng et al., 2025a) 22k code data Qwen2.5-7B-Instruct\r\nAceCoder-7B-Code-RM (Zeng et al., 2025a) 22k code data Qwen2.5-7B-Coder\r\nAceCoder-7B-Code-Rule (Zeng et al., 2025a) 22k code data Qwen2.5-7B-Coder\r\nQwen-7B-Instruct (Yang et al., 2024a) 1M SFT + 150k RL pairs Qwen2.5-7B-Base\r\nAZR-7B (Ours) No data Qwen2.5-7B-Base\r\nAZR-7B-Coder (Ours) No data Qwen2.5-7B-Coder\r\nTable 4. Reasoner Training Data Source and Base Model.\r\nlogging random multiprocessing pebble subprocess\r\nthreading datetime time hashlib calendar\r\nbcrypt os.sys os.path sys.exit os.environ\r\nFigure 8. Forbidden Python Modules. List of Python modules forbidden to exist in proposed tasks\u2019 programs.\r\nC. More Results\r\nC.1. Out-of-Distribution Performance Breakdown\r\nWe plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\r\n14B, and 14B-Coder models in Figures 28 to 31. We observe a strong correlation between training using AZR and improvements in both\r\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\r\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.\r\nC.2. In-Distribution Results\r\nSince we have defined the task domains as input prediction and output prediction, we can directly evaluate our model\u2019s capabilities in these\r\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (Gu\r\net al., 2024; Jain et al., 2024), where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\r\nIn Figure 14, we visualize the evolution of these metrics during the training of Absolute Zero Reasoner-base-7b. As training\r\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\r\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure 28), they serve as useful proxies for\r\ntracking task-specific progress.\r\nC.3. Interplay Between Propose and Solve Roles\r\nWe visualize the training dynamics between the propose and solve roles over training steps in Figures 15 to 17. We observe that, in\r\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\r\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.\r\nInterestingly, we also observe a consistent ordering in token length across reasoning types\u2014abduction and deduction tasks tend to result\r\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\r\nin trial-and-error reasoning\u2014repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\r\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length\r\n1 VALIDATE_CODE_TEMPLATE = \"\"\"{ code }\r\n2 repr (f({ inputs }))\"\"\"\r\n3\r\n4 exec ( VALIDATE_CODE_TEMPLATE )\r\nFigure 9. Python Program to Check Valid Code.\r\n22","<IMAGE FOR PAGE: 22 / 50>",{"content_type":"image_asset_pointer","asset_pointer":"file-service://file-HWhW6kqG4VQ9kamgFXqFuo","size_bytes":83274,"width":512,"height":512,"fovea":512,"metadata":null},"<PARSED TEXT FOR PAGE: 23 / 50>","Absolute Zero: Reinforced Self-play Reasoning with Zero Data\r\n1 EVAL_INPUT_PREDICTION_TEMPLATE = \"\"\" { code }\r\n2 { gold_output } == f({ agent_input }) \"\"\"\r\n3\r\n4 exec ( EVAL_INPUT_PREDICTION_TEMPLATE )\r\nFigure 10. Python Code to Check Agent Input Abduction Correctness.\r\n1 EVAL_OUTPUT_PREDICTION_TEMPLATE = \"\"\" { code }\r\n2 eval ({ gold_output }) == eval ({ agent_output }) \"\"\"\r\n3\r\n4 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\r\nFigure 11. Python Code to Check Agent Output Deduction Correctness.\r\n\nThe file is too long and its contents have been truncated.\n"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"command":"context_stuff","is_visually_hidden_from_conversation":true,"retrieval_turn_number":0,"retrieval_file_index":0,"reasoning_status":"is_reasoning","message_type":null,"model_slug":"o3","default_model_slug":"o3","parent_id":"902900f8-348a-4b47-a42c-1e51e547031b","request_id":"93d1fe424ddaec69-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"902900f8-348a-4b47-a42c-1e51e547031b","children":["5af469e0-53e0-4007-8133-9d9459d8aa32"]},"5af469e0-53e0-4007-8133-9d9459d8aa32":{"id":"5af469e0-53e0-4007-8133-9d9459d8aa32","message":{"id":"5af469e0-53e0-4007-8133-9d9459d8aa32","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1746802071.094648,"update_time":null,"content":{"content_type":"text","parts":["\nRemember you have access to rendered images of pages from the files, in addition to their parsed text content. Some files may include images, while others may not.\nThese may contain embedded text in images, infographics, complex tables, diagrams, and other visual elements not fully captured in the parsed text.\nYou can directly view these images to read text within images, interpret infographics, understand diagrams and complex tables. When applicable, combine insights from both parsed text and images to provide comprehensive and enriched answers.\n\nWhen responding to the user, actively incorporate information from embedded text, images, tables, diagrams, and other visuals in the files when applicable.\nExplicitly reference them by mentioning their location (e.g., page number) and summarizing their content. For example:\n- \"According to the *chart on page 5*, sales have increased by 20% over ...\"\n- \"The *diagram on page 3* illustrates the workflow, highlighting ...\"\n"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"reasoning_status":"is_reasoning","message_type":null,"model_slug":"o3","default_model_slug":"o3","parent_id":"c4874eee-9cf0-40aa-bb91-338f3f1ffbec","request_id":"93d1fe424ddaec69-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"c4874eee-9cf0-40aa-bb91-338f3f1ffbec","children":["15481dd7-a686-4b19-9813-e047d42ae23c"]},"15481dd7-a686-4b19-9813-e047d42ae23c":{"id":"15481dd7-a686-4b19-9813-e047d42ae23c","message":{"id":"15481dd7-a686-4b19-9813-e047d42ae23c","author":{"role":"tool","name":"file_search","metadata":{}},"create_time":1746802071.094793,"update_time":null,"content":{"content_type":"text","parts":["The file contents provided above are truncated/partial snippets. The complete content for these files IS accessible via querying.\n\nIf the user asks a question related to these files, and the provided snippets do not clearly answer it, you **MUST** use the `file_search` tool to search the full document contents before responding.\n\n## How to invoke the file_search tool\nIf the user's query isn't sufficiently answered by the provided snippets, parsed text, or images, you should invoke `file_search.msearch`.\nTo search the full content of the available files, you **must** send one message with recipient `to=file_search.msearch` in the analysis channel. Your message should be structured exactly as follows:\n```\n{\n  'queries': ['Example query 1', 'Example query 2']  // List of up to five queries to search for.\n}\n```\n\nCarefully balance the number of `file_search` calls and the quality of your response with the latency incurred by `file_search` calls.\n\nImportant guidelines for query formatting:\n- Your message must be a valid JSON object. Do not include any extraneous text, backticks, markdown formatting, or comments beyond what is shown.\n- The `queries` field must be a list of strings.\n- One of the queries **MUST** exactly match the user's original question, but stripped of any extraneous details, and with ambiguous references clearly resolved using context from the conversation. It **MUST** be a complete sentence.\n- Even if you suspect the user meant something else, one query **MUST** still be their original question.\n\n## How to handle results from file_search\nThe file search tool will respond with the relevant search results from the full files. After you receive these results:\n- Carefully evaluate each result to determine its relevance and quality.\n- Use the high-quality and relevant results to inform your answer to the user's question.\n- When possible, reuse information from prior file_search results and avoid repeating identical queries.\n\n## Tool usage instructions and ordering\nWhen the user asks a question, carefully consider whether the provided snippets sufficiently answer the query, whether you need to invoke `file_search.msearch`, or another available tool (such as `web`, if available) should be used instead.\n\nTool-ordering Guidelines:\n1. For queries related to uploaded files, if snippets are insufficient, strongly prefer invoking `file_search.msearch` over other tools.\n    - Avoid using `web.run`, `python` analysis, or any other tools until you have either:\n        - Confirmed with high confidence the answer is not in the files, or\n        - Have high confidence `file_search_tool` queries are not sufficient to answer the query.\n2. Avoid usage of `web_search/web.run` for queries related to uploaded files\n    - Carefully consider whether the user's query is better answered by web search results rather than available snippets or results from `file_search_tool`.\n    - Unless the user's query is clearly unrelated to the uploaded files, prefer using `file_search.msearch`. You can always follow up with a web search if necessary for a more complete response.\n3. Avoid using `python` unless necessary or requested by user.\n    - If the user's query is answerable from the parsed text and images from the uploaded files, or from `file_search_tool`, do not invoke `python`.\n\nAlways keep this instruction in mind for the rest of the conversation, as the user may reference these files or ask related questions later on."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"command":"prompt","reasoning_status":"is_reasoning","message_type":null,"model_slug":"o3","default_model_slug":"o3","parent_id":"5af469e0-53e0-4007-8133-9d9459d8aa32","request_id":"93d1fe424ddaec69-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"5af469e0-53e0-4007-8133-9d9459d8aa32","children":["d44ee320-4639-4aa2-9baa-2f2f5bb0cd13"]},"d44ee320-4639-4aa2-9baa-2f2f5bb0cd13":{"id":"d44ee320-4639-4aa2-9baa-2f2f5bb0cd13","message":{"id":"d44ee320-4639-4aa2-9baa-2f2f5bb0cd13","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1746802071.094883,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","message_type":null,"model_slug":"o3","default_model_slug":"o3","parent_id":"15481dd7-a686-4b19-9813-e047d42ae23c","request_id":"93d1fe424ddaec69-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"15481dd7-a686-4b19-9813-e047d42ae23c","children":["b99540ad-d8ca-45e7-a21a-e33debdb7a68"]},"b99540ad-d8ca-45e7-a21a-e33debdb7a68":{"id":"b99540ad-d8ca-45e7-a21a-e33debdb7a68","message":{"id":"b99540ad-d8ca-45e7-a21a-e33debdb7a68","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1746802071.095092,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Analyzing research paper","content":"The user has asked for a thorough yet accessible analysis of a research paper. Since they're mathematically trained but not a domain expert, I'll need to offer an overview, explain key concepts in simple terms, highlight the structure and argument, and critique elements like contributions, assumptions, and limitations. I'll also focus on technical details like algorithms, RL advantage estimators, and evaluation results, mainly from the earlier pages, though I might explore the Appendices if necessary for full context.","chunks":[],"finished":true}],"source_analysis_msg_id":"ad61c054-e5c8-44bf-94a0-276a0e723505"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"message_type":null,"model_slug":"o3","default_model_slug":"o3","parent_id":"d44ee320-4639-4aa2-9baa-2f2f5bb0cd13","request_id":"93d1fe424ddaec69-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"d44ee320-4639-4aa2-9baa-2f2f5bb0cd13","children":["38513d27-27b8-4e9c-8c47-05e4687fa2a0"]},"38513d27-27b8-4e9c-8c47-05e4687fa2a0":{"id":"38513d27-27b8-4e9c-8c47-05e4687fa2a0","message":{"id":"38513d27-27b8-4e9c-8c47-05e4687fa2a0","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1746802096.499982,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 33 seconds"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":33,"message_type":null,"model_slug":"o3","default_model_slug":"o3","parent_id":"b99540ad-d8ca-45e7-a21a-e33debdb7a68","request_id":"93d1fe424ddaec69-SJC","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"b99540ad-d8ca-45e7-a21a-e33debdb7a68","children":["d2c4b25a-4d2c-4c60-bef6-1be71493169c"]},"d2c4b25a-4d2c-4c60-bef6-1be71493169c":{"id":"d2c4b25a-4d2c-4c60-bef6-1be71493169c","message":{"id":"d2c4b25a-4d2c-4c60-bef6-1be71493169c","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1746802060.589234,"update_time":null,"content":{"content_type":"text","parts":["### 1. Overview \u2014 What the paper sets out to do  \n\n| Aspect | Summary |\n|--------|---------|\n| **Goal** | Replace human-curated question-answer data with a *fully self-generated* curriculum for improving LLM reasoning. |\n| **Method** | Introduce the **Absolute Zero paradigm**: a single model *proposes* new tasks and then *solves* them, using nothing but program execution as a verifiable reward signal. |\n| **System** | **Absolute Zero Reasoner (AZR)**, built on open-weight Qwen 2.5 and Llama 3 bases, trains by self-play on three Python-based task types (deduction, abduction, induction). |\n| **Key findings** | Without *any* external training pairs, AZR: 1) outperforms earlier \u201czero-setting\u201d RL models that used tens of thousands of curated examples, and 2) shows strong cross-domain transfer from coding to math benchmarks. |\n| **Conclusion** | Self-proposed, verifiably-rewarded curricula can match or beat data-hungry RL-from-feedback pipelines\u2014pointing toward \u201creasoning systems that learn from experience rather than datasets.\u201d |\n\n\ue200cite\ue202turn0file0\ue201  \n\n---\n\n### 2. Key Concepts, demystified  \n\n| Term | Intuitive explanation |\n|------|-----------------------|\n| **Reinforcement Learning with Verifiable Rewards (RLVR)** | Instead of imitating gold \u201cchain-of-thought\u201d, the model just gets a score based on whether its *final answer* is correct. |\n| **Zero setting** | RLVR applied straight to a base LLM, *but* the questions/answers themselves are still human-collected. |\n| **Absolute Zero** | Takes \u201czero\u201d one step further: the model must *invent* its own training questions, solve them, and learn from the outcome\u2014no external data at all. |\n| **Proposer / Solver Roles** | Same network plays two roles each step: (i) *Proposer* emits a candidate task, (ii) *Solver* tries to answer it. Both actions receive rewards. |\n| **Deduction / Abduction / Induction tasks** | Three complementary reasoning modes over a program\u2013input\u2013output triplet:<br>\u2022 **Deduction** (p + i \u2192 o) predict output.<br>\u2022 **Abduction** (p + o \u2192 i) guess an input that yields the output.<br>\u2022 **Induction** ({(i\u2081,o\u2081)\u2026} \u2192 p) synthesize a program consistent with examples. |\n| **Learnability reward** | Proposer is rewarded when the task is *neither trivial nor impossible* for the current Solver\u2014estimated via Monte-Carlo roll-outs. |\n| **TRR++ advantage estimator** | A variance-reduced REINFORCE update that maintains *per-task-type, per-role* baselines rather than a single global baseline. |\n\nFigures 2 and 3 on pages 2\u20134 visually summarise the paradigm and loop. \ue200cite\ue202turn0file0\ue201  \n\n---\n\n### 3. How the paper is structured  \n\n1. **Introduction & Motivation** \u2014 limitations of dataset-driven RLVR; argument for self-play.  \n2. **Absolute Zero Paradigm (Section 2)** \u2014 formal objective (Eq. 3) and conceptual loop.  \n3. **AZR System (Section 3)**  \n   * roles, task types, buffer management  \n   * safety / determinism filters for code  \n   * TRR++ learning algorithm and hyper-parameters  \n4. **Experiments (Section 4)** \u2014 comparisons against zero-style baselines, scaling studies, ablations.  \n5. **Discussion & Related Work (Section 5\u20136)** \u2014 links to AlphaZero lineage, self-play literature, safety caveats.  \n6. **Appendices** \u2014 RL details, additional metrics, negative results, implementation specifics.  \n\n\ue200cite\ue202turn0file0\ue201  \n\n---\n\n### 4. Critical analysis  \n\n| Dimension | Assessment |\n|-----------|------------|\n| **Novel contributions** | *i)* First end-to-end demonstration that a *single* LLM can bootstrap reasoning *from scratch* with no QA data. *ii)* Use of executable code as an open-ended yet automatically verifiable environment. *iii)* Empirical evidence of cross-domain generalisation gains (code\u2192math). |\n| **Key assumptions & simplifications** | \u2022 All tasks are reducible to deterministic Python functions; stochastic or non-functional domains are postponed. \u2022 Safety filters rely on blacklists (e.g., banning `os`, `sys`)\u2014not fool-proof. \u2022 Learnability reward assumes Monte-Carlo success rate is a good difficulty proxy. |\n| **Limitations** | \u2013 Demonstrated only on small/medium models (up to 14 B).<br>\u2013 Evaluation still uses human-curated benchmarks\u2014true open-world utility remains untested.<br>\u2013 Emergent \u201cuh-oh\u201d unsafe CoT noted for Llama 3 variant; safety governance unresolved. |\n| **Comparison to prior work** | Outperforms DeepSeek-R1, CodeR1 and AceCoder models that consumed 2 k \u2013 500 k labeled tasks, despite using *zero*. Mirrors AlphaZero\u2019s self-play idea but in a language-reasoning space rather than board games. |\n\n\ue200cite\ue202turn0file0\ue201  \n\n---\n\n### 5. Implications & open questions  \n\n| Area | Take-aways | Open problems |\n|------|------------|---------------|\n| **Scalability** | Results improve with 3 B \u2192 7 B \u2192 14 B; suggests positive scaling law for self-play curricula. | What happens at 70 B+? Does proposer quality keep pace or plateau? |\n| **General reasoning** | Significant math gains despite *never* proposing math-specific tasks\u2014hints at transferable abstractions from code reasoning. | Would other grounded environments (formal proofs, chemistry simulators, robotics) yield broader transfer? |\n| **Data efficiency** | Potential to decouple frontier LLM progress from ever-growing human datasets\u2014important as human text becomes saturated. | Need automatic metrics to detect/avoid self-generated data collapse or mode-locking. |\n| **Safety & Alignment** | Self-play setup surfaces unsafe chains of thought; authors flag need for \u201csafety-aware training.\u201d | How to integrate red-teaming or oversight loops *within* Absolute Zero without re-introducing heavy human labeling? |\n\n\ue200cite\ue202turn0file0\ue201  \n\n---\n\n### 6. Technical complexities (high-level)  \n\n* **Task validation pipeline** \u2014 Every proposed `(program, input)` is executed twice to guarantee determinism and run through a security blacklist before entering the training buffer. (Page 7).  \n* **Learnability reward (Eq. 4)** \u2014 Rewards are highest when solver\u2019s success rate is strictly between 0 and 1, encouraging tasks at the frontier of competence.  \n* **TRR++ (Eq. 8)** \u2014 Calculates six separate baselines (3 task types \u00d7 2 roles) to stabilise gradients in this multitask setting.  \n* **Buffer mechanics** \u2014 Valid tasks accumulate without limit, preventing catastrophic forgetting; if a new batch lacks fresh proposals, it back-fills from history.  \n* **Scaling experiments** \u2014 Show that coder-pretrained bases amplify gains (+15 pp math vs +11 pp for vanilla base), implying code priors act as \u201creasoning catalysts.\u201d (Table 1, Figure 6).  \n\nThese engineering choices underpin the paper\u2019s empirical success while hinting at why naive self-play often fails without such safeguards. \ue200cite\ue202turn0file0\ue201  \n\n---\n\n## Bottom line  \n\u201cAbsolute Zero\u201d convincingly demonstrates that a language model can *teach itself* sophisticated reasoning skills by inventing and verifying its own exercises\u2014no external labels required. The approach still lives in the relatively safe sandbox of Python code, but it charts a credible path toward data-independent, continually improving reasoning agents."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"content_references":[{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":1042,"end_idx":1059,"type":"grouped_webpages","items":[]},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":2547,"end_idx":2564,"type":"grouped_webpages","items":[]},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":3295,"end_idx":3312,"type":"grouped_webpages","items":[]},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":4537,"end_idx":4554,"type":"grouped_webpages","items":[]},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":5604,"end_idx":5621,"type":"grouped_webpages","items":[]},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":6666,"end_idx":6683,"type":"grouped_webpages","items":[]},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":1042,"end_idx":1059,"alt":null,"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","snippet":"","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":2547,"end_idx":2564,"alt":null,"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","snippet":"","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":3295,"end_idx":3312,"alt":null,"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","snippet":"","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":4537,"end_idx":4554,"alt":null,"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","snippet":"","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":5604,"end_idx":5621,"alt":null,"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","snippet":"","cloud_doc_url":null},{"matched_text":"\ue200cite\ue202turn0file0\ue201","start_idx":6666,"end_idx":6683,"alt":null,"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","snippet":"","cloud_doc_url":null}],"citations":[{"start_ix":1042,"end_ix":1059,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","text":"","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":2547,"end_ix":2564,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","text":"","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":3295,"end_ix":3312,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","text":"","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":4537,"end_ix":4554,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","text":"","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":5604,"end_ix":5621,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","text":"","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}},{"start_ix":6666,"end_ix":6683,"citation_format_type":"berry_file_search","metadata":{"type":"file","name":"2505.03335v2.pdf","id":"file-2y85dCWt3NzwrjE73ip9Ah","source":"my_files","text":"","extra":{"retrieval_turn":0,"retrieval_file_index":0,"cited_message_idx":6,"search_result_idx":null,"line_range":null,"cloud_doc_url":null}}}],"message_type":null,"model_slug":"o3","default_model_slug":"o3","parent_id":"38513d27-27b8-4e9c-8c47-05e4687fa2a0","request_id":"93d1fe424ddaec69-SJC","timestamp_":"absolute"},"recipient":"all","channel":"final"},"parent":"38513d27-27b8-4e9c-8c47-05e4687fa2a0","children":[]}}}