{"id":"677a0783-3518-8007-a424-694dcd926d4d","title":"Byte Pair Tokenizer Overview","create_time":1736050563.379625,"update_time":1736063957.901168,"current_node":"bac7f49a-899e-40d1-82fa-2ace654dcfa5","mapping":{"aaa13d14-2a1e-402a-aab0-67ef71b3ce12":{"id":"aaa13d14-2a1e-402a-aab0-67ef71b3ce12","message":null,"parent":null,"children":["0b474510-05b3-4fba-8c6a-a8dd081007ca"]},"0b474510-05b3-4fba-8c6a-a8dd081007ca":{"id":"0b474510-05b3-4fba-8c6a-a8dd081007ca","message":{"id":"0b474510-05b3-4fba-8c6a-a8dd081007ca","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"aaa13d14-2a1e-402a-aab0-67ef71b3ce12","children":["df974528-5f19-4f8f-b219-23c95baebc90"]},"df974528-5f19-4f8f-b219-23c95baebc90":{"id":"df974528-5f19-4f8f-b219-23c95baebc90","message":{"id":"df974528-5f19-4f8f-b219-23c95baebc90","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```About Myself: I have mathematical knowledge up to the post-graduate level.  I also have a keen interest in theoretical physics.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Coding Instructions:\n- When writing machine learning code in Python, use Pytorch unless there are good reasons for using another library such as TensorFlow\n- Make sure all Python code is compatible with Python version 3.11.\n\nGeneral instructions:\n- Provide examples if they are appropriate and do not make conversation unduly verbose\n- Always point out what you perceive as errors in my reasoning, or where I have failed to consider alternative perspectives.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"About Myself: I have mathematical knowledge up to the post-graduate level.  I also have a keen interest in theoretical physics.\n\nThese are my current research areas or areas of study:\n(1) coding: mainly Python and full-stack web development\n(2) machine learning and artificial intelligence","about_model_message":"Coding Instructions:\n- When writing machine learning code in Python, use Pytorch unless there are good reasons for using another library such as TensorFlow\n- Make sure all Python code is compatible with Python version 3.11.\n\nGeneral instructions:\n- Provide examples if they are appropriate and do not make conversation unduly verbose\n- Always point out what you perceive as errors in my reasoning, or where I have failed to consider alternative perspectives."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"0b474510-05b3-4fba-8c6a-a8dd081007ca","children":["aaa2e5ef-86df-40cd-a94f-bf9004a72533"]},"aaa2e5ef-86df-40cd-a94f-bf9004a72533":{"id":"aaa2e5ef-86df-40cd-a94f-bf9004a72533","message":{"id":"aaa2e5ef-86df-40cd-a94f-bf9004a72533","author":{"role":"user","name":null,"metadata":{}},"create_time":1736050563.388952,"update_time":null,"content":{"content_type":"text","parts":["The following code is a Python module dedicated to training a byte-level BPT tokenizer.\r\n\r\nWrite a docstring that gives an overview of how the code works and a concise summary of the various functions.\r\n\r\n```python\r\n\r\nimport os \r\nimport pathlib\r\nfrom typing import Dict, Tuple\r\nimport regex as re\r\nimport json \r\n\r\n\r\n\"\"\" \r\n# 1 \r\n\"\"\" \r\n\r\n\r\ndef process_text_file(\r\n        input_path: str | os.PathLike, \r\n        special_tokens: list[str],\r\n) -> list[str]:\r\n    \"\"\"\r\n    Reads a text file and processes it into a list of lines, splitting each line by special tokens.\r\n\r\n    Args:\r\n        input_path (str | os.PathLike): The path to the input text file.\r\n        special_tokens (list[str]): A list of special tokens to split each line by.\r\n    \r\n    Returns:\r\n        list[str]: A list of lines from the input file, split by the special tokens.\r\n    \"\"\"\r\n    output_lines = []\r\n    with open(input_path, 'r', encoding='utf-8') as file:\r\n        for line in file:\r\n            line = line.strip()\r\n            splits = [line]\r\n            for token in special_tokens:\r\n                temp_splits = []\r\n                for segment in splits:\r\n                    temp_splits.extend(segment.split(token))\r\n                splits = temp_splits\r\n            output_lines.extend([segment.strip() for segment in splits if segment.strip()])\r\n    return output_lines\r\n\r\n\r\n\"\"\" \r\n# 2 \r\n\"\"\" \r\n\r\ndef bytes_list_to_formatted_string(\r\n        byte_strings: list[bytes]\r\n) -> str:\r\n    \"\"\"\r\n    Converts a list of byte strings to a formatted string of integers.\r\n    Each byte string is converted to a list of integers, which are then joined by '|'.\r\n    The formatted strings are then joined by '-'.\r\n\r\n    Args:\r\n        byte_strings (list[bytes]): A list of byte strings to convert to a formatted string.\r\n\r\n    Returns:\r\n        str: A formatted string of integers representing the input byte strings.\r\n    \r\n    Usage Example:\r\n        byte_strings = [b'\\x01\\x02\\x03', b'\\x10\\x20', b'\\xFF\\x00']\r\n        formatted_string = bytes_list_to_formatted_string(byte_strings)\r\n        print(formatted_string)  \r\n            >>> 1|2|3-16|32-255|0\r\n    \"\"\"\r\n    # Convert each byte string to a formatted string of integers\r\n    formatted_segments = []\r\n    for byte_string in byte_strings:\r\n        # Convert the byte string to a list of integers and join with '|'\r\n        segment = '|'.join(str(byte) for byte in byte_string)\r\n        formatted_segments.append(segment)\r\n    # Join all segments with '-' as the separator\r\n    return '-'.join(formatted_segments)\r\n\r\ndef formatted_string_to_bytes_list(\r\n        formatted_string: str\r\n) -> list[bytes]:\r\n    \"\"\"\r\n    Converts a formatted string of integers to a list of byte strings.\r\n    The formatted string is split by '-', and each segment is split by '|'.\r\n    Each integer in the segment is then converted to a byte string.\r\n\r\n    Args:\r\n        formatted_string (str): A formatted string of integers to convert to byte strings.\r\n    \r\n    Returns:\r\n        list[bytes]: A list of byte strings converted from the input formatted    \r\n    \r\n    Usage Example:\r\n        formatted_string = '1|2|3-16|32-255|0'\r\n        byte_strings = formatted_string_to_bytes_list(formatted_string)\r\n        print(byte_strings)\r\n            >>> [b'\\x01\\x02\\x03', b'\\x10\\x20', b'\\xff\\x00']\r\n    \"\"\"\r\n    # Split the formatted string by '-' to get each segment\r\n    segments = formatted_string.split('-')\r\n    # Convert each segment back to a byte string\r\n    byte_strings = []\r\n    for segment in segments:\r\n        # Split the segment by '|' and convert each integer to a byte\r\n        byte_string = bytes(int(num) for num in segment.split('|'))\r\n        byte_strings.append(byte_string)\r\n    return byte_strings\r\n\r\ndef byte_tuples_dict_to_json(\r\n        byte_tuples_dict: Dict[Tuple[bytes, ...], int],\r\n        output_path: str | os.PathLike,\r\n) -> None:\r\n    \"\"\"\r\n    Converts a dictionary with byte tuple keys to a JSON file with formatted string keys.\r\n    String keys must be used in order to be compatible with JSON serialization.\r\n\r\n    Args:\r\n        byte_tuples_dict (Dict[Tuple[bytes, ...], int]): \r\n            A dictionary with byte tuple keys to be converted to JSON.\r\n        output_path (str | os.PathLike): \r\n            The path to the output JSON file where the dictionary will be saved.\r\n    \r\n    Returns:\r\n        None\r\n    \"\"\"\r\n    # Convert the byte tuple keys to formatted string keys\r\n    formatted_dict = {\r\n        bytes_list_to_formatted_string(list(key)): value for key, value in byte_tuples_dict.items()\r\n    }\r\n    # Save the formatted dictionary to a JSON file\r\n    with open(output_path, 'w', encoding='utf-8') as file:\r\n        json.dump(formatted_dict, file, ensure_ascii=False)\r\n\r\ndef json_to_byte_tuples_dict(\r\n        input_path: str | os.PathLike\r\n) -> Dict[Tuple[bytes, ...], int]:\r\n    \"\"\"\r\n    Converts a JSON file with formatted string keys to a dictionary with byte tuple keys.\r\n\r\n    Args:\r\n        input_path (str | os.PathLike): \r\n            The path to the input JSON file containing the formatted dictionary.\r\n    \r\n    Returns:\r\n        Dict[Tuple[bytes, ...], int]: A dictionary with byte tuple keys loaded from the JSON file.\r\n    \"\"\"\r\n    # Load the formatted dictionary from the JSON file\r\n    with open(input_path, 'r', encoding='utf-8') as file:\r\n        formatted_dict = json.load(file)\r\n    # Convert the formatted string keys back to byte tuple keys\r\n    byte_tuples_dict = {\r\n        tuple(formatted_string_to_bytes_list(key)): value for key, value in formatted_dict.items()\r\n    }\r\n    return byte_tuples_dict \r\n\r\n\r\n\"\"\" \r\n# 3 \r\n\"\"\" \r\n\r\ndef pretokenize_and_vocab_initialization(\r\n        pretoken_pattern: re.Pattern,\r\n        special_tokens: list[str],\r\n        textlines: list[str],\r\n) -> Tuple[Dict[bytes, int], Dict[Tuple[bytes, ...], int], Dict[Tuple[bytes, bytes], int]]:\r\n    \"\"\"\r\n    Pre-tokenizes a list of text lines, encodes the tokens into byte representations, and initializes vocabulary \r\n    and pair frequency dictionaries.\r\n\r\n    Args:\r\n        pretoken_pattern (re.Pattern): A compiled regex pattern for splitting the text into pre-tokens.\r\n        special_tokens (list[str]): A list of special tokens to include in the vocabulary.\r\n        textlines (list[str]): A list of text lines to be processed.\r\n\r\n    Returns:\r\n        Tuple[Dict[bytes, int], Dict[Tuple[bytes, ...], int], Dict[Tuple[bytes, bytes], int]]:\r\n            - A vocabulary dictionary mapping individual bytes and special tokens to unique integer indices.\r\n            - A dictionary mapping tuples each containing a sequence of pre-token byte encodings to the tuples' counts.\r\n            - A dictionary mapping byte pairs (tuples of two bytes) to their frequencies across the text.\r\n    \"\"\"\r\n    token_dict = {}\r\n    pairs_dict = {}\r\n    for line in textlines:\r\n        list_of_pretokens = pretoken_pattern.findall(line.strip())\r\n        list_of_encoded_pretokens = [pretoken.encode(\"utf-8\") for pretoken in list_of_pretokens]\r\n        list_of_list_of_bytes = [[bytes([x]) for x in encoded_pretoken] for encoded_pretoken in list_of_encoded_pretokens]\r\n        for list_of_bytes in list_of_list_of_bytes:\r\n            if len(list_of_bytes) > 1:\r\n                for i in range(len(list_of_bytes) - 1):\r\n                    byte_pair = (list_of_bytes[i], list_of_bytes[i + 1])\r\n                    pairs_dict[byte_pair] = pairs_dict.get(byte_pair, 0) + 1\r\n        list_of_tuples_of_bytes = [tuple(x) for x in list_of_list_of_bytes]\r\n        for tuple_of_bytes in list_of_tuples_of_bytes:\r\n            token_dict[tuple_of_bytes] = token_dict.get(tuple_of_bytes, 0) + 1           \r\n    vocab = {bytes([i]): i for i in range(256)}  \r\n    special_token_indices = range(256, 256 + len(special_tokens))\r\n    vocab.update({special_tokens[i - 256].encode(\"utf-8\"): i for i in special_token_indices})  \r\n    return vocab, token_dict, pairs_dict\r\n\r\n\r\n\"\"\" \r\n# 4 \r\n\"\"\" \r\n\r\n\r\ndef process_byte_pairs(\r\n        pairs_dict: Dict[Tuple[bytes, bytes], int],\r\n        token_dict: Dict[Tuple[bytes, ...], int], \r\n        vocab: Dict[bytes, int],\r\n        merges: list[Tuple[bytes, bytes]],\r\n) -> Tuple[\r\n    Dict[Tuple[bytes, bytes], int], \r\n    Dict[Tuple[bytes, ...], int], \r\n    Dict[bytes, int], \r\n    list[Tuple[bytes, bytes]]\r\n    ]:\r\n    \"\"\"\r\n    Merges the most frequent byte pair in the current vocabulary and updates relevant dictionaries.\r\n\r\n    This function performs a single merge operation in the Byte Pair Encoding (BPE) algorithm.\r\n    It identifies the most frequent byte pair from `pairs_dict`, merges it into a new byte sequence,\r\n    updates the vocabulary (`vocab`), the token frequency dictionary (`token_dict`), and the \r\n    byte pair frequency dictionary (`pairs_dict`). The merge operation is also recorded in the\r\n    `merges` list.\r\n\r\n    Args:\r\n        pairs_dict (Dict[Tuple[bytes, bytes], int]): \r\n            A dictionary mapping byte pairs to their frequency counts.\r\n        token_dict (Dict[Tuple[bytes, ...], int]): \r\n            A dictionary mapping tuples each containing a sequence of bytes to the tuples' frequency counts.\r\n        vocab (Dict[bytes, int]): \r\n            The current vocabulary mapping byte sequences to unique integer indices.\r\n        merges (list[Tuple[bytes, bytes]]): \r\n            A list of byte pairs that have been merged so far.\r\n\r\n    Returns:\r\n        Tuple[\r\n            Dict[Tuple[bytes, bytes], int], \r\n            Dict[Tuple[bytes, ...], int], \r\n            Dict[bytes, int], \r\n            list[Tuple[bytes, bytes]]\r\n        ]: \r\n            A tuple containing:\r\n            - Updated `pairs_dict` with the merged pair removed and new pairs added.\r\n            - Updated `token_dict` with tuples modified to contain the newly merged tokens \r\n                  alongside the original tokens wherever applicable and the tuples' updated frequencies.\r\n            - Updated `vocab` including the new merged byte sequence.\r\n            - Updated `merges` list with the newly merged byte pair appended.\r\n    \r\n    Raises:\r\n        KeyError: \r\n            If `pairs_dict` is empty and no byte pairs are available to merge.\r\n    \"\"\"\r\n    if not pairs_dict:\r\n        raise KeyError(\"No byte pairs available to merge.\")\r\n\r\n    highest_key = max(\r\n        pairs_dict.keys(),\r\n        key=lambda k: (pairs_dict[k], k)  # Sort by value, then lexicographically\r\n    )\r\n\r\n    byte1 = highest_key[0]\r\n    byte2 = highest_key[1]\r\n    new_byte = byte1 + byte2   \r\n    vocab[new_byte] = len(vocab)\r\n    merges.append((byte1, byte2))\r\n    del pairs_dict[(byte1, byte2)]\r\n    keys_to_add = {}\r\n    keys_to_remove = []\r\n    for key, value in token_dict.items():\r\n        key_list = list(key)\r\n        i = 0\r\n        while i < len(key_list) - 1:\r\n            if key_list[i] == byte1 and key_list[i + 1] == byte2:\r\n                                \r\n                # Perform the merge on key_list \r\n                key_list[i:i + 2] = [new_byte]\r\n\r\n                # Update list of keys to be removed from token_dict\r\n                keys_to_remove.append(key)\r\n                \r\n                # Record the (before byte, new byte) pair and\r\n                # delete the (before byte, first byte) pair if applicable\r\n                if i > 0:\r\n                    before_byte = key_list[i - 1]\r\n                    pairs_dict[(before_byte, byte1)] = pairs_dict.get((before_byte, byte1), 0) - value\r\n                    if pairs_dict[(before_byte, byte1)] <= 0:\r\n                        del pairs_dict[(before_byte, byte1)]\r\n                    pairs_dict[(before_byte, new_byte)] = pairs_dict.get((before_byte, new_byte), 0) + value\r\n                \r\n                # Record the (new byte, after byte) pair and\r\n                # delete the (second byte, after byte) pair if applicable\r\n                if i < len(key_list) - 1:\r\n                    after_byte = key_list[i + 1]\r\n                    pairs_dict[(byte2, after_byte)] = pairs_dict.get((byte2, after_byte), 0) - value\r\n                    if pairs_dict[(byte2, after_byte)] <= 0:\r\n                        del pairs_dict[(byte2, after_byte)]\r\n                    pairs_dict[(new_byte, after_byte)] = pairs_dict.get((new_byte, after_byte), 0) + value\r\n            \r\n            i += 1\r\n                \r\n        newkey = tuple(key_list)\r\n        keys_to_add[newkey] = keys_to_add.get(newkey, 0) + value\r\n\r\n    # Remove the keys marked for removal and add the new keys\r\n    token_dict = {key: value for key, value in token_dict.items() if key not in keys_to_remove}\r\n    token_dict.update(keys_to_add)\r\n\r\n    return pairs_dict, token_dict, vocab, merges\r\n\r\n\r\n\"\"\" \r\n# 5 \r\n\"\"\"\r\n\r\ndef gpt2_bytes_to_unicode() -> dict[int, str]:\r\n    \"\"\"\r\n    Creates a one-to-one mapping from all 256 possible byte values (0-255) to a single Unicode \r\n    character each. \r\n    GPT-2 does this so that any raw byte string can be encoded as a sequence of \u201cprintable\u201d or \r\n    otherwise unambiguous Unicode characters.\r\n\r\n    The function maps:\r\n    1. Visually printable byte values to their corresponding characters.\r\n    2. Non-printable byte values to unique Unicode characters with an offset of 256.\r\n\r\n    Returns:\r\n        dict[int, str]: A dictionary mapping each byte value (0-255) to a Unicode character.\r\n    \"\"\"\r\n    # Define printable byte ranges\r\n    printable_bytes = (\r\n        list(range(ord(\"!\"), ord(\"~\") + 1))\r\n        + list(range(ord(\"\u00a1\"), ord(\"\u00ac\") + 1))\r\n        + list(range(ord(\"\u00ae\"), ord(\"\u00ff\") + 1))\r\n    )\r\n\r\n    # Start with printable characters\r\n    char_codes = printable_bytes[:]\r\n\r\n    # Add mappings for non-printable characters\r\n    next_offset = 0\r\n    for byte in range(256):\r\n        if byte not in printable_bytes:\r\n            printable_bytes.append(byte)\r\n            char_codes.append(256 + next_offset)\r\n            next_offset += 1\r\n\r\n    # Create the final mapping\r\n    byte_to_unicode = {byte: chr(code) for byte, code in zip(printable_bytes, char_codes)}\r\n    return byte_to_unicode\r\n\r\ndef byte_string_to_unicode_string(byte_string: bytes, byte_to_unicode: dict[int, str]) -> str:\r\n    \"\"\"\r\n    Converts a byte string to a Unicode string using the provided byte-to-Unicode mapping.\r\n\r\n    Args:\r\n        byte_string (bytes): A byte string to convert to Unicode.\r\n        byte_to_unicode (dict[int, str]): A dictionary mapping byte values to Unicode characters.\r\n\r\n    Returns:\r\n        str: The Unicode string representation of the input byte string.\r\n    \r\n    Usage Example:\r\n        byte_string = b'\\x48\\x65\\x6C\\x6C\\x6F'\r\n        byte_to_unicode = gpt2_bytes_to_unicode()\r\n        unicode_string = byte_string_to_unicode_string(byte_string, byte_to_unicode)\r\n        print(unicode_string)\r\n            >>> 'Hello'\r\n    \"\"\"\r\n    return \"\".join(byte_to_unicode[byte] for byte in byte_string)\r\n\r\ndef unicode_to_gpt2_bytes_mapping(gpt2_bytes_to_unicode_dict: dict[int, str]) -> dict[str, int]:\r\n    \"\"\"\r\n    Takes a dictionary generated by gpt2_bytes_to_unicode and creates the inverse mapping,\r\n    which maps Unicode characters back to their corresponding byte values.\r\n\r\n    Args:\r\n        gpt2_bytes_to_unicode_dict (dict[int, str]): \r\n            A dictionary mapping byte values (0-255) to Unicode string characters.\r\n\r\n    Returns:\r\n        dict[str, int]: A dictionary mapping Unicode characters to their corresponding byte values.\r\n    \"\"\"\r\n    # Use dictionary comprehension to swap keys and values of the input dictionary\r\n    inverse_mapping = {v: k for k, v in gpt2_bytes_to_unicode_dict.items()}\r\n    return inverse_mapping\r\n\r\ndef unicode_string_to_byte_string(unicode_string: str, unicode_to_byte: dict[str, int]) -> bytes:\r\n    \"\"\"\r\n    Converts a Unicode string to a byte string using the provided Unicode-to-byte mapping.\r\n\r\n    Args:\r\n        unicode_string (str): A Unicode string to convert to bytes.\r\n        unicode_to_byte (dict[str, int]): A dictionary mapping Unicode characters to byte values.\r\n\r\n    Returns:\r\n        bytes: The byte string representation of the input Unicode string.\r\n    \r\n    Usage Example:\r\n        unicode_string = 'Hello'\r\n        unicode_to_byte = unicode_to_gpt2_bytes_mapping(gpt2_bytes_to_unicode())\r\n        byte_string = unicode_string_to_byte_string(unicode_string, unicode_to_byte)\r\n        print(byte_string)\r\n            >>> b'\\x48\\x65\\x6c\\x6c\\x6f'\r\n    \"\"\"\r\n    return bytes([unicode_to_byte[char] for char in unicode_string])\r\n\r\ndef load_merges_from_file(\r\n        merges_saved_file: str | os.PathLike,\r\n        unicode_to_byte: dict[str, int],\r\n) -> list[Tuple[bytes, bytes]]:\r\n    \"\"\"\r\n    Loads merge operations from a text file and converts Unicode strings to byte strings.\r\n\r\n    Args:\r\n        merges_saved_file (str | os.PathLike): The path to the file containing saved merge operations.\r\n        unicode_to_byte (dict[str, int]): A dictionary mapping Unicode characters to byte values.\r\n    \r\n    Returns:\r\n        list[Tuple[bytes, bytes]]: A list of byte pairs loaded from the file.\r\n    \"\"\"\r\n    with open(merges_saved_file, 'r', encoding='utf-8') as file:\r\n        lines = file.readlines()\r\n    lines_to_parse = [line.strip() for line in lines]\r\n    merges = []\r\n    for line in lines_to_parse:\r\n        unicode1, unicode2 = line.split(\" \")  # Split the line on whitespace into two Unicode strings\r\n        merges.append((unicode_string_to_byte_string(unicode1, unicode_to_byte), unicode_string_to_byte_string(unicode2, unicode_to_byte)))\r\n    return merges\r\n\r\ndef save_merges_to_file(\r\n        merges: list[Tuple[bytes, bytes]],\r\n        output_path_merges: str | os.PathLike,\r\n        byte_to_unicode: dict[int, str],\r\n) -> None:\r\n    \"\"\"\r\n    Saves merge operations to a text file, converting byte strings to Unicode strings.\r\n\r\n    Args:\r\n        merges (list[Tuple[bytes, bytes]]): A list of byte pairs to save.\r\n        output_path_merges (str | os.PathLike): The path to the output file where merge operations will be saved.\r\n        byte_to_unicode (dict[int, str]): A dictionary mapping byte values to Unicode characters\r\n\r\n    Returns:\r\n        None \r\n    \"\"\"\r\n    inspectable_merges = [\r\n        (byte_string_to_unicode_string(byte1, byte_to_unicode), byte_string_to_unicode_string(byte2, byte_to_unicode)) for byte1, byte2 in merges\r\n    ]\r\n    with open(output_path_merges, \"w\", encoding='utf-8') as file:\r\n        for byte1, byte2 in inspectable_merges:\r\n            file.write(f\"{byte1} {byte2}\\n\")\r\n\r\ndef save_token_dict_to_file_latin(\r\n        vocab: dict[bytes, int],\r\n        output_path_vocab: str | os.PathLike,\r\n) -> None:\r\n    \"\"\"\r\n    Saves a token dictionary to a JSON file, converting byte keys to Latin-1 encoded string keys.\r\n    Latin-1 encoding is used to ensure compatibility with JSON serialization.\r\n\r\n    Args:\r\n        vocab (dict[bytes, int]): A dictionary mapping byte sequences to integer indices.\r\n        output_path_vocab (str | os.PathLike): The path to the output file.\r\n    \r\n    Returns:\r\n        None\r\n    \"\"\"\r\n    with open(output_path_vocab, \"w\", encoding=\"utf-8\") as file:\r\n        vocab_keys_latin_1 = {k.decode('latin-1'): v for k, v in vocab.items()}\r\n        json.dump(vocab_keys_latin_1, file, ensure_ascii=False)\r\n\r\ndef load_token_dict_from_file_latin(\r\n        input_path_vocab: str | os.PathLike,\r\n) -> dict[bytes, int]:\r\n    \"\"\"\r\n    Loads a token dictionary from a JSON file, converting Latin-1 encoded keys to byte string keys.\r\n    Byte string keys are used in the internal representation of the dictionary so that\r\n    they can be directly used for byte pair merging operations.\r\n\r\n    Args:\r\n        input_path_vocab (str | os.PathLike): The path to the input file.\r\n    \r\n    Returns:\r\n        dict[bytes, int]: A dictionary mapping byte sequences to integer indices.\r\n    \"\"\"\r\n    with open(input_path_vocab, \"r\", encoding=\"utf-8\") as file:\r\n        vocab_latin_1 = json.load(file)\r\n        vocab = {k.encode('latin-1'): v for k, v in vocab_latin_1.items()}\r\n    return vocab    \r\n\r\n\r\n\"\"\"\r\n# 6\r\n\"\"\"\r\n\r\n\r\n\r\ndef is_valid_path(path: str | None = None) -> bool:\r\n    \"\"\"\r\n    Validates the given file path for invalid characters or invalid strings.\r\n\r\n    Args:\r\n        path (str): The file path to validate.\r\n\r\n    Returns:\r\n        bool: True if the path is valid, False otherwise.\r\n    \"\"\"\r\n    if not path:\r\n        return False\r\n\r\n    # Combine invalid characters\r\n    # We define many more characters to be invalid \r\n    # so as to force file names to be clean\r\n    invalid_chars = set('+`#~<>:;,\"\\'|?*!@$^&[]{}').union(set('\\0'))\r\n\r\n    # Set of reserved names\r\n    reserved_names = {\r\n        \"CON\", \"PRN\", \"AUX\", \"NUL\",\r\n        \"COM1\", \"COM2\", \"COM3\", \"COM4\", \"COM5\", \"COM6\", \"COM7\", \"COM8\", \"COM9\",\r\n        \"LPT1\", \"LPT2\", \"LPT3\", \"LPT4\", \"LPT5\", \"LPT6\", \"LPT7\", \"LPT8\", \"LPT9\"\r\n    }\r\n\r\n    # Split the path into parts and validate each part\r\n    try:\r\n        p = pathlib.Path(path)\r\n        for part in p.parts:\r\n            # Check for invalid characters\r\n            if any(char in invalid_chars for char in part):\r\n                return False\r\n            # Check for reserved names (case-insensitive)\r\n            if part.upper() in reserved_names:\r\n                return False\r\n        return True\r\n    except Exception:\r\n        return False\r\n\r\n\r\n\r\ndef train_utf8_bytelevel_bpe(\r\n    input_path: str,\r\n    output_path_vocab_final: str,\r\n    output_path_merges_final: str,\r\n    vocab_size: int,\r\n    special_tokens: list[str],\r\n    pretoken_pattern: str,\r\n    save_work_periodically: bool = False,\r\n    save_vocab_frequency: int = 50,\r\n    halt_on_vocab_frequency: int = 1000,\r\n    save_directory: str | None = None,\r\n    resume_training: bool = False,\r\n) -> Tuple[Dict[int, bytes], list[Tuple[bytes, bytes]]]:\r\n    \"\"\"\r\n    Trains a UTF-8 Byte-Level Byte Pair Encoding (BPE) tokenizer on a given text corpus.\r\n\r\n    This function processes the input text file, initializes the necessary data structures,\r\n    and iteratively merges the most frequent byte pairs until the desired vocabulary size\r\n    is achieved. The resulting vocabulary and merge operations are saved to the specified\r\n    output files.\r\n\r\n    Args:\r\n        input_path (str): \r\n            The file path to the input text corpus used for training the BPE tokenizer.\r\n        output_path_vocab_final (str): \r\n            The file path where the final resulting vocabulary JSON will be saved.\r\n        output_path_merges_final (str): \r\n            The file path where the complete collection of merge operations (unicode pairs) \r\n            will be saved as a text file.\r\n        vocab_size (int): \r\n            The target size of the vocabulary. The training will continue until this size is reached.\r\n        special_tokens (list[str]): \r\n            A list of special tokens (e.g., `<PAD>`, `<BOS>`, `<EOS>`) to include in the vocabulary.\r\n        pretoken_pattern (str): \r\n            A regex pattern string used to pre-tokenize the input text lines.\r\n        save_work_periodically (bool):\r\n            A flag indicating whether to save the training progress periodically.\r\n        save_vocab_frequency (int):\r\n            The frequency (in terms of number of vocab items) at which the training progress will be saved.\r\n        halt_on_vocab_frequency (int):\r\n            The frequency (in terms of number of vocab items) at which the training will halt.\r\n            The training progress will be saved if save_work_periodically is True.\r\n        save_directory (str):\r\n            The directory where the training progress files will be saved.\r\n        resume_training (bool):\r\n            A flag indicating whether to resume training from a previous checkpoint.\r\n\r\n    Returns:\r\n        Tuple[Dict[int, bytes], list[Tuple[bytes, bytes]]]:\r\n            A tuple containing:\r\n            - `enumerated_vocab`: A dictionary mapping unique integer indices to their corresponding byte sequences.\r\n            - `merges`: A list of byte pairs that were merged during the training process.\r\n\r\n    Raises:\r\n        ValueError: \r\n            If the specified `vocab_size` is smaller than the initial vocabulary size.\r\n    \"\"\"\r\n\r\n    SAVE_VOCAB_FILE_NAME = \"saved_vocab.json\"\r\n    SAVE_TOKEN_DICT_FILE_NAME = \"saved_token_dict.json\"\r\n    SAVE_PAIRS_DICT_FILE_NAME = \"saved_pairs_dict.json\"\r\n    SAVE_MERGES_FILE_NAME = \"saved_merges.txt\"\r\n    byte_to_unicode = gpt2_bytes_to_unicode()\r\n    unicode_to_byte = unicode_to_gpt2_bytes_mapping(byte_to_unicode)\r\n\r\n    if (save_directory is None) or (not is_valid_path(save_directory)):\r\n        resume_training = False\r\n        save_work_periodically = False\r\n    else:\r\n        save_path = pathlib.Path(save_directory)\r\n        directory_exists = save_path.is_dir()\r\n        vocab_saved_file = save_path / SAVE_VOCAB_FILE_NAME\r\n        token_dict_saved_file = save_path / SAVE_TOKEN_DICT_FILE_NAME\r\n        pairs_dict_saved_file = save_path / SAVE_PAIRS_DICT_FILE_NAME\r\n        merges_saved_file = save_path / SAVE_MERGES_FILE_NAME\r\n        vocab_file_exists = vocab_saved_file.is_file()\r\n        token_dict_file_exists = token_dict_saved_file.is_file()\r\n        pairs_dict_file_exists = pairs_dict_saved_file.is_file()\r\n        merges_file_exists = merges_saved_file.is_file()\r\n        if (not directory_exists) or (not vocab_file_exists) or (not token_dict_file_exists) or (not pairs_dict_file_exists):\r\n            print(\"Non-existing directory or files. Training will proceed from scratch.\")\r\n            resume_training = False\r\n\r\n    if resume_training:        \r\n        try:\r\n            vocab = load_token_dict_from_file_latin(vocab_saved_file)\r\n            print(\"Previously saved vocabulary dictionary loaded.\")\r\n            token_dict = json_to_byte_tuples_dict(token_dict_saved_file)\r\n            print(\"Previously saved token dictionary loaded.\")\r\n            pairs_dict = json_to_byte_tuples_dict(pairs_dict_saved_file)\r\n            print(\"Previously saved pairs dictionary loaded.\")\r\n            if merges_file_exists:\r\n                merges = load_merges_from_file(\r\n                    merges_saved_file=merges_saved_file, \r\n                    unicode_to_byte=unicode_to_byte,\r\n                )\r\n                print(\"Previously saved merges loaded.\")\r\n            else:\r\n                merges = [] # Initialize merges list if merge files is not found\r\n                print(\"No previously saved merges found. Training will proceed from initial vocabulary.\")\r\n        except Exception as e:\r\n            print(f\"Error processing file: {e}\")\r\n            print(f\"Cannot resume training. Restarting training from scratch.\")\r\n            resume_training = False\r\n\r\n    if save_work_periodically:\r\n        if not save_path.exists():\r\n            try:\r\n                save_path.mkdir()\r\n                print(f\"Created save directory at: {save_path}\")\r\n            except Exception as e:\r\n                print(f\"Error creating save directory: {e}\")\r\n                print(\"Work will NOT be saved periodically.\")\r\n                save_work_periodically = False\r\n\r\n    if not resume_training:\r\n        textlines = process_text_file(input_path, special_tokens)\r\n        vocab, token_dict, pairs_dict = pretokenize_and_vocab_initialization(\r\n            pretoken_pattern=pretoken_pattern,\r\n            special_tokens=special_tokens,\r\n            textlines=textlines,\r\n        )\r\n        merges = []\r\n    \r\n    if vocab_size < len(vocab):\r\n        raise ValueError(f\"Vocabulary size must be at least {len(vocab)}\")\r\n    \r\n    if save_work_periodically and (not resume_training):\r\n        save_token_dict_to_file_latin(\r\n            vocab=vocab,\r\n            output_path_vocab=vocab_saved_file,\r\n        )\r\n        byte_tuples_dict_to_json(\r\n            byte_tuples_dict=token_dict,\r\n            output_path=token_dict_saved_file,\r\n        )\r\n        byte_tuples_dict_to_json(\r\n            byte_tuples_dict=pairs_dict,\r\n            output_path=pairs_dict_saved_file,\r\n        )\r\n\r\n    while (len(vocab) < vocab_size):\r\n        pairs_dict, token_dict, vocab, merges = process_byte_pairs(\r\n            pairs_dict=pairs_dict,\r\n            token_dict=token_dict,\r\n            vocab=vocab,\r\n            merges=merges,\r\n        )\r\n        if len(vocab) % 200 == 0:\r\n            print(f\"Vocabulary Length is now: {len(vocab)}.\")\r\n        if save_work_periodically and ((len(vocab) % save_vocab_frequency == 0) or (len(vocab) % halt_on_vocab_frequency == 0)):\r\n            save_token_dict_to_file_latin(\r\n                vocab=vocab,\r\n                output_path_vocab=vocab_saved_file,\r\n            )\r\n            byte_tuples_dict_to_json(\r\n                byte_tuples_dict=token_dict,\r\n                output_path=token_dict_saved_file,\r\n            )\r\n            byte_tuples_dict_to_json(\r\n                byte_tuples_dict=pairs_dict,\r\n                output_path=pairs_dict_saved_file,\r\n            )\r\n            save_merges_to_file(\r\n                merges=merges,\r\n                output_path_merges=merges_saved_file,\r\n                byte_to_unicode=byte_to_unicode,\r\n            )\r\n        if (len(vocab) % halt_on_vocab_frequency == 0) or (len(vocab) >= halt_on_vocab_frequency):\r\n            print(f\"Vocabulary Length is now: {len(vocab)}.\")\r\n            print(f\"Training halted.\")\r\n            break   \r\n    \r\n    save_merges_to_file(\r\n        merges=merges,\r\n        output_path_merges=output_path_merges_final,\r\n        byte_to_unicode=byte_to_unicode,\r\n    )\r\n    save_token_dict_to_file_latin(\r\n        vocab=vocab,\r\n        output_path_vocab=output_path_vocab_final,\r\n    )\r\n    enumerated_vocab = {v: k for k, v in vocab.items()}\r\n    return enumerated_vocab, merges\r\n\r\n\r\n\"\"\"\r\n# 7\r\n\"\"\"\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    FIXTURES_PATH = pathlib.Path().resolve() / \"../tests/fixtures\"\r\n    input_path = FIXTURES_PATH / \"corpus.en\"\r\n    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\", \"<|endoftext|>\"]\r\n    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\r\n    pretoken_pattern = re.compile(PAT, re.UNICODE)\r\n\r\n    vocab, merges = train_utf8_bytelevel_bpe(\r\n        input_path=input_path,\r\n        output_path_vocab_final=\"vocab.json\",\r\n        output_path_merges_final=\"merges.txt\",\r\n        vocab_size=450,\r\n        special_tokens=special_tokens,\r\n        pretoken_pattern=pretoken_pattern,\r\n        save_work_periodically=True,\r\n        save_vocab_frequency=50,\r\n        halt_on_vocab_frequency=400,\r\n        save_directory=\"bpe_training\",\r\n        resume_training=True,\r\n    )\r\n\r\n    print(vocab)\r\n    print(merges)\r\n\r\n```"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"8fd0a6927e0281ba-SIN","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"df974528-5f19-4f8f-b219-23c95baebc90","children":["4416ea89-dca0-4d05-aefb-d8dc46ad7cae"]},"4416ea89-dca0-4d05-aefb-d8dc46ad7cae":{"id":"4416ea89-dca0-4d05-aefb-d8dc46ad7cae","message":{"id":"4416ea89-dca0-4d05-aefb-d8dc46ad7cae","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1736050593.594604,"update_time":null,"content":{"content_type":"text","parts":["**Crafting a docstring**\n\nI'm piecing together a comprehensive docstring for a Python module focused on Byte Pair Token (BPT) tokenization. This docstring will detail the module's function and briefly outline each specific function within.\n\n**Crafting the docstring**\n\nI'm outlining a structured docstring for the module, detailing its functions from text file processing to BPE training, including byte sequence handling and utility functions."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Thought about byte-level BPE tokenizer for 9 seconds","initial_text":"Thinking","message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"aaa2e5ef-86df-40cd-a94f-bf9004a72533","request_id":"8fd0a6927e0281ba-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"aaa2e5ef-86df-40cd-a94f-bf9004a72533","children":["7dd8f32a-307a-4bab-a39c-09261eda94fa"]},"7dd8f32a-307a-4bab-a39c-09261eda94fa":{"id":"7dd8f32a-307a-4bab-a39c-09261eda94fa","message":{"id":"7dd8f32a-307a-4bab-a39c-09261eda94fa","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1736050593.594722,"update_time":null,"content":{"content_type":"text","parts":["```python\n\"\"\"\nThis module provides functionality to train a UTF-8 byte-level Byte Pair Encoding (BPE) tokenizer. \nIt contains utilities to preprocess text, encode and decode byte sequences, track and merge \nfrequent byte pairs, and ultimately build a BPE-based vocabulary. The key steps include:\n\n1. **Text File Processing**  \n   - Splits input lines by special tokens and returns processed text lines ready for tokenization.\n\n2. **Byte Sequence Representation**  \n   - Provides bidirectional conversions between byte sequences and formatted strings (e.g., \n     `bytes_list_to_formatted_string` and `formatted_string_to_bytes_list`) for easier \n     serialization and inspection.\n   - Facilitates saving and loading dictionaries with byte tuple keys by converting them \n     into JSON-compatible string formats (`byte_tuples_dict_to_json`, `json_to_byte_tuples_dict`).\n\n3. **Pre-tokenization and Vocabulary Initialization**  \n   - Uses regex-based pre-tokenization (`pretokenize_and_vocab_initialization`) to segment text \n     into tokens, encodes them to bytes, and initializes frequencies of individual byte tokens \n     (vocab) and byte sequences (token_dict), as well as pair frequencies (pairs_dict).\n\n4. **Byte Pair Merging**  \n   - Identifies the most frequent byte pairs, merges them, and updates the vocabulary using \n     `process_byte_pairs`. Each merge operation reduces token granularity and grows the \n     tokenizer's byte-level vocabulary.\n\n5. **GPT-2 Byte-to-Unicode Mapping**  \n   - Implements GPT-2's strategy for mapping the full range of bytes (0\u2013255) to a unique set \n     of Unicode characters (`gpt2_bytes_to_unicode`, `unicode_to_gpt2_bytes_mapping`), ensuring \n     arbitrary byte sequences can be losslessly converted to valid Unicode text.\n\n6. **Saving and Loading Artifacts**  \n   - Functions to save and load merges, vocabulary dictionaries, and token dictionaries \n     to/from disk, facilitating checkpointing and resuming training (`save_merges_to_file`, \n     `load_merges_from_file`, `save_token_dict_to_file_latin`, `load_token_dict_from_file_latin`).\n\n7. **Path Validation**  \n   - Includes a simple utility (`is_valid_path`) to detect invalid file paths or reserved names.\n\n8. **Training Workflow**  \n   - The primary entry point, `train_utf8_bytelevel_bpe`, orchestrates the entire training \n     pipeline: from reading and preprocessing a text corpus, through iterative byte pair merges, \n     to saving the final vocabulary. It supports periodic checkpointing and resuming from a \n     prior state.\n\nBelow is a concise summary of the main functions:\n\n- **process_text_file(input_path, special_tokens)**  \n  Reads a text file line by line, splits lines by special tokens, and returns processed lines.\n\n- **bytes_list_to_formatted_string(byte_strings)**  \n  Converts a list of byte strings to a human-readable string of integers.\n\n- **formatted_string_to_bytes_list(formatted_string)**  \n  Converts a formatted integer string back into a list of byte strings.\n\n- **byte_tuples_dict_to_json(byte_tuples_dict, output_path)**  \n  Saves a dictionary with byte tuple keys to JSON (serialized as strings).\n\n- **json_to_byte_tuples_dict(input_path)**  \n  Loads a JSON file and reconstructs the dictionary\u2019s byte tuple keys.\n\n- **pretokenize_and_vocab_initialization(pretoken_pattern, special_tokens, textlines)**  \n  Splits text lines into pre-tokens, encodes them as byte sequences, and initializes the \n  token frequency dictionaries.\n\n- **process_byte_pairs(pairs_dict, token_dict, vocab, merges)**  \n  Performs a single BPE merge operation on the most frequent byte pair, updating relevant \n  dictionaries and merges list.\n\n- **gpt2_bytes_to_unicode()**  \n  Generates GPT-2\u2019s byte-to-Unicode mapping.\n\n- **byte_string_to_unicode_string(byte_string, byte_to_unicode)**  \n  Converts a raw byte string to a Unicode string using GPT-2 mapping.\n\n- **unicode_to_gpt2_bytes_mapping(gpt2_bytes_to_unicode_dict)**  \n  Produces the inverse mapping of GPT-2\u2019s byte-to-Unicode dictionary.\n\n- **unicode_string_to_byte_string(unicode_string, unicode_to_byte)**  \n  Converts a GPT-2 Unicode string back to its original byte string form.\n\n- **load_merges_from_file(merges_saved_file, unicode_to_byte)**  \n  Loads merge pairs from a text file, converting them from Unicode form to raw bytes.\n\n- **save_merges_to_file(merges, output_path_merges, byte_to_unicode)**  \n  Saves merge pairs to a text file, converting raw bytes to GPT-2 Unicode form.\n\n- **save_token_dict_to_file_latin(vocab, output_path_vocab)**  \n  Stores a byte-based vocabulary dictionary as JSON by encoding keys in Latin-1.\n\n- **load_token_dict_from_file_latin(input_path_vocab)**  \n  Loads a Latin-1-encoded vocabulary JSON file into a dictionary with byte keys.\n\n- **is_valid_path(path)**  \n  Validates a potential file path for unsafe characters and reserved names.\n\n- **train_utf8_bytelevel_bpe(...)**  \n  The main training procedure: reads text, initializes structures, and executes iterative \n  merges until the vocabulary reaches the desired size. Optionally supports checkpointing \n  and resuming.\n\nUse `train_utf8_bytelevel_bpe` to run an end-to-end workflow that processes an input text \nfile, merges frequent byte pairs, and outputs a trained byte-level BPE tokenizer.\n\"\"\"\n```"]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"4416ea89-dca0-4d05-aefb-d8dc46ad7cae","request_id":"8fd0a6927e0281ba-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"4416ea89-dca0-4d05-aefb-d8dc46ad7cae","children":["aaa292bb-bafe-49d7-bc0f-1d229b950732"]},"aaa292bb-bafe-49d7-bc0f-1d229b950732":{"id":"aaa292bb-bafe-49d7-bc0f-1d229b950732","message":{"id":"aaa292bb-bafe-49d7-bc0f-1d229b950732","author":{"role":"user","name":null,"metadata":{}},"create_time":1736050671.58767,"update_time":null,"content":{"content_type":"text","parts":["write a usage example"]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"8fd0a935ec9381ba-SIN","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"7dd8f32a-307a-4bab-a39c-09261eda94fa","children":["b285c47d-1db3-4174-bf93-5674f0733bee"]},"b285c47d-1db3-4174-bf93-5674f0733bee":{"id":"b285c47d-1db3-4174-bf93-5674f0733bee","message":{"id":"b285c47d-1db3-4174-bf93-5674f0733bee","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1736050690.294035,"update_time":null,"content":{"content_type":"text","parts":["**Creating a simple example**\n\nI\u2019m working on a Python script to train a byte-level BPE model. This involves importing functions, setting up the main function, and using process_text_file."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Thought about usage example for BPE for 7 seconds","initial_text":"Thinking","message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"aaa292bb-bafe-49d7-bc0f-1d229b950732","request_id":"8fd0a935ec9381ba-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"aaa292bb-bafe-49d7-bc0f-1d229b950732","children":["601c3b93-d120-4057-8a4e-ffbba1cae733"]},"601c3b93-d120-4057-8a4e-ffbba1cae733":{"id":"601c3b93-d120-4057-8a4e-ffbba1cae733","message":{"id":"601c3b93-d120-4057-8a4e-ffbba1cae733","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1736050690.294169,"update_time":null,"content":{"content_type":"text","parts":["Below is a simple usage example demonstrating how to import the module and run the training procedure on a sample text file. It reads the text corpus from `corpus.txt`, sets up a regex-based pretokenization pattern, includes some special tokens, and then calls `train_utf8_bytelevel_bpe` to produce a byte-level BPE vocabulary and merges file.\n\n```python\nimport re\nfrom your_bpe_module import train_utf8_bytelevel_bpe\n\ndef main():\n    # Path to the training corpus\n    input_path = \"corpus.txt\"\n\n    # Output paths for the final vocab and merges\n    output_vocab_final = \"vocab.json\"\n    output_merges_final = \"merges.txt\"\n\n    # Define some special tokens\n    special_tokens = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n\n    # A sample regex pattern for pre-tokenizing the text\n    # This pattern splits on words, numbers, punctuation, and whitespace\n    pretoken_pattern = re.compile(\n        r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n        re.UNICODE\n    )\n\n    # Desired size of the BPE vocabulary\n    vocab_size = 1000\n\n    # Call the main BPE training function\n    enumerated_vocab, merges = train_utf8_bytelevel_bpe(\n        input_path=input_path,\n        output_path_vocab_final=output_vocab_final,\n        output_path_merges_final=output_merges_final,\n        vocab_size=vocab_size,\n        special_tokens=special_tokens,\n        pretoken_pattern=pretoken_pattern,\n        save_work_periodically=False,  # No periodic saving/checkpointing in this example\n        resume_training=False,         # Start training from scratch\n    )\n\n    print(\"Training finished!\")\n    print(f\"Final vocabulary size: {len(enumerated_vocab)}\")\n    print(f\"Number of merges: {len(merges)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Steps in this example**:\n\n1. **Regex Definition**: The pattern `pretoken_pattern` controls how the text is split into initial tokens (e.g., words, numbers, punctuation).  \n2. **Run Training**: `train_utf8_bytelevel_bpe` processes the text, identifies and merges frequent byte pairs, and grows the vocabulary up to your desired size (`vocab_size = 1000`).  \n3. **Saving Results**: The final vocabulary (`vocab.json`) and merge operations (`merges.txt`) are written to disk for later use.  \n4. **Inspecting Results**: The returned `enumerated_vocab` (mapping integer IDs to byte sequences) and `merges` (list of merged byte pairs) can be printed or used directly in your code."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"b285c47d-1db3-4174-bf93-5674f0733bee","request_id":"8fd0a935ec9381ba-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"b285c47d-1db3-4174-bf93-5674f0733bee","children":["aaa2cfe8-7e44-4225-8ef9-dca040de96f5"]},"aaa2cfe8-7e44-4225-8ef9-dca040de96f5":{"id":"aaa2cfe8-7e44-4225-8ef9-dca040de96f5","message":{"id":"aaa2cfe8-7e44-4225-8ef9-dca040de96f5","author":{"role":"user","name":null,"metadata":{}},"create_time":1736056658.599404,"update_time":null,"content":{"content_type":"text","parts":["I wish to verify that periodic saving and loading of save data works as intended. Devise unit tests to be trained on a data set  that will verify that data is saved in the format as intended, and that when training is completed all at once, the output is identical to when it is broken into stages."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"8fd13b60a88a8814-SIN","message_source":null,"timestamp_":"absolute","message_type":null},"recipient":"all","channel":null},"parent":"601c3b93-d120-4057-8a4e-ffbba1cae733","children":["f0da79c3-9dd1-4dbe-af6a-b4353d1d318b"]},"f0da79c3-9dd1-4dbe-af6a-b4353d1d318b":{"id":"f0da79c3-9dd1-4dbe-af6a-b4353d1d318b","message":{"id":"f0da79c3-9dd1-4dbe-af6a-b4353d1d318b","author":{"role":"tool","name":"a8km123","metadata":{}},"create_time":1736056745.806892,"update_time":null,"content":{"content_type":"text","parts":["**Testing training strategies**\n\nI'm checking if a BPE model's training can be split into sections or done all at once. By comparing final results and JSON outputs, I see if this approach works.\n\n**Crafting unit tests**\n\nI\u2019m setting up unit tests for BPE model training. These tests will check comprehensive details like training progression, final results, and JSON format accuracy to ensure thorough validation.\n\n**Validating checkpoint accuracy**\n\nI'm setting up tests to ensure partial merges and resuming training maintain final results consistently, using a temporary directory to avoid system pollution."]},"status":"finished_successfully","end_turn":null,"weight":0.0,"metadata":{"finished_text":"Thought about unit tests for BPE training for 17 seconds","initial_text":"Thinking","message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"aaa2cfe8-7e44-4225-8ef9-dca040de96f5","request_id":"8fd13b60a88a8814-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"aaa2cfe8-7e44-4225-8ef9-dca040de96f5","children":["bac7f49a-899e-40d1-82fa-2ace654dcfa5"]},"bac7f49a-899e-40d1-82fa-2ace654dcfa5":{"id":"bac7f49a-899e-40d1-82fa-2ace654dcfa5","message":{"id":"bac7f49a-899e-40d1-82fa-2ace654dcfa5","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1736056745.806953,"update_time":null,"content":{"content_type":"text","parts":["Below is an outline of how you can structure unit tests (shown in **pytest** style for brevity) to verify that:\n\n1. **Periodic saving** works as intended (the intermediate artifacts are saved in the correct format).  \n2. **Resuming training** from saved data yields the same result as training in a single run.\n\nThese tests assume you have the following or similar methods available in your code:\n\n- `train_utf8_bytelevel_bpe(...)`  \n- `load_token_dict_from_file_latin(...)`  \n- `load_merges_from_file(...)`  \n- `is_valid_path(...)`  \n\nThe unit tests will:\n\n- Create a small, known corpus for testing.  \n- Run BPE training **in a single run**.  \n- Run BPE training **in multiple stages**, saving the state periodically and resuming from those checkpoints.  \n- Compare the final artifacts (vocab, merges, etc.) from both runs to ensure they are identical.  \n- Optionally, inspect the intermediate saved JSON and text files to confirm their format is as expected.\n\n---\n\n## Example Test File\n\nCreate a file named `test_bpe_training.py` in your tests directory, alongside your main code.\n\n```python\nimport json\nimport os\nimport shutil\nimport tempfile\nimport pytest\nimport re\n\nfrom pathlib import Path\n\n# Import the relevant BPE functions from your module\n# e.g.:\n# from your_bpe_module import (\n#     train_utf8_bytelevel_bpe,\n#     load_token_dict_from_file_latin,\n#     load_merges_from_file,\n#     is_valid_path\n# )\n\n@pytest.fixture\ndef small_corpus_file():\n    \"\"\"\n    Creates a small text corpus for testing and returns its file path.\n    The content is deliberately small and repetitive to ensure merges happen quickly.\n    \"\"\"\n    corpus_content = \"\"\"\\\nHello World\nHello Byte Pair\nByte Pair Encoding\nHello Encoding\n\"\"\"\n    temp_dir = tempfile.mkdtemp()\n    file_path = os.path.join(temp_dir, \"small_corpus.txt\")\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(corpus_content)\n    \n    yield file_path  # Provide the path to the corpus file for tests\n\n    # Cleanup after tests finish\n    shutil.rmtree(temp_dir, ignore_errors=True)\n\n\n@pytest.fixture\ndef pretoken_pattern():\n    \"\"\"\n    Sample regex pattern for splitting text into words, punctuation, or whitespace tokens.\n    \"\"\"\n    pattern = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n    return re.compile(pattern, re.UNICODE)\n\n\n@pytest.mark.parametrize(\"vocab_size\", [50]) \ndef test_bpe_training_single_run(small_corpus_file, pretoken_pattern, vocab_size):\n    \"\"\"\n    Tests BPE training from start to finish without intermediate checkpoints.\n    We store the final merges and vocab in memory and compare them later in other tests.\n    \"\"\"\n    final_vocab_path = \"vocab_single_run.json\"\n    final_merges_path = \"merges_single_run.txt\"\n\n    # Make sure we start clean\n    if os.path.exists(final_vocab_path):\n        os.remove(final_vocab_path)\n    if os.path.exists(final_merges_path):\n        os.remove(final_merges_path)\n\n    special_tokens = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n\n    enumerated_vocab, merges = train_utf8_bytelevel_bpe(\n        input_path=small_corpus_file,\n        output_path_vocab_final=final_vocab_path,\n        output_path_merges_final=final_merges_path,\n        vocab_size=vocab_size,\n        special_tokens=special_tokens,\n        pretoken_pattern=pretoken_pattern,\n        save_work_periodically=False,\n        resume_training=False,\n    )\n\n    # Check that the final vocab and merges files exist\n    assert os.path.exists(final_vocab_path), \"Final vocab file not created.\"\n    assert os.path.exists(final_merges_path), \"Final merges file not created.\"\n\n    # Also verify that enumerated_vocab and merges are not empty\n    assert len(enumerated_vocab) > 0, \"Enumerated vocab is empty.\"\n    assert len(merges) >= 0, \"Merges list should not be None.\"\n\n    # Return artifacts for other tests to use\n    return enumerated_vocab, merges\n\n\n@pytest.mark.parametrize(\"vocab_size\", [50])\ndef test_bpe_training_checkpointed_run(small_corpus_file, pretoken_pattern, vocab_size):\n    \"\"\"\n    Same as above, but training is done with periodic saving and resume.\n    We'll compare the final output with the single-run output afterwards.\n    \"\"\"\n    special_tokens = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n\n    # Use a temporary directory to store intermediate files\n    temp_dir = tempfile.mkdtemp()\n\n    final_vocab_path = os.path.join(temp_dir, \"vocab_checkpointed.json\")\n    final_merges_path = os.path.join(temp_dir, \"merges_checkpointed.txt\")\n\n    # We'll force saving every 10 merges to ensure we do multiple checkpoints\n    save_vocab_frequency = 10\n    halt_on_vocab_frequency = 200  # Something well above 50 for this test\n\n    # First run partway, then resume\n    enumerated_vocab_partial, merges_partial = train_utf8_bytelevel_bpe(\n        input_path=small_corpus_file,\n        output_path_vocab_final=final_vocab_path,\n        output_path_merges_final=final_merges_path,\n        vocab_size=vocab_size,  # final size\n        special_tokens=special_tokens,\n        pretoken_pattern=pretoken_pattern,\n        save_work_periodically=True,  # Periodic saving\n        save_vocab_frequency=save_vocab_frequency,\n        halt_on_vocab_frequency=halt_on_vocab_frequency,\n        save_directory=temp_dir,  # checkpoint directory\n        resume_training=False,    # Start from scratch\n    )\n\n    # Now, simulate resuming training until we reach vocab_size (if not reached).\n    # We'll just call the training function again with resume=True\n    enumerated_vocab_checkpointed, merges_checkpointed = train_utf8_bytelevel_bpe(\n        input_path=small_corpus_file,\n        output_path_vocab_final=final_vocab_path,\n        output_path_merges_final=final_merges_path,\n        vocab_size=vocab_size,\n        special_tokens=special_tokens,\n        pretoken_pattern=pretoken_pattern,\n        save_work_periodically=True,\n        save_vocab_frequency=save_vocab_frequency,\n        halt_on_vocab_frequency=halt_on_vocab_frequency,\n        save_directory=temp_dir,\n        resume_training=True,  # Resume from the partial run\n    )\n\n    # Final check: file paths must exist after the resumed training\n    assert os.path.exists(final_vocab_path), \"Final vocab file not created after resume.\"\n    assert os.path.exists(final_merges_path), \"Final merges file not created after resume.\"\n\n    # Return final artifacts so we can compare them to the single-run approach\n    return enumerated_vocab_checkpointed, merges_checkpointed\n\n\ndef test_bpe_end_to_end_equivalence(\n    small_corpus_file,\n    pretoken_pattern\n):\n    \"\"\"\n    High-level test:\n    1) Trains in a single run (no checkpoints)\n    2) Trains in multiple checkpoints (resuming from partially saved data)\n    3) Compares final merges & vocab from both approaches\n    \"\"\"\n    # 1) Single-run\n    single_vocab_artifacts = test_bpe_training_single_run(small_corpus_file, pretoken_pattern, vocab_size=50)\n    enumerated_vocab_single, merges_single = single_vocab_artifacts\n\n    # 2) Checkpointed run\n    checkpointed_artifacts = test_bpe_training_checkpointed_run(small_corpus_file, pretoken_pattern, vocab_size=50)\n    enumerated_vocab_checkpointed, merges_checkpointed = checkpointed_artifacts\n\n    # Compare final enumerated vocab lengths and merges\n    assert len(enumerated_vocab_single) == len(enumerated_vocab_checkpointed), \\\n        \"Vocab sizes differ between single-run and checkpointed-run.\"\n\n    assert merges_single == merges_checkpointed, \\\n        \"Final merges differ between single-run and checkpointed-run.\"\n\n    # If you want, also compare the dictionary contents entry-by-entry\n    # Note: enumerated_vocab is typically { int: bytes }, so you can do:\n    # for idx, byte_seq in enumerated_vocab_single.items():\n    #     assert idx in enumerated_vocab_checkpointed\n    #     assert enumerated_vocab_checkpointed[idx] == byte_seq\n    #\n    # Or, you can do a direct equality check if ordering doesn't matter:\n    # assert enumerated_vocab_single == enumerated_vocab_checkpointed\n\n    print(\"End-to-end equivalence test passed: single-run and checkpointed-run match.\")\n\n\ndef test_intermediate_files_format():\n    \"\"\"\n    Optional: check intermediate JSON and text files for correct format.\n    For example:\n      1) Ensure JSON is valid\n      2) Confirm merges text file has lines with two space-separated tokens\n      3) Validate no unexpected keys\n    \"\"\"\n    temp_dir = tempfile.mkdtemp()\n    # (Hypothetically) create or copy some known JSON and merges files into temp_dir\n    vocab_test_file = os.path.join(temp_dir, \"test_vocab.json\")\n    merges_test_file = os.path.join(temp_dir, \"test_merges.txt\")\n\n    # Write sample JSON to vocab_test_file\n    with open(vocab_test_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"Hello\": 10, \"World\": 12}, f)\n\n    # Write sample merges to merges_test_file\n    with open(merges_test_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"He llo\\nWo rld\\n\")\n\n    # 1) Validate JSON\n    with open(vocab_test_file, \"r\", encoding=\"utf-8\") as f:\n        loaded = json.load(f)\n    assert \"Hello\" in loaded\n    assert loaded[\"Hello\"] == 10\n\n    # 2) Validate merges format\n    with open(merges_test_file, \"r\", encoding=\"utf-8\") as f:\n        lines = [l.strip() for l in f.readlines()]\n    for line in lines:\n        splits = line.split(\" \")\n        assert len(splits) == 2, f\"Merge line '{line}' should contain exactly two space-separated items.\"\n\n    shutil.rmtree(temp_dir, ignore_errors=True)\n    print(\"Intermediate files format test passed.\")\n```\n\n### Explanation of Key Tests\n\n1. **`test_bpe_training_single_run`**  \n   - Trains your BPE tokenizer on a small corpus from scratch, **without** periodic saving or resuming.  \n   - Verifies that the final vocabulary and merges files are created, and that the in-memory structures (`enumerated_vocab`, `merges`) are not empty.\n\n2. **`test_bpe_training_checkpointed_run`**  \n   - Uses the same corpus but **enables periodic saving**.  \n   - Partially trains the tokenizer and saves the intermediate state, then calls `train_utf8_bytelevel_bpe(...)` **again** with `resume_training=True`.  \n   - Ensures the final artifacts (vocab and merges) exist after resume.\n\n3. **`test_bpe_end_to_end_equivalence`**  \n   - Invokes both the single-run test and the checkpointed-run test.  \n   - Compares the final outputs (vocabulary size, merges) to confirm that splitting training into stages yields the **identical** final result as a single uninterrupted training run.\n\n4. **`test_intermediate_files_format`** *(Optional)*  \n   - Illustrates checking the **format** (e.g., valid JSON, expected merges lines) of any intermediate output files.  \n   - This test can be extended or customized to suit your file format requirements.\n\n---\n\n## How To Run These Tests\n\n1. **Install pytest** (if not already):  \n   ```bash\n   pip install pytest\n   ```\n2. **Place the above file** (`test_bpe_training.py`) in your tests directory (or in the same directory as your module).\n3. **Run the tests** from the command line:  \n   ```bash\n   pytest test_bpe_training.py\n   ```\n4. **Observe the output**. If all tests pass, you should see output similar to:\n   ```\n   ========================= test session starts =========================\n   collected 4 items\n\n   test_bpe_training.py ...                                          [100%]\n\n   ========================== 4 passed in Xs ===========================\n   ```\n\nBy following this structure, you validate that:\n\n- **Periodic saving** to disk produces valid, readable JSON and text files at each checkpoint.  \n- **Resuming** from those saved checkpoints recovers the training state accurately.  \n- **No difference** exists in the final merges or vocabulary, whether the training completes in a single run or in multiple runs with checkpoints."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"message_type":null,"model_slug":"o1","default_model_slug":"o1","parent_id":"f0da79c3-9dd1-4dbe-af6a-b4353d1d318b","request_id":"8fd13b60a88a8814-SIN","timestamp_":"absolute"},"recipient":"all","channel":null},"parent":"f0da79c3-9dd1-4dbe-af6a-b4353d1d318b","children":[]}}}