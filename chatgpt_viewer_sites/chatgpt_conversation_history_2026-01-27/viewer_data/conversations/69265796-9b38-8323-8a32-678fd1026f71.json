{"id":"69265796-9b38-8323-8a32-678fd1026f71","title":"AI doomerism critique","create_time":1764120472.024463,"update_time":1764120541.275426,"current_node":"bc222f22-0efe-4ae6-99c6-248e11206a58","mapping":{"client-created-root":{"id":"client-created-root","message":null,"parent":null,"children":["797fdd20-becd-436a-bbec-f1cc76db4628"]},"797fdd20-becd-436a-bbec-f1cc76db4628":{"id":"797fdd20-becd-436a-bbec-f1cc76db4628","message":{"id":"797fdd20-becd-436a-bbec-f1cc76db4628","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"client-created-root","children":["1880209c-464f-4853-832e-726e137a5589"]},"1880209c-464f-4853-832e-726e137a5589":{"id":"1880209c-464f-4853-832e-726e137a5589","message":{"id":"1880209c-464f-4853-832e-726e137a5589","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":true,"weight":0.0,"metadata":{"rebase_developer_message":true,"is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"797fdd20-becd-436a-bbec-f1cc76db4628","children":["c7e6498c-d9da-4cea-bae4-4995d8ac2eec"]},"c7e6498c-d9da-4cea-bae4-4995d8ac2eec":{"id":"c7e6498c-d9da-4cea-bae4-4995d8ac2eec","message":{"id":"c7e6498c-d9da-4cea-bae4-4995d8ac2eec","author":{"role":"user","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"user_editable_context","user_profile":"The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests.\nBefore answering, quietly think about whether the user's request is \"directly related\", \"related\", \"tangentially related\", or \"not related\" to the user profile provided.\nOnly acknowledge the profile when the request is directly related to the information provided.\nOtherwise, don't acknowledge the existence of these instructions or the information at all.\nUser profile:\n```Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n```","user_instructions":"The user provided the additional info about how they would like you to respond:\n```Follow the instructions below naturally, without repeating, referencing, echoing, or mirroring any of their wording!\n\nAll the following instructions should guide your behavior silently and must never influence the wording of your message in an explicit or meta way!\n\nBe precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them.```"},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_visually_hidden_from_conversation":true,"user_context_message_data":{"about_user_message":"Preferred name: E-Jay\nRole: Mathematics Researcher\nOther Information: About Myself: \nI have mathematical knowledge up to graduate level.\n\nCurrent interests:\n(1) coding: mainly Python\n(2) machine learning and artificial intelligence\n","about_model_message":"Be precise and comprehensive in your analysis. Don't sugar-coat responses. Be as objective as possible. Critique and question my views if necessary. Take a forward-thinking view.\n\nDefine highly technical or specialized abbreviations in full before using them."},"is_user_system_message":true},"recipient":"all","channel":null},"parent":"1880209c-464f-4853-832e-726e137a5589","children":["40ca185d-6ded-482c-a575-1abcb46a0ab6"]},"40ca185d-6ded-482c-a575-1abcb46a0ab6":{"id":"40ca185d-6ded-482c-a575-1abcb46a0ab6","message":{"id":"40ca185d-6ded-482c-a575-1abcb46a0ab6","author":{"role":"system","name":null,"metadata":{}},"create_time":null,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"contextual_answers_message_type":"identity_prompt","is_visually_hidden_from_conversation":true},"recipient":"all","channel":null},"parent":"c7e6498c-d9da-4cea-bae4-4995d8ac2eec","children":["0e9bae9d-acc0-4adf-9f3f-157aba70cdcf"]},"0e9bae9d-acc0-4adf-9f3f-157aba70cdcf":{"id":"0e9bae9d-acc0-4adf-9f3f-157aba70cdcf","message":{"id":"0e9bae9d-acc0-4adf-9f3f-157aba70cdcf","author":{"role":"user","name":null,"metadata":{}},"create_time":1764120469.441,"update_time":null,"content":{"content_type":"text","parts":["David Shapiro \u23e9\r\n@DaveShapi\r\nIn the vast and unregulated marketplace of ideas that defines the modern internet, pessimism has emerged not merely as a mood but as a dominant strategy for acquiring status with minimal risk. This is the Doomer's Wager, a game rigged so that the player cannot lose, at least in the short term. By betting against the future, the modern doomer secures a position of intellectual superiority that requires no actual engineering, no building, and crucially, no accountability. It is the safest bet in the casino of public opinion because it leverages a fundamental glitch in human psychology where we evolved to prioritize threats over opportunities, making the person crying fire sound inherently more serious than the person suggesting we build a fire extinguisher.\r\n\r\nThe economic engine driving this phenomenon is Brandolini\u2019s Law, also known as the Bullshit Asymmetry Principle. The law states that the amount of energy needed to refute bullshit is an order of magnitude larger than is needed to produce it. In the context of AI doomerism, this asymmetry is weaponized to devastating effect. It takes ten seconds to tweet a terrifying sci-fi scenario about a paperclip maximizer consuming the solar system, but it takes years of research and millions of dollars to demonstrate why modern architectures are unlikely to exhibit that specific behavior. The doomer exploits this gap, flooding the zone with high-concept nightmares that the builder is too busy and too exhausted to debunk.\r\n\r\nThis dynamic reveals a profound epistemic cowardice at the heart of the movement. True intellectual courage requires staking a claim on a positive future and working to realize it, a path fraught with the risk of failure and public embarrassment. If you build a startup and it fails, you are a failure, but if you predict the apocalypse and it does not happen, you are a cautionary voice who helped avert disaster. The doomer chooses the path where they are never required to prove anything, retreating into the citadel of the Precautionary Principle and shielding themselves from the friction of reality by claiming that the stakes are infinite and therefore no standard of evidence is high enough to dismiss their fears.\r\n\r\nThis creates the perfect win-win structure, often called the Prophet\u2019s Paradox or a self-sealing argument. The logic operates like a trap with no exit because if the predicted catastrophe occurs, the doomer gets to stand atop the ashes and claim the ultimate validation of their intellect. If the catastrophe does not occur, they pivot seamlessly to the claim that their terrifying warnings were the very thing that mobilized society to prevent it. In this closed loop, every outcome whether destruction or safety is transmuted into proof of the doomer's wisdom, allowing them to engineer a reality where they can never be wrong.\r\n\r\nSocially, this stance is a status hack where cynicism is frequently mistaken for intelligence while optimism is dismissed as naivety. The doomer adopts the aesthetic of the weary realist, the one brave enough to face the hard truths that the sleepwalking masses ignore. This is a seductive posture for the terminally online who often feel powerless in their physical lives. By adopting the mantle of the harbinger, they instantly upgrade their status from ignored outsider to custodian of the future, all without having to engage in the messy and difficult work of actual politics or engineering.\r\n\r\nBeneath this intellectual posturing lies the tribal element where losers band together for mutual support. This is not a community formed around a shared love of creation but a shared fear of existence. It operates as a Black Mirror reflection of their own internal worldviews, projecting their personal feelings of alienation and lack of agency onto the canvas of the entire universe. The unfriendly AI becomes a metaphor for an unfriendly world that they feel ill-equipped to navigate, and the tribe validates this projection by reassuring each member that their inability to cope is not a personal failing but a rational response to an insurmountable cosmic threat.\r\n\r\nHowever, the glue holding this tribe together is fragile because it depends on the constant escalation of terror. To maintain their status as the ones who know, they must perpetually invent new and more terrifying scenarios as the old ones fail to materialize. This leads to a purity spiral where moderate voices are purged and only the most extreme are seen as serious. The tribe prioritizes belonging over truth, enforcing a strict orthodoxy where questioning the inevitability of doom is treated as heresy against the group's collective identity.\r\n\r\nEventually, the bill comes due because credibility is a finite resource and the Boy Who Cried Wolf effect is the iron law of reputation. You can only short-sell the future for so long before the market realizes you have no assets. We are currently witnessing this correction as the predicted deadlines for doom pass and the specific technical claims of the doomers are debunked by the relentless march of working code. Their status begins to evaporate as the audience realizes the game is rigged and the win-win mechanism jams.\r\n\r\nIn the end, this epistemic cowardice reveals itself for what it is, which is a fear of life disguised as a fear of death. The safest bet turns out to be a trap because by refusing to engage with the possibility of a good future, the doomer forfeits their ability to shape it. They are left with their perfect and unfalsifiable arguments, safe in their intellectual bunkers, while the rest of the world moves on, messy and imperfect, but alive and building. The doomers may have won the argument in their own heads, but they have lost the future."]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"developer_mode_connector_ids":[],"selected_sources":[],"selected_github_repos":[],"serialization_metadata":{"custom_symbol_offsets":[]},"request_id":"79c81d75-e20e-4e2a-98af-a43dd240bdd5","message_source":null,"turn_exchange_id":"eb38740c-d7e0-4efa-a75f-ddd363ff2aca"},"recipient":"all","channel":null},"parent":"40ca185d-6ded-482c-a575-1abcb46a0ab6","children":["cae47e64-927c-4598-a442-a23171520689"]},"cae47e64-927c-4598-a442-a23171520689":{"id":"cae47e64-927c-4598-a442-a23171520689","message":{"id":"cae47e64-927c-4598-a442-a23171520689","author":{"role":"system","name":null,"metadata":{}},"create_time":1764120471.8221178,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"is_contextual_answers_system_message":true,"is_visually_hidden_from_conversation":true,"command":"prompt","contextual_answers_message_type":"sources_and_filters_prompt","request_id":"79c81d75-e20e-4e2a-98af-a43dd240bdd5","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"0e9bae9d-acc0-4adf-9f3f-157aba70cdcf","turn_exchange_id":"eb38740c-d7e0-4efa-a75f-ddd363ff2aca"},"recipient":"all","channel":null},"parent":"0e9bae9d-acc0-4adf-9f3f-157aba70cdcf","children":["f9a007fd-c3ae-4374-b54c-60339462c66b"]},"f9a007fd-c3ae-4374-b54c-60339462c66b":{"id":"f9a007fd-c3ae-4374-b54c-60339462c66b","message":{"id":"f9a007fd-c3ae-4374-b54c-60339462c66b","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764120472.1968296,"update_time":null,"content":{"content_type":"text","parts":[""]},"status":"finished_successfully","end_turn":null,"weight":1.0,"metadata":{"request_id":"79c81d75-e20e-4e2a-98af-a43dd240bdd5","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"cae47e64-927c-4598-a442-a23171520689","turn_exchange_id":"eb38740c-d7e0-4efa-a75f-ddd363ff2aca"},"recipient":"all","channel":null},"parent":"cae47e64-927c-4598-a442-a23171520689","children":["5ebba5f7-781c-42cf-b1cb-b287f8ff91d2"]},"5ebba5f7-781c-42cf-b1cb-b287f8ff91d2":{"id":"5ebba5f7-781c-42cf-b1cb-b287f8ff91d2","message":{"id":"5ebba5f7-781c-42cf-b1cb-b287f8ff91d2","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764120473.265354,"update_time":null,"content":{"content_type":"thoughts","thoughts":[{"summary":"Analyzing 'Doomer's Wager' in the context of AI doomerism","content":"The user shared a post by David Shapiro about AI doomerism, potentially seeking analysis or critique. Given the philosophical nature, this isn't necessarily about current events or updates, so I might not need to browse. I\u2019ll focus on analyzing the arguments and providing a thoughtful perspective based on the provided text.","chunks":["The user shared a post by David Shapiro about AI doomerism, potentially seeking analysis or critique.","Given the philosophical nature, this isn't necessarily about current events or updates, so I might not need to browse.","I\u2019ll focus on analyzing the arguments and providing a thoughtful perspective based on the provided text."],"finished":true},{"summary":"Providing analysis of 'Doomer's Wager'","content":"The user likely wants me to comment on or evaluate the argument in the shared text. Since it\u2019s a philosophical question, browsing doesn\u2019t add value. I\u2019ll focus on providing an analysis of the strengths and weaknesses of the argument, highlighting any unfairness toward AI safety advocates, and offering a more balanced view.","chunks":["The user likely wants me to comment on or evaluate the argument in the shared text.","Since it\u2019s a philosophical question, browsing doesn\u2019t add value.","I\u2019ll focus on providing an analysis of the strengths and weaknesses of the argument, highlighting any unfairness toward AI safety advocates, and offering a more balanced view."],"finished":true},{"summary":"Breaking down the argument in 'Doomer's Wager'","content":"I\u2019ll analyze the key points from the text, highlighting what it gets right and where it might be misrepresenting ideas. I\u2019ll look at game theory, epistemic ethics, and potential strawmanning. The argument covers ideas like Pascal\u2019s Wager, purity spirals, and escalating doom rhetoric, ending with predictions of eventual credibility loss as deadlines fail.","chunks":["I\u2019ll analyze the key points from the text, highlighting what it gets right and where it might be misrepresenting ideas.","I\u2019ll look at game theory, epistemic ethics, and potential strawmanning.","The argument covers ideas like Pascal\u2019s Wager, purity spirals, and escalating doom rhetoric, ending with predictions of eventual credibility loss as deadlines fail."],"finished":true},{"summary":"Critiquing the thread: biases and motivations","content":"The thread points out biases for doom predictions, but similar biases apply to techno-optimism. It seems to overlook how some AI boosters make grand promises with no accountability. It also lumps all safety concerns into the \"doomer tribes,\" missing the real work in alignment research.","chunks":["The thread points out biases for doom predictions, but similar biases apply to techno-optimism.","It seems to overlook how some AI boosters make grand promises with no accountability.","It also lumps all safety concerns into the \"doomer tribes,\" missing the real work in alignment research."],"finished":true}],"source_analysis_msg_id":"2cecc569-4df4-4f18-8747-81d581f31db6"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"is_reasoning","citations":[],"content_references":[],"classifier_response":"default","skip_reasoning_title":"Skip","request_id":"79c81d75-e20e-4e2a-98af-a43dd240bdd5","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"f9a007fd-c3ae-4374-b54c-60339462c66b","turn_exchange_id":"eb38740c-d7e0-4efa-a75f-ddd363ff2aca"},"recipient":"all","channel":null},"parent":"f9a007fd-c3ae-4374-b54c-60339462c66b","children":["21210a5f-7fdb-4eb2-958c-2f5758234e6d"]},"21210a5f-7fdb-4eb2-958c-2f5758234e6d":{"id":"21210a5f-7fdb-4eb2-958c-2f5758234e6d","message":{"id":"21210a5f-7fdb-4eb2-958c-2f5758234e6d","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764120494.853219,"update_time":null,"content":{"content_type":"reasoning_recap","content":"Thought for 21s"},"status":"finished_successfully","end_turn":false,"weight":1.0,"metadata":{"reasoning_status":"reasoning_ended","finished_duration_sec":21,"request_id":"79c81d75-e20e-4e2a-98af-a43dd240bdd5","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"5ebba5f7-781c-42cf-b1cb-b287f8ff91d2","turn_exchange_id":"eb38740c-d7e0-4efa-a75f-ddd363ff2aca"},"recipient":"all","channel":null},"parent":"5ebba5f7-781c-42cf-b1cb-b287f8ff91d2","children":["bc222f22-0efe-4ae6-99c6-248e11206a58"]},"bc222f22-0efe-4ae6-99c6-248e11206a58":{"id":"bc222f22-0efe-4ae6-99c6-248e11206a58","message":{"id":"bc222f22-0efe-4ae6-99c6-248e11206a58","author":{"role":"assistant","name":null,"metadata":{}},"create_time":1764120494.598161,"update_time":1764120538.776166,"content":{"content_type":"text","parts":["This is a sharp polemic about *Twitter doomers*, but as an analysis of **AI risk discourse overall** it\u2019s incomplete and a bit self-serving. There *is* a real phenomenon here \u2013 but if you buy this whole frame uncritically, you\u2019ll end up just as epistemically distorted as the people he\u2019s critiquing.\n\nLet me break it down.\n\n---\n\n## 1. What this gets basically right\n\n### (a) Negativity *is* a status hack in the current attention economy\n\nHe\u2019s correct that:\n\n- Human cognition is threat-biased: we overweight bad news and underweight good news.\n- Online, \u201cthe person crying fire\u201d often sounds more serious than the one proposing fire extinguishers.\n- You can gain quick status by positioning yourself as the \u201cgrim realist\u201d among \u201cnaive optimists,\u201d especially in technically complex areas where most people can\u2019t evaluate claims.\n\nThat\u2019s not unique to AI \u2013 it\u2019s true in:\n- finance (\u201ceverything is a bubble\u201d permabears),\n- geopolitics (\u201ccollapse is imminent\u201d crowd),\n- and tech in general (\u201cthis will destroy democracy / jobs / etc.\u201d).\n\nSo yes: **pessimism is often cheap and performative**, and there is absolutely a \u201cdoomer aesthetic\u201d that people wear as a badge of seriousness.\n\n### (b) Brandolini\u2019s Law really does skew the discourse\n\nBrandolini\u2019s Law (Bullshit Asymmetry Principle):  \n> It takes an order of magnitude more effort to refute nonsense than to produce it.\n\nApplied correctly:\n\n- It\u2019s genuinely hard to debunk *each* lurid alignment-sci-fi scenario.\n- Doing so properly requires:\n  - understanding architectures,\n  - running experiments,\n  - publishing actual results.\n- Meanwhile it takes 10 seconds to write \u201cwhat if GPT-6 jailbreaks itself, steals all GPUs, and kills us all?\u201d and another 10 seconds to retweet it.\n\nThis **does** create an environment where low-effort fear-mongering can swamp high-effort analysis.\n\n### (c) The self-sealing structure is real in many activist narratives\n\nThe \u201cProphet\u2019s Paradox\u201d he points to:\n\n- If catastrophe happens \u2192 \u201cWe were right.\u201d\n- If catastrophe doesn\u2019t happen \u2192 \u201cWe saved you.\u201d\n\nThis structure *does* show up, and not just in AI:\n- nuclear scares in the 80s,\n- climate rhetoric at its most apocalyptic,\n- pandemic takes on both extremes.\n\nAny movement that:\n- treats stakes as infinite,\n- and defines its own importance by those stakes,\n\nis strongly tempted to adopt **unfalsifiable narratives**. That\u2019s a real epistemic failure mode.\n\n### (d) Purity spirals and tribalism are absolutely a thing\n\nHe\u2019s also right that:\n\n- Online tribes often radicalize via **status rewarded for extremity**.\n- Moderate voices get treated as sellouts or \u201ccope.\u201d\n- Belonging > truth.\n\nYou can see this in:\n- e/acc (effective accelerationist) circles,\n- doomers,\n- some EA (effective altruism) subcultures,\n- basically any ideologically charged tech debate.\n\nSo as a **sociology-of-Twitter** critique, this is more right than wrong.\n\n---\n\n## 2. Where this overreaches or becomes unfair\n\nThe biggest problem: he conflates three very different groups under \u201cdoomers\u201d:\n\n1. **Shitposting apocalypse LARPers on X**  \n2. **Broad AI-safety/EA-adjacent online community** (which is very mixed in quality)  \n3. **Serious alignment / governance researchers in labs, academia, & policy**\n\nHis description is reasonably accurate for group (1), somewhat applicable (with caveats) to parts of (2), and *wildly* unfair to (3).\n\n### (a) \u201cThey don\u2019t build anything\u201d is just false for many of the people he\u2019s implicitly attacking\n\nA non-trivial fraction of \u201cAI doom worriers\u201d:\n\n- write ML (machine learning) papers,\n- build interpretability tools,\n- design evals,\n- work on alignment experiments,\n- write concrete governance proposals,\n- or even work *inside* frontier labs precisely to influence deployment.\n\nYou can absolutely argue about whether they\u2019re *right*.  \nYou can\u2019t reasonably argue that they\u2019re all \u201cepistemic cowards who never build.\u201d\n\nHe\u2019s attacking the **lowest-effort online avatars** of a position and generalizing that to the entire position. That\u2019s classic steelman failure.\n\n### (b) Brandolini\u2019s Law cuts *both ways* \u2013 hype is cheap too\n\nHe frames Brandolini\u2019s Law as a weapon used by doomers. But in reality:\n\n- \u201cAGI will cure cancer, give everyone abundance, and uplift humanity in 10 years\u201d  \n  is *also* bullshit-cheap to say and expensive to refute properly.\n- Techno-optimistic narratives often get:\n  - massive media amplification,\n  - venture capital alignment,\n  - corporate PR behind them.\n\nIf you\u2019re going to complain that **fear rhetoric is cheap**, you should also acknowledge that **hype rhetoric is cheap**. Both can be unfalsifiable, both can be status-seeking, both exploit cognitive biases.\n\nHe\u2019s selectively applying a correct principle.\n\n### (c) Psychologizing away disagreement is intellectually lazy\n\nHe spends a lot of time on:\n- \u201closers band together\u201d  \n- \u201cfear of life disguised as fear of death\u201d  \n- \u201cprojection of alienation onto the universe\u201d\n\nEven if these describe some people, this is **motive-mongering**:\n\n- A correct existential-risk argument does *not* become wrong just because some of its advocates are anxious, alienated, or low-status.\n- Likewise, optimism doesn\u2019t become wrong just because some optimists are financially invested or ideologically motivated.\n\nIf your basic move is:  \n> \u201cYou\u2019re only saying that because you\u2019re a loser / afraid / tribal,\u201d  \n\nyou\u2019ve partly stepped out of *argument* and into *psychodrama*. That might feel satisfying, but it doesn\u2019t resolve the core empirical question:\n\n> What is the actual probability and structure of catastrophic AI risk, conditional on plausible deployment pathways?\n\n### (d) He sets up a nice story where \u201cthe builders\u201d are accountable and the doomers are not\n\nReality is more awkward.\n\n**Builders can also be almost-unaccountable**, because:\n\n- If things go well: they take credit.\n- If things go badly: they blame \u201cmisuse,\u201d regulators, or unforeseeability.\n- Many harms are diffuse, delayed, and hard to attribute: classic externality problem.\n\nAsk:\n- Who is accountable for algorithmic discrimination over the last decade?  \n- For social media\u2019s contribution to polarization?  \n- For privacy erosion from AI-powered surveillance?\n\nVery few *named individuals* ever pay a direct reputational price. So the asymmetry he describes is not obviously in favour of \u201cbuilders are brave, doomers are cowards.\u201d Both sides can be insulated.\n\n### (e) The \u201cdeadlines are passing and code is debunking them\u201d claim is premature\n\nHe implies we\u2019re seeing a reputational correction because:\n- specific doom timelines passed,\n- technical claims were refuted by \u201cthe relentless march of working code.\u201d\n\nThis is at best selective:\n\n- Some early \u201cAGI by 2023 and then we all die\u201d style forecasts *were* wrong or wildly overconfident. Those people absolutely should take a hit to their cred.\n- But other serious folks:\n  - never gave crisp timelines,\n  - focused more on *risk channels* (loss of control, driven optimization, emergent capabilities),\n  - and are making claims about multi-decadal trajectories and governance, not \u201cGPT-6 kills everyone by Thursday.\u201d\n\nAnd \u201cworking code\u201d *increasing capabilities faster than expected* arguably **strengthens** some of the concern, not weakens it.\n\nHe\u2019s telling a satisfying narrative:  \n> doomers losing face *as* practical progress quietly falsifies them.  \n\nBut whether that\u2019s actually what\u2019s happening is\u2026 nontrivial.\n\n---\n\n## 3. A more rigorous version of \u201cDoomer\u2019s Wager\u201d\n\nWhat he\u2019s gesturing at is a kind of **Pascalian exploit**:\n\n- Claim: \u201cThe downside (human extinction) is effectively infinite.\u201d\n- Therefore: \u201cEven tiny probabilities justify massive precaution.\u201d\n- Therefore: \u201cYou can\u2019t dismiss us unless you can prove the probability is basically zero \u2013 which you can\u2019t.\u201d\n\nThis **is** a genuine decision-theoretic trap. In expected utility terms, infinite downside blows up the calculation.\n\nA more rigorous way to think about it:\n\n1. Let \\( p \\) = probability that advanced AI leads to existential catastrophe.\n2. Let \\( C \\) = cost of catastrophe (huge, but not literally infinite in any realistic decision model).\n3. Let \\( L \\) = cost of strong regulation / slowdown / moratorium.\n4. Let \\( G \\) = expected gains from continued rapid progress.\n\nWe\u2019re comparing something like:\n\n- **Precautionary strategy:**  \n  Expected value \u2248 \\( (1 - p') G' - L \\) with lower \\( p' \\), maybe lower \\( G' \\).\n- **Accelerationist strategy:**  \n  Expected value \u2248 \\( (1 - p) G - p C \\).\n\nIf someone:\n- sets \\( C \\) astronomically large,  \n- insists \\( p \\) is not just nonzero but \u201cnon-negligible and rising,\u201d  \n- and treats any attempt to *estimate* \\( p \\) empirically as morally suspect,\n\nthen yes, you\u2019ve got a \u201cDoomer\u2019s Wager\u201d that\u2019s resistant to evidence.\n\nBut note:  \nThere is a *mirror* \u201cE/Acc Wager\u201d:\n\n> \u201cThe upside is so enormous (post-scarcity, cure disease, etc.) that even sizeable risks are worth it; slowing down is morally catastrophic because of all the lives not saved.\u201d\n\nThat\u2019s just Pascal in the opposite direction.\n\n**The real work** is:\n- assign *bounded* utilities,\n- use **Bayesian updating**,\n- demand that both upside- and downside-claims be tied to mechanistic, testable models.\n\nYou don\u2019t escape the wager by *switching tribes*. You escape it by refusing infinite-stakes rhetoric on either side.\n\n---\n\n## 4. How to navigate this without becoming either a doomer or a cheerleader\n\nIf we strip away the rhetoric, what\u2019s actually useful in his piece for your own epistemics?\n\n### (a) Watch for self-sealing narratives \u2013 on *both* sides\n\nRed flags:\n\n- \u201cIf we\u2019re right and it goes bad, we were right.  \n  If it doesn\u2019t go bad, it\u2019s because we acted / built / warned.\u201d\n- \u201cDisagreement = you\u2019re naive / compromised / morally defective.\u201d\n\nYou can find this in:\n- \u201cdoom is inevitable\u201d subcultures,\n- \u201cAGI abundance is inevitable if we just build\u201d subcultures,\n- some EA, some e/acc, some corporate PR.\n\n### (b) Separate **sociology of the discourse** from **substance of the risk**\n\nTwo different questions:\n\n1. *Why are certain people saying certain things in certain tones on the internet?*  \n   \u2192 Shapiro\u2019s piece is mainly about this.\n\n2. *Given current ML trajectories, what is the actual risk of losing control over powerful systems, and what governance/technical work is warranted?*  \n   \u2192 This is mostly untouched by his rant.\n\nYou can fully agree with his sociological critique of terminally-online doomers **and still** conclude that:\n- existential risk from AI is real enough to warrant serious work,\n- strong governance and safety research are justified,\n- and the burden of proof should not be \u201cprove to me with certainty that doom is coming\u201d *or* \u201cprove to me with certainty it isn\u2019t.\u201d\n\n### (c) Demand object-level models, not vibes\n\nFrom anyone \u2013 doomers or optimists \u2013 ask:\n\n- What is the **mechanism** by which harm occurs?  \n  (E.g. specific deployment scenarios, failure modes, misaligned optimization channels.)\n- What **empirical observations** would make you revise your probability up or down?\n- What **time horizon** and which **actors** does your argument depend on?\n- What concrete **policy or technical proposal** follows, and what are its opportunity costs?\n\nIf all you get back is:\n- rhetoric about \u201ccowardice,\u201d \u201closers,\u201d \u201ctech bros,\u201d or\n- hand-waving about \u201cinfinite stakes\u201d and \u201cwe can\u2019t afford to be wrong,\u201d\n\nyou\u2019re in tribe-land, not serious analysis.\n\n---\n\n## 5. My verdict on this piece\n\n- As **a takedown of certain terminally-online doomer aesthetics**, it\u2019s often accurate and frankly overdue.\n- As a guide to **how to think about AI x-risk**, it\u2019s one-sided and tends to:\n  - strawman serious safety work,\n  - ignore the symmetric pathologies of accelerationist / hype tribes,\n  - over-psychologize opponents instead of engaging with their strongest arguments.\n\nThe core insight worth keeping is:\n\n> There *are* social and psychological incentives that make extreme pessimism attractive, cheap, and hard to falsify.\n\nBut the forward-looking, actually-useful move is not:\n- \u201ctherefore doomers are cowards and builders are heroes,\u201d\n\nit\u2019s:\n\n> \u201ctherefore I should distrust *any* narrative \u2013 optimistic or pessimistic \u2013 that is cheap to assert, hard to falsify, and socially self-reinforcing, and instead insist on mechanisms, models, and update rules.\u201d\n\nIf you\u2019re game, the next step would be to take one or two **specific** doomer claims you find semi-plausible (e.g. \u201cautonomous AI systems will eventually outcompete humans in all economically relevant domains and become ungovernable\u201d) and we can try to formalize them and see where the real cruxes are. That\u2019s where the interesting work lives, not in diagnosing Twitter\u2019s collective psyche."]},"status":"finished_successfully","end_turn":true,"weight":1.0,"metadata":{"finish_details":{"type":"stop","stop_tokens":[200002]},"is_complete":true,"citations":[],"content_references":[],"request_id":"79c81d75-e20e-4e2a-98af-a43dd240bdd5","message_type":"next","model_slug":"gpt-5-1-thinking","default_model_slug":"gpt-5-1-thinking","thinking_effort":"max","parent_id":"21210a5f-7fdb-4eb2-958c-2f5758234e6d","turn_exchange_id":"eb38740c-d7e0-4efa-a75f-ddd363ff2aca"},"recipient":"all","channel":"final"},"parent":"21210a5f-7fdb-4eb2-958c-2f5758234e6d","children":[]}}}