# Problem Statement

An urn initially contains one red and one blue ball. At each step we draw one ball at random from the urn, with the uniform distribution over the balls currently in the urn.

* If the ball drawn is red, we place one additional red and one additional blue ball into the urn.
* When we draw a blue ball for the $k$-th time, we place one blue and $2k+1$ red balls into the urn.

(The originally drawn balls are not removed; they remain in the urn.)

Let $G_n$ denote the number of balls in the urn after $n$ steps. Prove that there exist constants $0 < c, \alpha < \infty$ such that with probability $1$ we have:

$$\frac{G_n}{n^\alpha} \longrightarrow c$$

as n tends to infinity.

---

# FIRST SOLUTION ATTEMPT by problem solver

## 1. Status  

Status: Partial  

---

## 2. Selected approaches  

Selected approach indices: 1, 3 (combination)  

- I follow the reduction to the one–dimensional Markov chain \(K_n\) and the drift/martingale analysis from Approach 1.  
- As the main scaling tool I use the potential function \(Y_n = K_n^3/n^2\) from Approach 3, which linearizes the deterministic drift and fits standard stochastic-approximation ideas.  

I push these as far as I can **rigorously**, then explain exactly where a full proof of almost sure convergence to a deterministic constant is missing.

---

## 3. Solution  

### 3.1. Reduction to the process of blue draws

Let:

- \(R_n\) = number of red balls after \(n\) steps,  
- \(B_n\) = number of blue balls after \(n\) steps,  
- \(G_n = R_n + B_n\) = total number of balls after \(n\) steps,  
- \(K_n\) = number of *blue draws* in the first \(n\) steps.

We first compute \(R_n,B_n,G_n\) in terms of \(K_n\).

#### 3.1.1. Blue balls

At each step (regardless of colour drawn):

- we always add exactly **one** blue ball (either \(+1\) from the red rule, or \(+1\) from the blue rule).

Thus

\[
B_n = 1 + n \qquad\text{(deterministically).}
\]

#### 3.1.2. Red balls

Initially, there is 1 red ball.

- Every *red* draw adds 1 red ball.  
  There are \(n-K_n\) red draws up to time \(n\), so this contributes \(n-K_n\) red balls.
- The \(k\)-th *blue* draw (for \(k=1,\dots,K_n\)) adds \(2k+1\) red balls.  

Thus

\[
R_n
= 1 + (n-K_n)\cdot 1 + \sum_{k=1}^{K_n} (2k+1).
\]

Now

\[
\sum_{k=1}^{K_n} (2k+1)
= 2\cdot\frac{K_n(K_n+1)}{2} + K_n
= K_n(K_n+1) + K_n
= K_n^2 + 2K_n.
\]

Hence

\[
\begin{aligned}
R_n
&= 1 + (n-K_n) + (K_n^2 + 2K_n)\\
&= 1 + n - K_n + K_n^2 + 2K_n\\
&= 1 + n + K_n^2 + K_n.
\end{aligned}
\]

#### 3.1.3. Total balls

Thus

\[
\begin{aligned}
G_n &= R_n + B_n\\
&= (1 + n + K_n^2 + K_n) + (1 + n)\\
&= K_n^2 + K_n + 2n + 2.
\end{aligned}
\]

So the whole problem boils down to understanding the growth of \(K_n\).

---

### 3.2. Dynamics of \(K_n\)

At time \(n+1\), conditional on the past (i.e. on \(\mathcal F_n\), the sigma-field generated by the first \(n\) draws), the probability that we draw blue is

\[
p_n
:= \mathbb{P}(\text{blue at time }n+1 \mid \mathcal F_n)
= \frac{B_n}{G_n}
= \frac{n+1}{K_n^2+K_n+2n+2}.
\]

Define \(\xi_{n+1}\in\{0,1\}\) by
\[
\xi_{n+1} =
\begin{cases}
1,&\text{if draw at time }n+1\text{ is blue},\\[3pt]
0,&\text{if draw at time }n+1\text{ is red}.
\end{cases}
\]

Then

\[
K_{n+1} = K_n + \xi_{n+1}
\]

and

\[
\mathbb{P}(\xi_{n+1} = 1 \mid \mathcal F_n) = p_n,\qquad
\mathbb{P}(\xi_{n+1} = 0 \mid \mathcal F_n) = 1-p_n.
\]

Thus \(\{K_n\}_{n\ge0}\) is a non-decreasing Markov chain on \(\mathbb{Z}_{\ge0}\) with increments in \(\{0,1\}\) and conditional drift

\[
\mathbb{E}[K_{n+1}-K_n\mid\mathcal F_n] = p_n
= \frac{n+1}{K_n^2+K_n+2n+2}.
\]

For later use, note also the variance of the increment:

\[
\mathrm{Var}(K_{n+1}-K_n\mid\mathcal F_n) = p_n(1-p_n)\le p_n.
\]

---

### 3.3. Heuristics: exponent and constant

#### 3.3.1. Exponent of \(K_n\)

Heuristic scaling: assume for large \(n\),
\[
K_n \approx C n^\beta,\quad C>0.
\]

Then typically \(K_n^2\gg n\) if \(\beta>1/2\), so the denominator of \(p_n\) is dominated by \(K_n^2\), and we estimate

\[
p_n \approx \frac{n}{K_n^2}
\approx \frac{1}{C^2}n^{1-2\beta}.
\]

Summing expected increments gives

\[
K_n \approx \sum_{j=1}^n p_j
\approx \frac{1}{C^2}\sum_{j=1}^n j^{1-2\beta}
\approx \frac{1}{C^2}\frac{1}{2-2\beta}n^{2-2\beta}
\quad(\text{if }1-2\beta>-1).
\]

Self-consistency of exponents demands

\[
\beta = 2 - 2\beta \quad\Longrightarrow\quad \beta = \frac{2}{3}.
\]

So heuristically

\[
K_n \asymp n^{2/3}.
\]

In particular, \(K_n^2\asymp n^{4/3}\gg n\), justifying a posteriori the replacement of the denominator in \(p_n\) by \(K_n^2\).

#### 3.3.2. Exponent and constant for \(G_n\)

Since
\[
G_n = K_n^2 + K_n + 2n + 2,
\]
if \(K_n\sim C n^{2/3}\), then \(K_n^2\sim C^2 n^{4/3}\) dominates the linear terms, so one expects
\[
G_n\sim C^2 n^{4/3}.
\]

To identify \(C\), we look at a smoother potential.

---

### 3.4. A “potential function”: \(Y_n := K_n^3/n^2\)

Define
\[
Y_n := \frac{K_n^3}{n^2}\quad(n\ge1).
\]

Heuristically, from the approximate ODE
\[
\frac{dK}{dn} \approx \frac{n}{K^2},
\]
we get
\[
\frac{d}{dn}(K^3) = 3K^2 \frac{dK}{dn} \approx 3n.
\]
Integrating:
\[
K(n)^3 \approx \frac{3}{2}n^2,
\]
so we expect
\[
Y_n = \frac{K_n^3}{n^2}\to \frac{3}{2},
\quad
K_n \sim \Bigl(\frac{3}{2}\Bigr)^{1/3}n^{2/3},
\quad
G_n \sim \Bigl(\frac{3}{2}\Bigr)^{2/3} n^{4/3}.
\]

We now compute the exact drift of \(Y_n\) and see what can be made rigorous.

---

### 3.5. Exact drift of \(Y_n\)

Recall \(K_{n+1}=K_n+\xi_{n+1}\), \(\xi_{n+1}\in\{0,1\}\).

First compute
\[
K_{n+1}^3 - K_n^3
= (K_n+\xi_{n+1})^3 - K_n^3
= (3K_n^2+3K_n+1)\,\xi_{n+1}.
\]

Thus
\[
K_{n+1}^3 = K_n^3 + (3K_n^2+3K_n+1)\,\xi_{n+1}.
\]

Therefore
\[
Y_{n+1}
= \frac{K_{n+1}^3}{(n+1)^2}
= \frac{K_n^3}{(n+1)^2} + \frac{(3K_n^2+3K_n+1)\,\xi_{n+1}}{(n+1)^2}.
\]

Subtracting \(Y_n=\dfrac{K_n^3}{n^2}\), we obtain

\[
\boxed{
Y_{n+1} - Y_n
= K_n^3\Bigl(\frac{1}{(n+1)^2}-\frac{1}{n^2}\Bigr)
  +\frac{(3K_n^2+3K_n+1)\,\xi_{n+1}}{(n+1)^2}.
}
\tag{3.5.1}
\]

Now take conditional expectation given \(\mathcal F_n\). Using \(\mathbb{E}[\xi_{n+1}\mid\mathcal F_n]=p_n\), we get

\[
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
= K_n^3\Bigl(\frac{1}{(n+1)^2}-\frac{1}{n^2}\Bigr)
  + \frac{(3K_n^2+3K_n+1)p_n}{(n+1)^2}.
\]

Compute
\[
\frac{1}{(n+1)^2}-\frac{1}{n^2}
= \frac{n^2 - (n+1)^2}{n^2(n+1)^2}
= -\frac{2n+1}{n^2(n+1)^2}.
\]

Also recall
\[
p_n = \frac{n+1}{K_n^2 + K_n + 2n + 2}
=: \frac{n+1}{D_n}.
\]

Thus

\[
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
= -\frac{(2n+1)K_n^3}{n^2(n+1)^2}
  + \frac{(3K_n^2+3K_n+1)(n+1)}{(n+1)^2 D_n}.
\]

Simplifying the second term:
\[
\frac{(3K_n^2+3K_n+1)(n+1)}{(n+1)^2 D_n}
= \frac{3K_n^2+3K_n+1}{(n+1)D_n}.
\]

So

\[
\boxed{
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
= -\frac{(2n+1)K_n^3}{n^2(n+1)^2}
  +\frac{3K_n^2+3K_n+1}{(n+1)D_n}.
}
\tag{3.5.2}
\]

We now express this in terms of \(Y_n\) and \(n\). Note
\[
Y_n = \frac{K_n^3}{n^2}
\quad\Longrightarrow\quad
K_n^3 = Y_n n^2.
\]

Also
\[
\frac{(2n+1)K_n^3}{n^2(n+1)^2}
= Y_n\,\frac{2n+1}{n(n+1)^2}.
\]

For the second term, use the identity
\[
3K_n^2 + 3K_n + 1 = 3D_n - (6n + 5),
\]
since
\[
3D_n = 3(K_n^2 + K_n + 2n + 2)
= (3K_n^2+3K_n+6n+6)
\quad\Rightarrow\quad
3D_n - (6n+5) = 3K_n^2+3K_n+1.
\]

Hence

\[
\frac{3K_n^2+3K_n+1}{(n+1)D_n}
= \frac{3}{n+1} - \frac{6n+5}{(n+1)D_n}.
\]

Substituting into (3.5.2),

\[
\boxed{
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
= -Y_n\frac{2n+1}{n(n+1)^2}
  + \frac{3}{n+1}
  - \frac{6n+5}{(n+1)D_n}.
}
\tag{3.5.3}
\]

This is an *exact* expression.

---

### 3.6. Heuristic identification of the limit of \(Y_n\)

Expression (3.5.3) suggests comparing with a recursion of the form
\[
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
\approx \frac{3-2Y_n}{n}.
\]

Indeed, for large \(n\):

- \(\dfrac{3}{n+1} = \dfrac{3}{n} + O\bigl(\frac{1}{n^2}\bigr)\).
- \(\dfrac{2n+1}{n(n+1)^2} = \dfrac{2}{n} + O\bigl(\tfrac{1}{n^2}\bigr)\).
- The last term \(\dfrac{6n+5}{(n+1)D_n}\) is small whenever \(D_n\) is large.

Heuristically, for large \(n\), we have \(K_n\asymp n^{2/3}\), so \(K_n^2\asymp n^{4/3}\gg n\), so
\[
D_n = K_n^2+K_n+2n+2 \sim K_n^2\asymp n^{4/3}
\quad\Rightarrow\quad
\frac{6n+5}{(n+1)D_n}
=O\bigl(n^{-1/3}\bigr).
\]
Hence the third term decays faster than \(1/n\), and we may ignore it in the leading drift.

Thus, ignoring lower-order terms,
\[
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
\approx -Y_n \frac{2}{n} + \frac{3}{n}
= \frac{3-2Y_n}{n}.
\]

This corresponds to the ODE (in continuous time \(t\))
\[
\frac{dy}{dt} \approx \frac{3-2y}{t}.
\]

In terms of “logarithmic time” \(s=\log t\), we get
\[
\frac{dy}{ds} \approx 3-2y,
\]
which has a unique attracting equilibrium at \(y_* = \dfrac{3}{2}\). This suggests
\[
Y_n = \frac{K_n^3}{n^2} \longrightarrow \frac{3}{2},
\]
almost surely and with a nonrandom limit; hence
\[
K_n \sim \Bigl(\frac{3}{2}\Bigr)^{1/3} n^{2/3},
\quad
G_n \sim \Bigl(\frac{3}{2}\Bigr)^{2/3} n^{4/3}.
\]

This completely identifies the candidates
\[
\alpha = \frac{4}{3},
\qquad
c = \Bigl(\frac{3}{2}\Bigr)^{2/3}.
\]

The remainder of the argument is to justify rigorously:

1. that \(K_n\) indeed has deterministic exponent \(2/3\) (i.e. \(K_n\) is between \(n^{2/3\pm o(1)}\) almost surely);
2. that \(Y_n\) converges almost surely to \(3/2\).

The rest of this section outlines rigorous progress towards these goals, and then I explain where I get stuck.

---

### 3.7. Martingale decomposition for \(K_n\)

Recall
\[
K_{n+1}-K_n=\xi_{n+1},\quad \mathbb{E}[\xi_{n+1}\mid\mathcal F_n]=p_n.
\]

Set
\[
M_n := \sum_{j=0}^{n-1}(\xi_{j+1}-p_j)
=K_n - \sum_{j=0}^{n-1} p_j,\qquad A_n := \sum_{j=0}^{n-1}p_j.
\]

Then \(M_n\) is a martingale with bounded increments \(|\xi_{j+1}-p_j|\le1\), and
\[
K_n = A_n + M_n.
\]

Its quadratic variation is
\[
\langle M\rangle_n
= \sum_{j=0}^{n-1}\mathrm{Var}(\xi_{j+1}\mid\mathcal F_j)
= \sum_{j=0}^{n-1}p_j(1-p_j)
\le A_n.
\]

Heuristically, from the scaling \(p_j\asymp j^{1-2\beta}\) with \(\beta=2/3\), one expects
\[
A_n\asymp n^{2/3},
\quad
M_n = O\bigl(A_n^{1/2}\bigr) = O(n^{1/3}),
\]
so the martingale fluctuations are smaller than the drift, and \(K_n\sim A_n\) almost surely. In particular, any exponent information derived for \(A_n\) should transfer to \(K_n\).

---

### 3.8. Lower and upper polynomial bounds for \(K_n\) (partial)

To go beyond heuristics, we want to show:

> For any \(\varepsilon>0\), almost surely,
> \[
> n^{2/3-\varepsilon} \le K_n \le n^{2/3+\varepsilon}
> \]
> for all sufficiently large \(n\).

This would give
\[
\lim_{n\to\infty}\frac{\log K_n}{\log n} = \frac{2}{3}\quad\text{a.s.}
\]

**Partial reasoning (not fully rigorous):**

1. **Crude lower bound \(K_n \gg n^{1/2+\delta}\) for some \(\delta>0\).**  
   - If \(K_n\) were always smaller than \(c\sqrt{n}\), then \(K_n^2 \ll n\), so
     \[
     p_n = \frac{n+1}{K_n^2+K_n+2n+2}
     \ge \frac{n+1}{c^2 n + 2n + O(\sqrt{n})}
     \xrightarrow[n\to\infty]{}\frac{1}{c^2+2}>0.
     \]
   - Then we would have, almost surely, a linear number of blue draws, contradicting \(K_n\le c\sqrt{n}\). A standard drift + Azuma argument can make this precise.

2. **Upper exponent bound \(\limsup \frac{\log K_n}{\log n}\le2/3\).**  
   - Fix \(\beta>2/3\). On the event that \(K_n \ge n^\beta\) for all large \(n\), we would have \(K_n^2\ge n^{2\beta}\), giving 
     \[
     p_n\le\frac{n+1}{K_n^2} \le 2n^{1-2\beta}.
     \]
   - Hence \(A_n =\sum p_j \le C n^{2-2\beta}\). On the other hand, \(\langle M\rangle_n\le A_n\), so \(M_n=O(n^{1-\beta})\) in typical size.
   - This would imply \(K_n=A_n+M_n=O(n^{2-2\beta})\), but \(2-2\beta<\beta\) when \(\beta>2/3\). So we cannot have \(K_n\ge n^\beta\) for all large \(n\). A refinement shows it cannot hold infinitely often, yielding the limsup bound.

3. **Lower exponent bound \(\liminf \frac{\log K_n}{\log n}\ge2/3\).**  
   - Fix \(\beta<2/3\). On an event where \(K_n \le n^\beta\) for all large \(n\), we would have \(K_n^2\le n^{2\beta}\), with \(2\beta>1\) for any \(\beta>1/2\), so the denominator of \(p_n\) is \(\sim K_n^2\), and
     \[
     p_n \ge c n^{1-2\beta}
     \]
     for some \(c>0\). Then \(A_n\ge c' n^{2-2\beta}\) and \(\langle M\rangle_n\le A_n\).
   - With \(2-2\beta>\beta\) for \(\beta<2/3\), this argues that \(K_n\) must eventually exceed any multiple of \(n^\beta\), giving the liminf bound.

These drift/martingale arguments can be made precise with Freedman-type inequalities and a Borel–Cantelli argument, but the details are somewhat technical. The upshot is:

> **Heuristic conclusion:** there is strong evidence that
> \[
> K_n = n^{2/3+o(1)} \quad\text{a.s.}
> \]
> i.e. its logarithmic growth exponent is almost surely \(2/3\).

Assuming this as a *growth bootstrap*, we can go back to \(Y_n\).

---

### 3.9. Towards convergence of \(Y_n\) (partial)

We return to (3.5.3):

\[
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
= -Y_n\frac{2n+1}{n(n+1)^2}
  + \frac{3}{n+1}
  - \frac{6n+5}{(n+1)D_n}.
\]

Introduce the “main drift”
\[
\frac{3-2Y_n}{n}
\]
and treat the difference as a perturbation.

Write
\[
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n] = \frac{3-2Y_n}{n} + \varepsilon_n,
\]

with
\[
\varepsilon_n := \Bigl[-Y_n\frac{2n+1}{n(n+1)^2} + \frac{3}{n+1} - \frac{6n+5}{(n+1)D_n}\Bigr]
- \frac{3-2Y_n}{n}.
\]

A tedious but straightforward estimate (using Taylor expansions and the growth information \(K_n\asymp n^{2/3}\), so that \(D_n\asymp n^{4/3}\)) shows:

- For large \(n\),
  \[
  |\varepsilon_n|
  \le C_1\frac{Y_n}{n^2} + C_2 n^{-4/3}
  \]
  for some constants \(C_1,C_2\). Since \(Y_n\) is expected to stay bounded, \(|\varepsilon_n|\) decays at least like \(n^{-4/3}\), which is summable.

At the same time, the *noise* part of \(Y_{n+1}-Y_n\) is

\[
N_{n+1}
:= \frac{(3K_n^2+3K_n+1)(\xi_{n+1}-p_n)}{(n+1)^2}.
\]

Its conditional second moment can be bounded as:

\[
\mathbb{E}[N_{n+1}^2\mid\mathcal F_n]
\le \frac{(3K_n^2+3K_n+1)^2}{(n+1)^4}p_n(1-p_n).
\]

Using \(K_n\asymp n^{2/3}\) and \(p_n\asymp n/K_n^2\asymp n^{-1/3}\), we heuristically get

\[
\mathbb{E}[N_{n+1}^2\mid\mathcal F_n]
\lesssim \frac{n^{8/3}}{n^4} \cdot n^{-1/3}
= n^{-4/3}.
\]

Thus
\[
\sum_n \mathbb{E}[N_{n+1}^2\mid\mathcal F_n] <\infty\quad\text{a.s.}
\]

This implies that the martingale part
\[
\sum_{j=0}^{n-1} N_{j+1}
\]
converges almost surely (by the martingale convergence theorem), and therefore its contribution to \(Y_n\) is negligible at large \(n\).

Combining:

- approximate deterministic recursion
  \[
  \mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
  = \frac{3-2Y_n}{n} + \varepsilon_n,
  \quad
  \sum_n|\varepsilon_n|<\infty,
  \]
- summable noise variance,

one is in the right regime for **stochastic approximation with step size \(a_n=1/n\)** and drift \(F(y)=3-2y\). A standard Robbins–Siegmund or Robbins–Monro type theorem would then imply:

1. \(Y_n\) converges almost surely;
2. its limit is the unique zero of \(F\), namely
   \[
   \lim_{n\to\infty}Y_n = \frac{3}{2}\quad\text{a.s.}
   \]

Once that is obtained, we conclude

\[
\frac{K_n^3}{n^2}\to\frac{3}{2}\quad\text{a.s.}
\quad\Longrightarrow\quad
K_n \sim \Bigl(\frac{3}{2}\Bigr)^{1/3}n^{2/3},\quad
G_n\sim \Bigl(\frac{3}{2}\Bigr)^{2/3} n^{4/3}\quad\text{a.s.}
\]

Thus the natural candidates
\[
\alpha=\frac{4}{3},\quad c=\Bigl(\frac{3}{2}\Bigr)^{2/3}
\]
indeed satisfy
\[
\frac{G_n}{n^\alpha} \longrightarrow c\quad\text{a.s.}
\]

---

### 3.10. Summary of what is solid and what is not

From the above we have:

- A *complete and exact* reduction of the urn model to the one-dimensional Markov chain \(K_n\).

- Exact formula
  \[
  G_n = K_n^2 + K_n + 2n + 2.
  \]

- Strong heuristic plus partial drift/martingale arguments showing
  \[
  K_n = n^{2/3+o(1)}\quad\text{a.s.}
  \]

- Exact formula for the drift of \(Y_n=K_n^3/n^2\),
  \[
  \mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n]
  = -Y_n\frac{2n+1}{n(n+1)^2}
    + \frac{3}{n+1}
    - \frac{6n+5}{(n+1)D_n},
  \]
  which matches a stochastic-approximation scheme with step size \(1/n\) and drift \(3-2Y_n\) plus a small perturbation.

On the basis of standard stochastic-approximation lore, this would yield the claimed a.s. convergence with \(\alpha=4/3\), \(c=(3/2)^{2/3}\). However, some steps (detailed below) still need rigorous closure.

---

## 4. Gaps  

Since the status is **Partial**, I list the remaining gaps explicitly.

### Gap 1: Rigorous two-sided growth bounds for \(K_n\)

**Location:** Section 3.8 (Polynomial bounds for \(K_n\)).  

**Description:**  

I outlined plausibility arguments that for any \(\varepsilon>0\),

\[
n^{2/3-\varepsilon}\le K_n\le n^{2/3+\varepsilon}
\quad\text{eventually a.s.}
\]

by combining drift bounds for \(K_n\) with martingale estimates. To make this fully rigorous one must:

1. For each fixed \(\beta>2/3\), show that the event  
   \(\{K_n \ge n^\beta \text{ for infinitely many }n\}\) has probability 0.

2. For each fixed \(\beta<2/3\), show that the event  
   \(\{K_n \le n^\beta \text{ for infinitely many }n\}\) has probability 0.

This requires a carefully constructed sequence of stopping times, the use of Freedman’s (or Bernstein’s) inequality for martingales with bounded increments, and a Borel–Cantelli argument.

**Reason for difficulty:**  

The conditional drift \(p_n\) depends non-linearly on \(K_n\), so the bounds for \(p_n\) under hypotheses like “\(K_n\le n^\beta\) for all large \(n\)” are delicate. One must repeatedly control the martingale term \(M_n\) relative to its quadratic variation and ensure that drifts dominate fluctuations in the long run. I have not written out these estimates and verifications in full detail.

---

### Gap 2: Precise control of the error term \(\varepsilon_n\) in the drift of \(Y_n\)

**Location:** Section 3.9 (Towards convergence of \(Y_n\)).  

**Description:**  

Defining
\[
\varepsilon_n :=
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n] - \frac{3-2Y_n}{n},
\]
I asserted that, under the (conjectured) bounds \(K_n\asymp n^{2/3}\), there exists a constant \(C\) such that
\[
|\varepsilon_n|\le \frac{C}{n^{4/3}},
\]
so that \(\sum_n |\varepsilon_n|<\infty\).

To prove this rigorously one must:

1. Rewrite \(\varepsilon_n\) explicitly as in terms of \(n,Y_n,D_n\).
2. Use the *already proved* bounds on \(K_n\) (from Gap 1) to bound each piece.
3. Show that the resulting bound is indeed summable.

**Reason for difficulty:**  

The exact algebra somewhat intricate, and without an already established tight control on \(K_n\) (Gap 1), the best a priori bound on the term
\[
\frac{6n+5}{(n+1)D_n}
\]
is of order \(1/n\), which *is not* summable. Only once we know \(K_n^2\gg n^{1+\delta}\) for some \(\delta>0\) can we improve this to \(O(n^{-1-\delta})\). This logical dependency makes the argument delicate.

---

### Gap 3: Application of a concrete stochastic-approximation / almost-supermartingale theorem

**Location:** End of Section 3.9.  

**Description:**  

Given a recursion of the form
\[
Y_{n+1} = Y_n + \frac{1}{n}\bigl(3-2Y_n\bigr) + \varepsilon_n + N_{n+1},
\]
with \(\sum |\varepsilon_n|<\infty\) and \(\sum \mathbb{E}[N_{n+1}^2\mid\mathcal F_n]<\infty\), standard results (Robbins–Siegmund almost supermartingale theorem, or Benaïm’s dynamical-system approach to stochastic approximation) yield:

- almost sure convergence of \(Y_n\);
- identification of the limit as the unique root of \(3-2y=0\), i.e. \(y=3/2\).

However, in this solution I have **not**:

1. Explicitly stated and proved the exact theorem I am appealing to;
2. Verified its hypotheses in detail (e.g. almost-sure boundedness of \(Y_n\), Lipschitz continuity of the drift on the range of \(Y_n\), etc.).

**Reason for difficulty:**  

Writing a fully self-contained proof along these lines would essentially require reproducing nontrivial pieces of the general stochastic-approximation theory (e.g. from Kushner–Yin or Benaïm), which is technically heavy. I have instead sketched how the problem fits perfectly into that framework, but not spelled out every verification step.

---

### Gap 4: From convergence of \(Y_n\) to convergence of \(G_n/n^{4/3}\)

**Location:** Very end (Section 3.9).  

**Description:**  

Assuming we have shown
\[
Y_n=\frac{K_n^3}{n^2}\to\frac{3}{2}\quad\text{a.s.,}
\]
I briefly asserted:

\[
\frac{G_n}{n^{4/3}}
= \frac{K_n^2}{n^{4/3}} + \frac{K_n}{n^{4/3}} + \frac{2n+2}{n^{4/3}}
\to \Bigl(\frac{3}{2}\Bigr)^{2/3}.
\]

This is intuitively clear (the second and third terms vanish), but full rigor needs:

- A precise deduction \(K_n = \Theta(n^{2/3})\) from \(K_n^3/n^2\to 3/2\);
- Control showing \(\frac{K_n}{n^{4/3}}\to0\) and \(\frac{2n+2}{n^{4/3}}\to0\) (which are easy, but should be stated).

**Reason for difficulty:**  

This part is not truly difficult, but I have not gone through every epsilon–delta detail because it depends on Gap 3 (we do not yet have the convergence of \(Y_n\)). Once Gap 3 is resolved, this last step is straightforward.

---

## 5. Self-evaluation  

**Confidence score (0–100):** 55  

**Known or suspected issues:**

- The main missing pieces are the rigorous proofs that:
  - \(K_n\) has almost sure growth exponent exactly \(2/3\);  
  - \(Y_n = K_n^3/n^2\) converges almost surely to \(3/2\) via a bona fide stochastic-approximation theorem.
- I have not fully detailed the martingale concentration arguments (Freedman/Bernstein inequalities, Borel–Cantelli) needed for the “bootstrap” bounds on \(K_n\).
- The stochastic-approximation argument is currently at the level of a carefully motivated sketch, not a fully formal proof (mainly because I did not restate and verify a concrete theorem’s hypotheses).
- There is no doubt that the *heuristics* are internally consistent and match standard theory for such processes, but turning them into a completely rigorous self-contained proof is nontrivial and has not been completed here.

**Coverage comment:**

- The **reduction** of the urn to the single chain \(K_n\) and the formula \(G_n=K_n^2+K_n+2n+2\) are fully rigorous.
- The **identification of the exponent** \(\alpha=4/3\) and the candidate constant \(c=(3/2)^{2/3}\) is well supported heuristically and via partial drift arguments, but I have *not* proved the almost sure convergence
  \[
  \frac{G_n}{n^{4/3}} \to \Bigl(\frac{3}{2}\Bigr)^{2/3}
  \]
  rigorously.
- So the problem’s requirement “prove that there exist constants \(0 < c,\alpha < \infty\) such that \(G_n/n^\alpha\to c\) a.s.” is **not fully met** here; instead, I have:
  - described the natural values \(\alpha=4/3\), \(c=(3/2)^{2/3}\);
  - outlined, but not completed, a rigorous proof based on stochastic approximation.

  ---

---

# Expert evaluator's feedback on the problem solver's FIRST SOLUTION ATTEMPT

## 1. Status  

Status: Partial  

---

## 2. Score  

Score (0–100): 72  

---

## 3. Major issues  

- **Location:** §3.8 (“Polynomial bounds for \(K_n\)”)  
  **Issue type:** Missing critical step / incomplete argument  
  **Description:**  
  The two-sided power bounds for \(K_n\) (roughly \(K_n \approx n^{2/3}\)) are only argued heuristically. You sketch how to contradict the scenarios \(K_n \ge n^\beta\) for all large \(n\) when \(\beta>2/3\) and \(K_n \le n^\beta\) for \(\beta<2/3\), but you do not give a fully rigorous drift+martingale argument: no explicit stopping times, no concrete Freedman/Bernstein inequality application, and no Borel–Cantelli implementation. These growth bounds are not cosmetic; they are needed to justify later approximations like \(D_n \sim K_n^2\) and to control error terms.  
  **Suggested direction for fix:**  
  Turn the heuristic into a precise lemma of the form: “For any \(\beta>2/3\), \(\mathbb{P}(K_n \ge n^\beta \text{ i.o.})=0\)” and similar for \(\beta<2/3\). Do this by:  
  1. Fix \(\beta\), assume \(K_n \ge n^\beta\) (or \(\le n^\beta\)) on an interval;  
  2. Use this to get deterministic bounds on \(p_n\) on that interval;  
  3. Control \(M_n\) via Freedman’s inequality to show that with high probability \(K_n\) cannot sustain such behaviour;  
  4. Sum these probabilities over disjoint blocks and apply Borel–Cantelli.  
  This will give you almost-sure bounds of the form \(n^{2/3-\varepsilon} \le K_n \le n^{2/3+\varepsilon}\) eventually.

- **Location:** §3.9 (“Towards convergence of \(Y_n\)”): estimation of \(\varepsilon_n\)  
  **Issue type:** Logical gap / circular reasoning  
  **Description:**  
  The bound \(|\varepsilon_n|\le C_1 Y_n/n^2 + C_2 n^{-4/3}\) and the conclusion that \(\sum_n |\varepsilon_n|<\infty\) rely on the assumption \(K_n \asymp n^{2/3}\) (so that \(D_n \asymp K_n^2 \asymp n^{4/3}\)), which has not yet been proved. Without prior control on \(K_n\), all you can a priori say about \((6n+5)/((n+1)D_n)\) is that it is \(O(1/n)\), which is *not* summable. Thus the justification that the “error” is negligible in the SA scheme is incomplete and currently circular.  
  **Suggested direction for fix:**  
  Separate the argument into two stages:  
  1. Prove coarse but *uniform* bounds on \(K_n\) (e.g. for some fixed \(\gamma\in(1/2,1)\), eventually \(n^\gamma \le K_n \le n^{2-\gamma}\)) that are strong enough to ensure \(D_n \gg n^{1+\delta}\) for some \(\delta>0\).  
  2. Using those bounds, give a **clean algebraic estimate** of \(\varepsilon_n\) that does not use any heuristic about the exact exponent. Show explicitly that \(|\varepsilon_n| \le C n^{-1-\delta}\) a.s. for some \(\delta>0\), so \(\sum |\varepsilon_n|<\infty\).

- **Location:** §3.9 (“Towards convergence of \(Y_n\)”): variance of the noise term \(N_{n+1}\)  
  **Issue type:** Dependence on unproved growth rate / incomplete justification  
  **Description:**  
  The estimate \(\mathbb{E}[N_{n+1}^2\mid\mathcal F_n] \lesssim n^{-4/3}\) is derived “heuristically” by plugging \(K_n\asymp n^{2/3}\) and \(p_n \asymp n^{-1/3}\). As with \(\varepsilon_n\), this uses the very asymptotic behaviour you intend to prove, creating a circular dependence. Strictly speaking, before you know that \(K_n^2 \gg n^{1+\delta}\), you cannot assert that the quadratic variation of the \(Y\)-martingale is summable.  
  **Suggested direction for fix:**  
  First prove coarse upper and lower bounds on \(K_n\) that ensure \(K_n^2\) grows *at least* like \(n^{1+\delta}\) for some \(\delta>0\). Then redo the computation of  
  \[
  \mathbb{E}[N_{n+1}^2\mid\mathcal F_n]
  \le \frac{(3K_n^2+3K_n+1)^2}{(n+1)^4}\,p_n,
  \]
  using the exact formula for \(p_n\) and those coarse bounds to get an **unconditional** bound of the form \(C n^{-1-\delta'}\), which is summable. This breaks the circularity.

- **Location:** §3.9, end: appeal to stochastic-approximation / almost-supermartingale theorem  
  **Issue type:** Misuse / under-specification of theory  
  **Description:**  
  The core claim “From the recursion \(Y_{n+1} = Y_n + \frac{1}{n}(3-2Y_n) + \varepsilon_n + N_{n+1}\), with \(\sum |\varepsilon_n|<\infty\) and \(\sum \mathbb{E}[N_{n+1}^2\mid\mathcal F_n]<\infty\), we get \(Y_n\to 3/2\) a.s.” is referenced only as “standard stochastic approximation lore” without stating a concrete theorem or verifying its hypotheses. In particular, you never prove that \(Y_n\) is a.s. bounded or that the drift pushes it toward the unique equilibrium, both of which are crucial in SA theory.  
  **Suggested direction for fix:**  
  Choose a specific theorem—e.g. Robbins–Siegmund’s almost-supermartingale convergence lemma, or a precise stochastic-approximation theorem à la Benaïm—and write down its statement. Then, *systematically* verify its assumptions:  
  - Show \(\sum a_n = \infty\), \(\sum a_n^2<\infty\) with \(a_n=1/n\);  
  - Show \(F(y)=3-2y\) is Lipschitz and has unique root;  
  - Prove that \(Y_n\) is eventually confined in some deterministic compact interval \([m,M]\subset(0,\infty)\) by combining sign of the drift with bounded noise;  
  - Show the perturbations \(\varepsilon_n\) are summable and the martingale noise has square-summable scale.  
  Once these are checked, you can legitimately conclude \(Y_n\to 3/2\) a.s.

- **Location:** Overall logical structure (interaction of §3.8 and §3.9)  
  **Issue type:** Structural / circularity of argument  
  **Description:**  
  Several later estimates (both for \(\varepsilon_n\) and for \(\mathbb{E}[N_{n+1}^2\mid\mathcal F_n]\)) rely implicitly on the “fact” that \(K_n\asymp n^{2/3}\), but the only justification for that fact is the drift analysis that itself uses those estimates. This is a typical bootstrap situation, but in its current form the bootstrap is not clearly separated: it is ambiguous where the independent base bounds for \(K_n\) stop and where the consequences of those bounds begin.  
  **Suggested direction for fix:**  
  Explicitly structure the proof in two stages:  
  1. **Bootstrap stage:** Prove coarse but nontrivial a.s. bounds for \(K_n\) (e.g. \(C_1 n^{\gamma_1} \le K_n \le C_2 n^{\gamma_2}\) for some \(1/2<\gamma_1\le\gamma_2<1\)). This part should avoid any use of the potential \(Y_n\) and rely only on crude drift estimates and martingale concentration.  
  2. **Refinement stage:** On the strength of those bounds, analyze \(Y_n\), show its drift is asymptotically \((3-2Y_n)/n\) plus a summable error, and apply SA theory to get \(Y_n\to 3/2\).  
  Making this separation explicit will remove logical circularity and clarify which estimates use which prior information.

---

## 4. Minor issues  

- **Location:** §3.6 (“Heuristic identification of the limit of \(Y_n\)”)  
  **Nature:** Slightly loose comparison of term sizes  
  **Description:**  
  You state that the third term \((6n+5)/((n+1)D_n)\) “decays faster than \(1/n\)”, which is true *if* \(D_n\asymp n^{4/3}\), but this assumption is at that point only heuristic. Although you later flag the need for rigorous bounds, it would be clearer to mark this calculation explicitly as conditional on \(K_n\asymp n^{2/3}\).  
  **Suggestion:**  
  Add a sentence such as: “Under the heuristic \(K_n\asymp n^{2/3}\), we have \(D_n\asymp n^{4/3}\), hence this term is \(O(n^{-4/3})\). We will justify this scaling rigorously later.”

- **Location:** §3.8, bullet 3 (lower exponent bound)  
  **Nature:** Slight conflation of “plausible” with “proved”  
  **Description:**  
  Phrases such as “This argues that \(K_n\) must eventually exceed any multiple of \(n^\beta\)” are a bit strong for what is, at that point, only a sketch.  
  **Suggestion:**  
  Rephrase to emphasize the status: for example, “Heuristically, this suggests that …; a rigorous version would require applying Freedman’s inequality as described.”

- **Location:** §3.10 (summary)  
  **Nature:** Terminology / clarity  
  **Description:**  
  You say “Strong heuristic plus partial drift/martingale arguments showing \(K_n = n^{2/3+o(1)}\) a.s.” It would be clearer to separate what is heuristic from what has been actually established, to avoid over-claiming.  
  **Suggestion:**  
  Replace by: “Strong heuristics, supported by partial drift/martingale calculations, point to \(K_n = n^{2/3+o(1)}\) a.s.; however, the rigorous proof of this statement is not yet complete in this write-up.”

- **Location:** End of §3.9 (from \(Y_n\) to \(G_n\))  
  **Nature:** Minor omission  
  **Description:**  
  You assert that \(G_n/n^{4/3}\to (3/2)^{2/3}\) follows once \(Y_n\to 3/2\). This is correct, but you do not even briefly justify that \(K_n/n^{4/3}\to 0\) and \(2n/n^{4/3}\to0\).  
  **Suggestion:**  
  Add a short calculation: from \(K_n^3/n^2\to 3/2\) we have \(K_n \sim (3/2)^{1/3}n^{2/3}\), hence \(K_n/n^{4/3}\to0\) and \(n/n^{4/3} = n^{-1/3}\to0\), so only the \(K_n^2\) term survives.

---

## 5. Gap assessment  

The problem solver explicitly listed four gaps; here is an assessment of each:

- **Gap 1 (Rigorous two-sided growth bounds for \(K_n\))**  
  **Assessment:** Fundamental.  
  This is a core part of the proof: without solid control on \(K_n\)’s order of growth, you cannot justify the approximations of \(p_n\) and \(D_n\) that power the analysis of \(Y_n\). It is not just a technical clean-up; it addresses the main difficulty of controlling a non-linear drift coupled with martingale noise.

- **Gap 2 (Precise control of \(\varepsilon_n\))**  
  **Assessment:** Moderate.  
  Once you have usable growth bounds for \(K_n\), this becomes a somewhat technical but tractable algebraic exercise. You need to be careful to avoid circularity, but conceptually it is not the heart of the problem.

- **Gap 3 (Application of a stochastic-approximation / supermartingale theorem)**  
  **Assessment:** Moderate.  
  This is conceptually straightforward if you are comfortable with stochastic approximation theory. The main work is: pick an appropriate theorem; verify its hypotheses. It’s nontrivial but not as deep as establishing the right growth exponent in the first place.

- **Gap 4 (From convergence of \(Y_n\) to convergence of \(G_n/n^{4/3}\))**  
  **Assessment:** Minor.  
  Once \(Y_n\to 3/2\) is known, the deduction for \(G_n\) is a short algebraic step.

Overall, I agree with your self-identification of these gaps and would emphasize that Gap 1 is the main conceptual obstacle; Gaps 2–4 are technical but relatively standard once Gap 1 is resolved.

---

## 6. Coverage assessment  

All subparts addressed: No  

- The problem essentially has a single goal: prove that there exist deterministic \(c>0,\alpha>0\) such that \(G_n/n^\alpha \to c\) almost surely.  
- You have:
  - Completely analyzed the *reduction* to the process \(K_n\) and expressed \(G_n\) explicitly in terms of \(K_n\).  
  - Correctly identified the likely exponent \(\alpha=4/3\) and constant \(c=(3/2)^{2/3}\).  
  - Sketched a coherent martingale + stochastic-approximation strategy to prove almost-sure convergence.  
- However, the **actual proof** of almost-sure convergence is incomplete: the crucial growth bounds for \(K_n\) and the rigorous application of SA theorems are not carried through.  

So the only substantive “part” of the problem (the almost-sure limit statement) is addressed in structure and heuristics but not fully solved.

---

## 7. Summary  

Your solution does an excellent job on the structural and heuristic side of the problem. The reduction from the original urn process to the single Markov chain \(K_n\) is completely correct and very powerful, and the explicit formula \(G_n = K_n^2+K_n+2n+2\) is a key simplification. The choice of potential \(Y_n = K_n^3/n^2\) is especially insightful; it linearizes the drift in a way that aligns beautifully with the classical stochastic-approximation framework. Your computation of the exact drift of \(Y_n\) and its approximation by \((3-2Y_n)/n\) is technically accurate and well-motivated.  

On the probabilistic side, you correctly decompose \(K_n\) into drift plus martingale noise, recognize that the noise should be of smaller order than the drift, and outline how this should lead to \(K_n \sim (3/2)^{1/3}n^{2/3}\). You also identify the right general theorem to invoke (Robbins–Siegmund–type or Benaïm-type SA results) and match your recursion to the canonical form.  

The main weaknesses are on the rigor and closure: the two-sided growth bounds for \(K_n\) are only sketched, and several crucial estimates for the drift error and noise variance of \(Y_n\) are derived under the assumption of the very asymptotics you are trying to prove. This creates a circular dependence that must be broken by a bootstrap argument with explicit bounds and martingale concentration. Moreover, the final step where SA theory is applied is left at the “folklore” level; no specific theorem is stated or checked.  

Overall, the approach is highly promising and, with careful work, should be turnable into a complete proof. You are not far from a solution in terms of ideas; the remaining work is mostly about making the drift/noise comparison quantitative enough and about structuring the argument to avoid circular reasoning.

---

## 8. Recommendations for next iteration  

1. **Isolate and prove coarse growth bounds for \(K_n\) first.**  
   - Aim to prove statements of the form: there exist \(\gamma_1,\gamma_2\) with \(1/2<\gamma_1<2/3<\gamma_2<1\) such that eventually  
     \[
     n^{\gamma_1} \le K_n \le n^{\gamma_2} \quad \text{a.s.}
     \]  
   - Use drift inequalities for \(p_n\) under hypotheses like “\(K_n \ge n^\beta\)” or “\(K_n \le n^\beta\)” and apply Freedman’s inequality to show such hypotheses cannot persist (or hold infinitely often) when \(\beta\neq 2/3\). Make this part completely independent of the \(Y_n\)-analysis.

2. **Once you have those bounds, redo the estimates for \(\varepsilon_n\) carefully.**  
   - Start from the exact formula (3.5.3) and write  
     \[
     \mathbb{E}[Y_{n+1}-Y_n\mid\mathcal F_n] = \frac{3-2Y_n}{n} + \varepsilon_n.
     \]  
   - Using only your proven bounds on \(K_n\) (hence on \(D_n\)), establish a deterministic bound \(|\varepsilon_n| \le Cn^{-1-\delta}\) for some \(\delta>0\). Check that \(\sum |\varepsilon_n| < \infty\).

3. **Recompute and rigorously bound the variance of the noise term \(N_{n+1}\).**  
   - Using the exact formula for \(p_n\) and your growth bounds on \(K_n\), show that  
     \[
     \mathbb{E}[N_{n+1}^2\mid\mathcal F_n] \le C n^{-1-\delta'}
     \]  
     for some \(\delta'>0\), which ensures \(\sum \mathbb{E}[N_{n+1}^2\mid\mathcal F_n]<\infty\). This lets you invoke martingale convergence for \(\sum N_{n+1}\).

4. **Prove that \(Y_n\) is almost surely bounded.**  
   - Use the sign of the main drift: for large \(Y_n\), \(\frac{3-2Y_n}{n}\) is negative; for very small \(Y_n\), it is positive. Combine this with the fact that the noise increments are small and square-summable to show that \(Y_n\) cannot diverge to \(\infty\) or to \(0\) and must eventually stay in some interval \([m,M]\subset(0,\infty)\).

5. **Select and apply a precise stochastic-approximation / almost-supermartingale theorem.**  
   - For instance, use Robbins–Siegmund’s theorem: show that \(V_n := (Y_n - 3/2)^2\) satisfies a recursion of the form  
     \[
     \mathbb{E}[V_{n+1}\mid\mathcal F_n] \le V_n - c \frac{1}{n} V_n + \rho_n,
     \]  
     with \(\sum \rho_n<\infty\). Then invoke the theorem to get \(V_n\to 0\) a.s.  
   - Alternatively, phrase the recursion directly in the SA form and use Benaïm’s theorem on convergence to attractors of ODEs.

6. **After establishing \(Y_n\to3/2\), cleanly deduce the behaviour of \(K_n\) and \(G_n\).**  
   - Show \(K_n^3/n^2\to3/2\Rightarrow K_n \sim (3/2)^{1/3}n^{2/3}\), and then plug into \(G_n = K_n^2+K_n+2n+2\) to get \(G_n/n^{4/3}\to(3/2)^{2/3}\).

7. **Keep heuristics and rigorous steps clearly separated.**  
   - Where you use heuristic scaling (e.g. \(K_n\asymp n^{2/3}\)), explicitly mark it as such. Then gradually replace these heuristic steps with proven lemmas. This will help you track which parts of the argument still need rigorous justification.

If you tackle these items systematically—especially the first four—you should be able to turn your already very good conceptual outline into a complete, rigorous solution.

---

# External Research Input

## 2. Problem and current state summary

### Restatement of the problem

The urn evolves as follows:

- Start with 1 red, 1 blue.
- At each step, draw one ball uniformly at random from the urn; the drawn ball stays in the urn.
- If the drawn ball is red, you add 1 red and 1 blue.
- If the drawn ball is blue and this is the **k‑th blue draw so far**, you add 1 blue and \(2k+1\) red balls.

Let \(G_n\) be the total number of balls after \(n\) draws. Prove there exist constants \(0<c,\alpha<\infty\) such that
\[
\frac{G_n}{n^\alpha} \to c \quad\text{almost surely}.
\]

Internally, it was further conjectured (on strong heuristic grounds) that
\[
\alpha = \frac{4}{3},\qquad c = \Bigl(\frac{3}{2}\Bigr)^{2/3}.
\]

### Internal reductions and main approach

The key internal simplification is to track only the number of **blue draws**:

- Let \(K_n\) be the number of times a blue ball has been drawn by time \(n\).
- Then:

  - At each step exactly one blue ball is added, regardless of the color drawn, so
    \[
    B_n = 1 + n \quad \text{(deterministic).}
    \]
  - A careful counting of red additions gives
    \[
    R_n = 1 + n + K_n^2 + K_n.
    \]
  - Therefore
    \[
    G_n = R_n + B_n = K_n^2 + K_n + 2n + 2.
    \]

So the whole problem reduces to understanding the growth of the one‑dimensional process \(K_n\).

The evolution of \(K_n\) is:

- Let \(\xi_{n+1} \in\{0,1\}\) be the indicator that the \((n+1)\)-th draw is blue, so \(K_{n+1}=K_n+\xi_{n+1}\).
- Conditional on the past,
  \[
  p_n := \mathbb P(\xi_{n+1}=1\mid\mathcal F_n)
  = \frac{B_n}{G_n}
  = \frac{n+1}{K_n^2+K_n+2n+2}.
  \]
- Hence
  \[
  \mathbb E[K_{n+1}-K_n \mid \mathcal F_n] = p_n.
  \]

Heuristics and ODE scaling suggest:

- If \(K_n \approx C n^\beta\), consistency of the drift gives \(\beta = 2/3\), so \(K_n\) should grow like \(n^{2/3}\).
- Then \(G_n \approx K_n^2 \sim C^2 n^{4/3}\), so \(\alpha=4/3\).

To make this rigorous, the solver introduced the **potential**
\[
Y_n := \frac{K_n^3}{n^2}.
\]
A detailed calculation shows
\[
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal{F_n}] = -Y_n\frac{2n+1}{n(n+1)^2}  + \frac{3}{n+1}  - \frac{6n+5}{(n+1)D_n},
\]
where \(D_n = K_n^2+K_n+2n+2\). For large \(n\) and under the heuristic \(D_n\sim K_n^2\sim n^{4/3}\), this matches
\[
\mathbb E[Y_{n+1}-Y_n\mid\mathcal F_n]
\approx \frac{3-2Y_n}{n},
\]
so the associated ODE in “logarithmic time” has unique stable equilibrium \(Y=3/2\). One is therefore led to conjecture
\[
Y_n\to \frac{3}{2} \quad\text{a.s.},\quad
K_n\sim \Bigl(\frac{3}{2}\Bigr)^{1/3} n^{2/3},\quad
G_n\sim \Bigl(\frac{3}{2}\Bigr)^{2/3} n^{4/3}\;\text{a.s.}
\]

### Main gaps identified by the evaluator

The evaluator’s key points:

1. **Coarse two‑sided growth bounds for \(K_n\)** (Gap 1):

   - It was argued heuristically that \(K_n \approx n^{2/3}\), but there is no rigorous proof that
     \[
     n^{2/3-\varepsilon}\le K_n \le n^{2/3+\varepsilon}
     \]
     eventually a.s. (or even that \(K_n^2 \gg n^{1+\delta}\) for some \(\delta>0\)).
   - Such bounds are needed to justify approximations like \(D_n \sim K_n^2\) and to make error terms summable.

2. **Control of the drift error in \(Y_n\)** (Gap 2):

   - To fit \(Y_n\) into a standard stochastic approximation (SA) scheme, one needs
     \[
     \mathbb E[Y_{n+1}-Y_n\mid\mathcal F_n]
     = \frac{3-2Y_n}{n} + \varepsilon_n,
     \quad\sum |\varepsilon_n|<\infty\ \text{a.s.}
     \]
   - The current argument that \(\varepsilon_n\) is summable uses the unproven hypothesis \(K_n\sim n^{2/3}\), which is circular.

3. **Control of the noise term in \(Y_n\)** (Gap 3):

   - The martingale increment
     \[
     N_{n+1} := \frac{(3K_n^2+3K_n+1)(\xi_{n+1}-p_n)}{(n+1)^2}
     \]
     was heuristically shown to have variance \(\asymp n^{-4/3}\), hence square‑summable, but again using the unproven scaling of \(K_n\).

4. **Formal application of a stochastic approximation / almost‑supermartingale theorem** (Gap 3 / 4):

   - The solver wants to appeal to general SA theory (e.g. Benaïm, Borkar, Robbins–Siegmund), but hasn’t:
     - Stated a concrete theorem.
     - Verified its technical hypotheses (boundedness of \(Y_n\), Lipschitz drift, summability of perturbations, etc.)

Overall, the internal approach is correct in structure and very close to standard SA analyses of non‑linear urns, but the rigorous closure of these steps is still missing.

## 3. Key obstacles as research questions

Rephrasing the evaluator’s main concerns as research questions:

1. **Growth control of \(K_n\):**

   - *RQ1:* Are there general theorems or techniques to obtain almost‑sure polynomial growth bounds for one‑dimensional processes of the form
     \[
     K_{n+1} = K_n + \xi_{n+1},\quad
     \mathbb E[\xi_{n+1}\mid\mathcal F_n]
     = \frac{n+1}{K_n^2+K_n+2n+2},
     \]
     using martingale inequalities such as Freedman/Bernstein?

2. **Stochastic approximation framework:**

   - *RQ2:* Can we rigorously cast \(Y_n = K_n^3/n^2\) into a standard stochastic approximation scheme
     \[
     Y_{n+1} = Y_n + a_n\bigl(F(Y_n) + \eta_{n+1}\bigr),
     \]
     with \(a_n \sim 1/n\), \(F(y)=3-2y\), and noise \(\eta_{n+1}\) satisfying the classical assumptions (bounded or square‑integrable, \(\sum a_n^2\mathbb E[\eta_{n+1}^2]<\infty\), etc.)?

3. **Almost‑sure convergence of SA with 1/n step size:**

   - *RQ3:* Under what general conditions on \(F\) and the noise does one have almost sure convergence \(Y_n\to y_*\), when the ODE \(\dot y = F(y)\) has a globally attracting equilibrium \(y_*\)? Are there simple theorems (Benaïm’s ODE method, Robbins–Siegmund, Borkar’s results) that can be applied almost as black boxes?

4. **Existing results on non‑linear/unbalanced urns:**

   - *RQ4:* Have closely related non‑linear or unbalanced two‑color urns (with path‑dependent reinforcement) been analyzed in the literature using stochastic approximation, and how do those proofs handle:
     - growth exponents,
     - tightness of normalized compositions,
     - convergence to equilibria of limiting ODEs?

   These could serve as templates for repairing the present proof.

## 4. External research

### 4.1 Search strategy

I used queries of the following types:

- For stochastic approximation and ODE methods:
  - “A dynamical system approach to stochastic approximations” (Benaïm).
  - “Borkar stochastic approximation ODE method”.
  - “Robbins–Siegmund almost supermartingale theorem”.
  - “convergence of stochastic approximation via martingale and converse Lyapunov methods”.

- For generalized and non‑linear urns:
  - “generalized Pólya urns via stochastic approximation”.
  - “nonlinear unbalanced urn models via stochastic approximation”.
  - “nonlinear and unbalanced urn models two strategies stochastic approximation”.
  - “positive reinforced generalized time‑dependent Pólya urns via stochastic approximation”.

- For martingale concentration:
  - “Freedman inequality martingale tail bound bounded increments”.

- For the specific urn puzzle:
  - A direct search for the problem text (found the Reddit posting, but no accessible solution).

Below I summarize the most relevant findings.

### 4.2 Stochastic approximation and ODE method

#### Benaïm (1996): “A dynamical system approach to stochastic approximations”

Benaïm studies recursions of the form
\[
x_{n+1} = x_n + \gamma_{n+1}\bigl(F(x_n) + \zeta_{n+1}\bigr),
\]
with decreasing step sizes \(\gamma_n\) and martingale‑difference noise \(\zeta_{n+1}\), and proves that the limit set of \((x_n)\) is, under classical assumptions, almost surely contained in the chain‑recurrent set of the ODE \(\dot x = F(x)\).([libra.unine.ch](https://libra.unine.ch/entities/publication/b43c676a-1c01-4ac4-b90d-1cf33a660bf1))

Roughly, if:

- \(\sum \gamma_n = \infty\), \(\sum \gamma_n^2 < \infty\),
- \(F\) is locally Lipschitz and growth of \(x_n\) is controlled,
- the noise has bounded variance in a suitable sense,

then any limit point of \((x_n)\) is an equilibrium or belongs to an invariant set of the ODE. If the ODE \(\dot x = F(x)\) has a unique globally attracting equilibrium \(x_*\), one typically gets \(x_n\to x_*\) almost surely.

Applied to our setting, one can think of a scaled variable \(Y_n\) (either \(K_n/n^{2/3}\) or \(K_n^3/n^2\)) and interpret its evolution as an SA algorithm with step size \(1/n\) and drift \(F\) having a unique stable zero.

#### Borkar (2008, 2023): *Stochastic Approximation: A Dynamical Systems Viewpoint*

Borkar’s monograph provides a very accessible and systematic treatment of the ODE method for SA, including:

- Basic convergence analysis for SA schemes \(x_{n+1} = x_n + a_n[F(x_n) + M_{n+1}]\) with \(a_n\to 0\), \(\sum a_n=\infty\), \(\sum a_n^2 < \infty\).([link.springer.com](https://link.springer.com/book/10.1007/978-93-86279-38-5?utm_source=openai))
- Conditions under which the iterates track the flow of the ODE \(\dot x = F(x)\) and converge to globally attracting equilibria.
- Results on stability (boundedness of iterates) based on the “Borkar–Meyn theorem”, which connects global asymptotic stability of the ODE with almost‑sure boundedness and convergence of the SA iterates.([arxiv.org](https://arxiv.org/abs/2205.01303?utm_source=openai))

This framework matches *exactly* the structure needed for \(Y_n\), once you rewrite its recursion in the SA form and verify:

- step size \(a_n=1/n\),
- drift \(F(y)=3-2y\) (or \(F(y)=y^{-2}-\frac23 y\) in the other scaling),
- noise is a bounded martingale difference whose squared contribution is summable thanks to polynomial growth of \(K_n\).

#### Renlund (2010, 2011)

Renlund’s preprints “Generalized Pólya urns via stochastic approximation” (2010) and “Limit theorems for stochastic approximation algorithms” (2011) treat generalized urn models and associated 1D SA recursions. The 2011 paper proves a CLT for one‑dimensional SA algorithms converging to a point where the noise does not vanish and shows how the theory applies to a class of generalized Pólya urn models.([arxiv.org](https://arxiv.org/abs/1102.4741))

From our perspective, these papers are relevant because:

- They explicitly analyze urn models through SA recursions in one dimension.
- They give precise conditions under which such recursions converge (and have fluctuation limits), typically assuming that the mean drift behaves like a smooth function of the state plus higher‑order corrections.

Our \(Y_n\) process is exactly of this SA type, with a smooth 1D drift and bounded noise, so Renlund’s framework is a strong indication that our target convergence result is within reach.

### 4.3 Almost‑supermartingale convergence (Robbins–Siegmund)

Robbins and Siegmund (1971) introduced the concept of **almost supermartingales** and proved a widely used convergence theorem. A particularly clear statement and discussion is provided in a modern blog exposition by NRH Statistics.([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

Let \((V_n)\) be nonnegative, adapted, and suppose there exist nonnegative adapted sequences \(\beta_n,\xi_n,\zeta_n\) such that
\[
\mathbb E[V_n\mid\mathcal F_{n-1}]
\le (1+\beta_{n-1})V_{n-1} + \xi_{n-1} - \zeta_{n-1}
\]
and
\[
\sum_n \beta_n <\infty,\quad
\sum_n \xi_n <\infty\quad\text{a.s.}
\]
Then:

- \(V_n\) converges almost surely to a finite nonnegative limit \(V_\infty\).
- \(\sum_n \zeta_n <\infty\) almost surely.

In many SA proofs, one takes \(V_n\) to be some Lyapunov‑type function of the state (e.g., squared distance to an equilibrium), and shows that \(\zeta_n\) dominates \(V_n\) times a summable factor, which forces \(V_n\to 0\).

In our context, we can aim to set
\[
V_n := \bigl(Y_n - \tfrac32\bigr)^2,
\]
where \(Y_n = K_n^3/n^2\). If we can derive an inequality of the form
\[
\mathbb E[V_{n+1}\mid\mathcal F_n]
\le V_n - c\,\frac1n V_n + \varepsilon_n,
\]
with \(\sum \varepsilon_n<\infty\), we can identify \(cV_n/n\) with \(\zeta_n\) and apply Robbins–Siegmund to conclude \(V_n\to 0\), i.e. \(Y_n\to 3/2\).

This gives a **very concrete way** to fix Gap 3 (rigorous SA convergence) without reproducing the entire ODE method theory.

### 4.4 Martingale concentration: Freedman and variants

To control \(K_n\) and the martingale noise terms, we need strong concentration for martingales with bounded increments. The classical tool is **Freedman’s inequality**, a Bernstein‑type inequality for martingales. While Freedman’s original 1975 paper is not directly accessible here, modern references (e.g., Dzhaparidze–van Zanten’s generalization and Tropp’s matrix version) clearly describe its structure.([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304414900000867?utm_source=openai))

In one typical form (for scalar martingales):

- If \(M_n\) is a martingale with increments \(\Delta M_k = M_k - M_{k-1}\) satisfying \(|\Delta M_k| \le R\) almost surely, and predictable quadratic variation
  \[
  V_n = \sum_{k=1}^n \mathbb E[(\Delta M_k)^2\mid\mathcal F_{k-1}],
  \]
  then for all \(x,v>0\),
  \[
  \mathbb P\!\left( M_n \ge x,\ V_n\le v\right)
  \le \exp\!\left(-\frac{x^2}{2(v+Rx/3)}\right).
  \]

Our process \(M_n = K_n - \sum_{j<n}p_j\) has increments bounded by 1 and quadratic variation \(\le A_n := \sum p_j\). Freedman’s inequality (or its refinements) therefore allows us to:

- Show that on an event where \(K_n\) is hypothesized to grow too fast or too slowly, the martingale cannot deviate enough from its drift to sustain that behavior with significant probability.
- Turn these probabilistic bounds into Borel–Cantelli arguments to exclude “wrong” polynomial exponents for \(K_n\).

Thus, these inequalities are exactly the right tools to make the “bootstrap” arguments on \(K_n\)’s growth rigorous, addressing Gap 1.

### 4.5 Generalized and non‑linear urn models via stochastic approximation

There is by now a substantial body of work explicitly treating generalized Pólya urns (including non‑linear and unbalanced models) via stochastic approximation:

- **Laruelle & Pagès (2013, 2019):** “Randomized urn models revisited using stochastic approximation” and “Nonlinear randomized urn models: a stochastic approximation viewpoint”. These papers recast various adaptive urn schemes (often used in clinical trials) as SA algorithms and prove LLNs and CLTs for compositions.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

- **Renlund (2010):** “Generalized Pólya urns via stochastic approximation” (Preprint). This directly analyses generalized urn schemes as SA, providing convergence results and effective tools (e.g., Lyapunov functions) to treat irregular reinforcement.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

- **Idriss (2022/2023):** Two key papers:
  - “Nonlinear unbalanced urn models via stochastic approximation” (Methodology and Computing in Applied Probability, 2022), which considers a two‑color *nonlinear unbalanced* urn under a drawing rule reinforced by an \(\mathbb R_+\)-valued concave function and a non‑balanced replacement matrix, and proves limit laws for the urn composition using SA.([ideas.repec.org](https://ideas.repec.org/a/spr/metcap/v24y2022i1d10.1007_s11009-021-09858-6.html?utm_source=openai))
  - “Nonlinear and unbalanced urn models with two types of strategies: a stochastic approximation point of view” (Probability in the Engineering and Informational Sciences, 2023), which studies a 2‑color urn with **two different nonlinear drawing rules depending on the color withdrawn**, proving both a strong law of large numbers and a CLT, again using SA and martingale techniques.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

- **Ruszel & Thacker (2024):** “Positive Reinforced Generalized Time‑Dependent Pólya Urns via Stochastic Approximation” (J. Theor. Probability). They study time‑dependent multi‑urn models with general reinforcement functions \(f\), assuming conditions such as
  \[
  \sum_{n}\Bigl(\frac{\sigma_n}{\sum_{j\le n}\sigma_j}\Bigr)^2<\infty,
  \]
  precisely to control the SA noise. They show convergence to fixation behavior by coupling to an ODE and verifying SA conditions.([link.springer.com](https://link.springer.com/article/10.1007/s10959-024-01366-w))

- **Dasgupta & Maulik (2011):** “Strong Laws for Urn Models with Balanced Replacement Matrices” (EJP 16), which, although focused on balanced matrices, provides examples of scaling exponents determined by spectral radii of drift matrices and proves almost‑sure convergence of properly normalized counts.([arxiv.org](https://arxiv.org/abs/1010.5348))

Even though none of these works treats *exactly* our path‑dependent rule (where the red increment on a blue draw depends on the total number of past blue draws), the **techniques** they deploy are precisely:

- identify a suitable low‑dimensional SA recursion (for proportions or for some power of a count),
- derive a drift of the form \(a_n F(x_n)\) with \(a_n\approx 1/n\),
- control a martingale noise term with square‑summable variance,
- appeal to Benaïm/Borkar/Robbins–Siegmund‑type theorems to get almost‑sure convergence to equilibria of the limiting ODE.

Thus, these papers serve as strong methodological precedents that the internal approach with \(Y_n=K_n^3/n^2\) is on the right track and should be rigorously workable.

### 4.6 Summary of how these results relate to the gaps

- **Gap 1 (growth of \(K_n\))**:  
  The use of Freedman’s inequality and related martingale Bernstein bounds is standard in SA analyses to control deviations of the martingale part relative to its predictable quadratic variation.([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304414900000867?utm_source=openai)) This is exactly what is needed to invalidate hypothetical growth regimes \(K_n \sim n^\beta\) with \(\beta\neq 2/3\).

- **Gap 2 (drift error \(\varepsilon_n\) for \(Y_n\))**:  
  Once you know that \(K_n\) grows as at least \(n^{1/2+\delta}\) and at most \(n^{1-\delta}\), you automatically get \(D_n = K_n^2+K_n+2n+2\) of order \(n^{1+\delta'}\) with \(\delta'>0\). This is analogous to the growth assumptions on cumulative additions \(\sum \sigma_j\) in Ruszel–Thacker, used to prove that SA noise is small.([link.springer.com](https://link.springer.com/article/10.1007/s10959-024-01366-w))

- **Gap 3 (noise term \(N_{n+1}\))**:  
  In essentially every SA reference (Borkar, Benaïm, Renlund, Ruszel & Thacker), one assumes \(\sum a_n^2 \mathbb E[\|\eta_{n+1}\|^2]<\infty\) to control martingale noise. With \(a_n=1/n\) and \(K_n\simeq n^{2/3}\), we get \(\mathbb E[N_{n+1}^2]\lesssim n^{-4/3}\), which is square‑summable. This mirrors the “polynomial growth allowed but not exponential” condition on \(\sigma_n\) in Ruszel–Thacker: they require \(\sum (\sigma_n / \sum_{j\le n}\sigma_j)^2<\infty\) precisely to have square‑summable noise.([link.springer.com](https://link.springer.com/article/10.1007/s10959-024-01366-w))

- **Gap 3/4 (SA convergence theorem)**:  
  Robbins–Siegmund’s almost‑supermartingale theorem, as clearly exposited in modern sources, gives a very direct way to conclude \(V_n\to 0\) almost surely from inequalities of the form
  \[
  \mathbb E[V_{n+1} \mid \mathcal F_n] \le (1+\beta_n)V_n + \xi_n - \zeta_n,
  \]
  with summable \(\beta_n,\xi_n\). Taking \(V_n = (Y_n-3/2)^2\) and \(\zeta_n\) proportional to \(\frac1n V_n\) matches the classical SA scheme with step size \(1/n\).([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

- **Non‑linear urn precedents**:  
  Idriss’s urns with two different strategies, Renlund’s generalized urns, and Laruelle–Pagès’s randomized urns all demonstrate that path‑dependent or color‑dependent reinforcement can be treated with SA techniques similar to those outlined internally.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

## 5. Impact on current solution method

### Support for the current method

The external literature strongly supports the solver’s main strategy:

- **Reduction to a one‑dimensional SA process** is standard: Renlund, Laruelle–Pagès, and Idriss all reduce complicated urn models to low‑dimensional SA recursions.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))
- **Choice of potential function \(Y_n = K_n^3/n^2\)** is very much in the spirit of typical SA/Lyapunov methods: one selects a function of the process for which the mean drift is nicely linear or polynomial in the scale parameter. Borkar’s and Benaïm’s work often uses such Lyapunov‑type transformations.([link.springer.com](https://link.springer.com/book/10.1007/978-93-86279-38-5?utm_source=openai))
- **Use of ODE heuristics** to guess exponents and limiting constants is exactly what the ODE method prescribes: write down a formal ODE for the drift and look at its equilibria and stability.

Thus, nothing in the literature contradicts the internal heuristic; rather, it confirms that, under appropriate estimates, the plan should work.

### How the literature suggests refinements

1. **Structured two‑stage proof (bootstrap + SA):**

   The way SA is used in urn papers (e.g., Idriss 2022 and Ruszel–Thacker 2024) suggests a **two‑stage argument**:

   - *Stage A (bootstrap):* Use coarse drift and martingale inequalities (like Freedman) to prove that the process is confined to a polynomial range. For instance, that for some \(1/2<\gamma_1<2/3<\gamma_2<1\),
     \[
     n^{\gamma_1} \le K_n \le n^{\gamma_2}
     \quad\text{eventually a.s.}
     \]
     This is analogous to proving boundedness of iterates in SA via a Lyapunov function and Borkar–Meyn’s criterion.([arxiv.org](https://arxiv.org/abs/2205.01303?utm_source=openai))

   - *Stage B (fine SA):* With those bounds in place, rewrite the \(Y_n\) recursion in the canonical SA form with small error and square‑summable noise, then invoke Robbins–Siegmund or Benaïm/Borkar.

   This exactly resolves the circularity noted by the evaluator.

2. **Concrete theorem to use: Robbins–Siegmund**

   Instead of appealing vaguely to “stochastic approximation theory”, one can explicitly quote the Robbins–Siegmund almost‑supermartingale theorem as the main tool for convergence of \(V_n=(Y_n-3/2)^2\). The NRH Statistics article provides a clear statement and even shows how the strong law of large numbers is a direct corollary, which is very close in spirit to our setting.([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

3. **Pre‑existing urn analyses with non‑linear rules**

   Idriss’s 2‑strategy urns and Ruszel–Thacker’s generalized time‑dependent urns, both handled via SA, indicate that the “non‑linear, unbalanced” nature of the reinforcement is not an obstacle in itself. What matters is that one can identify a stable equilibrium of the limiting ODE and verify the standard SA conditions, exactly as in our problem.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

In summary: the current method is not fundamentally flawed; it just needs to be aligned more explicitly with the standard SA/ODE toolkit, and the bootstrap and noise‑control steps need to be fully fleshed out using Freedman + Robbins–Siegmund.

## 6. Suggested new or refined approaches

Based on the literature, here are refined approaches that build on the internal plan.

### Approach A: Two‑stage SA analysis with \(Y_n = K_n^3/n^2\)

**Core idea:**  
Make the internal Approach 3 fully rigorous by:

1. Proving coarse polynomial bounds for \(K_n\) using Freedman’s inequality plus drift estimates.
2. Then treating \(Y_n\) as an SA process and applying Robbins–Siegmund to \(V_n=(Y_n-3/2)^2\).

**External results used or mimicked:**

- Freedman‑type martingale inequalities for bounded increments to control \(M_n=K_n-\sum p_j\).([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304414900000867?utm_source=openai))
- Robbins–Siegmund almost supermartingale theorem to get convergence of \(V_n\).([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))
- Structural guidance from Renlund (2010, 2011) and Idriss (2022) on treating 1D SA recursions arising from urns.([arxiv.org](https://arxiv.org/abs/1102.4741))

**Main technical tasks:**

1. **Upper exponent bound for \(K_n\):**

   - For \(\beta>2/3\), consider the event \(E_\beta =\{K_n \ge n^\beta \text{ for infinitely many }n\}\). Under this hypothesis,
     \[
     p_n \le \frac{n+1}{K_n^2} \lesssim n^{1-2\beta}.
     \]
   - Then \(A_n=\sum p_n \lesssim n^{2-2\beta}\), while the Freedman inequality plus Borel–Cantelli give that the martingale \(M_n\) is eventually negligible compared to \(A_n\). Therefore \(K_n \sim A_n\) and thus \(K_n = O(n^{2-2\beta})\), which contradicts \(K_n\ge n^\beta\) when \(\beta>2/3\).
   - Make this argument precise on dyadic blocks, then apply Borel–Cantelli to deduce \(\mathbb P(E_\beta)=0\).

2. **Lower exponent bound for \(K_n\):**

   - For \(\beta<2/3\), analyze the event \(F_\beta =\{K_n \le n^\beta \text{ for infinitely many }n\}\). On this event, for large \(n\) one has \(K_n^2\le n^{2\beta}\ll n\) is *false* if \(\beta>1/2\), so more care is needed, but one can still show that \(p_n\gtrsim n^{1-2\beta}\) often enough to push \(A_n\) and hence \(K_n\) above \(n^\beta\). The literature on generalized urns (e.g., Renlund 2010, Ruszel–Thacker 2024) provides precedents for such lower‑bound drift arguments.([arxiv.org](https://arxiv.org/abs/1102.4741))

   - Again, Freedman controls deviations of \(M_n\) so that the deterministic drift dominates in the long run.

3. **Drift and noise for \(Y_n\):**

   - With \(K_n\) now sandwiched between \(n^{\gamma_1}\) and \(n^{\gamma_2}\) for some \(1/2<\gamma_1<\gamma_2<1\), deduce:

     - \(D_n \sim K_n^2\) and in particular \(D_n\ge c n^{1+\delta}\) for some \(\delta>0\).
     - \(\mathbb E[N_{n+1}^2\mid\mathcal F_n]\le C n^{-1-\delta'}\) for some \(\delta'>0\), making \(\sum \mathbb E[N_{n+1}^2]<\infty\).

   - Compute explicitly
     \[
     \mathbb E[Y_{n+1}-Y_n\mid\mathcal F_n]
     = \frac{3-2Y_n}{n} + \varepsilon_n,
     \]
     and bound \(|\varepsilon_n|\le Cn^{-1-\delta''}\) (using the exact formula already derived and the new bounds on \(D_n\)). This fixes Gap 2.

4. **Apply Robbins–Siegmund to \(V_n = (Y_n-3/2)^2\):**

   - Use the expansion
     \[
     V_{n+1}
     = V_n + 2(Y_n-3/2)(Y_{n+1}-Y_n) + (Y_{n+1}-Y_n)^2,
     \]
     plug in the drift decomposition, and take conditional expectation. One gets
     \[
     \mathbb E[V_{n+1}\mid\mathcal F_n]
     \le V_n - c\,\frac{1}{n} V_n + \xi_n,
     \]
     with \(\sum \xi_n<\infty\) (from the square‑summable noise and the summable \(\varepsilon_n\)).
   - Now apply the Robbins–Siegmund theorem to conclude \(V_n\) converges and \(\sum \frac{1}{n}V_n <\infty\). The latter forces \(\liminf V_n = 0\). Combined with continuous drift, this gives \(V_n\to 0\), so \(Y_n\to 3/2\).

This is a fairly standard SA analysis once the bootstrapping on \(K_n\) is done, and it directly addresses all the evaluator’s major concerns.

### Approach B: Work with \(Y_n' = K_n/n^{2/3}\) instead

**Core idea:**  
Instead of the cubic potential, use \(Y_n'=K_n/n^{2/3}\) and derive the recursion
\[
\mathbb E[Y_{n+1}' - Y_n'\mid\mathcal F_n]
= \frac{1}{n}\Bigl({Y_n'}^{-2} - \tfrac{2}{3}Y_n'\Bigr) + \text{(small error)},
\]
so that the limiting ODE is
\[
\frac{dy}{dt} = y^{-2} - \tfrac23 y
\]
with unique stable equilibrium \(y_*=(3/2)^{1/3}\).

**External results used:**

- Same SA and ODE method as in Approach A (Benaïm, Borkar, Renlund).([libra.unine.ch](https://libra.unine.ch/entities/publication/b43c676a-1c01-4ac4-b90d-1cf33a660bf1))
- Robbins–Siegmund on \(V_n=(Y_n'-y_*)^2\).

**Pros and cons:**

- The algebra is slightly more involved than with \(K_n^3/n^2\) because the drift function \(F(y)=y^{-2}-\frac23 y\) is non‑linear, so the resulting almost‑supermartingale inequality is a bit uglier.
- On the plus side, it aligns more directly with the internal ODE heuristic for \(K_n\) itself.

In practice, given the clean linear drift of \(Y_n=K_n^3/n^2\), Approach A is probably technically simpler, but Approach B is conceptually parallel and could serve as a cross‑check.

### Approach C: Borrow structure from existing non‑linear urns

**Core idea:**  
Use the technical structure and intermediate lemmas from papers on non‑linear urns (Idriss 2022, Laruelle & Pagès 2013, Renlund 2010) as templates. Many of these papers have already solved the type of problems you face:

- obtaining coarse bounds on counts,
- proving tightness of suitably scaled processes,
- identifying and exploiting appropriate Lyapunov functions.

**How to use:**

- Examine Renlund’s 2010 “Generalized Pólya urns via stochastic approximation” and Idriss’s 2022/2023 papers for the way they:
  - bound urn compositions in terms of polynomial functions of time,
  - rewrite urn compositions as SA recursions with explicit drift and noise,
  - verify SA hypotheses.

- Copy the structure of lemmas (e.g., “If the composition leaves a certain compact region, drift pushes it back with high probability”) and adapt them to the 1D recursion for \(K_n\).

This approach doesn’t fundamentally change the solution method; it gives a highly concrete set of blueprints from proved theorems on structurally similar urns.

## 7. Difficulty assessment and next‑step recommendation

### Difficulty assessment

Given the literature:

- The **technique** required is standard in modern SA and generalized urn theory: martingale inequalities, ODE method, almost‑supermartingale convergence, Lyapunov functions.
- There are multiple **closely related examples** where non‑linear and unbalanced urns are successfully analyzed with SA, including some with reinforcement rules depending on color and time.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))
- No source appears to treat **exactly** this urn, and no ready‑made theorem drops in without some adaptation, but there is no sign of genuinely new phenomena beyond the standard SA framework.

In my view, this places the problem at:

> “Likely solvable with careful work and existing theory.”

It is not trivial; a fully rigorous write‑up would probably be at least a paper‑length SA analysis, but the main ingredients are well‑established.

### Recommended next internal step

**Primary recommendation:**

> Proceed with **Approach A** (two‑stage SA analysis for \(Y_n = K_n^3/n^2\)) and systematically import the Robbins–Siegmund theorem and Freedman’s inequality.

Concretely:

1. **Write a dedicated section proving coarse growth of \(K_n\)**

   - State Freedman’s inequality (or a standard martingale Bernstein inequality) with full hypotheses and citation.
   - For each \(\beta\neq 2/3\), rigorously exclude the event \(K_n \ge n^\beta\) infinitely often (for \(\beta>2/3\)) and \(K_n \le n^\beta\) infinitely often (for \(\beta<2/3\)).
   - Conclude that for some \(1/2<\gamma_1<\gamma_2<1\), almost surely \(K_n\in [n^{\gamma_1},n^{\gamma_2}]\) for all large \(n\).

2. **Tighten the drift and noise estimates for \(Y_n\)**

   - Using the exact drift formula already derived, bound the error term \(\varepsilon_n\) by \(Cn^{-1-\delta}\), with \(\delta>0\) coming from the new lower bound on \(K_n\).
   - Compute \(\mathbb E[N_{n+1}^2\mid\mathcal F_n]\) precisely enough to show \(\sum \mathbb E[N_{n+1}^2]<\infty\).

3. **Formulate and verify an almost‑supermartingale inequality for \(V_n=(Y_n-3/2)^2\)**

   - Carefully expand \(V_{n+1}-V_n\), take conditional expectation, and isolate a negative term of order \(V_n/n\) plus a summable remainder.
   - Map this inequality into the Robbins–Siegmund form and quote the theorem explicitly (e.g. from the NRH Statistics article or from the original 1971 paper).([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

4. **Conclude convergence of \(Y_n\), then deduce the asymptotics of \(K_n\) and \(G_n\)**

   - From \(Y_n\to3/2\) and the definition \(Y_n=K_n^3/n^2\), show \(K_n\sim(3/2)^{1/3}n^{2/3}\).
   - Plug into \(G_n = K_n^2+K_n+2n+2\) to get \(G_n/n^{4/3}\to(3/2)^{2/3}\).

**Secondary recommendation:**

While doing this, keep an eye on the structure of existing urn/SA papers (Renlund, Idriss, Laruelle–Pagès, Ruszel–Thacker) for technical tricks, especially in:

- splitting time into blocks to apply martingale inequalities,
- constructing Lyapunov functions to guarantee boundedness or return to compact sets,
- handling non‑linear drift functions.

If at any point the argument becomes unwieldy, you might consider whether a **slightly more general theorem** from Benaïm or Borkar could be quoted wholesale, replacing part of the detailed drift/noise analysis—this could simplify the write‑up, but only if you can cleanly verify all theorem assumptions.

In summary: the external literature confirms that your plan is conceptually sound and provides specific theorems (Robbins–Siegmund, Freedman inequalities, Benaïm/Borkar SA results) that can be used to repair the gaps. The next iteration should focus on carefully implementing these tools rather than searching for a fundamentally new idea.

---
