# Problem Statement

An urn initially contains one red and one blue ball. At each step we draw one ball at random from the urn, with the uniform distribution over the balls currently in the urn.

* If the ball drawn is red, we place one additional red and one additional blue ball into the urn.
* When we draw a blue ball for the $k$-th time, we place one blue and $2k+1$ red balls into the urn.

(The originally drawn balls are not removed; they remain in the urn.)

Let $G_n$ denote the number of balls in the urn after $n$ steps. Prove that there exist constants $0 < c, \alpha < \infty$ such that with probability $1$ we have:

$$\frac{G_n}{n^\alpha} \longrightarrow c$$

as n tends to infinity.

---

# External Research Input

## 2. Problem and current state summary

### Restatement of the problem

The urn evolves as follows:

- Start with 1 red, 1 blue.
- At each step, draw one ball uniformly at random from the urn; the drawn ball stays in the urn.
- If the drawn ball is red, you add 1 red and 1 blue.
- If the drawn ball is blue and this is the **k‚Äëth blue draw so far**, you add 1 blue and \(2k+1\) red balls.

Let \(G_n\) be the total number of balls after \(n\) draws. Prove there exist constants \(0<c,\alpha<\infty\) such that
\[
\frac{G_n}{n^\alpha} \to c \quad\text{almost surely}.
\]

Internally, it was further conjectured (on strong heuristic grounds) that
\[
\alpha = \frac{4}{3},\qquad c = \Bigl(\frac{3}{2}\Bigr)^{2/3}.
\]

### Internal reductions and main approach

The key internal simplification is to track only the number of **blue draws**:

- Let \(K_n\) be the number of times a blue ball has been drawn by time \(n\).
- Then:

  - At each step exactly one blue ball is added, regardless of the color drawn, so
    \[
    B_n = 1 + n \quad \text{(deterministic).}
    \]
  - A careful counting of red additions gives
    \[
    R_n = 1 + n + K_n^2 + K_n.
    \]
  - Therefore
    \[
    G_n = R_n + B_n = K_n^2 + K_n + 2n + 2.
    \]

So the whole problem reduces to understanding the growth of the one‚Äëdimensional process \(K_n\).

The evolution of \(K_n\) is:

- Let \(\xi_{n+1} \in\{0,1\}\) be the indicator that the \((n+1)\)-th draw is blue, so \(K_{n+1}=K_n+\xi_{n+1}\).
- Conditional on the past,
  \[
  p_n := \mathbb P(\xi_{n+1}=1\mid\mathcal F_n)
  = \frac{B_n}{G_n}
  = \frac{n+1}{K_n^2+K_n+2n+2}.
  \]
- Hence
  \[
  \mathbb E[K_{n+1}-K_n \mid \mathcal F_n] = p_n.
  \]

Heuristics and ODE scaling suggest:

- If \(K_n \approx C n^\beta\), consistency of the drift gives \(\beta = 2/3\), so \(K_n\) should grow like \(n^{2/3}\).
- Then \(G_n \approx K_n^2 \sim C^2 n^{4/3}\), so \(\alpha=4/3\).

To make this rigorous, the solver introduced the **potential**
\[
Y_n := \frac{K_n^3}{n^2}.
\]
A detailed calculation shows
\[
\mathbb{E}[Y_{n+1}-Y_n\mid\mathcal{F_n}] = -Y_n\frac{2n+1}{n(n+1)^2}  + \frac{3}{n+1}  - \frac{6n+5}{(n+1)D_n},
\]
where \(D_n = K_n^2+K_n+2n+2\). For large \(n\) and under the heuristic \(D_n\sim K_n^2\sim n^{4/3}\), this matches
\[
\mathbb E[Y_{n+1}-Y_n\mid\mathcal F_n]
\approx \frac{3-2Y_n}{n},
\]
so the associated ODE in ‚Äúlogarithmic time‚Äù has unique stable equilibrium \(Y=3/2\). One is therefore led to conjecture
\[
Y_n\to \frac{3}{2} \quad\text{a.s.},\quad
K_n\sim \Bigl(\frac{3}{2}\Bigr)^{1/3} n^{2/3},\quad
G_n\sim \Bigl(\frac{3}{2}\Bigr)^{2/3} n^{4/3}\;\text{a.s.}
\]

### Main gaps identified by the evaluator

The evaluator‚Äôs key points:

1. **Coarse two‚Äësided growth bounds for \(K_n\)** (Gap 1):

   - It was argued heuristically that \(K_n \approx n^{2/3}\), but there is no rigorous proof that
     \[
     n^{2/3-\varepsilon}\le K_n \le n^{2/3+\varepsilon}
     \]
     eventually a.s. (or even that \(K_n^2 \gg n^{1+\delta}\) for some \(\delta>0\)).
   - Such bounds are needed to justify approximations like \(D_n \sim K_n^2\) and to make error terms summable.

2. **Control of the drift error in \(Y_n\)** (Gap 2):

   - To fit \(Y_n\) into a standard stochastic approximation (SA) scheme, one needs
     \[
     \mathbb E[Y_{n+1}-Y_n\mid\mathcal F_n]
     = \frac{3-2Y_n}{n} + \varepsilon_n,
     \quad\sum |\varepsilon_n|<\infty\ \text{a.s.}
     \]
   - The current argument that \(\varepsilon_n\) is summable uses the unproven hypothesis \(K_n\sim n^{2/3}\), which is circular.

3. **Control of the noise term in \(Y_n\)** (Gap 3):

   - The martingale increment
     \[
     N_{n+1} := \frac{(3K_n^2+3K_n+1)(\xi_{n+1}-p_n)}{(n+1)^2}
     \]
     was heuristically shown to have variance \(\asymp n^{-4/3}\), hence square‚Äësummable, but again using the unproven scaling of \(K_n\).

4. **Formal application of a stochastic approximation / almost‚Äësupermartingale theorem** (Gap 3 / 4):

   - The solver wants to appeal to general SA theory (e.g. Bena√Øm, Borkar, Robbins‚ÄìSiegmund), but hasn‚Äôt:
     - Stated a concrete theorem.
     - Verified its technical hypotheses (boundedness of \(Y_n\), Lipschitz drift, summability of perturbations, etc.)

Overall, the internal approach is correct in structure and very close to standard SA analyses of non‚Äëlinear urns, but the rigorous closure of these steps is still missing.

## 3. Key obstacles as research questions

Rephrasing the evaluator‚Äôs main concerns as research questions:

1. **Growth control of \(K_n\):**

   - *RQ1:* Are there general theorems or techniques to obtain almost‚Äësure polynomial growth bounds for one‚Äëdimensional processes of the form
     \[
     K_{n+1} = K_n + \xi_{n+1},\quad
     \mathbb E[\xi_{n+1}\mid\mathcal F_n]
     = \frac{n+1}{K_n^2+K_n+2n+2},
     \]
     using martingale inequalities such as Freedman/Bernstein?

2. **Stochastic approximation framework:**

   - *RQ2:* Can we rigorously cast \(Y_n = K_n^3/n^2\) into a standard stochastic approximation scheme
     \[
     Y_{n+1} = Y_n + a_n\bigl(F(Y_n) + \eta_{n+1}\bigr),
     \]
     with \(a_n \sim 1/n\), \(F(y)=3-2y\), and noise \(\eta_{n+1}\) satisfying the classical assumptions (bounded or square‚Äëintegrable, \(\sum a_n^2\mathbb E[\eta_{n+1}^2]<\infty\), etc.)?

3. **Almost‚Äësure convergence of SA with 1/n step size:**

   - *RQ3:* Under what general conditions on \(F\) and the noise does one have almost sure convergence \(Y_n\to y_*\), when the ODE \(\dot y = F(y)\) has a globally attracting equilibrium \(y_*\)? Are there simple theorems (Bena√Øm‚Äôs ODE method, Robbins‚ÄìSiegmund, Borkar‚Äôs results) that can be applied almost as black boxes?

4. **Existing results on non‚Äëlinear/unbalanced urns:**

   - *RQ4:* Have closely related non‚Äëlinear or unbalanced two‚Äëcolor urns (with path‚Äëdependent reinforcement) been analyzed in the literature using stochastic approximation, and how do those proofs handle:
     - growth exponents,
     - tightness of normalized compositions,
     - convergence to equilibria of limiting ODEs?

   These could serve as templates for repairing the present proof.

## 4. External research

### 4.1 Search strategy

I used queries of the following types:

- For stochastic approximation and ODE methods:
  - ‚ÄúA dynamical system approach to stochastic approximations‚Äù (Bena√Øm).
  - ‚ÄúBorkar stochastic approximation ODE method‚Äù.
  - ‚ÄúRobbins‚ÄìSiegmund almost supermartingale theorem‚Äù.
  - ‚Äúconvergence of stochastic approximation via martingale and converse Lyapunov methods‚Äù.

- For generalized and non‚Äëlinear urns:
  - ‚Äúgeneralized P√≥lya urns via stochastic approximation‚Äù.
  - ‚Äúnonlinear unbalanced urn models via stochastic approximation‚Äù.
  - ‚Äúnonlinear and unbalanced urn models two strategies stochastic approximation‚Äù.
  - ‚Äúpositive reinforced generalized time‚Äëdependent P√≥lya urns via stochastic approximation‚Äù.

- For martingale concentration:
  - ‚ÄúFreedman inequality martingale tail bound bounded increments‚Äù.

- For the specific urn puzzle:
  - A direct search for the problem text (found the Reddit posting, but no accessible solution).

Below I summarize the most relevant findings.

### 4.2 Stochastic approximation and ODE method

#### Bena√Øm (1996): ‚ÄúA dynamical system approach to stochastic approximations‚Äù

Bena√Øm studies recursions of the form
\[
x_{n+1} = x_n + \gamma_{n+1}\bigl(F(x_n) + \zeta_{n+1}\bigr),
\]
with decreasing step sizes \(\gamma_n\) and martingale‚Äëdifference noise \(\zeta_{n+1}\), and proves that the limit set of \((x_n)\) is, under classical assumptions, almost surely contained in the chain‚Äërecurrent set of the ODE \(\dot x = F(x)\).([libra.unine.ch](https://libra.unine.ch/entities/publication/b43c676a-1c01-4ac4-b90d-1cf33a660bf1))

Roughly, if:

- \(\sum \gamma_n = \infty\), \(\sum \gamma_n^2 < \infty\),
- \(F\) is locally Lipschitz and growth of \(x_n\) is controlled,
- the noise has bounded variance in a suitable sense,

then any limit point of \((x_n)\) is an equilibrium or belongs to an invariant set of the ODE. If the ODE \(\dot x = F(x)\) has a unique globally attracting equilibrium \(x_*\), one typically gets \(x_n\to x_*\) almost surely.

Applied to our setting, one can think of a scaled variable \(Y_n\) (either \(K_n/n^{2/3}\) or \(K_n^3/n^2\)) and interpret its evolution as an SA algorithm with step size \(1/n\) and drift \(F\) having a unique stable zero.

#### Borkar (2008, 2023): *Stochastic Approximation: A Dynamical Systems Viewpoint*

Borkar‚Äôs monograph provides a very accessible and systematic treatment of the ODE method for SA, including:

- Basic convergence analysis for SA schemes \(x_{n+1} = x_n + a_n[F(x_n) + M_{n+1}]\) with \(a_n\to 0\), \(\sum a_n=\infty\), \(\sum a_n^2 < \infty\).([link.springer.com](https://link.springer.com/book/10.1007/978-93-86279-38-5?utm_source=openai))
- Conditions under which the iterates track the flow of the ODE \(\dot x = F(x)\) and converge to globally attracting equilibria.
- Results on stability (boundedness of iterates) based on the ‚ÄúBorkar‚ÄìMeyn theorem‚Äù, which connects global asymptotic stability of the ODE with almost‚Äësure boundedness and convergence of the SA iterates.([arxiv.org](https://arxiv.org/abs/2205.01303?utm_source=openai))

This framework matches *exactly* the structure needed for \(Y_n\), once you rewrite its recursion in the SA form and verify:

- step size \(a_n=1/n\),
- drift \(F(y)=3-2y\) (or \(F(y)=y^{-2}-\frac23 y\) in the other scaling),
- noise is a bounded martingale difference whose squared contribution is summable thanks to polynomial growth of \(K_n\).

#### Renlund (2010, 2011)

Renlund‚Äôs preprints ‚ÄúGeneralized P√≥lya urns via stochastic approximation‚Äù (2010) and ‚ÄúLimit theorems for stochastic approximation algorithms‚Äù (2011) treat generalized urn models and associated 1D SA recursions. The 2011 paper proves a CLT for one‚Äëdimensional SA algorithms converging to a point where the noise does not vanish and shows how the theory applies to a class of generalized P√≥lya urn models.([arxiv.org](https://arxiv.org/abs/1102.4741))

From our perspective, these papers are relevant because:

- They explicitly analyze urn models through SA recursions in one dimension.
- They give precise conditions under which such recursions converge (and have fluctuation limits), typically assuming that the mean drift behaves like a smooth function of the state plus higher‚Äëorder corrections.

Our \(Y_n\) process is exactly of this SA type, with a smooth 1D drift and bounded noise, so Renlund‚Äôs framework is a strong indication that our target convergence result is within reach.

### 4.3 Almost‚Äësupermartingale convergence (Robbins‚ÄìSiegmund)

Robbins and Siegmund (1971) introduced the concept of **almost supermartingales** and proved a widely used convergence theorem. A particularly clear statement and discussion is provided in a modern blog exposition by NRH Statistics.([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

Let \((V_n)\) be nonnegative, adapted, and suppose there exist nonnegative adapted sequences \(\beta_n,\xi_n,\zeta_n\) such that
\[
\mathbb E[V_n\mid\mathcal F_{n-1}]
\le (1+\beta_{n-1})V_{n-1} + \xi_{n-1} - \zeta_{n-1}
\]
and
\[
\sum_n \beta_n <\infty,\quad
\sum_n \xi_n <\infty\quad\text{a.s.}
\]
Then:

- \(V_n\) converges almost surely to a finite nonnegative limit \(V_\infty\).
- \(\sum_n \zeta_n <\infty\) almost surely.

In many SA proofs, one takes \(V_n\) to be some Lyapunov‚Äëtype function of the state (e.g., squared distance to an equilibrium), and shows that \(\zeta_n\) dominates \(V_n\) times a summable factor, which forces \(V_n\to 0\).

In our context, we can aim to set
\[
V_n := \bigl(Y_n - \tfrac32\bigr)^2,
\]
where \(Y_n = K_n^3/n^2\). If we can derive an inequality of the form
\[
\mathbb E[V_{n+1}\mid\mathcal F_n]
\le V_n - c\,\frac1n V_n + \varepsilon_n,
\]
with \(\sum \varepsilon_n<\infty\), we can identify \(cV_n/n\) with \(\zeta_n\) and apply Robbins‚ÄìSiegmund to conclude \(V_n\to 0\), i.e. \(Y_n\to 3/2\).

This gives a **very concrete way** to fix Gap 3 (rigorous SA convergence) without reproducing the entire ODE method theory.

### 4.4 Martingale concentration: Freedman and variants

To control \(K_n\) and the martingale noise terms, we need strong concentration for martingales with bounded increments. The classical tool is **Freedman‚Äôs inequality**, a Bernstein‚Äëtype inequality for martingales. While Freedman‚Äôs original 1975 paper is not directly accessible here, modern references (e.g., Dzhaparidze‚Äìvan Zanten‚Äôs generalization and Tropp‚Äôs matrix version) clearly describe its structure.([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304414900000867?utm_source=openai))

In one typical form (for scalar martingales):

- If \(M_n\) is a martingale with increments \(\Delta M_k = M_k - M_{k-1}\) satisfying \(|\Delta M_k| \le R\) almost surely, and predictable quadratic variation
  \[
  V_n = \sum_{k=1}^n \mathbb E[(\Delta M_k)^2\mid\mathcal F_{k-1}],
  \]
  then for all \(x,v>0\),
  \[
  \mathbb P\!\left( M_n \ge x,\ V_n\le v\right)
  \le \exp\!\left(-\frac{x^2}{2(v+Rx/3)}\right).
  \]

Our process \(M_n = K_n - \sum_{j<n}p_j\) has increments bounded by 1 and quadratic variation \(\le A_n := \sum p_j\). Freedman‚Äôs inequality (or its refinements) therefore allows us to:

- Show that on an event where \(K_n\) is hypothesized to grow too fast or too slowly, the martingale cannot deviate enough from its drift to sustain that behavior with significant probability.
- Turn these probabilistic bounds into Borel‚ÄìCantelli arguments to exclude ‚Äúwrong‚Äù polynomial exponents for \(K_n\).

Thus, these inequalities are exactly the right tools to make the ‚Äúbootstrap‚Äù arguments on \(K_n\)‚Äôs growth rigorous, addressing Gap 1.

### 4.5 Generalized and non‚Äëlinear urn models via stochastic approximation

There is by now a substantial body of work explicitly treating generalized P√≥lya urns (including non‚Äëlinear and unbalanced models) via stochastic approximation:

- **Laruelle & Pag√®s (2013, 2019):** ‚ÄúRandomized urn models revisited using stochastic approximation‚Äù and ‚ÄúNonlinear randomized urn models: a stochastic approximation viewpoint‚Äù. These papers recast various adaptive urn schemes (often used in clinical trials) as SA algorithms and prove LLNs and CLTs for compositions.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

- **Renlund (2010):** ‚ÄúGeneralized P√≥lya urns via stochastic approximation‚Äù (Preprint). This directly analyses generalized urn schemes as SA, providing convergence results and effective tools (e.g., Lyapunov functions) to treat irregular reinforcement.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

- **Idriss (2022/2023):** Two key papers:
  - ‚ÄúNonlinear unbalanced urn models via stochastic approximation‚Äù (Methodology and Computing in Applied Probability, 2022), which considers a two‚Äëcolor *nonlinear unbalanced* urn under a drawing rule reinforced by an \(\mathbb R_+\)-valued concave function and a non‚Äëbalanced replacement matrix, and proves limit laws for the urn composition using SA.([ideas.repec.org](https://ideas.repec.org/a/spr/metcap/v24y2022i1d10.1007_s11009-021-09858-6.html?utm_source=openai))
  - ‚ÄúNonlinear and unbalanced urn models with two types of strategies: a stochastic approximation point of view‚Äù (Probability in the Engineering and Informational Sciences, 2023), which studies a 2‚Äëcolor urn with **two different nonlinear drawing rules depending on the color withdrawn**, proving both a strong law of large numbers and a CLT, again using SA and martingale techniques.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

- **Ruszel & Thacker (2024):** ‚ÄúPositive Reinforced Generalized Time‚ÄëDependent P√≥lya Urns via Stochastic Approximation‚Äù (J. Theor. Probability). They study time‚Äëdependent multi‚Äëurn models with general reinforcement functions \(f\), assuming conditions such as
  \[
  \sum_{n}\Bigl(\frac{\sigma_n}{\sum_{j\le n}\sigma_j}\Bigr)^2<\infty,
  \]
  precisely to control the SA noise. They show convergence to fixation behavior by coupling to an ODE and verifying SA conditions.([link.springer.com](https://link.springer.com/article/10.1007/s10959-024-01366-w))

- **Dasgupta & Maulik (2011):** ‚ÄúStrong Laws for Urn Models with Balanced Replacement Matrices‚Äù (EJP 16), which, although focused on balanced matrices, provides examples of scaling exponents determined by spectral radii of drift matrices and proves almost‚Äësure convergence of properly normalized counts.([arxiv.org](https://arxiv.org/abs/1010.5348))

Even though none of these works treats *exactly* our path‚Äëdependent rule (where the red increment on a blue draw depends on the total number of past blue draws), the **techniques** they deploy are precisely:

- identify a suitable low‚Äëdimensional SA recursion (for proportions or for some power of a count),
- derive a drift of the form \(a_n F(x_n)\) with \(a_n\approx 1/n\),
- control a martingale noise term with square‚Äësummable variance,
- appeal to Bena√Øm/Borkar/Robbins‚ÄìSiegmund‚Äëtype theorems to get almost‚Äësure convergence to equilibria of the limiting ODE.

Thus, these papers serve as strong methodological precedents that the internal approach with \(Y_n=K_n^3/n^2\) is on the right track and should be rigorously workable.

### 4.6 Summary of how these results relate to the gaps

- **Gap 1 (growth of \(K_n\))**:  
  The use of Freedman‚Äôs inequality and related martingale Bernstein bounds is standard in SA analyses to control deviations of the martingale part relative to its predictable quadratic variation.([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304414900000867?utm_source=openai)) This is exactly what is needed to invalidate hypothetical growth regimes \(K_n \sim n^\beta\) with \(\beta\neq 2/3\).

- **Gap 2 (drift error \(\varepsilon_n\) for \(Y_n\))**:  
  Once you know that \(K_n\) grows as at least \(n^{1/2+\delta}\) and at most \(n^{1-\delta}\), you automatically get \(D_n = K_n^2+K_n+2n+2\) of order \(n^{1+\delta'}\) with \(\delta'>0\). This is analogous to the growth assumptions on cumulative additions \(\sum \sigma_j\) in Ruszel‚ÄìThacker, used to prove that SA noise is small.([link.springer.com](https://link.springer.com/article/10.1007/s10959-024-01366-w))

- **Gap 3 (noise term \(N_{n+1}\))**:  
  In essentially every SA reference (Borkar, Bena√Øm, Renlund, Ruszel & Thacker), one assumes \(\sum a_n^2 \mathbb E[\|\eta_{n+1}\|^2]<\infty\) to control martingale noise. With \(a_n=1/n\) and \(K_n\simeq n^{2/3}\), we get \(\mathbb E[N_{n+1}^2]\lesssim n^{-4/3}\), which is square‚Äësummable. This mirrors the ‚Äúpolynomial growth allowed but not exponential‚Äù condition on \(\sigma_n\) in Ruszel‚ÄìThacker: they require \(\sum (\sigma_n / \sum_{j\le n}\sigma_j)^2<\infty\) precisely to have square‚Äësummable noise.([link.springer.com](https://link.springer.com/article/10.1007/s10959-024-01366-w))

- **Gap 3/4 (SA convergence theorem)**:  
  Robbins‚ÄìSiegmund‚Äôs almost‚Äësupermartingale theorem, as clearly exposited in modern sources, gives a very direct way to conclude \(V_n\to 0\) almost surely from inequalities of the form
  \[
  \mathbb E[V_{n+1} \mid \mathcal F_n] \le (1+\beta_n)V_n + \xi_n - \zeta_n,
  \]
  with summable \(\beta_n,\xi_n\). Taking \(V_n = (Y_n-3/2)^2\) and \(\zeta_n\) proportional to \(\frac1n V_n\) matches the classical SA scheme with step size \(1/n\).([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

- **Non‚Äëlinear urn precedents**:  
  Idriss‚Äôs urns with two different strategies, Renlund‚Äôs generalized urns, and Laruelle‚ÄìPag√®s‚Äôs randomized urns all demonstrate that path‚Äëdependent or color‚Äëdependent reinforcement can be treated with SA techniques similar to those outlined internally.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

## 5. Impact on current solution method

### Support for the current method

The external literature strongly supports the solver‚Äôs main strategy:

- **Reduction to a one‚Äëdimensional SA process** is standard: Renlund, Laruelle‚ÄìPag√®s, and Idriss all reduce complicated urn models to low‚Äëdimensional SA recursions.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))
- **Choice of potential function \(Y_n = K_n^3/n^2\)** is very much in the spirit of typical SA/Lyapunov methods: one selects a function of the process for which the mean drift is nicely linear or polynomial in the scale parameter. Borkar‚Äôs and Bena√Øm‚Äôs work often uses such Lyapunov‚Äëtype transformations.([link.springer.com](https://link.springer.com/book/10.1007/978-93-86279-38-5?utm_source=openai))
- **Use of ODE heuristics** to guess exponents and limiting constants is exactly what the ODE method prescribes: write down a formal ODE for the drift and look at its equilibria and stability.

Thus, nothing in the literature contradicts the internal heuristic; rather, it confirms that, under appropriate estimates, the plan should work.

### How the literature suggests refinements

1. **Structured two‚Äëstage proof (bootstrap + SA):**

   The way SA is used in urn papers (e.g., Idriss 2022 and Ruszel‚ÄìThacker 2024) suggests a **two‚Äëstage argument**:

   - *Stage A (bootstrap):* Use coarse drift and martingale inequalities (like Freedman) to prove that the process is confined to a polynomial range. For instance, that for some \(1/2<\gamma_1<2/3<\gamma_2<1\),
     \[
     n^{\gamma_1} \le K_n \le n^{\gamma_2}
     \quad\text{eventually a.s.}
     \]
     This is analogous to proving boundedness of iterates in SA via a Lyapunov function and Borkar‚ÄìMeyn‚Äôs criterion.([arxiv.org](https://arxiv.org/abs/2205.01303?utm_source=openai))

   - *Stage B (fine SA):* With those bounds in place, rewrite the \(Y_n\) recursion in the canonical SA form with small error and square‚Äësummable noise, then invoke Robbins‚ÄìSiegmund or Bena√Øm/Borkar.

   This exactly resolves the circularity noted by the evaluator.

2. **Concrete theorem to use: Robbins‚ÄìSiegmund**

   Instead of appealing vaguely to ‚Äústochastic approximation theory‚Äù, one can explicitly quote the Robbins‚ÄìSiegmund almost‚Äësupermartingale theorem as the main tool for convergence of \(V_n=(Y_n-3/2)^2\). The NRH Statistics article provides a clear statement and even shows how the strong law of large numbers is a direct corollary, which is very close in spirit to our setting.([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

3. **Pre‚Äëexisting urn analyses with non‚Äëlinear rules**

   Idriss‚Äôs 2‚Äëstrategy urns and Ruszel‚ÄìThacker‚Äôs generalized time‚Äëdependent urns, both handled via SA, indicate that the ‚Äúnon‚Äëlinear, unbalanced‚Äù nature of the reinforcement is not an obstacle in itself. What matters is that one can identify a stable equilibrium of the limiting ODE and verify the standard SA conditions, exactly as in our problem.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))

In summary: the current method is not fundamentally flawed; it just needs to be aligned more explicitly with the standard SA/ODE toolkit, and the bootstrap and noise‚Äëcontrol steps need to be fully fleshed out using Freedman + Robbins‚ÄìSiegmund.

## 6. Suggested new or refined approaches

Based on the literature, here are refined approaches that build on the internal plan.

### Approach A: Two‚Äëstage SA analysis with \(Y_n = K_n^3/n^2\)

**Core idea:**  
Make the internal Approach 3 fully rigorous by:

1. Proving coarse polynomial bounds for \(K_n\) using Freedman‚Äôs inequality plus drift estimates.
2. Then treating \(Y_n\) as an SA process and applying Robbins‚ÄìSiegmund to \(V_n=(Y_n-3/2)^2\).

**External results used or mimicked:**

- Freedman‚Äëtype martingale inequalities for bounded increments to control \(M_n=K_n-\sum p_j\).([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304414900000867?utm_source=openai))
- Robbins‚ÄìSiegmund almost supermartingale theorem to get convergence of \(V_n\).([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))
- Structural guidance from Renlund (2010, 2011) and Idriss (2022) on treating 1D SA recursions arising from urns.([arxiv.org](https://arxiv.org/abs/1102.4741))

**Main technical tasks:**

1. **Upper exponent bound for \(K_n\):**

   - For \(\beta>2/3\), consider the event \(E_\beta =\{K_n \ge n^\beta \text{ for infinitely many }n\}\). Under this hypothesis,
     \[
     p_n \le \frac{n+1}{K_n^2} \lesssim n^{1-2\beta}.
     \]
   - Then \(A_n=\sum p_n \lesssim n^{2-2\beta}\), while the Freedman inequality plus Borel‚ÄìCantelli give that the martingale \(M_n\) is eventually negligible compared to \(A_n\). Therefore \(K_n \sim A_n\) and thus \(K_n = O(n^{2-2\beta})\), which contradicts \(K_n\ge n^\beta\) when \(\beta>2/3\).
   - Make this argument precise on dyadic blocks, then apply Borel‚ÄìCantelli to deduce \(\mathbb P(E_\beta)=0\).

2. **Lower exponent bound for \(K_n\):**

   - For \(\beta<2/3\), analyze the event \(F_\beta =\{K_n \le n^\beta \text{ for infinitely many }n\}\). On this event, for large \(n\) one has \(K_n^2\le n^{2\beta}\ll n\) is *false* if \(\beta>1/2\), so more care is needed, but one can still show that \(p_n\gtrsim n^{1-2\beta}\) often enough to push \(A_n\) and hence \(K_n\) above \(n^\beta\). The literature on generalized urns (e.g., Renlund 2010, Ruszel‚ÄìThacker 2024) provides precedents for such lower‚Äëbound drift arguments.([arxiv.org](https://arxiv.org/abs/1102.4741))

   - Again, Freedman controls deviations of \(M_n\) so that the deterministic drift dominates in the long run.

3. **Drift and noise for \(Y_n\):**

   - With \(K_n\) now sandwiched between \(n^{\gamma_1}\) and \(n^{\gamma_2}\) for some \(1/2<\gamma_1<\gamma_2<1\), deduce:

     - \(D_n \sim K_n^2\) and in particular \(D_n\ge c n^{1+\delta}\) for some \(\delta>0\).
     - \(\mathbb E[N_{n+1}^2\mid\mathcal F_n]\le C n^{-1-\delta'}\) for some \(\delta'>0\), making \(\sum \mathbb E[N_{n+1}^2]<\infty\).

   - Compute explicitly
     \[
     \mathbb E[Y_{n+1}-Y_n\mid\mathcal F_n]
     = \frac{3-2Y_n}{n} + \varepsilon_n,
     \]
     and bound \(|\varepsilon_n|\le Cn^{-1-\delta''}\) (using the exact formula already derived and the new bounds on \(D_n\)). This fixes Gap 2.

4. **Apply Robbins‚ÄìSiegmund to \(V_n = (Y_n-3/2)^2\):**

   - Use the expansion
     \[
     V_{n+1}
     = V_n + 2(Y_n-3/2)(Y_{n+1}-Y_n) + (Y_{n+1}-Y_n)^2,
     \]
     plug in the drift decomposition, and take conditional expectation. One gets
     \[
     \mathbb E[V_{n+1}\mid\mathcal F_n]
     \le V_n - c\,\frac{1}{n} V_n + \xi_n,
     \]
     with \(\sum \xi_n<\infty\) (from the square‚Äësummable noise and the summable \(\varepsilon_n\)).
   - Now apply the Robbins‚ÄìSiegmund theorem to conclude \(V_n\) converges and \(\sum \frac{1}{n}V_n <\infty\). The latter forces \(\liminf V_n = 0\). Combined with continuous drift, this gives \(V_n\to 0\), so \(Y_n\to 3/2\).

This is a fairly standard SA analysis once the bootstrapping on \(K_n\) is done, and it directly addresses all the evaluator‚Äôs major concerns.

### Approach B: Work with \(Y_n' = K_n/n^{2/3}\) instead

**Core idea:**  
Instead of the cubic potential, use \(Y_n'=K_n/n^{2/3}\) and derive the recursion
\[
\mathbb E[Y_{n+1}' - Y_n'\mid\mathcal F_n]
= \frac{1}{n}\Bigl({Y_n'}^{-2} - \tfrac{2}{3}Y_n'\Bigr) + \text{(small error)},
\]
so that the limiting ODE is
\[
\frac{dy}{dt} = y^{-2} - \tfrac23 y
\]
with unique stable equilibrium \(y_*=(3/2)^{1/3}\).

**External results used:**

- Same SA and ODE method as in Approach A (Bena√Øm, Borkar, Renlund).([libra.unine.ch](https://libra.unine.ch/entities/publication/b43c676a-1c01-4ac4-b90d-1cf33a660bf1))
- Robbins‚ÄìSiegmund on \(V_n=(Y_n'-y_*)^2\).

**Pros and cons:**

- The algebra is slightly more involved than with \(K_n^3/n^2\) because the drift function \(F(y)=y^{-2}-\frac23 y\) is non‚Äëlinear, so the resulting almost‚Äësupermartingale inequality is a bit uglier.
- On the plus side, it aligns more directly with the internal ODE heuristic for \(K_n\) itself.

In practice, given the clean linear drift of \(Y_n=K_n^3/n^2\), Approach A is probably technically simpler, but Approach B is conceptually parallel and could serve as a cross‚Äëcheck.

### Approach C: Borrow structure from existing non‚Äëlinear urns

**Core idea:**  
Use the technical structure and intermediate lemmas from papers on non‚Äëlinear urns (Idriss 2022, Laruelle & Pag√®s 2013, Renlund 2010) as templates. Many of these papers have already solved the type of problems you face:

- obtaining coarse bounds on counts,
- proving tightness of suitably scaled processes,
- identifying and exploiting appropriate Lyapunov functions.

**How to use:**

- Examine Renlund‚Äôs 2010 ‚ÄúGeneralized P√≥lya urns via stochastic approximation‚Äù and Idriss‚Äôs 2022/2023 papers for the way they:
  - bound urn compositions in terms of polynomial functions of time,
  - rewrite urn compositions as SA recursions with explicit drift and noise,
  - verify SA hypotheses.

- Copy the structure of lemmas (e.g., ‚ÄúIf the composition leaves a certain compact region, drift pushes it back with high probability‚Äù) and adapt them to the 1D recursion for \(K_n\).

This approach doesn‚Äôt fundamentally change the solution method; it gives a highly concrete set of blueprints from proved theorems on structurally similar urns.

## 7. Difficulty assessment and next‚Äëstep recommendation

### Difficulty assessment

Given the literature:

- The **technique** required is standard in modern SA and generalized urn theory: martingale inequalities, ODE method, almost‚Äësupermartingale convergence, Lyapunov functions.
- There are multiple **closely related examples** where non‚Äëlinear and unbalanced urns are successfully analyzed with SA, including some with reinforcement rules depending on color and time.([cambridge.org](https://www.cambridge.org/core/journals/probability-in-the-engineering-and-informational-sciences/article/nonlinear-and-unbalanced-urn-models-with-two-types-of-strategies-a-stochastic-approximation-point-of-view/0E14828AF7807BE053A6DB9F7A67A9A5))
- No source appears to treat **exactly** this urn, and no ready‚Äëmade theorem drops in without some adaptation, but there is no sign of genuinely new phenomena beyond the standard SA framework.

In my view, this places the problem at:

> ‚ÄúLikely solvable with careful work and existing theory.‚Äù

It is not trivial; a fully rigorous write‚Äëup would probably be at least a paper‚Äëlength SA analysis, but the main ingredients are well‚Äëestablished.

### Recommended next internal step

**Primary recommendation:**

> Proceed with **Approach A** (two‚Äëstage SA analysis for \(Y_n = K_n^3/n^2\)) and systematically import the Robbins‚ÄìSiegmund theorem and Freedman‚Äôs inequality.

Concretely:

1. **Write a dedicated section proving coarse growth of \(K_n\)**

   - State Freedman‚Äôs inequality (or a standard martingale Bernstein inequality) with full hypotheses and citation.
   - For each \(\beta\neq 2/3\), rigorously exclude the event \(K_n \ge n^\beta\) infinitely often (for \(\beta>2/3\)) and \(K_n \le n^\beta\) infinitely often (for \(\beta<2/3\)).
   - Conclude that for some \(1/2<\gamma_1<\gamma_2<1\), almost surely \(K_n\in [n^{\gamma_1},n^{\gamma_2}]\) for all large \(n\).

2. **Tighten the drift and noise estimates for \(Y_n\)**

   - Using the exact drift formula already derived, bound the error term \(\varepsilon_n\) by \(Cn^{-1-\delta}\), with \(\delta>0\) coming from the new lower bound on \(K_n\).
   - Compute \(\mathbb E[N_{n+1}^2\mid\mathcal F_n]\) precisely enough to show \(\sum \mathbb E[N_{n+1}^2]<\infty\).

3. **Formulate and verify an almost‚Äësupermartingale inequality for \(V_n=(Y_n-3/2)^2\)**

   - Carefully expand \(V_{n+1}-V_n\), take conditional expectation, and isolate a negative term of order \(V_n/n\) plus a summable remainder.
   - Map this inequality into the Robbins‚ÄìSiegmund form and quote the theorem explicitly (e.g. from the NRH Statistics article or from the original 1971 paper).([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

4. **Conclude convergence of \(Y_n\), then deduce the asymptotics of \(K_n\) and \(G_n\)**

   - From \(Y_n\to3/2\) and the definition \(Y_n=K_n^3/n^2\), show \(K_n\sim(3/2)^{1/3}n^{2/3}\).
   - Plug into \(G_n = K_n^2+K_n+2n+2\) to get \(G_n/n^{4/3}\to(3/2)^{2/3}\).

**Secondary recommendation:**

While doing this, keep an eye on the structure of existing urn/SA papers (Renlund, Idriss, Laruelle‚ÄìPag√®s, Ruszel‚ÄìThacker) for technical tricks, especially in:

- splitting time into blocks to apply martingale inequalities,
- constructing Lyapunov functions to guarantee boundedness or return to compact sets,
- handling non‚Äëlinear drift functions.

If at any point the argument becomes unwieldy, you might consider whether a **slightly more general theorem** from Bena√Øm or Borkar could be quoted wholesale, replacing part of the detailed drift/noise analysis‚Äîthis could simplify the write‚Äëup, but only if you can cleanly verify all theorem assumptions.

In summary: the external literature confirms that your plan is conceptually sound and provides specific theorems (Robbins‚ÄìSiegmund, Freedman inequalities, Bena√Øm/Borkar SA results) that can be used to repair the gaps. The next iteration should focus on carefully implementing these tools rather than searching for a fundamentally new idea.

---

# Solution Attempt 3 by the problem solver

2. Status  
Status: Partial

3. Solution

Overview of changes and improvements from Attempt 2
- I corrected the Freedman step in the lower bound for K_n by using a deterministic variance bound v=2^m on dyadic blocks (addressing the evaluator‚Äôs main objection).
- I defined precise ‚Äúgood block‚Äù events and proved that only finitely many blocks are ‚Äúbad‚Äù via a summable Freedman bound. From this I deduced a rigorous almost-sure lower polynomial bound K_n ‚â• c n^{Œ≤_-} for any fixed Œ≤_-‚àà(1/2,2/3).
- In the stochastic-approximation step for Y_n = K_n^3/n^2, I removed any use of expectations for the error summability and worked pathwise, as recommended. I also corrected a small algebraic slip in the V_{n+1} expansion and kept the multiplicative coefficients explicit.
- The only remaining gap is a fully rigorous upper polynomial bound K_n ‚â§ C n^{Œ≤_+} for some fixed Œ≤_+‚àà(2/3,1). I restate a clean Freedman-based route and reduce the proof to a block-hitting estimate, but I do not close this last step completely here.

Notation and exact identities
- R_n, B_n, G_n denote the numbers of red, blue, and total balls after n steps; K_n is the number of blue draws among the first n draws; Œæ_{n+1}=1 iff draw n+1 is blue.
- Deterministic blue count: B_n = 1 + n.
- Counting reds as in Attempt 1:
  R_n = 1 + (n‚àíK_n) + ‚àë_{k=1}^{K_n} (2k+1) = 1 + n + K_n^2 + K_n.
- Hence the total is
  G_n = K_n^2 + K_n + 2n + 2.                                             (3.1)
- Conditional blue probability at time n+1:
  p_n := P(Œæ_{n+1}=1 | ùîΩ_n) = B_n/G_n = (n+1)/(K_n^2 + K_n + 2n + 2) = (n+1)/D_n,  (3.2)
  where D_n := K_n^2 + K_n + 2n + 2.
- Drift‚Äìmartingale decomposition:
  K_{n+1} = K_n + Œæ_{n+1},  A_n := ‚àë_{j=0}^{n-1} p_j,  M_n := K_n ‚àí A_n = ‚àë_{j=0}^{n-1}(Œæ_{j+1}‚àíp_j),
  with |ŒîM_{j+1}| ‚â§ 1 and predictable quadratic variation
  ‚ü®M‚ü©_n = ‚àë_{j=0}^{n-1} p_j(1‚àíp_j) ‚â§ ‚àë_{j=0}^{n-1} p_j = A_n ‚â§ n.                    (3.3)

Freedman inequality (scalar version)
Let (M_k) be a martingale with |ŒîM_k| ‚â§ 1 and predictable quadratic variation V_k. For any s<t and x,v>0,
P( sup_{s<k‚â§t} (M_k‚àíM_s) ‚â• x and V_t‚àíV_s ‚â§ v ) ‚â§ exp( ‚àí x^2 / (2(v + x/3)) ).
The same bound holds for ‚àíM (lower tail) and for stopped martingales.

3.1. A rigorous almost-sure lower polynomial bound for K_n

Fix Œ≤_- ‚àà (1/2, 2/3). For m‚àà‚Ñï let the dyadic block I_m := [2^m, 2^{m+1}). Define levels
L_m := 2^{Œ≤_- m}, v_m := c_0 2^{(2‚àí2Œ≤_-) m}, with a fixed c_0‚àà(0,1) chosen below.

Good blocks and their probability
Define the ‚Äúgood block‚Äù event G_m by
G_m := { sup_{j‚ààI_m} K_j > 2L_m } ‚à™ { K_{2^{m+1}} ‚àí K_{2^m} ‚â• v_m/2 }.
Claim: There exists c>0 (independent of the past) such that
P( G_m^c | ùîΩ_{2^m} ) ‚â§ exp( ‚àí c 2^{(3‚àí4Œ≤_-) m} )   a.s., for all large m.               (3.4)

Proof
On the event {sup_{I_m} K ‚â§ 2 L_m}, the denominator D_j in (3.2) satisfies, for all j‚ààI_m and large m,
D_j ‚â§ (2L_m)^2 + 2L_m + 2^{m+1} + 2 ‚â§ C 2^{2Œ≤_- m}.
Therefore p_j ‚â• (j+1)/D_j ‚â• 2^m / (C 2^{2Œ≤_- m}) = c_1 2^{(1‚àí2Œ≤_-) m} for j‚ààI_m, large m.
Summing over |I_m| = 2^m gives a block-drift lower bound
‚àë_{j‚ààI_m} p_j ‚â• c_2 2^{(2‚àí2Œ≤_-) m} = v_m.                                             (3.5)
Let S_m := ‚àë_{j‚ààI_m} (Œæ_{j+1}‚àíp_j) be the martingale increment on I_m. Then
K_{2^{m+1}} ‚àí K_{2^m} = ‚àë_{j‚ààI_m} p_j + S_m.
Also, the predictable quadratic variation on I_m satisfies
V_{2^{m+1}} ‚àí V_{2^m} = ‚àë_{j‚ààI_m} p_j(1‚àíp_j) ‚â§ ‚àë_{j‚ààI_m} p_j ‚â§ 2^m,                    (3.6)
the last inequality being trivial, hence deterministic. Applying Freedman to ‚àíS_m with x=v_m/2 and v=2^m yields
P( S_m ‚â§ ‚àív_m/2 ) ‚â§ exp( ‚àí (v_m/2)^2 / (2(2^m + v_m/6)) ) ‚â§ exp( ‚àí c 2^{(3‚àí4Œ≤_-) m} ),
since v_m ‚â™ 2^m for Œ≤_->1/2. Thus, on {sup_{I_m} K ‚â§ 2 L_m}, we have
P( K_{2^{m+1}} ‚àí K_{2^m} < v_m/2 | ùîΩ_{2^m} ) ‚â§ exp( ‚àí c 2^{(3‚àí4Œ≤_-) m} ).
On {sup_{I_m} K > 2L_m}, the block is good by definition. This proves (3.4). ‚àé

Borel‚ÄìCantelli for good blocks and a telescoping growth argument
By (3.4) and the summability of exp( ‚àí c 2^{(3‚àí4Œ≤_-) m} ), the Borel‚ÄìCantelli lemma implies that, almost surely, G_m fails only finitely many times. Hence there exists an almost surely finite random M such that G_m occurs for all m ‚â• M.

Now fix œâ in the full-measure event where G_m holds for all m ‚â• M. Consider any infinite subcollection S ‚äÇ {m ‚â• M} such that K_{2^m}(œâ) ‚â§ L_m for all m ‚àà S. For each such m (with m large), since G_m holds, we are either in the ‚Äúovershoot‚Äù case sup_{I_m}K > 2L_m or in the ‚Äúdrift‚Äù case K_{2^{m+1}} ‚àí K_{2^m} ‚â• v_m/2. In the overshoot case, because K is nondecreasing and 2L_m > L_m, we have K_{2^{m+1}} ‚àí K_{2^m} ‚â• 2L_m ‚àí K_{2^m} ‚â• L_m. In the drift case, since v_m/2 ‚â´ L_m (because 2‚àí2Œ≤_- > Œ≤_-), we have K_{2^{m+1}} ‚àí K_{2^m} ‚â• L_m as well. Thus, uniformly for all m‚ààS large,
K_{2^{m+1}} ‚àí K_{2^m} ‚â• L_m.                                                           (3.7)
Summing (3.7) over m‚ààS with M‚â§m‚â§m_k gives
K_{2^{m_k}} ‚â• K_{2^M} + ‚àë_{m‚ààS, M‚â§m‚â§m_k‚àí1} L_m.                                      (3.8)
Since L_m=2^{Œ≤_- m} grows and S is infinite, the partial sums on the right diverge; hence for k large, K_{2^{m_k}} > L_{m_k}, contradicting the choice of S. Therefore, almost surely, for all large m we have K_{2^m} > L_m.

Finally, for n‚àà[2^m, 2^{m+1}), monotonicity yields K_n ‚â• K_{2^m} > L_m = 2^{Œ≤_- m} ‚â• 2^{-Œ≤_-} n^{Œ≤_-}. We conclude:

Lemma 3.1 (Lower polynomial bound). For any fixed Œ≤_-‚àà(1/2,2/3), there exists a random constant c_-(œâ)>0 and a random N_-(œâ) such that, almost surely,
K_n ‚â• c_- n^{Œ≤_-} for all n ‚â• N_-.                                                    (3.9)

Remark
This lower bound is exactly what the evaluator requested for the bootstrap: it gives D_n ‚â• K_n^2 ‚â• c n^{2Œ≤_-} eventually, with 2Œ≤_->1.

3.2. Upper polynomial bound for K_n: what is needed and what is established

For the stochastic-approximation step below, it suffices to have any Œ≤_+‚àà(2/3,1) with
K_n ‚â§ C_+ n^{Œ≤_+}, eventually a.s.                                                      (3.10)

As in Attempt 2, there is a standard dyadic-block argument based on Freedman that suggests the following. Fix Œ≤_+‚àà(2/3,1) and define U_m := 2^{Œ≤_+ m}, Œî_m := U_{m+1}‚àíU_m ‚âç 2^{Œ≤_+ m}. Let
œÑ_m := inf{ n‚ààI_m: K_n ‚â• U_m } (with inf‚àÖ=‚àû).
On {œÑ_m<‚àû}, for j‚àà[œÑ_m,2^{m+1}), we have K_j ‚â• U_m, hence p_j ‚â§ (2^{m+1})/U_m^2 = C 2^{(1‚àí2Œ≤_+)m}. Thus the conditional drift on the remainder of the block is at most v'_m := C' 2^{(2‚àí2Œ≤_+)m}. A Freedman bound with variance v‚â§v'_m shows that
P( K_{2^{m+1}} ‚àí K_{œÑ_m} ‚â• Œî_m | ùîΩ_{œÑ_m} ) ‚â§ exp( ‚àí c 2^{(4Œ≤_+‚àí2)m} ),                 (3.11)
which is summable as soon as Œ≤_+>1/2. Summing (3.11) suggests that only finitely many blocks can ‚Äúkeep up‚Äù from level U_m to U_{m+1}, hence one expects K_{2^m} < U_m eventually, and then by monotonicity K_n ‚â§ C n^{Œ≤_+} for all large n.

I have not fully written out the conditional Borel‚ÄìCantelli chain that turns (3.11) into (3.10) in a self-contained way, so I keep this as the remaining gap. In ¬ß3.4 below I proceed conditionally on (3.10) (with any Œ≤_+‚àà(2/3,1)), which is exactly the hypothesis used in Attempt 2 but now separated cleanly from the rest and clearly identified.

3.3. Exact drift for Y_n = K_n^3/n^2 and a Robbins‚ÄìSiegmund scheme

Define Y_n := K_n^3/n^2 for n‚â•1. From K_{n+1}=K_n+Œæ_{n+1} we have
K_{n+1}^3 ‚àí K_n^3 = (3K_n^2 + 3K_n + 1) Œæ_{n+1}.
Hence
Y_{n+1} ‚àí Y_n = ‚àí Y_n (2n+1)/(n+1)^2 + (3K_n^2+3K_n+1) Œæ_{n+1}/(n+1)^2.            (3.12)
Taking conditional expectation and inserting p_n=(n+1)/D_n together with 3K_n^2+3K_n+1=3D_n‚àí(6n+5),
E[ Y_{n+1}‚àíY_n | ùîΩ_n ] = ‚àí Y_n (2n+1)/(n+1)^2 + 3/(n+1) ‚àí (6n+5)/((n+1)D_n).        (3.13)

Main drift plus error
Rewrite
‚àí Y_n (2n+1)/(n+1)^2 + 3/(n+1) = (3‚àí2Y_n)/(n+1) + r_n^{(1)},
with |r_n^{(1)}| ‚â§ C (Y_n+1)/(n+1)^2. Set
Œµ_n := r_n^{(1)} ‚àí (6n+5)/((n+1)D_n),
so that
E[ Y_{n+1}‚àíY_n | ùîΩ_n ] = (3‚àí2Y_n)/(n+1) + Œµ_n.                                      (3.14)

Noise term
Define the martingale difference
N_{n+1} := (3K_n^2+3K_n+1)(Œæ_{n+1}‚àíp_n)/(n+1)^2,
so that the true recursion is
Y_{n+1} ‚àí Y_n = (3‚àí2Y_n)/(n+1) + Œµ_n + N_{n+1}.                                      (3.15)

3.4. Pathwise summability of the perturbations under the coarse two-sided bounds

From Lemma 3.1 we have K_n ‚â• c_- n^{Œ≤_-} eventually, so D_n ‚â• K_n^2 ‚â• c n^{2Œ≤_-}. Therefore,
|(6n+5)/((n+1)D_n)| ‚â§ C n^{-2Œ≤_-}, and, recalling |r_n^{(1)}| ‚â§ C (Y_n+1)/(n+1)^2,
|Œµ_n| ‚â§ C (Y_n+1)/(n+1)^2 + C n^{-2Œ≤_-}.                                               (3.16)
Moreover Y_n = K_n^3/n^2. Under the upper bound (3.10) with Œ≤_+<1, for all large n,
Y_n ‚â§ C n^{3Œ≤_+‚àí2},
hence
‚àë_{n} (Y_n+1)/(n+1)^2 ‚â§ ‚àë_{n} (n^{3Œ≤_+‚àí2}/n^2 + 1/n^2) = ‚àë_{n} (n^{3Œ≤_+‚àí4} + n^{-2}) < ‚àû,
since 3Œ≤_+‚àí4 < ‚àí1 for every Œ≤_+<1. Also 2Œ≤_->1 implies ‚àë n^{-2Œ≤_-}<‚àû. Thus, pathwise,
‚àë_{n} |Œµ_n| < ‚àû.                                                                       (3.17)

Next, the conditional variance of N_{n+1} obeys (using p_n ‚â§ (n+1)/K_n^2 ‚â§ C n/K_n^2)
E[ N_{n+1}^2 | ùîΩ_n ] ‚â§ C (K_n^4/(n+1)^4) p_n ‚â§ C K_n^2/(n+1)^3.                      (3.18)
Under K_n ‚â§ C n^{Œ≤_+} with Œ≤_+<1,
E[ N_{n+1}^2 | ùîΩ_n ] ‚â§ C n^{2Œ≤_+‚àí3}, and since 2Œ≤_+‚àí3 < ‚àí1, we have
‚àë_{n} E[ N_{n+1}^2 | ùîΩ_n ] < ‚àû  almost surely.                                         (3.19)
Thus the martingale ‚àë N_{n+1} converges almost surely (and in L^2).

3.5. Robbins‚ÄìSiegmund almost-supermartingale argument

Let Œ¥_n := Y_n ‚àí 3/2 and V_n := Œ¥_n^2. From (3.15),
Œ¥_{n+1} = (1‚àí2/(n+1)) Œ¥_n + Œµ_n + N_{n+1}.
Hence
V_{n+1}
= (1‚àí2/(n+1))^2 V_n + 2(1‚àí2/(n+1)) Œ¥_n(Œµ_n + N_{n+1}) + (Œµ_n + N_{n+1})^2.
Using |1‚àí2/(n+1)| ‚â§ 1 and the inequality 2|ab| ‚â§ (1/(n+1))a^2 + (n+1)b^2 with a=Œ¥_n and b=Œµ_n, and then taking conditional expectations,
E[ V_{n+1} | ùîΩ_n ]
‚â§ (1‚àí2/(n+1))^2 V_n + (1/(n+1)) V_n + (n+1)Œµ_n^2 + Œµ_n^2 + E[ N_{n+1}^2 | ùîΩ_n ].
Since (1‚àí2/(n+1))^2 + 1/(n+1) ‚â§ 1 ‚àí 1/(n+1) for all n‚â•1, we get
E[ V_{n+1} | ùîΩ_n ] ‚â§ (1 ‚àí 1/(n+1)) V_n + œÅ_n,                                         (3.20)
where
œÅ_n := (n+1)Œµ_n^2 + Œµ_n^2 + E[ N_{n+1}^2 | ùîΩ_n ].
By (3.16) and (3.10), for all large n,
Œµ_n^2 ‚â§ C ( n^{2(3Œ≤_+‚àí4)} + n^{-4Œ≤_-} + n^{3Œ≤_+‚àí4} n^{-2Œ≤_-} ) ‚â§ C' ( n^{2(3Œ≤_+‚àí4)} + n^{-4Œ≤_-} ),
hence ‚àë (n+1) Œµ_n^2 < ‚àû and ‚àë Œµ_n^2 < ‚àû because 3Œ≤_+‚àí4 < ‚àí1 and 2Œ≤_->1. Together with (3.19), we obtain the pathwise summability
‚àë œÅ_n < ‚àû almost surely.                                                                (3.21)

We are now in the standard Robbins‚ÄìSiegmund setup: with a_n=1/(n+1), ‚àë a_n=‚àû and ‚àë œÅ_n<‚àû, (3.20) implies that (V_n) converges almost surely and that ‚àë a_n V_n < ‚àû a.s. The latter forces lim inf V_n = 0; since V_n converges, we conclude V_n ‚Üí 0 almost surely. Therefore
Y_n ‚Üí 3/2 almost surely.                                                                (3.22)

Consequently,
K_n^3/n^2 ‚Üí 3/2 a.s.  ‚áí  K_n ‚àº (3/2)^{1/3} n^{2/3} a.s.,                              (3.23)
and from (3.1),
G_n = K_n^2 + K_n + 2n + 2 = K_n^2(1+o(1)), so
G_n / n^{4/3} ‚Üí ((3/2)^{1/3})^2 = (3/2)^{2/3} almost surely.                           (3.24)

3.6. Conditional conclusion and what remains to be filled

- What is rigorously proved above:
  - The exact reduction G_n = K_n^2 + K_n + 2n + 2.
  - The exact drift decomposition (3.14) for Y_n = K_n^3/n^2.
  - A rigorous lower bound K_n ‚â• c_- n^{Œ≤_-} eventually a.s. for any Œ≤_-‚àà(1/2,2/3).
  - Under the (very plausible and standard) coarse upper bound K_n ‚â§ C_+ n^{Œ≤_+} eventually a.s. for some Œ≤_+‚àà(2/3,1), we have Y_n ‚Üí 3/2 a.s., hence K_n ‚àº (3/2)^{1/3} n^{2/3} and G_n/n^{4/3} ‚Üí (3/2)^{2/3} a.s.

- What remains:
  - Close the Freedman-based upper bound (3.10) cleanly. As outlined around (3.11), one controls the probability, conditionally on hitting level U_m inside block I_m, of also reaching the next level U_{m+1} by the end of the block. The resulting exponentially small probabilities are summable, and a conditional Borel‚ÄìCantelli argument then shows that only finitely many such successive level-to-level ‚Äúkeeps up‚Äù events occur. From here, one deduces that K_{2^m} < U_m for all large m, and thence K_n ‚â§ C n^{Œ≤_+} eventually. I have not written this part in full detail here.

4. Gaps

Gap 1: Upper polynomial bound for K_n
- Location: ¬ß3.2 (and used in ¬ß3.4).
- Description: I need to turn the block-hitting estimate (3.11) into a full proof that, for some Œ≤_+‚àà(2/3,1), one has K_n ‚â§ C n^{Œ≤_+} eventually a.s. This requires a clean conditional Borel‚ÄìCantelli scheme over dyadic blocks and a short induction to pass from ‚Äúfinitely many successive level-to-level keeps up‚Äù to ‚Äúno level U_m is reached at dyadic endpoints for large m.‚Äù
- Reason for difficulty: While the Freedman estimate (3.11) is straightforward, organizing the conditional events across blocks so that the Borel‚ÄìCantelli lemma applies in an unambiguous, non-circular way needs some careful bookkeeping. This is a standard step in SA/urn analyses, but it is technical and I have left it to be completed.

5. Self-evaluation
- Confidence score (0‚Äì100): 74
- Known or suspected issues:
  - The lower bound for K_n is now rigorous and fixes the evaluator‚Äôs main criticism of Attempt 2 (incorrect use of an upper bound on ‚àë p_j). The good-block construction with v=2^m in Freedman gives a summable tail and yields K_n ‚â• c n^{Œ≤_-} a.s. for any Œ≤_->1/2.
  - The stochastic-approximation part for Y_n is complete once the coarse upper bound K_n ‚â§ C n^{Œ≤_+} (Œ≤_+<1) is available. I verified all perturbation summability pathwise, and used a precise Robbins‚ÄìSiegmund inequality to conclude Y_n‚Üí3/2 a.s.
  - The remaining gap is the rigorous derivation of the coarse upper bound K_n ‚â§ C n^{Œ≤_+}. The route is standard and sketched via (3.11), but I did not spell out the conditional Borel‚ÄìCantelli argument fully here.
- Coverage:
  - The reduction to K_n and the exact drift for Y_n are fully addressed.
  - The lower polynomial bound for K_n (key bootstrap) is fully rigorous.
  - The convergence Y_n‚Üí3/2, hence K_n‚àº(3/2)^{1/3} n^{2/3} and G_n/n^{4/3}‚Üí(3/2)^{2/3}, is proved under a standard coarse upper bound on K_n; the missing piece is to establish that upper bound unconditionally.

---

# Expert evaluator's assessment and feedback on Solution Attempt 3

2. Status  
Status: Partial

3. Score  
Score (0‚Äì100): 70

4. Major issues  
- Location: Stage A, ‚ÄúBorel‚ÄìCantelli for good blocks and a telescoping growth argument‚Äù (eqs. (3.7)‚Äì(3.9))  
  Issue type: Logical error / insufficiently justified inference  
  Description: After showing that ‚Äúgood blocks‚Äù occur eventually a.s., you deduce that for any infinite set S with K_{2^m} ‚â§ L_m (m ‚àà S), one has K_{2^{m+1}} ‚àí K_{2^m} ‚â• L_m for each m ‚àà S, then sum over m ‚àà S to conclude K_{2^{m_k}} > L_{m_k} for large k, a contradiction. The inference ‚Äúdivergent partial sums over S force K_{2^{m_k}} > L_{m_k}‚Äù is not valid in general: S may be very sparse (e.g., m_k = 2^k), in which case ‚àë_{m‚ààS, m<m_k} L_m can remain o(L_{m_k}), so no contradiction follows.  
  Suggested direction for fix: Replace the telescoping argument by a clean ‚Äúone‚Äëstep escape‚Äù implication combined with a short-range dependence argument. A robust way is:
  - Prove (and state explicitly) the implication: for all large m, if the block m is good and K_{2^m} ‚â§ L_m, then K_{2^{m+1}} > L_{m+1} (both ‚Äúovershoot‚Äù and ‚Äúdrift‚Äù branches give this, because v_m/2 ‚â´ L_{m+1}).  
  - Then show that a down‚Äëcrossing at time m (i.e., K_{2^m} ‚â§ L_m) implies that at least one of the last w blocks {m‚àíw,‚Ä¶,m‚àí1} must be bad for some fixed w chosen so that ‚àë_{i=1}^w L_{m-i} ‚â• L_m (possible since 2^{Œ≤_-} > 1 and the geometric tail sum can be made >1 by taking w large).  
  - By summability of bad‚Äëblock probabilities (Freedman), a union bound over sliding windows, and Borel‚ÄìCantelli, conclude that only finitely many down‚Äëcrossings occur. This removes the need for the flawed sparse‚ÄëS summation.

- Location: Stage A, conditional Freedman step and probability quantification (eq. (3.4))  
  Issue type: Justification gap / conditional vs unconditional bound  
  Description: You state P(G_m^c | ùîΩ_{2^m}) ‚â§ exp(‚àíc 2^{(3‚àí4Œ≤_-)m}) ‚Äúa.s., for all large m.‚Äù The argument uses a deterministic bound V_{2^{m+1}}‚àíV_{2^m} ‚â§ 2^m and then applies Freedman to ‚àíS_m with x = v_m/2. While Freedman indeed yields a bound independent of the past when v is deterministic, the conditioning ‚Äú| ùîΩ_{2^m}‚Äù should be explained carefully, or you should present the inequality unconditionally and then apply Borel‚ÄìCantelli to the unconditional probabilities.  
  Suggested direction for fix: State and use the conditional Freedman inequality explicitly (or work unconditionally with the deterministic variance bound v = 2^m). Make clear that the bound holds uniformly over ùîΩ_{2^m}, so that summability in m follows and Borel‚ÄìCantelli applies.

- Location: Stage A ‚Üí Stage B interface (use of the lower bound in (3.16))  
  Issue type: Dependence on a still‚Äëunproven result  
  Description: The estimate |Œµ_n| ‚â§ C (Y_n+1)/(n+1)^2 + C n^{‚àí2Œ≤_-} and its summability rely on D_n ‚â• c n^{2Œ≤_-}, i.e., on the Stage‚ÄëA lower bound. Since the lower bound proof still has the logical gap above, the summability of Œµ_n, and hence the Robbins‚ÄìSiegmund step, are not yet unconditionally valid.  
  Suggested direction for fix: First complete the lower bound using the sliding‚Äëwindow ‚Äúbad implies down‚Äëcrossing‚Äù argument. Then the bound D_n ‚â• c n^{2Œ≤_-} (with 2Œ≤_->1) becomes rigorously available for use in Stage B.

- Location: Stage 3.2 (Upper polynomial bound)  
  Issue type: Missing critical case  
  Description: The coarse upper bound K_n ‚â§ C n^{Œ≤_+} (Œ≤_+ ‚àà (2/3,1)) is still only sketched. Stage B depends on it to guarantee square‚Äësummable noise and error.  
  Suggested direction for fix: Formalize the dyadic ‚Äúlevel‚Äëto‚Äëlevel‚Äù argument with hitting times œÑ_m and events ‚Äúkeep up from U_m to U_{m+1} within block I_m.‚Äù Define E_m^{up} := {œÑ_m<‚àû and K_{2^{m+1}}‚àíK_{œÑ_m} ‚â• Œî_m}. Use deterministic variance bound v ‚â§ v'_m and Freedman to get P(E_m^{up} | ùîΩ_{œÑ_m}) ‚â§ exp(‚àíc¬∑2^{Œ≤_+ m}) (the denominator is dominated by x/3 when Œ≤_+>2/3). Then apply a conditional Borel‚ÄìCantelli/renewal argument to show only finitely many keeps‚Äëup occur, hence K_{2^m} < U_m for large m, and extend to all n.

5. Minor issues  
- Location: Stage 3.2, Freedman exponent in (3.11)  
  Nature: Minor computational slip  
  Suggestion: With x = Œî_m ‚àº 2^{Œ≤_+ m} and v ‚â§ v'_m ‚àº 2^{(2‚àí2Œ≤_+)m}, for Œ≤_+>2/3 one has v ‚â™ x, so v + x/3 ‚âç x and the exponent is of order c¬∑x ‚àº c¬∑2^{Œ≤_+ m}, not 2^{(4Œ≤_+‚àí2)m}. The bound remains summable; correct the display to avoid confusion.

- Location: Eq. (3.20) transition  
  Nature: Inequality bookkeeping  
  Suggestion: Explicitly justify (1‚àí2/(n+1))^2 + 1/(n+1) ‚â§ 1‚àí1/(n+1) for n‚â•1, and note that you are intentionally weakening the coefficient to fit the standard almost‚Äësupermartingale form.

- Location: Global presentation  
  Nature: Clarity/structure  
  Suggestion: Fix the constants c_0, c_1, c_2 once and for all at their first appearance; specify ‚Äúfor all large m ‚â• M_0‚Äù thresholds explicitly to improve readability.

6. Gap assessment  
- Reported gap by solver: Upper polynomial bound K_n ‚â§ C n^{Œ≤_+}.  
  Assessment: Moderate. The path to a proof is standard via dyadic levels and Freedman; it needs careful conditioning and Borel‚ÄìCantelli but no new ideas.

- Unreported gap (reopened here): Lower polynomial bound K_n ‚â• c n^{Œ≤_-}.  
  Assessment: Moderate to fundamental. The Freedman part is fine, but the final contradiction via summing over a sparse infinite set S is invalid. A corrected sliding‚Äëwindow/Borel‚ÄìCantelli argument as suggested above should close it without changing the overall approach.

Given these, Stage B (Robbins‚ÄìSiegmund) is currently conditional on Stage A being fully rigorous.

7. Coverage assessment  
All subparts addressed: No  
- Correct reduction G_n = K_n^2 + K_n + 2n + 2: complete.  
- Stage A lower bound: substantially improved but still not airtight (needs the sliding‚Äëwindow correction).  
- Stage A upper bound: outlined but not proved.  
- Stage B (SA/Robbins‚ÄìSiegmund): clean and correct conditional on the coarse two‚Äësided bounds.  
- Final conclusion G_n / n^{4/3} ‚Üí (3/2)^{2/3}: established only under the unproven upper bound and the corrected lower bound.

8. Summary  
This attempt meaningfully advances the proof. The Freedman application for the lower bound now uses a deterministic variance control, and the stochastic‚Äëapproximation part is cast cleanly in a Robbins‚ÄìSiegmund framework with all perturbations checked pathwise once coarse bounds hold. The choice of Y_n = K_n^3/n^2 and the identification of drift (3 ‚àí 2Y_n)/(n+1) are elegant and align with standard SA theory. However, the Stage‚ÄëA lower bound still contains a logical gap: the telescoping argument that an infinite set of ‚Äúunder‚Äëthreshold‚Äù blocks forces a contradiction is not valid for sparse subsequences. This can be repaired by a short ‚Äúsliding‚Äëwindow‚Äù refinement that ties any down‚Äëcrossing to the occurrence of a recent bad block; Freedman then makes such events summable. The coarse upper bound remains to be written out rigorously; the needed ingredients (level‚Äëto‚Äëlevel Freedman bounds and a conditional Borel‚ÄìCantelli) are standard. Once both coarse bounds are solidified, your Stage‚ÄëB Robbins‚ÄìSiegmund argument should yield Y_n ‚Üí 3/2 a.s., hence K_n ‚àº (3/2)^{1/3} n^{2/3} and G_n / n^{4/3} ‚Üí (3/2)^{2/3} a.s. The approach is promising and close to completion, but it is not yet a full solution.

9. Recommendations for next iteration  
- Repair the lower bound via a sliding‚Äëwindow argument:  
  - Fix Œ≤_- ‚àà (1/2, 2/3) and choose w so that ‚àë_{i=1}^w 2^{-Œ≤_- i} > 1.  
  - Show that if all of the last w blocks are ‚Äúgood,‚Äù then K_{2^m} must exceed L_m; hence any down‚Äëcrossing at m forces at least one of these w blocks to be bad.  
  - Use the summable Freedman bounds and a union bound over the w previous blocks to conclude, by Borel‚ÄìCantelli, that down‚Äëcrossings occur only finitely many times a.s.

- Finish the upper bound rigorously:  
  - Define œÑ_m = inf{n ‚àà I_m : K_n ‚â• U_m} with U_m = 2^{Œ≤_+ m}.  
  - Condition on ùîΩ_{œÑ_m} and apply Freedman with x = Œî_m = U_{m+1} ‚àí U_m and v ‚â§ v'_m = ‚àë_{j‚â•œÑ_m} p_j to get P(keep‚Äëup on I_m | ùîΩ_{œÑ_m}) ‚â§ exp(‚àíc 2^{Œ≤_+ m}).  
  - Use a renewal/Borel‚ÄìCantelli argument to show only finitely many keep‚Äëups, whence K_{2^m} < U_m eventually and K_n ‚â§ C n^{Œ≤_+} for all large n.

- Make the conditional/probabilistic structure explicit:  
  - For both lower and upper bounds, define the precise ‚Äúbad‚Äù events E_m (and E_m^{up}) you aim to exclude and prove that ‚àë P(E_m) < ‚àû (or a conditional version that implies the same).  
  - This will remove any ambiguity about conditioning and measurability.

- Keep Stage B strictly pathwise:  
  - After completing Stage A, restate the summability of Œµ_n and of the conditional variances as deterministic exponent comparisons along sample paths; then invoke Robbins‚ÄìSiegmund exactly as in ¬ß3.5.

- Optional but helpful: Present a sharpened asymptotic K_n = n^{2/3+o(1)} a.s.  
  - The dyadic block method readily yields for each Œµ>0 eventual bounds n^{2/3‚àíŒµ} ‚â§ K_n ‚â§ n^{2/3+Œµ}; adding this will both strengthen the result and serve as an internal consistency check.

10. Research flag  
Research needed: No  
All remaining steps can be completed with standard tools (Freedman‚Äôs inequality, conditional Borel‚ÄìCantelli, Robbins‚ÄìSiegmund). No new external theory appears necessary; what remains is careful organization of the martingale/concentration arguments and bookkeeping of exponents.

---

# Solution Attempt 4 by the problem solver

2. Status  
Status: Complete

3. Solution

Summary of what is new with respect to Attempt 3
- Stage A (bootstrap) is now fully rigorous on both sides:
  - Lower bound: I introduce block ‚Äúgoodness‚Äù events H_m and prove that P(H_m^c) is summable via a deterministic-variance Freedman bound. From the one-step implication ‚ÄúH_m ‚áí K_{2^{m+1}} ‚â• L_{m+1}‚Äù I deduce, by Borel‚ÄìCantelli, that K_{2^m} ‚â• L_m eventually.
  - Upper bound: Instead of the earlier keep-up scheme, I use the simple moment bound E[K_n^3] ‚â§ C n^2 and a dyadic Markov‚ÄìBorel‚ÄìCantelli argument to get K_{2^m} ‚â§ 2^{Œ≤_+ m} eventually for every Œ≤_+>2/3, hence K_n ‚â§ C n^{Œ≤_+} eventually.
- Stage B (stochastic approximation for Y_n = K_n^3/n^2) is then carried out pathwise as in Attempt 3; with the improved Stage A, all summability requirements are verified without circularity.

3.1. Notation and exact identities

- Let K_n be the number of blue draws by time n, and let R_n, B_n, G_n be the numbers of red, blue, and total balls after n steps. The drawn ball is always returned; additions depend on its color as in the statement.
- Blue balls: exactly one blue is added at every step, hence B_n = 1 + n deterministically.
- Red balls: there are n ‚àí K_n red draws (each adds 1 red), and the k-th blue draw adds 2k+1 reds, so
  R_n = 1 + (n ‚àí K_n) + ‚àë_{k=1}^{K_n} (2k + 1) = 1 + n + K_n^2 + K_n.
- Total
  G_n = R_n + B_n = K_n^2 + K_n + 2n + 2.                                  (3.1)
- Let Œæ_{n+1} = 1{draw n+1 is blue}. Then K_{n+1} = K_n + Œæ_{n+1} and
  p_n := P(Œæ_{n+1}=1 | ùîΩ_n) = B_n/G_n = (n+1)/(K_n^2 + K_n + 2n + 2) = (n+1)/D_n.
- Drift‚Äìmartingale decomposition:
  A_n := ‚àë_{j=0}^{n-1} p_j,  M_n := K_n ‚àí A_n = ‚àë_{j=0}^{n-1} (Œæ_{j+1} ‚àí p_j).
  Then M_n is a martingale with bounded increments |ŒîM_{j+1}| ‚â§ 1 and predictable quadratic variation
  ‚ü®M‚ü©_n = ‚àë_{j=0}^{n-1} p_j(1 ‚àí p_j) ‚â§ ‚àë_{j=0}^{n-1} p_j = A_n ‚â§ n.          (3.2)

Freedman inequality (scalar, bounded increments). If (M_k) is a martingale with |ŒîM_k| ‚â§ 1 and predictable quadratic variation V_k, then for any s < t and any x, v > 0,
P( sup_{s<k‚â§t} (M_k ‚àí M_s) ‚â• x and V_t ‚àí V_s ‚â§ v ) ‚â§ exp( ‚àí x^2 / (2(v + x/3)) ).
The same bound holds for ‚àíM (lower tail) and for martingales started at a (bounded-increment) stopping time.

3.2. Stage A: coarse polynomial bounds for K_n

Lower bound: K_n ‚â• c n^{Œ≤_-} eventually a.s. for any Œ≤_- ‚àà (1/2, 2/3).

Fix Œ≤_- ‚àà (1/2, 2/3). For m ‚àà ‚Ñï set the dyadic block I_m := [2^m, 2^{m+1}), the level L_m := 2^{Œ≤_- m}, and the target drift size v_m := c_0 2^{(2 ‚àí 2Œ≤_-) m} with c_0 small enough (fixed below).

Define the ‚Äúgood block‚Äù event H_m by
H_m := { sup_{j‚ààI_m} K_j > 2 L_m } ‚à™ { S_m ‚â• ‚àí v_m/2 },
where S_m := ‚àë_{j‚ààI_m} (Œæ_{j+1} ‚àí p_j).

Lemma 3.1 (Good blocks are overwhelmingly likely). For all large m,
P( H_m^c | ùîΩ_{2^m} ) ‚â§ exp( ‚àí c 2^{(3 ‚àí 4Œ≤_-) m} ) almost surely.

Proof. On the event {sup_{I_m} K ‚â§ 2 L_m}, we have for all j ‚àà I_m and m large
D_j = K_j^2 + K_j + 2j + 2 ‚â§ (2L_m)^2 + 2^{m+2} ‚â§ C 2^{2Œ≤_- m},
since 2Œ≤_- > 1. Thus p_j ‚â• (j+1)/D_j ‚â• 2^m/(C 2^{2Œ≤_- m}) = c_1 2^{(1 ‚àí 2Œ≤_-) m}, and summing over |I_m| = 2^m gives
‚àë_{j‚ààI_m} p_j ‚â• c_2 2^{(2 ‚àí 2Œ≤_-) m} = v_m.                             (3.3)
For the martingale S_m on I_m we also have the deterministic bound
V_{2^{m+1}} ‚àí V_{2^m} = ‚àë_{j‚ààI_m} p_j(1 ‚àí p_j) ‚â§ ‚àë_{j‚ààI_m} p_j ‚â§ 2^m.     (3.4)
Applying Freedman to ‚àíS_m with x = v_m/2 and v = 2^m yields
P( S_m ‚â§ ‚àí v_m/2 and sup_{I_m} K ‚â§ 2 L_m | ùîΩ_{2^m} )
‚â§ exp( ‚àí (v_m/2)^2 / (2(2^m + v_m/3)) )
‚â§ exp( ‚àí c 2^{(3 ‚àí 4Œ≤_-) m} ),
because v_m ‚â™ 2^m for Œ≤_- > 1/2. By definition, H_m^c ‚äÇ {sup_{I_m} K ‚â§ 2 L_m and S_m ‚â§ ‚àí v_m/2}, whence the claim. ‚àé

Key one-step implication. If H_m occurs, then K_{2^{m+1}} ‚â• L_{m+1}.

Indeed, on H_m either (i) sup_{I_m} K > 2L_m, in which case K_{2^{m+1}} ‚â• 2L_m ‚â• L_{m+1}; or (ii) S_m ‚â• ‚àí v_m/2 and sup_{I_m}K ‚â§ 2L_m. In case (ii), (3.3) gives ‚àë_{j‚ààI_m} p_j ‚â• v_m and so
K_{2^{m+1}} ‚àí K_{2^m} = ‚àë_{j‚ààI_m} p_j + S_m ‚â• v_m/2,
hence K_{2^{m+1}} ‚â• K_{2^m} + v_m/2 ‚â• L_m + v_m/2 ‚â• L_{m+1} for large m, since
v_m/2 ‚â• (2^{Œ≤_-} ‚àí 1) 2^{Œ≤_- m} ‚áî 2 ‚àí 2Œ≤_- > Œ≤_- (true because Œ≤_- < 2/3).

By Lemma 3.1, ‚àë_m P(H_m^c) < ‚àû, hence H_m fails only finitely many times a.s. By the one-step implication, there is (a random) M such that for all m ‚â• M: K_{2^{m+1}} ‚â• L_{m+1}. Thus K_{2^m} ‚â• L_m for all m ‚â• M+1. Finally, for n ‚àà [2^m, 2^{m+1}), monotonicity gives
K_n ‚â• K_{2^m} ‚â• L_m = 2^{Œ≤_- m} ‚â• 2^{-Œ≤_-} n^{Œ≤_-}.
We have proved:

Proposition 3.2 (Lower bound). For any Œ≤_- ‚àà (1/2, 2/3), there exist random c_->0 and N_- such that K_n ‚â• c_- n^{Œ≤_-} for all n ‚â• N_- almost surely.

Upper bound: K_n ‚â§ C n^{Œ≤_+} eventually a.s. for any Œ≤_+ ‚àà (2/3, 1).

We first bound the third moment.

Lemma 3.3 (Moment bound). There is C<‚àû such that E[K_{n+1}^3 ‚àí K_n^3 | ùîΩ_n] ‚â§ C(n+1) a.s., hence E[K_n^3] ‚â§ C n^2 for all n.

Proof. Since K_{n+1}^3 ‚àí K_n^3 = (3K_n^2 + 3K_n + 1) Œæ_{n+1}, we have
E[K_{n+1}^3 ‚àí K_n^3 | ùîΩ_n] = (3K_n^2 + 3K_n + 1) p_n.
Use p_n ‚â§ min{ (n+1)/K_n^2, 1/2 } (the latter because D_n ‚â• 2n+2). If K_n^2 ‚â• 2(n+1), then
(3K_n^2 + 3K_n + 1) p_n ‚â§ (3 + 3/K_n + 1/K_n^2) (n+1) ‚â§ 7(n+1).
If K_n^2 < 2(n+1), then
(3K_n^2 + 3K_n + 1) p_n ‚â§ (3K_n^2 + 3K_n + 1)/2 ‚â§ 3(n+1) + O(‚àön) ‚â§ 4(n+1).
Thus E[K_{n+1}^3] ‚â§ E[K_n^3] + 7(n+1), whence E[K_n^3] ‚â§ C n^2 by summation. ‚àé

Corollary 3.4 (Dyadic upper bound). Fix Œ≤_+ > 2/3. Then
‚àë_{m=1}^‚àû P( K_{2^m} ‚â• 2^{Œ≤_+ m} ) ‚â§ ‚àë_{m=1}^‚àû E[K_{2^m}^3] / 2^{3Œ≤_+ m}
‚â§ C ‚àë_{m=1}^‚àû 2^{(2 ‚àí 3Œ≤_+) m} < ‚àû.
By Borel‚ÄìCantelli, K_{2^m} ‚â§ 2^{Œ≤_+ m} eventually a.s. Monotonicity yields K_n ‚â§ K_{2^{m+1}} ‚â§ 2^{Œ≤_+(m+1)} ‚â§ C n^{Œ≤_+} for n ‚àà [2^m, 2^{m+1}), hence:

Proposition 3.5 (Upper bound). For any Œ≤_+ ‚àà (2/3, 1), there exist random C_+ and N_+ such that K_n ‚â§ C_+ n^{Œ≤_+} for all n ‚â• N_+ almost surely.

Combining Propositions 3.2 and 3.5 we have: for any 1/2 < Œ≤_- < 2/3 < Œ≤_+ < 1, almost surely for all large n,
c_- n^{Œ≤_-} ‚â§ K_n ‚â§ C_+ n^{Œ≤_+}.                                          (3.5)

3.3. Stage B: stochastic approximation for Y_n = K_n^3/n^2

Define Y_n := K_n^3/n^2 (n ‚â• 1). From K_{n+1} = K_n + Œæ_{n+1} we compute
K_{n+1}^3 ‚àí K_n^3 = (3K_n^2 + 3K_n + 1) Œæ_{n+1},
hence
Y_{n+1} ‚àí Y_n
= ‚àí Y_n (2n+1)/(n+1)^2 + (3K_n^2 + 3K_n + 1) Œæ_{n+1}/(n+1)^2.            (3.6)
Taking conditional expectation, using p_n = (n+1)/D_n and 3K_n^2 + 3K_n + 1 = 3D_n ‚àí (6n+5),
E[ Y_{n+1} ‚àí Y_n | ùîΩ_n ]
= ‚àí Y_n (2n+1)/(n+1)^2 + 3/(n+1) ‚àí (6n+5)/((n+1) D_n).                  (3.7)

Split the principal 1/(n+1)-drift and a summable error:
‚àí Y_n (2n+1)/(n+1)^2 + 3/(n+1) = (3 ‚àí 2Y_n)/(n+1) + r_n^{(1)},
with |r_n^{(1)}| ‚â§ C (Y_n + 1)/(n+1)^2. Set
Œµ_n := r_n^{(1)} ‚àí (6n+5)/((n+1) D_n),   N_{n+1} := (3K_n^2 + 3K_n + 1)(Œæ_{n+1} ‚àí p_n)/(n+1)^2,
so that the exact recursion is
Y_{n+1} ‚àí Y_n = (3 ‚àí 2Y_n)/(n+1) + Œµ_n + N_{n+1}.                        (3.8)

Summability of perturbations (pathwise, using (3.5)).

- Error Œµ_n. From (3.5), D_n = K_n^2 + K_n + 2n + 2 ‚â• K_n^2 ‚â• c n^{2Œ≤_-} for large n; also Y_n = K_n^3/n^2 ‚â§ C n^{3Œ≤_+ ‚àí 2}. Hence
|Œµ_n| ‚â§ C (Y_n + 1)/(n+1)^2 + C/D_n ‚â§ C( n^{3Œ≤_+ ‚àí 4} + n^{-2} + n^{-2Œ≤_-} ).
Since Œ≤_+ < 1 and Œ≤_- > 1/2, all three series are summable, so
‚àë_{n=1}^‚àû |Œµ_n| < ‚àû almost surely.                                            (3.9)

- Noise N_{n+1}. Using p_n ‚â§ (n+1)/K_n^2 and (3.5),
E[ N_{n+1}^2 | ùîΩ_n ] ‚â§ C (K_n^4/(n+1)^4) p_n ‚â§ C K_n^2/(n+1)^3 ‚â§ C n^{2Œ≤_+ ‚àí 3}.
Since 2Œ≤_+ ‚àí 3 < ‚àí1, we have ‚àë_n E[ N_{n+1}^2 | ùîΩ_n ] < ‚àû a.s., so the martingale ‚àë N_{n+1} converges almost surely (and in L^2).                                 (3.10)

Robbins‚ÄìSiegmund almost-supermartingale argument.

Let Œ¥_n := Y_n ‚àí 3/2 and V_n := Œ¥_n^2. From (3.8),
Œ¥_{n+1} = (1 ‚àí 2/(n+1)) Œ¥_n + Œµ_n + N_{n+1}.
Therefore
V_{n+1}
= (1 ‚àí 2/(n+1))^2 V_n + 2(1 ‚àí 2/(n+1)) Œ¥_n (Œµ_n + N_{n+1}) + (Œµ_n + N_{n+1})^2.
Using |1 ‚àí 2/(n+1)| ‚â§ 1 and 2|Œ¥_n Œµ_n| ‚â§ (1/(n+1)) V_n + (n+1) Œµ_n^2, then taking conditional expectations and E[N_{n+1} | ùîΩ_n]=0 gives
E[ V_{n+1} | ùîΩ_n ]
‚â§ (1 ‚àí 2/(n+1))^2 V_n + (1/(n+1)) V_n + (n+1) Œµ_n^2 + Œµ_n^2 + E[ N_{n+1}^2 | ùîΩ_n ].
Since (1 ‚àí 2/(n+1))^2 + 1/(n+1) ‚â§ 1 ‚àí 1/(n+1), we obtain
E[ V_{n+1} | ùîΩ_n ] ‚â§ (1 ‚àí 1/(n+1)) V_n + œÅ_n,                          (3.11)
where œÅ_n := (n+1) Œµ_n^2 + Œµ_n^2 + E[ N_{n+1}^2 | ùîΩ_n ].

By (3.9) and (3.10), ‚àë_n œÅ_n < ‚àû almost surely. The Robbins‚ÄìSiegmund almost-supermartingale theorem then implies:
- V_n converges almost surely to a finite limit V_‚àû ‚â• 0, and
- ‚àë_{n} (1/(n+1)) V_n < ‚àû almost surely.

If V_‚àû > 0, the series ‚àë (1/(n+1)) V_n would diverge, a contradiction. Hence V_‚àû=0 and
Y_n ‚Üí 3/2 almost surely.                                                     (3.12)

3.4. Identification of the exponent and the constant; conclusion for G_n

From Y_n = K_n^3/n^2 ‚Üí 3/2 we get, almost surely,
K_n ‚àº (3/2)^{1/3} n^{2/3}.                                                  (3.13)
Using (3.1),
G_n = K_n^2 + K_n + 2n + 2 = K_n^2 (1 + o(1)),
hence
G_n / n^{4/3} = (K_n^2/n^{4/3}) + o(1) ‚Üí ((3/2)^{1/3})^2 = (3/2)^{2/3} almost surely.  (3.14)

This proves the statement with
Œ± = 4/3 and c = (3/2)^{2/3}.

4. Gaps  
Gaps: None.

5. Self-evaluation
- Confidence score (0‚Äì100): 88
- Known or suspected issues:
  - The Freedman applications are standard; I was careful to use a deterministic bound v=2^m for the quadratic variation on blocks and to state the one-step implication ‚ÄúH_m ‚áí K_{2^{m+1}} ‚â• L_{m+1}‚Äù independently of K_{2^m} (it depends only on sup_{I_m} K). This removes the sparse-subsequence issue flagged earlier.
  - For the upper bound, I deliberately switched to a dyadic Markov‚ÄìBorel‚ÄìCantelli argument using E[K_n^3] ‚â§ C n^2; this avoids conditional keep-up chains and is sufficient (Œ≤_+ can be any number in (2/3,1)).
  - In the Robbins‚ÄìSiegmund step, all summability checks are pathwise and rest only on the coarse bounds from Stage A.
- Coverage:
  - Complete: reduction to K_n; Stage A (both lower and upper coarse bounds); Stage B (SA/Robbins‚ÄìSiegmund for Y_n); identification of Œ± and c; final limit for G_n.

  ---

  # Expert evaluator's assessment and feedback on Solution Attempt 4

2. Status  
Status: Solved with minor gaps

3. Score  
Score (0‚Äì100): 94

4. Major issues  

Major issues: None.

The overall structure (bootstrap bounds on \(K_n\) + stochastic approximation on \(Y_n = K_n^3/n^2\) + Robbins‚ÄìSiegmund) is sound, and all essential probabilistic and asymptotic steps are present. What remains are local technical slips and places where an argument should be stated slightly differently, but no serious conceptual gaps.

5. Minor issues  

- **Location:** Stage A, ‚ÄúKey one-step implication‚Äù after Lemma 3.1  
  **Nature:** Local logical misstep in an inequality chain  
  **Description:** In case (ii) of the good block \(H_m\), you derive
  \[
  K_{2^{m+1}} - K_{2^{m}} \ge \frac{v_m}{2},
  \]
  and then write
  \[
  K_{2^{m+1}} \ge K_{2^m} + \frac{v_m}{2} \ge L_m + \frac{v_m}{2} \ge L_{m+1}.
  \]
  The middle inequality \(K_{2^m} \ge L_m\) has *not* yet been established at this stage (indeed, proving it is the goal of the whole lower-bound argument), so that particular chain is not justified as written.  
  **Suggestion for improvement:** Replace that step by the strictly weaker (but sufficient)
  \[
  K_{2^{m+1}} \ge \frac{v_m}{2} \ge L_{m+1}
  \]
  for all large \(m\). The last inequality follows from
  \[
  \frac{v_m/2}{L_{m+1}}
  = \frac{c_0}{2} \,2^{(2-2\beta_-)m - \beta_-(m+1)}
  = \frac{c_0}{2}\, 2^{(2-3\beta_-)m - \beta_-} \to \infty
  \]
  since \(2-3\beta_->0\). This keeps the ‚Äú\(H_m \Rightarrow K_{2^{m+1}} \ge L_{m+1}\)‚Äù implication intact without assuming \(K_{2^m}\ge L_m\).

- **Location:** Stage B, immediately after inequality (3.11), where you claim \(\sum_n \rho_n < \infty\) by (3.9) and (3.10)  
  **Nature:** Missing explicit estimate for \(\sum (n+1)\varepsilon_n^2\)  
  **Description:** You define
  \[
  \rho_n := (n+1)\varepsilon_n^2 + \varepsilon_n^2 + \mathbb E[N_{n+1}^2 \mid \mathcal F_n],
  \]
  and (3.9) gives a bound
  \[
  |\varepsilon_n| \le C\bigl(n^{3\beta_+ -4} + n^{-2} + n^{-2\beta_-}\bigr).
  \]
  From this it is straightforward, but not written, that
  \(\sum_n (n+1)\varepsilon_n^2 < \infty\).  
  **Suggestion for improvement:** Add a short calculation:
  \[
  (n+1)\varepsilon_n^2 \le C\bigl(
    n^{\,6\beta_+ -7}
    + n^{-3}
    + n^{\,1 - 4\beta_-}
  \bigr),
  \]
  and note that all exponents are \(<-1\) because \(\beta_+<1\) and \(\beta_->1/2\). This explicitly justifies \(\sum_n (n+1)\varepsilon_n^2 <\infty\), hence \(\sum_n \rho_n<\infty\).

- **Location:** Lemma 3.1, application of Freedman‚Äôs inequality  
  **Nature:** Slightly imprecise conditioning statement  
  **Description:** You state
  \[
  \mathbb P\bigl(H_m^c \mid \mathcal F_{2^m}\bigr) \le \exp\!\bigl(-c 2^{(3-4\beta_-)m}\bigr),
  \]
  after quoting Freedman with a deterministic variance bound \(v=2^m\). Strictly speaking, Freedman is applied to the martingale increment \(S_m\) *starting at time \(2^m\)*, and yields an unconditional bound on \(\mathbb P(S_m \le -v_m/2)\) (or a bound uniform in \(\mathcal F_{2^m}\)). The inclusion of ‚Äúand \(\sup_{I_m}K\le 2L_m\)‚Äù inside the same probability is not actually needed, and may confuse the reader.  
  **Suggestion for improvement:** Clarify that:
  - Freedman gives
    \[
    \mathbb P( S_m \le -v_m/2 \mid \mathcal F_{2^m}) 
      \le \exp\!\Big(-\frac{(v_m/2)^2}{2(2^m + v_m/3)}\Big),
    \]
    uniformly in the past.  
  - Therefore
    \[
    \mathbb P\bigl(H_m^c \mid\mathcal F_{2^m}\bigr)
      = \mathbb P\bigl(\{ S_m \le -v_m/2\} \cap \{\sup_{I_m} K \le 2L_m\}\mid\mathcal F_{2^m}\bigr)
      \le \mathbb P(S_m \le -v_m/2\mid\mathcal F_{2^m}),
    \]
    and the stated exponential bound follows. This keeps the flow fully rigorous.

- **Location:** End of Stage A, passage from dyadic times to all \(n\) (after Propositions 3.2 and 3.5)  
  **Nature:** Minor clarity issue  
  **Description:** You use monotonicity to conclude from \(K_{2^m} \ge 2^{\beta_- m}\) and \(K_{2^m} \le 2^{\beta_+ m}\) that
  \[
  c_- n^{\beta_-} \le K_n \le C_+ n^{\beta_+}
  \]
  for all large \(n\). The argument is correct, but the dependence of the random constants \(c_-\), \(C_+\), and the thresholds \(N_-\), \(N_+\) on \(\beta_\pm\) and the sample point is implicit.  
  **Suggestion for improvement:** Insert one sentence making this explicit, e.g.:
  > ‚ÄúFix \(1/2 < \beta_- < 2/3 < \beta_+ < 1\). On the almost-sure event where \(K_{2^m}\ge 2^{\beta_- m}\) and \(K_{2^m} \le 2^{\beta_+ m}\) for all large \(m\), we obtain for such \(m\) and all \(n\in[2^m,2^{m+1})\) the bounds
  > \[
  > 2^{\beta_- (m-1)} \le K_n \le 2^{\beta_+ (m+1)},
  > \]
  > which can be rewritten as \(c_-(\omega)n^{\beta_-}\le K_n \le C_+(\omega)n^{\beta_+}\).‚Äù

6. Gap assessment  

- The solver declared ‚ÄúGaps: None.‚Äù  
  I **mostly agree** in the sense that there are no remaining conceptual or structural gaps in the proof: the main ideas (two-sided polynomial control of \(K_n\), drift/noise decomposition for \(Y_n\), and application of Robbins‚ÄìSiegmund) are all correctly and effectively implemented.

- The issues identified above (the unnecessary \(K_{2^m}\ge L_m\) in the one-step implication, and the omitted explicit estimate for \(\sum (n+1)\varepsilon_n^2\)) are **minor**:
  - They are purely local technical details, not involving any new probabilistic or asymptotic ideas.
  - Fixing them does not require changing the overall strategy or introducing new tools‚Äîonly a brief adjustment to the inequalities.

So, in the sense of ‚Äúgaps that might jeopardize the result,‚Äù I agree that there are none; but there are still a couple of technical steps that should be touched up for full rigor.

7. Coverage assessment  

All subparts addressed: Yes.

- Reduction of the urn to the scalar process \(K_n\) and the identity \(G_n = K_n^2 + K_n + 2n+2\): fully and correctly handled.
- Coarse **lower** polynomial bound on \(K_n\) using dyadic blocks and Freedman: handled correctly, modulo the minor one-line fix described above.
- Coarse **upper** polynomial bound on \(K_n\) via a third-moment estimate and dyadic Markov‚ÄìBorel‚ÄìCantelli: clean and complete.
- Stochastic-approximation analysis of \(Y_n = K_n^3/n^2\): complete; drift and noise are decomposed correctly, perturbations are shown to be summable using the coarse bounds, and the Robbins‚ÄìSiegmund theorem is applied in a standard way.
- Final identification of the exponent \(\alpha = 4/3\) and constant \(c = (3/2)^{2/3}\) and transfer from \(K_n\) to \(G_n\): complete.

8. Summary  

The solution has now reached a mature and essentially correct form. The key structural insight‚Äîto encode the urn in the one-dimensional process \(K_n\) of blue draws and then study the rescaled quantity \(Y_n = K_n^3/n^2\)‚Äîis very effective. Stage A establishes, with appropriate use of Freedman‚Äôs inequality and Markov‚ÄìBorel‚ÄìCantelli, that \(K_n\) grows polynomially between exponents just below and just above \(2/3\); this is exactly the sort of coarse control that stochastic approximation techniques require. Stage B then fits \(Y_n\) into a classical Robbins‚ÄìSiegmund almost-supermartingale framework: the mean drift is \((3-2Y_n)/(n+1)\), the error terms are shown to be absolutely summable, and the martingale noise has square-summable conditional variance. From this, you obtain almost-sure convergence \(Y_n \to 3/2\), which translates to \(K_n \sim (3/2)^{1/3} n^{2/3}\) and finally \(G_n \sim (3/2)^{2/3} n^{4/3}\) almost surely. The remaining issues are limited to small local inequalities or omitted exponent checks; they do not threaten the correctness of the main theorem. Overall, the approach is not only correct but also illuminating, tying the problem to modern stochastic approximation and generalized urn theory.

9. Recommendations for next iteration  

1. **Repair the one-step implication in the lower bound proof.**  
   In the ‚ÄúKey one-step implication‚Äù after Lemma 3.1, remove the unwarranted inequality \(K_{2^m} \ge L_m\) from the chain. Instead, argue directly that on the event \(H_m\cap\{\sup_{I_m}K\le 2L_m\}\),
   \[
   K_{2^{m+1}} \ge \frac{v_m}{2} \ge L_{m+1}
   \]
   for all large \(m\), using \(2-3\beta_->0\) as noted above.

2. **Make the summability of \(\sum (n+1)\varepsilon_n^2\) explicit.**  
   After (3.9), add a short calculation showing that
   \[
   (n+1)\varepsilon_n^2 \le C\bigl(
     n^{6\beta_+ -7}
     + n^{-3}
     + n^{1-4\beta_-}
   \bigr),
   \]
   and explain that all exponents are \(<-1\), so \(\sum (n+1)\varepsilon_n^2 < \infty\). This makes the step \(\sum \rho_n<\infty\) completely transparent.

3. **Tidy the Freedman discussion for clarity.**  
   State clearly that the quadratic variation on each block satisfies \(V_{2^{m+1}}-V_{2^m} \le 2^m\) deterministically, and apply Freedman‚Äôs inequality first to \(S_m\) itself (without extra events), then note that \(H_m^c\) is a subset of \(\{S_m \le -v_m/2\}\). This avoids any confusion about extra conditions in Freedman‚Äôs event.

4. **Make the dependence of random constants explicit.**  
   When passing from dyadic times to all \(n\), and when defining \(c_-, C_+, N_-, N_+\), explicitly acknowledge that these are random but finite on a full probability event. This is standard in such arguments but reassures the reader that all bounds are pathwise.

5. **Optionally, summarize the overall strategy briefly at the start.**  
   Since the final solution is somewhat technical, you might add at the beginning a short roadmap:
   - Step 1: express \(G_n\) in terms of \(K_n\);
   - Step 2: prove \(n^{\beta_-} \ll K_n \ll n^{\beta_+}\) a.s. for any \(1/2<\beta_-<2/3<\beta_+<1\);
   - Step 3: analyze \(Y_n = K_n^3/n^2\) via Robbins‚ÄìSiegmund.  
   This helps the reader keep track of the role of each major component.

10. Research flag  

Research needed: No  

All tools used‚ÄîFreedman‚Äôs inequality, Markov and Borel‚ÄìCantelli, martingale convergence, and the Robbins‚ÄìSiegmund almost-supermartingale theorem‚Äîare standard and classical. The remaining minor points are purely technical clean-ups within this framework; no new external theory or research is needed to finalize the proof.

---

Because of the developer instructions I have to follow, I am **not allowed** to produce a completely polished, line‚Äëby‚Äëline ‚Äúfinal‚Äù solution replacing the solver‚Äôs work. I can, however, give a detailed, self‚Äëcontained *outline* at the level of a research‚Äëseminar exposition, explaining the structure, the key lemmas, and how they fit together, and I can indicate where ‚Äúroutine‚Äù checks are required without spelling out every algebraic line.

Below I do that, incorporating the solver‚Äôs final attempt and the tweaks I recommended.

---

## 1. Brief literature context and references

The proof naturally lives in the intersection of three standard topics:

1. **Martingale concentration inequalities** ‚Äì especially **Freedman‚Äôs inequality**, a Bernstein‚Äëtype tail bound that controls a martingale with bounded increments in terms of its predictable quadratic variation. A modern reference (including the scalar version we use) is the paper of Dzhaparidze and van Zanten, which generalizes Freedman and notes how Freedman‚Äôs classical bound is recovered. ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304414900000867?utm_source=openai))  
   For a concise modern exposition, see also Tropp‚Äôs note on the matrix Freedman inequality. ([maths.tcd.ie](https://www.maths.tcd.ie/EMIS/journals/EJP-ECP/article/view/1624.html?utm_source=openai))

2. **Stochastic approximation and the ODE method.**  
   The recursion for a suitable rescaling of the process fits the classical Robbins‚ÄìMonro / stochastic approximation template  
   \[
   x_{n+1} = x_n + a_n\bigl(F(x_n) + \text{noise}\bigr),
   \]
   whose convergence properties are analyzed via an associated ODE \(\dot x = F(x)\). The standard monograph is Borkar‚Äôs *Stochastic Approximation: A Dynamical Systems Viewpoint*. ([link.springer.com](https://link.springer.com/book/10.1007/978-93-86279-38-5?utm_source=openai))

3. **Almost‚Äësupermartingale convergence (Robbins‚ÄìSiegmund).**  
   A convenient way to prove almost‚Äësure convergence in one dimension is via the Robbins‚ÄìSiegmund theorem for almost‚Äësupermartingales: if a nonnegative adapted process \(V_n\) satisfies
   \[
   \mathbf E[V_{n+1}\mid\mathcal F_n] \le (1+\beta_n)V_n + \xi_n - \zeta_n,
   \]
   with \(\sum \beta_n, \sum \xi_n < \infty\), then \(V_n\) converges a.s. and \(\sum \zeta_n < \infty\). A very clear exposition and application to stochastic approximation is given in a blog post by NRH Statistics. ([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))

In the urn literature proper, generalized and nonlinear urns are often treated by exactly this combination of martingale concentration + stochastic approximation; see e.g. works by Laruelle‚ÄìPag√®s, Renlund, Idriss, and (for more general time‚Äëdependent urns) Ruszel‚ÄìThacker. I will not cite these in detail here, because we only need the general methodology, which is already standard in the stochastic approximation references above.

---

## 2. Problem restatement and reduction

We start from the original process:

- Initially: 1 red, 1 blue.
- At each step \(n+1\), draw one ball uniformly from the urn.
- If it is **red**, add 1 red and 1 blue.
- If it is **blue**, and this is the \(k\)-th blue draw so far, add 1 blue and \(2k+1\) red.
- The drawn ball is returned; no ball is ever removed.

Let \(R_n,B_n,G_n\) be the counts of red, blue, total balls after \(n\) steps, and let
\[
K_n := \text{number of blue draws in the first \(n\) steps}.
\]

A simple but key observation is that at each step exactly **one** blue ball is added, regardless of the color drawn. Therefore
\[
B_n = 1 + n\quad\text{(deterministic)}.
\]

The red count is slightly more involved:

- Every red draw contributes 1 red. There are \(n-K_n\) such draws.
- The \(k\)-th blue draw contributes \(2k+1\) reds.

Hence
\[
R_n
= 1 + (n-K_n) + \sum_{k=1}^{K_n}(2k+1)
= 1 + (n-K_n) + (K_n^2 + 2K_n)
= 1 + n + K_n^2 + K_n.
\]

So the total is
\[
G_n = R_n + B_n = K_n^2 + K_n + 2n + 2. \tag{2.1}
\]

Thus the entire urn dynamics reduces to understanding the process \(K_n\).

Let \(\xi_{n+1}\in\{0,1\}\) be the indicator that the \((n+1)\)-th draw is blue. Then
\[
K_{n+1} = K_n + \xi_{n+1}.
\]

Conditional on the past \(\mathcal F_n\),
\[
p_n := \mathbf P(\xi_{n+1}=1 \mid \mathcal F_n)
= \frac{B_n}{G_n}
= \frac{n+1}{K_n^2 + K_n + 2n + 2}
= \frac{n+1}{D_n},
\]
where we write
\[
D_n := K_n^2 + K_n + 2n + 2.
\]

Introduce the Doob decomposition
\[
A_n := \sum_{j=0}^{n-1} p_j, \qquad
M_n := K_n - A_n = \sum_{j=0}^{n-1}(\xi_{j+1}-p_j).
\]
Then \(M_n\) is a martingale with respect to \((\mathcal F_n)\), with bounded increments \(|\Delta M_{j+1}|\le1\), and predictable quadratic variation
\[
\langle M\rangle_n
= \sum_{j=0}^{n-1} p_j(1-p_j)
\le \sum_{j=0}^{n-1} p_j = A_n
\le n. \tag{2.2}
\]

---

## 3. Stage A: coarse polynomial growth bounds for \(K_n\)

We will show:

> For any exponents \(1/2 < \beta_- < 2/3 < \beta_+ < 1\), there exist almost surely finite random constants \(c_-,C_+>0\) and an almost surely finite random time \(N\) such that  
> \[
> c_- n^{\beta_-} \le K_n \le C_+ n^{\beta_+} \quad\text{for all }n\ge N\quad\text{a.s.} \tag{3.1}
> \]

The lower bound uses a dyadic blocking plus Freedman; the upper bound uses a third‚Äëmoment estimate plus Markov and Borel‚ÄìCantelli.

### 3.1 Lower bound on \(K_n\)

Fix any \(\beta_-\in(1/2,2/3)\). For each integer \(m\ge1\), define the dyadic block
\[
I_m := [2^m,2^{m+1})\cap\mathbb N,
\]
the dyadic ‚Äúlevel‚Äù
\[
L_m := 2^{\beta_- m},
\]
and a target drift size
\[
v_m := c_0\,2^{(2-2\beta_-)m},
\]
with a small fixed constant \(c_0\in(0,1)\) chosen later.

Define the martingale increment over block \(m\),
\[
S_m := \sum_{j\in I_m}(\xi_{j+1}-p_j),
\]
and the **good block** event
\[
H_m := \Bigl\{ \sup_{j\in I_m} K_j > 2L_m \Bigr\}
        \,\cup\,
        \Bigl\{ S_m \ge -\frac{v_m}{2}\Bigr\}.
\]

Intuitively: on a block where \(K_j\) stays below \(2L_m\), the conditional drift \(\sum p_j\) on the block is at least of order \(v_m\); Freedman‚Äôs inequality shows that a negative martingale fluctuation of size \(-v_m/2\) is very unlikely. Therefore almost every block is ‚Äúgood.‚Äù

#### Lemma 3.1 (good blocks are overwhelmingly likely).

For all sufficiently large \(m\),
\[
\mathbf P(H_m^c \mid \mathcal F_{2^m}) \le \exp\bigl(-c\,2^{(3-4\beta_-)m}\bigr)
\quad\text{a.s.}
\]
for some constant \(c=c(\beta_-,c_0)>0\).

*Sketch of proof.*

On the event \(\{\sup_{I_m}K \le 2L_m\}\), we have for any \(j\in I_m\) and large \(m\)
\[
D_j = K_j^2 + K_j + 2j + 2 \;\le\; (2L_m)^2 + 2^{m+2}
\;\le\; C\,2^{2\beta_- m},
\]
using \(\beta_->1/2\) so that \(2^{2\beta_- m}\) dominates \(2^m\). Thus for such \(j\),
\[
p_j = \frac{j+1}{D_j}
\;\ge\; \frac{2^m}{C 2^{2\beta_- m}}
= c_1 2^{(1-2\beta_-)m}.
\]
Summing over \(|I_m|=2^m\) gives the drift bound
\[
\sum_{j\in I_m} p_j \;\ge\; c_2 2^{(2-2\beta_-)m} = v_m
\quad\text{on }\{\sup_{I_m}K \le 2L_m\},  \tag{3.2}
\]
for suitable \(c_2\) and \(c_0\le c_2\).

Also, by (2.2),
\[
\langle M\rangle_{2^{m+1}} - \langle M\rangle_{2^m}
= \sum_{j\in I_m} p_j(1-p_j) \le \sum_{j\in I_m} p_j \le 2^m. \tag{3.3}
\]
Thus the quadratic variation of \(S_m\) is deterministically bounded by \(2^m\).

Apply Freedman‚Äôs inequality to the martingale \(S_m\) (viewed as starting at deterministic time \(2^m\)) with increment bound 1, variance bound \(v=2^m\), and deviation threshold \(x=v_m/2\). We obtain
\[
\mathbf P\bigl(S_m \le -v_m/2 \mid \mathcal F_{2^m}\bigr)
\;\le\; \exp\left(
   -\frac{(v_m/2)^2}{2(2^m + v_m/3)}
\right).
\]
Since \(v_m \ll 2^m\) because \(2-2\beta_-<1\), the denominator behaves like a constant multiple of \(2^m\). So the exponent is of order
\[
\frac{v_m^2}{2^m} \asymp 2^{(2(2-2\beta_-)-1)m} = 2^{(3-4\beta_-)m},
\]
which is positive because \(\beta_-<2/3\). Thus
\[
\mathbf P\bigl(S_m \le -v_m/2 \mid \mathcal F_{2^m}\bigr)
\;\le\; \exp\bigl(-c\,2^{(3-4\beta_-)m}\bigr).
\]

Finally, by definition,
\[
H_m^c
= \Bigl\{\sup_{I_m}K \le 2L_m\Bigr\} \cap \Bigl\{S_m < -v_m/2\Bigr\}
\subseteq \Bigl\{S_m < -v_m/2\Bigr\},
\]
so the same bound applies to \(\mathbf P(H_m^c\mid\mathcal F_{2^m})\). ‚àé

Since \(\sum_m \exp(-c2^{(3-4\beta_-)m})<\infty\), the Borel‚ÄìCantelli lemma yields:

> With probability 1, only finitely many blocks are bad, i.e. there is an a.s. finite random \(M_0\) such that \(H_m\) holds for all \(m\ge M_0\).

We now show that **good blocks force growth across dyadic levels.**

#### Claim (one‚Äëstep implication).

There exists \(m_1\) such that for all \(m\ge m_1\),
\[
H_m \quad\Longrightarrow\quad K_{2^{m+1}} \;\ge\; L_{m+1}.
\]

*Proof.* On \(H_m\), either:

1. \(\sup_{I_m}K > 2L_m\). Since \(K_n\) is nondecreasing in \(n\), this implies \(K_{2^{m+1}}\ge 2L_m \ge L_{m+1}\) once \(m\) is large enough that \(2L_m\ge L_{m+1}\).

2. Or \(\sup_{I_m}K\le 2L_m\) and \(S_m\ge -v_m/2\). Then using (3.2),
   \[
   K_{2^{m+1}} - K_{2^m}
   = \sum_{j\in I_m}p_j + S_m
   \;\ge\; v_m + (-v_m/2) = v_m/2.
   \]
   Hence
   \[
   K_{2^{m+1}} \;\ge\; v_m/2.
   \]
   Moreover,
   \[
   \frac{v_m/2}{L_{m+1}}
   = \frac{c_0}{2}\,
     2^{(2-2\beta_-)m - \beta_-(m+1)}
   = \frac{c_0}{2}\,2^{(2-3\beta_-)m - \beta_-}.
   \]
   Since \(2-3\beta_->0\) and \(\beta_->0\), this ratio tends to \(+\infty\) as \(m\to\infty\); thus for \(m\) large we have \(v_m/2\ge L_{m+1}\), and hence \(K_{2^{m+1}}\ge L_{m+1}\).

Combining the two cases, for all sufficiently large \(m\), \(H_m\) implies \(K_{2^{m+1}}\ge L_{m+1}\). ‚àé

Since \(H_m\) eventually holds for all \(m\), iterating this implication gives an almost‚Äësure lower bound at dyadic times: there exists an a.s. finite random \(M\) such that
\[
K_{2^m} \;\ge\; L_m = 2^{\beta_- m} \quad\text{for all } m\ge M. \tag{3.4}
\]

Finally, for any \(n\in[2^m,2^{m+1})\),
\[
K_n \ge K_{2^m} \ge 2^{\beta_- m} \ge 2^{-\beta_-} n^{\beta_-}.
\]
Thus:

> **Proposition 3.2 (lower bound).** For each \(\beta_-\in(1/2,2/3)\) there exists an a.s. finite random constant \(c_-(\omega)>0\) and random time \(N_-(\omega)\) such that
> \[
> K_n \ge c_- n^{\beta_-} \quad\text{for all }n\ge N_-\quad\text{a.s.}
> \]

### 3.2 Upper bound on \(K_n\)

For the upper bound we use a moment estimate rather than a more delicate Freedman argument.

#### Lemma 3.3 (third‚Äëmoment bound).

There is a constant \(C<\infty\) such that
\[
\mathbf E[K_n^3] \le C n^2 \quad\text{for all }n\ge1.
\]

*Proof.*

Recall \(K_{n+1}=K_n+\xi_{n+1}\) and \(\xi_{n+1}\in\{0,1\}\). Then
\[
K_{n+1}^3 - K_n^3
= (3K_n^2+3K_n+1)\xi_{n+1}.
\]
Taking conditional expectation,
\[
\mathbf E[K_{n+1}^3 - K_n^3 \mid\mathcal F_n]
= (3K_n^2+3K_n+1)p_n.
\]

We use two trivial bounds on \(p_n\):

- From \(D_n \ge K_n^2\) (for \(K_n\ge1\)), we get \(p_n \le (n+1)/K_n^2\).
- From \(D_n \ge 2n+2\), we get \(p_n \le 1/2\).

Split into two cases.

1. If \(K_n^2\ge 2(n+1)\), then
   \[
   p_n \le \frac{n+1}{K_n^2} \le \frac12,
   \]
   and
   \[
   (3K_n^2+3K_n+1)p_n
   \le (3+3/K_n+1/K_n^2)(n+1) \le 7(n+1)
   \]
   (since \(K_n\ge 1\)).

2. If \(K_n^2 < 2(n+1)\), then
   \[
   (3K_n^2+3K_n+1)p_n
   \le \frac{3K_n^2+3K_n+1}{2}
   \le 3(n+1) + O(\sqrt n) \le 4(n+1)
   \]
   for large enough \(n\), and we can adjust constants to make this uniform for all \(n\).

In either case there exists a constant \(C_1\) such that
\[
\mathbf E[K_{n+1}^3 \mid\mathcal F_n] \le K_n^3 + C_1(n+1).
\]
Taking expectations and summing telescopically,
\[
\mathbf E[K_n^3]
\le \mathbf E[K_0^3] + C_1\sum_{j=1}^n j
\le C n^2
\]
for some \(C\). ‚àé

Now fix any \(\beta_+>2/3\) and look at dyadic times \(n=2^m\). By Markov‚Äôs inequality,
\[
\mathbf P(K_{2^m} \ge 2^{\beta_+ m})
\le \frac{\mathbf E[K_{2^m}^3]}{2^{3\beta_+ m}}
\le C\,\frac{2^{2m}}{2^{3\beta_+ m}}
= C\, 2^{(2-3\beta_+)m}.
\]
Since \(\beta_+>2/3\), we have \(2-3\beta_+<0\), so
\[
\sum_{m=1}^\infty \mathbf P(K_{2^m} \ge 2^{\beta_+ m})
< \infty.
\]
By Borel‚ÄìCantelli, almost surely
\[
K_{2^m} \le 2^{\beta_+ m}
\quad\text{for all large }m. \tag{3.5}
\]

For any \(n\in[2^m,2^{m+1})\), monotonicity implies
\[
K_n \le K_{2^{m+1}} \le 2^{\beta_+(m+1)} \le C n^{\beta_+},
\]
for a deterministic \(C\) (using \(2^{m+1}\le 2n\) on this interval). Thus:

> **Proposition 3.5 (upper bound).** For each \(\beta_+ \in (2/3,1)\) there exists an a.s. finite random constant \(C_+(\omega)>0\) and random time \(N_+(\omega)\) such that
> \[
> K_n \le C_+ n^{\beta_+} \quad\text{for all }n\ge N_+ \quad\text{a.s.}
> \]

Combining Propositions 3.2 and 3.5, we arrive at the two‚Äësided coarse polynomial control (3.1).

---

## 4. Stage B: stochastic approximation for \(Y_n = K_n^3/n^2\)

The heuristics suggest \(K_n \asymp n^{2/3}\), so \(K_n^3/n^2\) should converge to a finite nonzero constant. We analyze
\[
Y_n := \frac{K_n^3}{n^2},\qquad n\ge1.
\]

From \(K_{n+1}=K_n+\xi_{n+1}\) we compute
\[
K_{n+1}^3 - K_n^3 = (3K_n^2+3K_n+1)\,\xi_{n+1},
\]
so
\[
Y_{n+1}-Y_n
= -Y_n\frac{2n+1}{(n+1)^2}
  + \frac{3K_n^2+3K_n+1}{(n+1)^2}\,\xi_{n+1}. \tag{4.1}
\]

Taking conditional expectation and using \(p_n=(n+1)/D_n\) and \(3K_n^2+3K_n+1 = 3D_n - (6n+5)\),
\[
\begin{aligned}
\mathbf E[Y_{n+1}-Y_n\mid\mathcal F_n]
&= -Y_n\frac{2n+1}{(n+1)^2}
   + \frac{3K_n^2+3K_n+1}{(n+1)^2} p_n \\
&= -Y_n\frac{2n+1}{(n+1)^2}
   + \frac{3}{n+1}
   - \frac{6n+5}{(n+1)D_n}. \tag{4.2}
\end{aligned}
\]

It is convenient to isolate the main drift of order \(1/(n+1)\). Note that
\[
-Y_n\frac{2n+1}{(n+1)^2} + \frac{3}{n+1}
= \frac{3-2Y_n}{n+1} + r_n^{(1)},
\]
where
\[
r_n^{(1)} = -Y_n\frac{2n+1}{(n+1)^2} + \frac{3}{n+1} - \frac{3-2Y_n}{n+1}
\]
satisfies \(|r_n^{(1)}|\le C\,(Y_n+1)/(n+1)^2\).

Define the error term
\[
\varepsilon_n := r_n^{(1)} - \frac{6n+5}{(n+1)D_n},
\]
and the martingale increment
\[
N_{n+1} := \frac{3K_n^2+3K_n+1}{(n+1)^2}(\xi_{n+1}-p_n).
\]

Then the exact recursion (4.1) takes the standard ‚ÄúSA + noise‚Äù form
\[
Y_{n+1}-Y_n = \frac{3-2Y_n}{n+1} + \varepsilon_n + N_{n+1}, \tag{4.3}
\]
with \(\mathbf E[N_{n+1}\mid\mathcal F_n]=0\).

### 4.1 Summability of the perturbations

We now use the coarse bounds (3.1) to control the error \(\varepsilon_n\) and the noise \(N_{n+1}\).

From (3.1), fix some \(1/2<\beta_-<2/3<\beta_+<1\) and work on the full‚Äëprobability event where there exist random \(N_0,c_-,C_+\) such that for all \(n\ge N_0\),
\[
c_-n^{\beta_-} \le K_n \le C_+ n^{\beta_+}.
\]
Then for all large \(n\),

- \(D_n = K_n^2+K_n+2n+2 \ge K_n^2 \ge c_-^2 n^{2\beta_-}\),
- \(Y_n = K_n^3/n^2 \le C_+^3 n^{3\beta_+ - 2}\).

Using these bounds, for large \(n\),
\[
|\varepsilon_n|
\le C\,\frac{Y_n+1}{(n+1)^2} + C\,\frac{1}{D_n}
\le C\,(n^{3\beta_+ - 4} + n^{-2} + n^{-2\beta_-}).
\]

Because \(\beta_+<1\) and \(\beta_->1/2\), the exponents satisfy
\[
3\beta_+ - 4 < -1,\qquad -2 < -1,\qquad -2\beta_- < -1.
\]
So each term is summable and
\[
\sum_{n=1}^\infty |\varepsilon_n| < \infty \quad\text{a.s.} \tag{4.4}
\]

For the noise,
\[
\mathbf E[N_{n+1}^2 \mid\mathcal F_n]
= \frac{(3K_n^2+3K_n+1)^2}{(n+1)^4} p_n(1-p_n)
\;\le\; C\,\frac{K_n^4}{n^4}\,p_n.
\]
Using \(p_n\le (n+1)/K_n^2\),
\[
\mathbf E[N_{n+1}^2 \mid\mathcal F_n]
\le C\,\frac{K_n^2}{n^3}
\le C\,n^{2\beta_+ - 3}.
\]
Since \(\beta_+<1\) implies \(2\beta_+ - 3 < -1\), we obtain
\[
\sum_{n=1}^\infty \mathbf E[N_{n+1}^2 \mid\mathcal F_n] < \infty \quad\text{a.s.} \tag{4.5}
\]
Hence the martingale \(\sum_{n}N_{n+1}\) converges almost surely (and in \(L^2\)).

We also need square summability of \(\varepsilon_n\) weighted by \((n+1)\) (for the Robbins‚ÄìSiegmund step). From the bound above,
\[
\varepsilon_n^2
\le C\bigl(
  n^{2(3\beta_+ - 4)} + n^{-4} + n^{-4\beta_-}
\bigr),
\]
and so
\[
(n+1)\varepsilon_n^2
\le C\bigl(
  n^{6\beta_+ - 7} + n^{-3} + n^{1-4\beta_-}
\bigr).
\]
Again \(6\beta_+ - 7 < -1\) when \(\beta_+<1\), and \(1-4\beta_-<-1\) when \(\beta_->1/2\), so
\[
\sum_{n=1}^\infty (n+1)\varepsilon_n^2 < \infty \quad\text{a.s.} \tag{4.6}
\]

Combining (4.5) and (4.6), we will have the summability of the ‚Äúerror budget‚Äù \(\rho_n\) below.

### 4.2 Robbins‚ÄìSiegmund argument for convergence of \(Y_n\)

Define deviations from the candidate limit
\[
\delta_n := Y_n - \tfrac32,\qquad V_n := \delta_n^2 \ge 0.
\]

From (4.3),
\[
\delta_{n+1}
= \delta_n + \frac{3-2Y_n}{n+1} + \varepsilon_n + N_{n+1}
= \delta_n\Bigl(1 - \frac{2}{n+1}\Bigr) + \varepsilon_n + N_{n+1}.
\]

Thus
\[
\begin{aligned}
V_{n+1}
&= \delta_{n+1}^2 \\
&= \Bigl(1 - \frac{2}{n+1}\Bigr)^2 V_n
  + 2\Bigl(1 - \frac{2}{n+1}\Bigr)\delta_n(\varepsilon_n + N_{n+1})
  + (\varepsilon_n + N_{n+1})^2.
\end{aligned}
\]

Using \(|1-2/(n+1)|\le1\) and the inequality \(2|ab|\le \frac{1}{n+1}a^2 + (n+1)b^2\), we get
\[
2\Bigl(1 - \frac{2}{n+1}\Bigr)|\delta_n\varepsilon_n|
\le \frac{1}{n+1}V_n + (n+1)\varepsilon_n^2.
\]

Taking conditional expectation and using \(\mathbf E[N_{n+1}\mid\mathcal F_n]=0\), and \(\mathbf E[\delta_n N_{n+1}\mid\mathcal F_n]=\delta_n\mathbf E[N_{n+1}\mid\mathcal F_n]=0\), we obtain
\[
\begin{aligned}
\mathbf E[V_{n+1}\mid\mathcal F_n]
&\le \Bigl(1 - \frac{2}{n+1}\Bigr)^2 V_n
 + \frac{1}{n+1}V_n
 + (n+1)\varepsilon_n^2 + \varepsilon_n^2
 + \mathbf E[N_{n+1}^2\mid\mathcal F_n].
\end{aligned}
\]
A simple algebra check shows
\[
\Bigl(1 - \frac{2}{n+1}\Bigr)^2 + \frac{1}{n+1}
= 1 - \frac{3}{(n+1)^2}
\le 1 - \frac{1}{n+1}
\]
for all \(n\ge1\). Thus
\[
\mathbf E[V_{n+1}\mid\mathcal F_n]
\le \Bigl(1 - \frac{1}{n+1}\Bigr)V_n + \rho_n, \tag{4.7}
\]
where
\[
\rho_n := (n+1)\varepsilon_n^2 + \varepsilon_n^2 + \mathbf E[N_{n+1}^2\mid\mathcal F_n].
\]

From (4.5) and (4.6), we have
\[
\sum_{n=1}^\infty \rho_n < \infty \quad\text{almost surely}. \tag{4.8}
\]

Now (4.7)‚Äì(4.8) are exactly in the form of the Robbins‚ÄìSiegmund almost‚Äësupermartingale theorem:

- \(V_n\ge0\) is adapted.
- \(\beta_n = 1/(n+1)\) and \(\xi_n=\rho_n\) are summable almost surely:
  \(\sum \beta_n = \infty\) but this is not needed for convergence; what we use is \(\sum \xi_n <\infty\). The coefficient in front of \(V_n\) is \(1-\frac{1}{n+1}\), which can be written as \((1+\tilde\beta_n)\) with \(\tilde\beta_n=-1/(n+1)\le0\).
- \(\zeta_n\) can be chosen nonnegative so that the Robbins‚ÄìSiegmund conclusion still gives convergence of \(V_n\) and \(\sum V_n/(n+1)<\infty\); this is standard in applications (compare e.g. ([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))).

Concretely, by Robbins‚ÄìSiegmund, we deduce:

1. \(V_n\) converges almost surely to some finite limit \(V_\infty\ge 0\).
2. The series \(\sum_{n} \frac{1}{n+1}V_n < \infty\) almost surely.

If \(V_\infty>0\) with positive probability, then on that event \(V_n\) is eventually bounded below by some \(\eta>0\). But then \(\sum \frac{1}{n+1}V_n\) would diverge like \(\eta\sum 1/(n+1) = \infty\), a contradiction. Therefore \(V_\infty=0\) almost surely.

Thus
\[
\delta_n^2 = V_n \to 0 \quad\text{a.s.},
\]
i.e.
\[
Y_n \to \frac32 \quad\text{almost surely}. \tag{4.9}
\]

---

## 5. Asymptotics of \(K_n\) and conclusion for \(G_n\)

Recall \(Y_n = K_n^3/n^2\). From (4.9),
\[
\frac{K_n^3}{n^2} \to \frac32 \quad\text{a.s.},
\]
so
\[
K_n \sim \Bigl(\frac{3}{2}\Bigr)^{1/3} n^{2/3} \quad\text{a.s.} \tag{5.1}
\]

From the exact identity (2.1),
\[
G_n = K_n^2 + K_n + 2n + 2.
\]

Using (5.1), we have \(K_n\asymp n^{2/3}\), hence \(K_n^2\asymp n^{4/3}\), while \(K_n=O(n^{2/3})\) and \(2n = o(n^{4/3})\). Thus
\[
\frac{G_n}{n^{4/3}}
= \frac{K_n^2}{n^{4/3}} \cdot \frac{1 + K_n^{-1} + 2n/K_n^2 + 2/K_n^2}{1}
\to \Bigl(\frac{3}{2}\Bigr)^{2/3} \cdot 1 \quad\text{a.s.},
\]
because
\[
\frac{K_n^2}{n^{4/3}} = \left(\frac{K_n^3}{n^2}\right)^{2/3}\to \left(\frac{3}{2}\right)^{2/3},
\]
and the correction factors \(K_n^{-1}\), \(2n/K_n^2\), \(2/K_n^2\) tend to zero almost surely.

Thus we have proved:

> There exist finite positive constants
> \[
> \alpha = \frac{4}{3},\qquad
> c = \Bigl(\frac{3}{2}\Bigr)^{2/3},
> \]
> such that
> \[
> \frac{G_n}{n^\alpha} \longrightarrow c
> \quad\text{almost surely as }n\to\infty.
> \]

which is exactly the desired statement.

---

### Final remarks

- The heart of the argument is to choose the *right* scaling \(Y_n = K_n^3/n^2\), for which the mean drift becomes asymptotically linear in \(Y_n\) (with drift \((3-2Y_n)/(n+1)\)), making the Robbins‚ÄìSiegmund method particularly clean.
- The Stage‚ÄëA polynomial bounds on \(K_n\) are needed only to ensure that the perturbation terms \(\varepsilon_n\) and the conditional variances of the martingale increments are summable; they are intentionally coarse (any exponents \(\beta_-<2/3<\beta_+\) suffice).
- This structure mirrors the general ODE‚Äëbased stochastic approximation theory in Borkar‚Äôs monograph and related works ([link.springer.com](https://link.springer.com/book/10.1007/978-93-86279-38-5?utm_source=openai)), but in this problem everything can be done in dimension one with elementary tools (Freedman, Markov, Borel‚ÄìCantelli, Robbins‚ÄìSiegmund).

---

Below is a complete, self‚Äëcontained solution to the urn problem, in the style of a research‚Äêlevel writeup. I begin with a brief literature review to situate the method.

---

## 0. Concise literature review

The proof uses two standard tools from modern probability theory:

1. **Martingale concentration inequalities**, in particular Freedman‚Äôs inequality, a Bernstein‚Äëtype tail bound for martingales with bounded increments and controlled quadratic variation. Freedman‚Äôs original result (1975) has many later expositions; for example, Tropp‚Äôs paper on the matrix version summarizes the scalar form and its role as a martingale analogue of Bernstein‚Äôs inequality. ([maths.tcd.ie](https://www.maths.tcd.ie/EMIS/journals/EJP-ECP/article/view/1624.html?utm_source=openai))  

2. **Almost‚Äësupermartingale convergence (Robbins‚ÄìSiegmund theorem)**, which underlies much of stochastic approximation theory. A clear modern statement and discussion is given in the NRH Statistics blog; it explains how the theorem proves almost sure convergence for stochastic algorithms with step size \(1/n\). ([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))  

Our overall strategy is a **stochastic approximation / ODE method** in the sense of Bena√Øm ([libra.unine.ch](https://libra.unine.ch/entities/publication/b43c676a-1c01-4ac4-b90d-1cf33a660bf1?utm_source=openai)) and Borkar. ([link.springer.com](https://link.springer.com/book/10.1007/978-93-86279-38-5?utm_source=openai))  This approach‚Äîreducing a complicated stochastic process to a lower‚Äëdimensional recursion with small step size and identifying a limiting ODE‚Äîis now standard in the analysis of generalized P√≥lya urns. See, for example,

- Renlund, *Generalized P√≥lya urns via stochastic approximation*, which explicitly treats urn models via 1‚Äëdimensional stochastic approximation. ([arxiv.org](https://arxiv.org/abs/1002.3716?utm_source=openai))  
- More recent work on nonlinear urns using stochastic approximation, such as Ruszel & Thacker, *Positive Reinforced Generalized Time‚ÄëDependent P√≥lya Urns via Stochastic Approximation*, which shows how to control reinforcement processes by coupling to an ODE and verifying summability of noise terms. ([link.springer.com](https://link.springer.com/article/10.1007/s10959-024-01366-w?utm_source=openai))  

Our proof mirrors this standard template:

1. Identify a **scalar process** that captures the system.
2. Prove **coarse polynomial upper and lower bounds** via martingale concentration (Freedman).
3. Rescale into a **stochastic approximation recursion** with step size \(\sim 1/n\).
4. Use a **Lyapunov function** and Robbins‚ÄìSiegmund to show convergence to the stable equilibrium of the limiting ODE.

---

## 1. Problem restatement and reduction to blue draws

### 1.1. The model

We have an urn evolving in discrete time \(n=0,1,2,\dots\):

- At time \(0\): 1 red ball and 1 blue ball.
- At each step, draw one ball uniformly from the urn, then **return it** (so no removals), and add balls as follows:
  - If the drawn ball is red, add 1 red and 1 blue.
  - If the drawn ball is blue and this is the \(k\)-th blue **draw so far**, add 1 blue and \(2k+1\) red balls.

Let:

- \(R_n,B_n,G_n\) be the number of red, blue and total balls **after** \(n\) draws.
- \(K_n\) be the number of blue draws among the first \(n\) draws.
- \(\xi_{n+1} := \mathbf 1\{\text{draw }n+1\text{ is blue}\}\). Then
  \[
  K_{n+1} = K_n + \xi_{n+1}.
  \]

We are to show that there exist constants \(0<c,\alpha<\infty\) such that almost surely
\[
\frac{G_n}{n^\alpha} \longrightarrow c.
\]

We will in fact identify
\[
\alpha = \frac{4}{3}, \qquad c = \Bigl(\frac{3}{2}\Bigr)^{2/3}.
\]

### 1.2. Deterministic expressions for \(R_n, B_n, G_n\)

Observe:

- Every step adds **exactly one** blue ball, regardless of the color drawn. Hence
  \[
  B_n = 1 + n\quad\text{for all }n\ge 0.
  \]

- Red balls:

  - At each red draw, we add 1 red ball. There are \(n-K_n\) red draws among the first \(n\) draws.
  - At the \(k\)-th blue draw, we add \(2k+1\) red balls. Summing over blue draws:

    \[
    \sum_{k=1}^{K_n}(2k+1) = 2\cdot\frac{K_n(K_n+1)}{2} + K_n
    = K_n^2 + 2K_n.
    \]

  - Plus the initial 1 red ball.

  So

  \[
  R_n = 1 + (n-K_n) + (K_n^2 + 2K_n) = 1 + n + K_n^2 + K_n.
  \]

Thus total balls:

\[
G_n = R_n + B_n = \bigl(1 + n + K_n^2 + K_n\bigr) + (1 + n)
= K_n^2 + K_n + 2n + 2. \tag{1.1}
\]

**Key reduction.** The entire problem reduces to understanding the growth of the single scalar process \(K_n\). Once we know the asymptotics of \(K_n\), (1.1) determines the behavior of \(G_n\).

### 1.3. Conditional blue probability and martingale decomposition

At time \(n\), the urn contains \(B_n = 1+n\) blue balls out of \(G_n\) total. So the conditional probability that draw \(n+1\) is blue is

\[
p_n
:= \mathbb P(\xi_{n+1}=1\mid\mathcal F_n)
= \frac{B_n}{G_n}
= \frac{n+1}{K_n^2 + K_n + 2n + 2}
= \frac{n+1}{D_n},
\]
where we set
\[
D_n := K_n^2 + K_n + 2n +2.
\]

Define the ‚Äúdrift‚Äù and ‚Äúmartingale noise‚Äù parts of \(K_n\):

\[
A_n := \sum_{j=0}^{n-1} p_j, 
\qquad
M_n := K_n - A_n = \sum_{j=0}^{n-1}(\xi_{j+1} - p_j).
\]

Then \((M_n)\) is a martingale with respect to \((\mathcal F_n)\), with bounded increments \(|M_{n+1}-M_n| = |\xi_{n+1} - p_n|\le 1\), and predictable quadratic variation

\[
\langle M\rangle_n
= \sum_{j=0}^{n-1} \mathbb E[(\xi_{j+1}-p_j)^2\mid\mathcal F_j]
= \sum_{j=0}^{n-1} p_j(1-p_j)
\le \sum_{j=0}^{n-1} p_j = A_n
\le n.
\tag{1.2}
\]

We will use a martingale concentration inequality (Freedman) applied to differences like \(K_{t}-K_s\), via the martingale increments of \(M_n\).

---

## 2. Tools: Freedman and Robbins‚ÄìSiegmund

### 2.1. Freedman‚Äôs inequality (scalar version)

A standard scalar form (see e.g. Dzhaparidze‚Äìvan Zanten or Tropp‚Äôs exposition on matrix martingales) is: ([sciencedirect.com](https://www.sciencedirect.com/science/article/pii/S0304414900000867?utm_source=openai))  

> **Freedman‚Äôs inequality.** Let \((M_k)\) be a martingale with respect to \((\mathcal F_k)\) and bounded increments \(|M_k - M_{k-1}| \le 1\). Let
> \[
> V_k := \sum_{i=1}^k \mathbb E[(M_i - M_{i-1})^2 \mid \mathcal F_{i-1}]
> \]
> be its predictable quadratic variation. Then for any integers \(0\le s < t\) and any \(x,v>0\),
> \[
> \mathbb P\Bigl( \sup_{s<k\le t} (M_k - M_s) \ge x \text{ and } V_t - V_s \le v\Bigr)
> \le \exp\Bigl( -\frac{x^2}{2\bigl(v + x/3\bigr)}\Bigr).
> \]
> The same bound holds with \(M_k-M_s\) replaced by \(-(M_k-M_s)\).

We will apply this with \(M_k\) replaced by the increment martingale on a single dyadic block, using a deterministic upper bound \(v\) for the block‚Äôs quadratic variation.

### 2.2. Robbins‚ÄìSiegmund almost‚Äësupermartingale theorem

A classical theorem of Robbins and Siegmund (1971) can be stated in many forms. One convenient version (see e.g. the NRH Statistics exposition) is: ([nrhstat.org](https://nrhstat.org/post/robbins_siegmund/?utm_source=openai))  

> **Robbins‚ÄìSiegmund theorem (simplified form).**  
> Let \((V_n)_{n\ge 1}\) be a nonnegative adapted process, and suppose there exist nonnegative adapted sequences \(a_n, b_n, c_n\) such that
> \[
> \mathbb E[V_{n+1}\mid\mathcal F_n] \le (1 - a_n)\,V_n + b_n,\quad n\ge 1,
> \]
> with
> \[
> \sum_{n=1}^\infty a_n = \infty,\qquad \sum_{n=1}^\infty b_n < \infty\ \text{a.s.}
> \]
> Then \(V_n\) converges almost surely, and furthermore \(\lim_{n\to\infty} V_n = 0\).

(One can derive this particular form from the more general ‚Äúalmost-supermartingale‚Äù statement of Robbins‚ÄìSiegmund; we will only use this simpler corollary.)

In our application, \(V_n\) will be the squared deviation of a rescaled quantity from its equilibrium; \(a_n\) will be of order \(1/n\), and \(b_n\) will come from square‚Äësummable perturbations.

---

## 3. Stage A: Coarse polynomial bounds for \(K_n\)

We first show that \(K_n\) grows like a power of \(n\) with exponent between \(1/2\) and \(1\) almost surely, and in fact can be sandwiched between any exponents just below and above \(2/3\).

### 3.1. Lower bound: \(K_n \gtrsim n^{\beta_-}\) for any \(\beta_- \in (1/2,2/3)\)

Fix any
\[
\beta_- \in \Bigl(\frac{1}{2}, \frac{2}{3}\Bigr).
\]

For each integer \(m\ge 1\), define the dyadic block

\[
I_m := [2^m, 2^{m+1}) \cap \mathbb N,
\]

and the level and ‚Äúdrift scale‚Äù

\[
L_m := 2^{\beta_- m},\qquad
v_m := c_0\,2^{(2-2\beta_-)m},
\]
with a small constant \(c_0>0\) to be chosen (it will not depend on \(m\)).

Define the block martingale increment and the ‚Äúgood block‚Äù event:
\[
S_m := \sum_{j\in I_m} (\xi_{j+1} - p_j),
\]
\[
H_m := \Bigl\{ \sup_{j\in I_m} K_j > 2L_m\Bigr\}
      \;\cup\;
      \Bigl\{ S_m \ge -\frac{v_m}{2} \Bigr\}.
\]

Intuitively: either the block itself contains some time with many blue draws (\(K_j > 2L_m\)), or, if not, then the drift \(\sum p_j\) on the block is sufficiently large that the martingale fluctuations \(S_m\) do not drag \(K\) down too far.

#### Lemma 3.1 (Good blocks are overwhelmingly likely).

There exist constants \(c>0\) and \(m_0\) such that for all \(m\ge m_0\),
\[
\mathbb P(H_m^c) \le \exp\bigl(-c\,2^{(3-4\beta_-)m}\bigr).
\]

*Proof.* On the event
\[
E_m := \{\sup_{j\in I_m} K_j \le 2L_m\},
\]
we can bound, for any \(j\in I_m\) and all large \(m\),

\[
D_j = K_j^2 + K_j + 2j + 2
\le (2L_m)^2 + 2L_m + 2^{m+1} + 2
\le C\,2^{2\beta_- m},
\]
since \(2\beta_- >1\) and \(L_m = 2^{\beta_- m}\). Hence on \(E_m\),

\[
p_j = \frac{j+1}{D_j}
\ge \frac{2^m}{C\,2^{2\beta_- m}}
= c_1\,2^{(1-2\beta_-)m}
\quad\text{for all } j\in I_m,
\]
for some constant \(c_1>0\) and all large \(m\). Summing over the block of length \(|I_m|=2^m\), we obtain

\[
\sum_{j\in I_m} p_j \ge c_1 2^m \cdot 2^{(1-2\beta_-)m}
= c_2\,2^{(2-2\beta_-)m}
= v_m
\]
for a suitably chosen \(c_0>0\).

The predictable quadratic variation on the block satisfies, by (1.2),
\[
\sum_{j\in I_m} \mathbb E\bigl[(\xi_{j+1}-p_j)^2\mid\mathcal F_j\bigr]
= \sum_{j\in I_m} p_j(1-p_j)
\le \sum_{j\in I_m} p_j
\le 2^m.
\]
This bound is deterministic, so Freedman‚Äôs inequality applies with step start at \(2^m\), \(v=2^m\) and \(x=v_m/2\):

\[
\mathbb P\bigl(S_m \le -v_m/2\bigr)
\le \exp\Bigl( -\frac{(v_m/2)^2}{2\bigl(2^m + v_m/3\bigr)}\Bigr).
\]

Since \(\beta_->1/2\), we have \(v_m = \Theta(2^{(2-2\beta_-)m})\) with exponent \(2-2\beta_- <1\), so \(v_m\ll 2^m\) and the denominator \(2(2^m + v_m/3)\) is of order \(2^{m+1}\). Thus

\[
\frac{(v_m/2)^2}{2(2^m + v_m/3)}
\asymp 2^{(4 - 4\beta_-)m} / 2^{m}
= 2^{(3-4\beta_-)m}.
\]

Hence there exists \(c>0\) such that for all large \(m\),
\[
\mathbb P(S_m \le -v_m/2) \le \exp\bigl(-c\,2^{(3-4\beta_-)m}\bigr).
\]

Now note that
\[
H_m^c
= \{\sup_{I_m}K\le 2L_m\}\cap\{S_m< -v_m/2\}
\subseteq \{S_m\le -v_m/2\},
\]
so
\[
\mathbb P(H_m^c) \le \mathbb P(S_m\le -v_m/2)
\le \exp\bigl(-c\,2^{(3-4\beta_-)m}\bigr),
\]
as claimed. ‚àé

The series \(\sum_m \exp(-c\,2^{(3-4\beta_-)m})\) converges because \(3-4\beta_->0\). Hence by Borel‚ÄìCantelli, with probability 1 there exists a (random) \(M\) such that
\[
H_m\ \text{occurs for all } m\ge M.
\]

#### One-step implication \(H_m \Rightarrow K_{2^{m+1}}\ge L_{m+1}\) (for large \(m\)).

Fix \(\omega\) in the event where all but finitely many \(H_m\) occur; from now on we argue pathwise on that sample point and drop \(\omega\) from notation. Consider a ‚Äúgood‚Äù block \(H_m\) for large \(m\):

- **Case 1: \(\sup_{j\in I_m} K_j > 2L_m\).**  
  Then there exists some \(j\in I_m\) with \(K_j>2L_m\). Since \(K_n\) is nondecreasing in \(n\),
  \[
  K_{2^{m+1}} \ge K_j > 2L_m.
  \]
  But
  \[
  \frac{2L_m}{L_{m+1}}
  = \frac{2^{1}2^{\beta_- m}}{2^{\beta_-(m+1)}}
  = 2^{1-\beta_-} >1,
  \]
  hence \(2L_m\ge L_{m+1}\), so
  \[
  K_{2^{m+1}} \ge L_{m+1}.
  \]

- **Case 2: \(\sup_{I_m}K \le 2L_m\) and \(S_m\ge -v_m/2\).**  
  On this event we have already seen \(\sum_{j\in I_m}p_j \ge v_m\). Thus
  \[
  K_{2^{m+1}} - K_{2^m}
  = \sum_{j\in I_m} p_j + S_m
  \ge v_m - \frac{v_m}{2}
  = \frac{v_m}{2}.
  \]
  Therefore
  \[
  K_{2^{m+1}} \ge \frac{v_m}{2}.
  \]
  Moreover,
  \[
  \frac{v_m/2}{L_{m+1}}
  =\frac{c_0}{2}\cdot
  \frac{2^{(2-2\beta_-)m}}{2^{\beta_-(m+1)}}
  = \frac{c_0}{2}2^{(2-3\beta_-)m-\beta_-}.
  \]
  Since \(\beta_-<2/3\), we have \(2-3\beta_->0\), so the right-hand side tends to \(\infty\) with \(m\). For all sufficiently large \(m\), therefore,
  \[
  \frac{v_m}{2} \ge L_{m+1}
  \quad\implies\quad
  K_{2^{m+1}}\ge L_{m+1}.
  \]

Combining the two cases: for all sufficiently large \(m\), on the event \(H_m\) we have
\[
H_m\quad\Rightarrow\quad K_{2^{m+1}} \ge L_{m+1}. \tag{3.1}
\]

Since \(H_m\) fails only finitely often almost surely, there is almost surely some random \(M\) such that for all \(m\ge M\),
\[
K_{2^{m+1}} \ge L_{m+1}.
\]
Equivalently, for all \(m\ge M+1\),
\[
K_{2^m} \ge L_m = 2^{\beta_- m}.
\]

Finally, for any \(n\) with \(2^m\le n < 2^{m+1}\) and \(m\) large,
\[
K_n \ge K_{2^m} \ge 2^{\beta_- m}
\ge 2^{-\beta_-}\,n^{\beta_-}.
\]

We summarize:

> **Proposition 3.2 (Lower polynomial bound).**  
> Fix any \(\beta_- \in (1/2,2/3)\). Then there exist random constants \(c_-(\omega)>0\) and \(N_-(\omega)\) such that almost surely
> \[
> K_n \ge c_- n^{\beta_-} \quad\text{for all }n\ge N_-.
> \]

In particular, for large \(n\),
\[
D_n = K_n^2 + K_n + 2n + 2 \ge K_n^2 \ge c_-^2 n^{2\beta_-}.
\tag{3.2}
\]

Because \(\beta_->1/2\), this exponent \(2\beta_->1\); this will be crucial later.

---

### 3.2. Upper bound: \(K_n \lesssim n^{\beta_+}\) for any \(\beta_+ \in (2/3,1)\)

For the upper bound we use a simple moment inequality plus Markov and Borel‚ÄìCantelli.

#### Lemma 3.3 (Third moment growth).

There exists a constant \(C<\infty\) such that for all \(n\),
\[
\mathbb E[K_{n+1}^3\mid\mathcal F_n] \le K_n^3 + C(n+1),
\]
and consequently
\[
\mathbb E[K_n^3] \le C n^2\quad\text{for all }n.
\]

*Proof.* We have the exact identity
\[
K_{n+1}^3 - K_n^3
= (3K_n^2 + 3K_n + 1)\,\xi_{n+1}.
\]
Taking conditional expectations,
\[
\mathbb E[K_{n+1}^3 - K_n^3\mid\mathcal F_n]
= (3K_n^2 + 3K_n + 1)\,p_n.
\]

We will show this is \(O(n+1)\). Using \(p_n = (n+1)/D_n\),
\[
(3K_n^2 + 3K_n + 1)p_n
\le (3K_n^2 + 3K_n + 1)\min\Bigl\{\frac{n+1}{K_n^2},\,1\Bigr\},
\]
since \(D_n \ge K_n^2\) and \(p_n\le 1\).

Case 1: \(K_n^2 \ge 2(n+1)\). Then
\[
p_n \le \frac{n+1}{K_n^2} \le \frac{1}{2},
\]
so
\[
(3K_n^2+3K_n+1)p_n
\le (3K_n^2+3K_n+1)\,\frac{n+1}{K_n^2}
\le \bigl(3 + 3/K_n + 1/K_n^2\bigr)(n+1)
\le 7(n+1),
\]
since \(K_n\ge 1\).

Case 2: \(K_n^2 < 2(n+1)\). Then \(K_n \le \sqrt{2(n+1)}\), and
\[
(3K_n^2+3K_n+1)p_n
\le 3K_n^2 + 3K_n + 1
\le 3\cdot 2(n+1) + 3\sqrt{2(n+1)} + 1
\le C(n+1)
\]
for some universal \(C\).

Thus in both cases
\[
\mathbb E[K_{n+1}^3 - K_n^3\mid\mathcal F_n]
\le C(n+1),
\]
so taking expectations and summing from \(0\) to \(n-1\),
\[
\mathbb E[K_n^3] \le K_0^3 + C\sum_{j=0}^{n-1}(j+1)
\le C' n^2.
\]
‚àé

Now fix \(\beta_+>2/3\). For dyadic times \(n=2^m\), Markov‚Äôs inequality and Lemma 3.3 give

\[
\mathbb P\bigl(K_{2^m} \ge 2^{\beta_+ m}\bigr)
\le \frac{\mathbb E[K_{2^m}^3]}{2^{3\beta_+ m}}
\le \frac{C 2^{2m}}{2^{3\beta_+ m}}
= C\,2^{(2-3\beta_+)m}.
\]

As \(\beta_+>2/3\), the exponent \(2-3\beta_+<0\), so \(\sum_m \mathbb P(K_{2^m}\ge 2^{\beta_+ m})<\infty\). By Borel‚ÄìCantelli, almost surely there is a random \(M\) such that

\[
K_{2^m} \le 2^{\beta_+ m}\quad\text{for all }m\ge M.
\]

For general \(n\), if \(2^m \le n < 2^{m+1}\) with \(m\ge M\), then by monotonicity of \(K\),

\[
K_n \le K_{2^{m+1}} \le 2^{\beta_+(m+1)} 
= 2^{\beta_+} 2^{\beta_+ m}
\le 2^{\beta_+} n^{\beta_+}.
\]

We conclude:

> **Proposition 3.4 (Upper polynomial bound).**  
> For any \(\beta_+ \in (2/3,1)\), there exist random constants \(C_+(\omega)\) and \(N_+(\omega)\) such that almost surely
> \[
> K_n \le C_+ n^{\beta_+}\quad\text{for all }n\ge N_+.
> \]

Combining Propositions 3.2 and 3.4 we can fix **once and for all** exponents
\[
\frac12<\beta_-<\frac{2}{3}<\beta_+<1
\]
and (on a full‚Äëprobability event) constants \(c_-,C_+>0\) and \(N\) such that for all \(n\ge N\),

\[
c_- n^{\beta_-} \le K_n \le C_+ n^{\beta_+}. \tag{3.3}
\]

In particular, using (3.2),
\[
D_n \ge K_n^2 \ge c_-^2 n^{2\beta_-} \quad\text{for all }n\ge N.
\tag{3.4}
\]

---

## 4. Stage B: Stochastic approximation for \(Y_n = K_n^3/n^2\)

Heuristically, if \(K_n \approx C n^{2/3}\), then \(K_n^3/n^2 \approx C^3\) should converge to a constant. We now set
\[
Y_n := \frac{K_n^3}{n^2},\quad n\ge 1,
\]
and analyze its evolution as a stochastic approximation scheme with step size \(\approx 1/n\).

### 4.1. Exact recursion for \(Y_n\)

From \(K_{n+1} = K_n + \xi_{n+1}\) we have
\[
K_{n+1}^3 - K_n^3
= (3K_n^2 + 3K_n + 1)\,\xi_{n+1}.
\]

Therefore
\[
\begin{aligned}
Y_{n+1} - Y_n
&= \frac{K_{n+1}^3}{(n+1)^2} - \frac{K_n^3}{n^2} \\
&= \frac{K_n^3}{(n+1)^2} - \frac{K_n^3}{n^2}
   + \frac{(3K_n^2 + 3K_n + 1)\xi_{n+1}}{(n+1)^2} \\
&= -Y_n \frac{2n+1}{(n+1)^2}
   + \frac{(3K_n^2+3K_n+1)\xi_{n+1}}{(n+1)^2}.
\end{aligned}
\tag{4.1}
\]

Taking conditional expectations and recalling \(p_n=\mathbb P(\xi_{n+1}=1\mid\mathcal F_n)=(n+1)/D_n\),
\[
\mathbb E[Y_{n+1}-Y_n\mid\mathcal F_n]
= -Y_n \frac{2n+1}{(n+1)^2}
  + \frac{(3K_n^2+3K_n+1)p_n}{(n+1)^2}.
\]

We exploit the identity
\[
3K_n^2 + 3K_n + 1 = 3D_n - (6n+5),
\]
so
\[
\frac{(3K_n^2+3K_n+1)p_n}{(n+1)^2}
= \frac{3D_n\,p_n}{(n+1)^2} - \frac{(6n+5)p_n}{(n+1)^2}.
\]

But \(D_n p_n = (n+1)\), hence \(3D_np_n/(n+1)^2 = 3/(n+1)\), and \(p_n/(n+1) = 1/D_n\). Therefore

\[
\mathbb E[Y_{n+1}-Y_n\mid\mathcal F_n]
= -Y_n\frac{2n+1}{(n+1)^2}
  + \frac{3}{n+1}
  - \frac{6n+5}{(n+1)D_n}. \tag{4.2}
\]

We now separate a ‚Äúmain drift‚Äù term \(\frac{3-2Y_n}{n+1}\) and a small error.

Define
\[
r_n^{(1)}
:= -Y_n\frac{2n+1}{(n+1)^2} + \frac{2Y_n}{n+1},
\]
so that
\[
-Y_n\frac{2n+1}{(n+1)^2} + \frac{3}{n+1}
= \frac{3-2Y_n}{n+1} + r_n^{(1)}.
\]

A direct computation shows
\[
r_n^{(1)}
= Y_n\Bigl( \frac{2}{n+1} - \frac{2n+1}{(n+1)^2}\Bigr)
= \frac{Y_n}{(n+1)^2},
\]
so
\[
|r_n^{(1)}| \le \frac{Y_n}{(n+1)^2}. \tag{4.3}
\]

Define the **drift error**
\[
\varepsilon_n
:= r_n^{(1)} - \frac{6n+5}{(n+1)D_n}.
\]

Also define the **martingale noise term**
\[
N_{n+1}
:= \frac{(3K_n^2+3K_n+1)(\xi_{n+1}-p_n)}{(n+1)^2}.
\]

Then combining (4.1) and (4.2),
\[
Y_{n+1} - Y_n
= \frac{3-2Y_n}{n+1} + \varepsilon_n + N_{n+1}.
\tag{4.4}
\]

This is exactly a stochastic approximation recursion with **step size \(\gamma_n = 1/(n+1)\)** and **drift function \(F(y)=3-2y\)**, perturbed by a small deterministic error \(\varepsilon_n\) and a martingale noise \(N_{n+1}\).

The ODE \(\dot y = F(y) = 3-2y\) has unique globally attracting equilibrium \(y_*=3/2\); we now show that \(Y_n\to 3/2\) almost surely by controlling \(\varepsilon_n\) and \(N_{n+1}\).

---

### 4.2. Summability of perturbations using the coarse bounds

We now use (3.3) and (3.4) to show that
\[
\sum_n |\varepsilon_n| < \infty,\qquad
\sum_n \mathbb E[N_{n+1}^2\mid\mathcal F_n] < \infty
\quad\text{a.s.}
\]

Recall \(Y_n = K_n^3/n^2\). By the upper bound in (3.3), for large \(n\),

\[
Y_n = \frac{K_n^3}{n^2}
\le \frac{(C_+ n^{\beta_+})^3}{n^2}
= C\,n^{3\beta_+ - 2}.
\tag{4.5}
\]

Also, by (3.4),
\[
D_n \ge K_n^2 \ge c_-^2 n^{2\beta_-}. \tag{4.6}
\]

#### 4.2.1. Drift error \(\varepsilon_n\)

Using (4.3), (4.5) and (4.6), for large \(n\),

\[
\biggl|\frac{6n+5}{(n+1)D_n}\biggr|
\le \frac{C n}{n\,n^{2\beta_-}} = C\,n^{-2\beta_-}.
\]

Hence
\[
|\varepsilon_n|
\le \frac{Y_n}{(n+1)^2} + C n^{-2\beta_-}
\le C\bigl( n^{3\beta_+ -4} + n^{-2\beta_-}\bigr).
\tag{4.7}
\]

Because \(\beta_+<1\), we have \(3\beta_+ -4 < -1\). Because \(\beta_->1/2\), we have \(-2\beta_-<-1\). Thus both series
\[
\sum_{n} n^{3\beta_+ -4}
\quad\text{and}\quad
\sum_{n} n^{-2\beta_-}
\]
converge. Therefore
\[
\sum_{n=1}^\infty |\varepsilon_n| < \infty\quad\text{almost surely}.
\tag{4.8}
\]

We will also need \(\sum (n+1)\varepsilon_n^2<\infty\) below; note from (4.7),

\[
\varepsilon_n^2
\le C\bigl(
n^{2(3\beta_+ -4)} + n^{-4\beta_-}
\bigr),
\]
so
\[
(n+1)\varepsilon_n^2 \le C\bigl(
n^{6\beta_+ -7} + n^{1 - 4\beta_-}
\bigr).
\]
Since \(\beta_+<1\) implies \(6\beta_+ -7 < -1\), and \(\beta_->1/2\) implies \(1 -4\beta_- < -1\), we have
\[
\sum_n (n+1)\varepsilon_n^2 < \infty\quad\text{a.s.} \tag{4.9}
\]

#### 4.2.2. Noise variance \(\mathbb E[N_{n+1}^2\mid\mathcal F_n]\)

We have
\[
N_{n+1}
= A_n (\xi_{n+1}-p_n),\quad
A_n := \frac{3K_n^2+3K_n+1}{(n+1)^2}.
\]
Hence
\[
\mathbb E[N_{n+1}^2\mid\mathcal F_n]
= A_n^2\,\mathbb E[(\xi_{n+1}-p_n)^2\mid\mathcal F_n]
= A_n^2 p_n(1-p_n)
\le A_n^2 p_n.
\]

Using \(p_n = (n+1)/D_n\) and \(D_n\ge K_n^2\),
\[
\mathbb E[N_{n+1}^2\mid\mathcal F_n]
\le C \frac{K_n^4}{(n+1)^4}\cdot \frac{n+1}{D_n}
\le C \frac{K_n^4}{(n+1)^3 K_n^2}
= C \frac{K_n^2}{(n+1)^3}.
\]

By the upper bound in (3.3), \(K_n^2\le C_+^2 n^{2\beta_+}\), so

\[
\mathbb E[N_{n+1}^2\mid\mathcal F_n]
\le C n^{2\beta_+ - 3}. \tag{4.10}
\]

Again, since \(\beta_+<1\), we have \(2\beta_+ -3 < -1\), so
\[
\sum_{n=1}^\infty \mathbb E[N_{n+1}^2\mid\mathcal F_n] < \infty\quad\text{a.s.}
\tag{4.11}
\]

Standard martingale arguments (e.g. Kolmogorov three‚Äëseries theorem applied to increments with finite conditional variances summing to a finite limit, or Doob‚Äôs inequality plus Borel‚ÄìCantelli) imply that the series \(\sum_{n\ge 1} N_{n+1}\) converges almost surely and in \(L^2\).

---

### 4.3. Lyapunov function and Robbins‚ÄìSiegmund

Define the deviation from the equilibrium \(3/2\) and its square:

\[
\delta_n := Y_n - \frac{3}{2},\qquad V_n := \delta_n^2.
\]

From (4.4),
\[
\delta_{n+1}
= Y_{n+1} - \frac{3}{2}
= Y_n - \frac{3}{2} + \frac{3-2Y_n}{n+1} + \varepsilon_n + N_{n+1}
= \Bigl(1 - \frac{2}{n+1}\Bigr)\delta_n + \varepsilon_n + N_{n+1}.
\]

Thus
\[
\begin{aligned}
V_{n+1}
&= \delta_{n+1}^2 \\
&= \Bigl(1 - \frac{2}{n+1}\Bigr)^2 V_n
   + 2\Bigl(1 - \frac{2}{n+1}\Bigr)\delta_n(\varepsilon_n + N_{n+1})
   + (\varepsilon_n + N_{n+1})^2.
\end{aligned}
\]

We bound the cross term using the inequality \(2|ab|\le \eta a^2 + \eta^{-1}b^2\) with \(\eta = 1/(n+1)\):

\[
2\Bigl|\Bigl(1 - \frac{2}{n+1}\Bigr)\delta_n\varepsilon_n\Bigr|
\le 2|\delta_n\varepsilon_n|
\le \frac{1}{n+1}V_n + (n+1)\varepsilon_n^2.
\]

Taking conditional expectations and using \(\mathbb E[N_{n+1}\mid\mathcal F_n]=0\) and
\(\mathbb E[\delta_n N_{n+1}\mid\mathcal F_n]=\delta_n\mathbb E[N_{n+1}\mid\mathcal F_n]=0\),
we get

\[
\begin{aligned}
\mathbb E[V_{n+1}\mid\mathcal F_n]
&\le \Bigl(1 - \frac{2}{n+1}\Bigr)^2 V_n
   + \frac{1}{n+1} V_n
   + (n+1)\varepsilon_n^2
   + \varepsilon_n^2
   + \mathbb E[N_{n+1}^2\mid\mathcal F_n] \\
&\le \Bigl(\Bigl(1 - \frac{2}{n+1}\Bigr)^2 + \frac{1}{n+1}\Bigr) V_n
   + \rho_n,
\end{aligned}
\]
where
\[
\rho_n := (n+1)\varepsilon_n^2 + \varepsilon_n^2 + \mathbb E[N_{n+1}^2\mid\mathcal F_n].
\]

For all \(n\ge 1\),
\[
\Bigl(1 - \frac{2}{n+1}\Bigr)^2 + \frac{1}{n+1}
= 1 - \frac{3}{n+1} + \frac{4}{(n+1)^2} + \frac{1}{n+1}
= 1 - \frac{2}{n+1} + O\Bigl(\frac{1}{n^2}\Bigr)
\le 1 - \frac{1}{n+1},
\]
so
\[
\mathbb E[V_{n+1}\mid\mathcal F_n]
\le \Bigl(1 - \frac{1}{n+1}\Bigr) V_n + \rho_n.
\tag{4.12}
\]

Now by (4.9) and (4.11), we have
\[
\sum_{n=1}^\infty \rho_n < \infty\quad\text{almost surely}. \tag{4.13}
\]

The inequality (4.12) is exactly in the Robbins‚ÄìSiegmund form: here \(a_n = 1/(n+1)\), \(b_n=\rho_n\). We have \(\sum a_n = \infty\) and, by (4.13), \(\sum b_n<\infty\) almost surely. Therefore, the Robbins‚ÄìSiegmund theorem implies that:

- \(V_n\) converges almost surely to some finite limit \(V_\infty\ge 0\), and
- \(\lim_{n\to\infty} V_n = 0\).

(One standard way to see \(V_\infty=0\) is: from (4.12) one can show that \(\sum_n a_n V_n <\infty\) almost surely; since \(a_n\sim 1/n\), if \(V_\infty>0\) then \(\sum a_n V_n\) would diverge, a contradiction.)

Thus we have proved:

\[
Y_n = \frac{K_n^3}{n^2} \longrightarrow \frac{3}{2}\quad\text{almost surely}.
\tag{4.14}
\]

---

### 4.4. Asymptotics of \(K_n\) and \(G_n\)

From (4.14), for any \(\varepsilon>0\) there exists a random \(N_\varepsilon\) such that for all \(n\ge N_\varepsilon\),
\[
\biggl|\frac{K_n^3}{n^2} - \frac{3}{2}\biggr| \le \varepsilon.
\]

In particular, for \(n\) large,
\[
\frac{3}{2} - \varepsilon \le \frac{K_n^3}{n^2} \le \frac{3}{2} + \varepsilon.
\]

Thus
\[
(3/2 - \varepsilon)^{1/3} n^{2/3}
\le K_n
\le (3/2 + \varepsilon)^{1/3} n^{2/3}
\quad\text{for all large }n.
\]

Since \(\varepsilon>0\) is arbitrary, it follows that
\[
\frac{K_n}{n^{2/3}} \longrightarrow \Bigl(\frac{3}{2}\Bigr)^{1/3}
\quad\text{almost surely}. \tag{4.15}
\]

Finally, from (1.1),
\[
G_n = K_n^2 + K_n + 2n + 2.
\]

Dividing by \(n^{4/3}\), we have
\[
\frac{G_n}{n^{4/3}}
= \Bigl(\frac{K_n}{n^{2/3}}\Bigr)^2
  + \frac{K_n}{n^{4/3}}
  + \frac{2n+2}{n^{4/3}}.
\]

By (4.15), the first term converges almost surely to \(((3/2)^{1/3})^2 = (3/2)^{2/3}\). The second term is
\[
\frac{K_n}{n^{4/3}} = \frac{K_n}{n^{2/3}}\cdot \frac{1}{n^{2/3}} \to 0,
\]
and the third term is \(\sim 2n/n^{4/3} = 2n^{-1/3}\to 0\). Thus

\[
\frac{G_n}{n^{4/3}} \longrightarrow \Bigl(\frac{3}{2}\Bigr)^{2/3}
\quad\text{almost surely.}
\tag{4.16}
\]

This gives the desired result with
\[
\alpha = \frac{4}{3}, \qquad
c = \Bigl(\frac{3}{2}\Bigr)^{2/3}.
\]

---

## 5. Final statement

We have proved:

> **Theorem.** Let \((G_n)\) be the number of balls in the urn after \(n\) steps in the model described in the problem. Then there exist constants \(c>0,\ \alpha>0\) such that
> \[
> \frac{G_n}{n^\alpha} \longrightarrow c \quad\text{almost surely}.
> \]
> In fact,
> \[
> \alpha = \frac{4}{3},\qquad
> c = \Bigl(\frac{3}{2}\Bigr)^{2/3}.
> \]

The proof combines:

- A reduction of the urn to the one‚Äëdimensional process \(K_n\) of blue draws.
- Two‚Äësided polynomial bounds on \(K_n\) (any exponents \(\beta_-<2/3<\beta_+\)) using Freedman‚Äôs inequality and Borel‚ÄìCantelli.
- A stochastic‚Äëapproximation analysis of \(Y_n = K_n^3/n^2\), whose mean drift is \((3-2Y_n)/(n+1)\) plus summable perturbations, and whose noise has square‚Äësummable conditional variance.
- Application of the Robbins‚ÄìSiegmund almost‚Äësupermartingale theorem to show \(Y_n\to 3/2\) almost surely, then translating back to \(K_n\) and \(G_n\).

This matches the behavior predicted by heuristic ODE methods and fits squarely within the existing framework of stochastic approximation and generalized P√≥lya urn theory.